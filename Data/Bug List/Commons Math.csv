Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Outward issue link (Cloners),Outward issue link (Container),Outward issue link (Duplicate),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Supercedes),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Date of First Response),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Machine Readable Info),Custom field (New-TLP-TLPName),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Severity),Custom field (Severity),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Test and Documentation Plan),Custom field (Testcase included),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
KS test throw MathInternalError: illegal state,MATH-1475,13226104,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,xuech,xuech,04/Apr/19 12:14,06/Apr/19 02:05,07/Apr/19 20:38,06/Apr/19 02:05,3.6.1,,,,,,,4.0,,,0,,,,,,,,"{code:java}
public class CC {
public static void main(String []args) throws Exception{
double[]x=new double[]{0.12350159883499146, -0.2601194679737091, -1.322849988937378, 0.379696249961853, 0.3987586498260498, -0.06924121081829071, -0.13951236009597778, 0.3213207423686981, 0.7949811816215515, -0.15811105072498322, 0.19912190735340118, -0.46363770961761475, -0.20019817352294922, 0.3062838613986969, -0.3872813880443573, 0.10733723640441895, 0.10910066962242126, 0.625770092010498, 0.2824835777282715, 0.3107619881629944, 0.1432388722896576, -0.08056988567113876, -0.5816712379455566, -0.09488576650619507, -0.2154506891965866, 0.2509046196937561, -0.06600788980722427, -0.01133995596319437, -0.22642627358436584, -0.12150175869464874, -0.21109570562839508, -0.17732949554920197, -0.2769380807876587, -0.3607368767261505, -0.07842907309532166, -0.2518743574619293, 0.035517483949661255, -0.6556509137153625, -0.360045850276947, -0.09371964633464813, -0.7284095883369446, -0.22719840705394745, -1.5540679693222046, -0.008972732350230217, -0.09106933325529099, -0.6465389132499695, 0.036245591938495636, 0.657580554485321, 0.32453101873397827, 0.6105462908744812, 0.25256943702697754, -0.194427490234375, 0.6238796710968018, 0.5203511118888855, -0.2708645761013031, 0.07761227339506149, 0.5315862894058228, 0.44320303201675415, 0.6283767819404602, 0.2618369162082672, 0.47253096103668213, 0.3889777660369873, 0.6856100559234619, 0.3007083833217621, 0.4963226914405823, 0.08229698985815048, 0.6170856952667236, 0.7501978874206543, 0.5744063258171082, 0.5233180522918701, 0.32654184103012085, 0.3014495372772217, 0.4082445800304413, -0.1075737327337265, -0.018864337354898453, 0.34642550349235535, 0.6414541602134705, 0.16678297519683838, 0.46028634905815125, 0.4151197075843811, 0.14407725632190704, 0.41751566529273987, -0.054958608001470566, 0.4995657801628113, 0.4485369324684143, 0.5600396990776062, 0.4098612368106842, 0.2748555839061737, 0.2562614381313324, 0.4324824810028076};
double[]y=new double[]{2.6881366763426717, 2.685469965655465, 2.261888917462379, -2.1933598759641226, -2.4279488152810145, -3.159389495849609, -2.3150004548153444, 2.468029206047388, 2.9442494682288953, 2.653360013462529, -2.1189940659194835, -2.121635289903703, -2.103092459792032, -2.737034221468073, -2.203389332350286, 2.1985949039005512, -2.5021604073154737, 2.2732754920764533, -2.3867025598454346, 2.135919387338413, 2.338120776050672, 2.2579794509726874, 2.083329059799027, -2.209733724709957, 2.297192240399189, -2.201703830825843, -3.460208691996806, 2.428839296615834, -3.2944259224581574, 2.0654875493620883, -2.743948930837782, -2.2240674680805212, -3.646366778182357, -2.12513198437294, 2.979166188824589, -2.6275491570089033, -2.3818176136461338, 2.882096356968376, -2.2147229261558334, -3.159389495849609, 2.312428759406432, 2.3313864098846477, -2.72802504046371, -2.4216068225364245, 3.0119599306499123, 2.5753099009496783, -2.9200121783556843, -2.519352725437922, -4.133932580227538, -2.30496316762808, 2.5381353678521363, 2.4818233632136697, 2.5277451177925685, -2.166465445816232, -2.1193897819471563, -2.109654332722425, 3.260211545834851, -3.9527673876059013, -2.199885089466947, 2.152573429747697, -3.1593894958496094, 2.5479522823226795, 3.342810742466116, -2.8197184957304007, -2.3407900299253765, -2.3303967152728537, 2.1760131201015565, 2.143930552944634, 2.33336231754409, 2.9126278362420575, -2.121169134387265, -2.2980208408109095, -2.285400411434817, -2.0742764640932903, 2.304178664095016, -2.2893825538911634, -3.7714771984158806, -2.7153698816026886, 2.8995011276220226, -2.158787087333056, -2.1045987952052547, 2.8478762016468147, -2.694578565956955, -2.696014432856399, -2.3190122657403496, -2.48225194403028, 3.3393947563371764, 2.7775468034263517, -3.396526561479875, -2.699967947404961};
KolmogorovSmirnovTest kst=new KolmogorovSmirnovTest();
double p=kst.kolmogorovSmirnovTest(x, y);
System.out.println(p);
}
}
{code}

Results:
run:
Exception in thread ""main"" org.apache.commons.math3.exception.MathInternalError: illegal state: internal error, please fill a bug report at https://issues.apache.org/jira/browse/MATH
at org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest.fixTies(KolmogorovSmirnovTest.java:1171)
at org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest.kolmogorovSmirnovTest(KolmogorovSmirnovTest.java:263)
at org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest.kolmogorovSmirnovTest(KolmogorovSmirnovTest.java:290)
at com.snplife.common.math.CC.main(CC.java:17)
C:\Users\xuechao\AppData\Local\NetBeans\Cache\8.2\executor-snippets\run.xml:53: Java returned: 1
","Windows 10 (Professional)

JDK 8",,,,,,,,,,,,,,,,,,,,04/Apr/19 12:19;xuech;Figure_1.png;https://issues.apache.org/jira/secure/attachment/12964859/Figure_1.png,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2019-04-04 13:29:03.266,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 06 01:58:22 UTC 2019,,,,,,0|z01gew:,9223372036854775807,,,,,,,,"04/Apr/19 13:29;erans;Thanks for the report.
However, please try the development version (4.0-SNAPSHOT) and let us know whether it fails too.",05/Apr/19 06:41;xuech;It fails too when I used the development version (4.0-SNAPSHOT)  with the same Exception.,05/Apr/19 06:42;xuech;It fails too when I used the development version (4.0-SNAPSHOT)  with the same Exception.,"05/Apr/19 13:53;erans;Which snaphot did you use?
 In any case, please retry either with the latest snapshot from the [CI system|https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math4/4.0-SNAPSHOT/] or with a JAR compiled from the latest version of the [source code|https://gitbox.apache.org/repos/asf?p=commons-math.git].

On my machine, running (JDK 8), your case passed (I copied it as a unit test).","06/Apr/19 01:58;xuech;I used the snaphot in [github|[https://github.com/jcastro-inf/commons-math4]].

The problem was solved when I used the snaphot you provide. Thank you! ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KolmogorSmirnovTest: Unit test fails,MATH-1472,13204057,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,erans,erans,12/Dec/18 17:57,12/Dec/18 19:25,07/Apr/19 20:38,,,,,,,,,,,,0,,,,,,,,"Upgrading the dependency of ""Commons Math"" to the just released v1.2 of ""Commons RNG"", a unit test now fails:
{noformat}
[ERROR] testTwoSampleWithManyTiesAndExtremeValues(org.apache.commons.math4.stat.inference.KolmogorovSmirnovTestTest)  Time elapsed: 0.032 s  <<< ERROR!
org.apache.commons.math4.exception.NotANumberException: NaN is not allowed
	at org.apache.commons.math4.stat.inference.KolmogorovSmirnovTestTest.testTwoSampleWithManyTiesAndExtremeValues(KolmogorovSmirnovTestTest.java:512)
{noformat}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,Important,,,,,,,,,9223372036854775807,,,Wed Dec 12 18:39:32 UTC 2018,,,,,,0|s01ge8:,9223372036854775807,,,,,,,,"12/Dec/18 18:02;erans;Bug is perhaps in the usage:

{code}
private static void jitter(double[] data,
                           UniformRandomProvider rng,
                           int ulp) {
    final int range = ulp * 2;
    for (int i = 0; i < data.length; i++) {
        final int rand = rng.nextInt(range) - ulp;
        data[i] += rand * Math.ulp(data[i]);
    }
}
{code}","12/Dec/18 18:39;erans;{quote}Bug is perhaps in the usage
{quote}
Confirming: In line
{code:java}
data[i] += rand * Math.ulp(data[i]);
{code}
the contents of {{data[i]}} is {{Infinity}} and {{rand}} is negative, resulting in {{NaN}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BicubicInterpolatingFunction not interpolating correctly for non discrete y value,MATH-1471,13191002,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,tswinicki,tswinicki,11/Oct/18 17:36,18/Oct/18 16:22,07/Apr/19 20:38,18/Oct/18 16:22,3.6.1,,,,,,,,,,0,,,,,,,,"Upon performing a bicubic interpolation with two point (x0, y0) and (x1, y1), the returned bicubic interpolating function returned returns the same result for variations in the estimated y value. 

For example, my inputs are (20, 20) and (25, 25) with f(20, 20) = 64 and f(25, 25) = 6468.

When I get the bicubic interpolating function for this and vary the estimated x, it works fine. For (21, 20), the function returns 730.016. When I input (20, 21), the function returns 64, which is f(20, 20). For any y value in between 20 and 25, the result is 64. This is the case for any function for which the y estimate is different from the value on the points. 

In other instances, it is varying x values that result in the same result while varying y estimates seem to work as expected.",JDK 1.8.0_181 ,,,,,,,,,,,,,,,,,,,,12/Oct/18 14:17;tswinicki;ApacheCommonsMathBiInterpolationTests.zip;https://issues.apache.org/jira/secure/attachment/12943651/ApacheCommonsMathBiInterpolationTests.zip,15/Oct/18 13:12;tswinicki;Interpolate.java;https://issues.apache.org/jira/secure/attachment/12943938/Interpolate.java,15/Oct/18 13:12;tswinicki;InterpolateTest.java;https://issues.apache.org/jira/secure/attachment/12943939/InterpolateTest.java,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2018-10-11 18:23:08.325,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 15 13:35:46 UTC 2018,,,,,,0|i3z3jr:,9223372036854775807,,,,,,,,"11/Oct/18 18:23;erans;bq. interpolating function returned returns the same result for variations in the estimated y value.

Thanks a lot for the report.
Could you please set up a unit test that demonstrates the bug, in the form of a patch (or ""pull request"") against the development version (i.e. the code in the ""master"" branch of the repository)?
A fix would also be welcome. :)",12/Oct/18 14:44;tswinicki;The pull request has been made and I have also attached the unit tests to this issue. Thanks for the prompt reply!,"12/Oct/18 23:35;erans;{quote}I have also attached the unit tests to this issue.
{quote}
What you have attached is an archive of your copy of the repository, plus your compiled files and the generated web site. :(
 Please upload a patch file.
{quote}The pull request has been made
{quote}
Where is it (I didn't get any notification)?
 Alternatively to the patch file, please copy here the link to the pull request. Thanks.","15/Oct/18 13:12;tswinicki;Sorry, the Apache Commons Math Library is included in the zip file, but under the src folder there is a class called Interpolate.java which implements the BicubicInterpolator in a method. Under tests folder, there is an InterpolateTest.java which contains a unit test for the BicubicInterpolatingFunction. I have uploaded just the java files as well for easier access,

 

Here is the pull request link! https://github.com/apache/commons-math/pull/90","15/Oct/18 13:35;erans;Hi Tom.

It's great that you want to contribute to the project but please read [how to do it|http://commons.apache.org/proper/commons-math/developers.html] (see especially the paragraph on using ""git"") in order to make it easy for us to apply the necessary changes.
 I don't know the steps for generating a PR on Github but if you post on the [""dev"" mailing list|http://commons.apache.org/proper/commons-math/mail-lists.html] asking for help, I hope that someone can explain them.
{quote}Here is the pull request link!
{quote}
Sorry but it is just a link to a ZIP file (same as you attached here, I guess) whereas it should only contain a ""diff"" wrt the ""master"" branch. In particular, the provided unit test should be a patch to apply to the [existing test suite for the class being tested|https://git1-us-west.apache.org/repos/asf?p=commons-math.git;a=blob;f=src/test/java/org/apache/commons/math4/analysis/interpolation/BicubicInterpolatorTest.java;h=bf3195c2a896ad4969b9b9cfa88687a1010fdfe0;hb=HEAD].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Precision.round(double...)'s use of Double.toString(x) rounds twice resulting in inaccuracy,MATH-1470,13188979,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,litesoft,litesoft,02/Oct/18 21:17,03/Oct/18 14:46,07/Apr/19 20:38,,3.6.1,,,,,,,,,,0,,,,,,,,"The use of Double.toString( x ) in the creation of the BigDecimal used by the Precision.round(double...) methods introduces a rounding that can then generate incorrect results when the rounding is applied to the BigDecimal.  Whenever possible rounding should only be applied to the most accurate value available.  Switching the BigDecimal construction to use the double value directly resolves the problem.

 

This problem can be seen by running the main method of the com.altoros.floatingpoint.PrecisionProblem class in the repo hosted at: [https://github.com/Altoros/precision-problem]

 

George

 ",,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-10-03 14:46:34.862,,,false,,,,https://github.com/Altoros/precision-problem,,,,,,,,,,9223372036854775807,,,Wed Oct 03 14:46:34 UTC 2018,,,,,,0|i3yr5z:,9223372036854775807,,,,,,,,"03/Oct/18 14:46;erans;Hi.

Thanks for the report and analysis.
Class {{Precision}} has been [moved to the ""Commons Numbers"" project|https://git1-us-west.apache.org/repos/asf?p=commons-numbers.git;a=blob;f=commons-numbers-core/src/main/java/org/apache/commons/numbers/core/Precision.java;h=11fe704383e46ee421a1c0c344dd86d86a2bcdbc;hb=HEAD], and so won't be part of the next major release (v4.0) of ""Commons Math"".
Could you please file an issue in the [bug tracking system for ""Numbers""|https://issues.apache.org/jira/projects/NUMBERS/issues/], linking that issue to this one?  Thanks.

It would of course be great if you could also provide a fix in the form of patch (or pull request) containing the necessary code changes and associated unit test(s).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CMAES Optimization Fails to find Actual Optimum if Solution Value is Negative,MATH-1466,13181429,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,rwolkoff,rwolkoff,27/Aug/18 18:19,29/Aug/18 00:26,07/Apr/19 20:38,,3.6.1,,,,,,,,,,0,Optimization,,,,,,,"Class CMAESOptimizer ([java.lang.Object|http://docs.oracle.com/javase/6/docs/api/java/lang/Object.html?is-external=true]

[org.apache.commons.math3.optim.BaseOptimizer|http://commons.apache.org/proper/commons-math/javadocs/api-3.4/org/apache/commons/math3/optim/BaseOptimizer.html]<PAIR>

[org.apache.commons.math3.optim.BaseMultivariateOptimizer|http://commons.apache.org/proper/commons-math/javadocs/api-3.4/org/apache/commons/math3/optim/BaseMultivariateOptimizer.html]<[PointValuePair|http://commons.apache.org/proper/commons-math/javadocs/api-3.4/org/apache/commons/math3/optim/PointValuePair.html]>

[org.apache.commons.math3.optim.nonlinear.scalar.MultivariateOptimizer|http://commons.apache.org/proper/commons-math/javadocs/api-3.4/org/apache/commons/math3/optim/nonlinear/scalar/MultivariateOptimizer.html]

org.apache.commons.math3.optim.nonlinear.scalar.noderiv.CMAESOptimizer)

I cannot provide code as I work for a private company with IP. However, my tests indicate that if the we are minimizing a cost function, and the function's value is a negative value, the resulting solution is incorrect.

For example, say I were minimizing y = x^2 - 100

The solution should be x = 0, with a value of -100. However, this CMAES optimizer would find random solutions, like x = 3 or x = -2 (with solutions of -91 and -96 respectively).

I used the BOBYQA solver, and it was able to find the solution of x = 0 with a value of y = -100. I tested y = x^2 + 100 and the CMAES solver was able to find a solution of x = 0, y = 100. I tested y = x^2 and the CMAES the CMAES solver was able to find a solution of x = 0, y = 0. I tested y = x^2 -1 and the CMAES solver was NOT able to find a solution of x = 0, y = -1.

I know it is highly inconvenient, but if someone wants to take a look at my cost function I can try to get an NDA. I think this bug is reproducible without it. Feel free to contact me at [rwolkoff@umich.edu|mailto:rwolkoff@umich.edu] if more info is needed.

I think it has to do with the following stop criteria, which ends early if the cost is negative:

if (stopFitness != 0 && bestFitness < (isMinimize ? stopFitness : -stopFitness)) {
 break generationLoop;
}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-08-29 00:26:48.393,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 29 00:26:48 UTC 2018,,,,,,0|i3xgyn:,9223372036854775807,,,,,,,,"29/Aug/18 00:26;erans;bq. I think it has to do with the following stop criteria, which ends early if the cost is negative

It is spot on, indeed.
But this condition is controlled by the {{stopFitness}} argument to the [constructor|http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math4/optim/nonlinear/scalar/noderiv/CMAESOptimizer.html#CMAESOptimizer-int-double-boolean-int-int-org.apache.commons.rng.UniformRandomProvider-boolean-org.apache.commons.math4.optim.ConvergenceChecker-].  Which value do you use?
I've added a [unit test|https://git1-us-west.apache.org/repos/asf?p=commons-math.git;a=commit;h=efb0230063b31d53bd077d9f6a4bb64fcbb9c97f] for {noformat}f(x) = x^2 - 100{noformat} that shows that the implementation should also work if the optimum has a negative fitness.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Exception thrown by ""IntegerSequence.Incrementor""",MATH-1463,13165553,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,erans,erans,12/Jun/18 12:50,12/Jun/18 13:13,07/Apr/19 20:38,12/Jun/18 13:13,,,,,,,,4.0,,,0,,,,,,,,"As {{IntegerSequence.Incrementor}} _implements_ {{java.util.Iterator}}, its {{next()}} method should abide by the interface's contract, i.e. throw {{java.util.NoSuchElementException}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 12 13:13:42 UTC 2018,,,,,,0|i3urnj:,9223372036854775807,,,,,,,,"12/Jun/18 13:13;erans;Commit 00a0c6cb866b01c0d48debae0faf884038279144 (""master"" branch).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EmpiricalDistribution:inverseCumulativeProbability return Infinity,MATH-1462,13165297,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,,,ebilarbi,ebilarbi,11/Jun/18 10:58,23/Jul/18 16:10,07/Apr/19 20:38,,3.6.1,,,,,,,,,,1,,,,,,,,"Hi,

inverseCumulativeProbability(0.5) return ""infinity"" which is absurd while it return correct values for 0.499999 and 0.511111, Here is the test :
{code:java}
double[] data = {6464.0205, 6449.1328, 6489.4569, 6497.5533, 6251.6487, 6252.6513, 6339.7883,
6356.2622, 6222.1251, 6157.3813, 6242.4741, 6332.5347, 6468.0633, 6471.2319, 6473.9929, 6589.1322, 
6511.2191, 6339.4349, 6307.7735, 6288.0915, 6354.0572, 6385.8283, 6325.3756, 6433.1699, 6433.6507, 
6424.6806, 6380.5268, 6407.6705, 6241.2198, 6230.3681, 6367.5943, 6358.4817, 6272.8039, 6269.0211, 
6312.9027, 6349.5926, 6404.0775, 6326.986, 6283.8685, 6309.9021, 6336.8554, 6389.1598, 6281.0372, 
6304.8852, 6359.2651, 6426.519, 6400.3926, 6440.6798, 6292.5812, 6398.4911, 6307.0002, 6284.2111, 6271.371, 6368.6377, 6323.3372, 6276.2155, 
6335.0117, 6319.2466, 6252.9969, 6445.2074, 6461.3944, 6384.1345};

EmpiricalDistribution ed = new EmpiricalDistribution(data.length);
ed.load(data);

double p50 = ed.inverseCumulativeProbability(0.5);
double p51 = ed.inverseCumulativeProbability(0.51111);
double p49 = ed.inverseCumulativeProbability(0.49999);

assertTrue(p51<6350);
assertTrue(p49<6341);
assertTrue(p50<7000);
{code}
 Any clue to fix this ?

 ",,,,,,,,,,,,,,,,,MATH-1353,MATH-1431,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-11 13:04:20.419,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 23 16:07:00 UTC 2018,,,,,,0|i3uq2v:,9223372036854775807,,,,,,,,"11/Jun/18 13:04;erans;This bug might be the same as reported in the issue which I've just linked.

You are most welcome to help fix this class. ;)","19/Jul/18 09:54;Dagguh;The empty bins mentioned in -MATH-1432- MATH-1431 did not help in our case.

We managed to work around this bug by abandoning the ""within-bin"" smoothing altogether:
{code:java}
public class RoughEmpiricalDistribution extends EmpiricalDistribution {

    @Override
    protected RealDistribution getKernel(SummaryStatistics bStats) {
        return new ConstantRealDistribution(bStats.getMean());
    }
}
{code}

","19/Jul/18 10:50;erans;bq. The empty bins mentioned in MATH-1432 did not help in our case.

I guess you meant MATH-1431 (?).
A unit test (patch) showing the problem would be useful.  Do you mean that the fix mentioned in the other report is not good enough?","23/Jul/18 16:07;Dagguh;Correct, MATH-1431 fix was not good enough.
I can't directly link to an actual executable test, because our Git repo is private. I'll inline key classes in the comment. It's in Kotlin, but I hope it's readable enough.

{code}
/**
 * Represents the [quantile function](https://en.wikipedia.org/wiki/Quantile_function).
 */
class QuantileFunction {

    fun plot(
        data: Collection<Number>
    ): List<Quantile> {
        if (data.isEmpty()) {
            return emptyList()
        }
        val distribution = RoughEmpiricalDistribution(
            binCount = 1000,
            data = data.map { it.toDouble() }.toDoubleArray()
        )
        return (0..100)
            .map { percentileIndex -> percentileIndex.toDouble() / 100 }
            .map { cumulativeProbability ->
                Quantile(
                    cumulativeProbability = cumulativeProbability,
                    value = distribution.inverseCumulativeProbability(cumulativeProbability)
                )
            }
    }
}
{code}

{code}
import org.hamcrest.Matchers.*
import org.junit.Assert.assertThat
import org.junit.Test

class QuantileFunctionTest {

    @Test
    fun shouldPlotPercentile32() {
        val data = listOf(
            18054,
            17548,
            17350,
            17860,
            17827,
            17653,
            18113,
            18405,
            17746,
            17647,
            18160,
            17955,
            17705,
            17890,
            17974,
            17857,
            13287,
            18645,
            17775,
            17730,
            17996,
            18263,
            17861,
            17161,
            17717,
            18134,
            18669,
            18340,
            17221,
            18292,
            18146,
            17520,
            18207,
            17829,
            18206,
            13301,
            18257,
            17626,
            18358,
            18340,
            18320,
            17852,
            17804,
            17577,
            17718,
            18099,
            13395,
            17763,
            17911,
            17978,
            12935,
            17519,
            17550,
            18728,
            18518,
            17698,
            18739,
            18553,
            17982,
            18113,
            17974,
            17961,
            17645,
            17867,
            17890,
            17498,
            18718,
            18191,
            18177,
            17923,
            18164,
            18155,
            6212,
            5961,
            711
        )

        val quantiles = QuantileFunction().plot(data)

        val p31 = quantiles[31].value
        val p32 = quantiles[32].value
        val p33 = quantiles[33].value
        assertThat(p32, greaterThanOrEqualTo(p31))
        assertThat(p32, lessThanOrEqualTo(p33))
    }
}
{code}

{code}
/**
 * Represents a [quantile](https://en.wikipedia.org/wiki/Quantile).
 */
data class Quantile(
    val cumulativeProbability: Double,
    val value: Double
)
{code}

{code}
import org.apache.commons.math3.distribution.ConstantRealDistribution
import org.apache.commons.math3.distribution.RealDistribution
import org.apache.commons.math3.random.EmpiricalDistribution
import org.apache.commons.math3.stat.descriptive.SummaryStatistics

/**
 * Works around [MATH-1462](https://issues.apache.org/jira/browse/MATH-1462).
 */
class RoughEmpiricalDistribution(
    binCount: Int,
    data: DoubleArray
) : EmpiricalDistribution(binCount) {

    init {
        super.load(data)
    }

    override fun getKernel(
        bStats: SummaryStatistics
    ): RealDistribution {
        return ConstantRealDistribution(bStats.mean)
    }
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IntegerSequence.Incrementor should fail earlier,MATH-1460,13156671,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,erans,erans,erans,02/May/18 17:54,18/Jan/19 14:53,07/Apr/19 20:38,18/Jan/19 14:53,3.6.1,,,,,,,4.0,,,0,,,,,,,,Method {{increment(int nTimes)}} allows increment beyond the maximal value.,,,,,,,,,,,,,,,,,MATH-1458,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed May 02 18:02:17 UTC 2018,,,,,,0|i3t9rz:,9223372036854775807,,,,,,,,02/May/18 18:02;erans;Tentative solution in commit f43069ac6d281b8367dad6f78def4b8336a11ff0 (please review).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simpson Integrator computes incorrect value at minimum iterations=1 and wastes an iteration,MATH-1458,13156336,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,alexherbert,alexherbert,01/May/18 13:56,18/Jan/19 14:52,07/Apr/19 20:38,18/Jan/19 14:52,3.6.1,,,,,,,,,,0,documentation,easyfix,newbie,patch,,,,"org.apache.commons.math3.analysis.integration.SimpsonIntergrator

When used with minimalIterationCount == 1 the integrator computes the wrong value due to the inlining of computation of stage 1 and stage 0 of the TrapezoidIntegrator. Each stage is successive since it relies on the result of the previous stage. So stage 0 must be computed first. This inlining causes stage 1 to be computed before stage 0:
{code:java}
return (4 * qtrap.stage(this, 1) - qtrap.stage(this, 0)) / 3.0;

{code}
This can be fixed using:
{code:java}
final double s0 = qtrap.stage(this, 0);
return (4 * qtrap.stage(this, 1) - s0) / 3.0;{code}
What is not clear is why setting minimum iterations to 1 results in no iteration. This is not documented. I would expect setting it to 1 would compute the first Simpson sum and then perform 1 refinement. This would make it functionality equivalent to the other Integrator classes which compute two sums for the first iteration and allow them to be compared if minimum iterations = 1. If convergence fails then each additional iteration computes an additional sum.

Note when used with minimalIterationCount > 1 the SimpsonIntegrator wastes a stage since it computes the following stages: 0, 0, 1, 2, 3. i.e. stage 0 is computed twice. This is because the iteration is incremented after the stage is computed:
{code:java}
final double t = qtrap.stage(this, getIterations());
incrementCount();
{code}
This should be:
{code:java}
incrementCount();
final double t = qtrap.stage(this, getIterations());
{code}
On the first iteration it thus computes the trapezoid sum and compares it to zero. This would  result in a bad computation if integrating a function whose trapezoid sum is zero (e.g. y=x^3 in the range -1 to 1). However since iteration only occurs for minimalIterationCount>1 no termination comparison is made on the first loop. The first termination comparison can be made at iteration=2 where the comparison will be between the Trapezoid sum and the first Simpson sum. This is a bug.

However I do not want to submit a formal patch as there is a lack of consistency across all the integrators in their doIntegrate() method with the use of incrementCount() and what the iteration number should be at the start of the while (true) loop:
 * IterativeLegendreGauss integrator uses getIterations()+1 to mark the current iteration inside the loop and calls incrementCount() at the end. 
 * TrapezoidIntegrator calls incrementCount() outside the loop, uses getIterations() to mark the current iteration and calls incrementCount() at the end.
 * The MidpointIntegrator calls incrementCount() at the start of the loop and uses getIterations() to mark the current iteration.
 * The RombergIntegrator calls incrementCount() outside the loop, uses getIterations() to mark the current iteration and calls incrementCount() in the middle of the loop before termination conditions have been checked. This allows it to fail when the iterations are equal to the maximum iterations even if convergence has been achieved (see Note*).
 * The SimpsonIntegrator uses getIterations() to mark the current iteration and calls incrementCount() immediately after.

Note*: This may not be discovered in a unit test since the incrementCount() uses a backing Incrementor where the Incrementor.increment() method calls Incrementor.increment(1) which ends up calling canIncrement(0) \{ instead of canIncrement(nTimes) } to check if the maxCountCallback should be triggered. I expect that all uses of the Incrementor actually trigger the maxCountCallback when the count has actually exceeded the maximalCount. I don't want to get into debugging that class since it also has an iterator using hasNext() with a call to canIncrement(0) and I do not know the contract that the iterator is working under.

A consistent approach would be:
 * Compute the first sum before the loop
 * Enter the loop and increment the iteration (so the first loop execution would be iteration 1)
 * Compute the next sum
 * Check termination conditions

An example for the SimpsonIntegrator is below:
{code:java}
protected double doIntegrate() throws 
  TooManyEvaluationsException, MaxCountExceededException
{
  // This is a modification from the default SimpsonIntegrator.
  // That only computed a single iteration if 
  // getMinimalIterationCount() == 1.

  // Simpson's rule requires at least two trapezoid stages.
  // So we set the first sum using two trapezoid stages.
  TrapezoidIntegrator qtrap = new TrapezoidIntegrator();

  final double s0 = qtrap.stage(this, 0);
  double oldt = qtrap.stage(this, 1);
  double olds = (4 * oldt - s0) / 3.0;
  while (true)
  {
    // The first iteration is now the first refinement of the sum.
    // This matches how the MidPointIntegrator works.
    incrementCount();
    final int i = getIterations();
    // 1-stage ahead of the iteration
    final double t = qtrap.stage(this, i + 1);
    final double s = (4 * t - oldt) / 3.0;
    if (i >= getMinimalIterationCount())
    {
      final double delta = FastMath.abs(s - olds);
      final double rLimit = getRelativeAccuracy() * 
        (FastMath.abs(olds) + FastMath.abs(s)) * 0.5;
      if ((delta <= rLimit) || (delta <= getAbsoluteAccuracy()))
      {
        return s;
      }
    }
    olds = s;
    oldt = t;
  }

}
{code}
Note: If this method is accepted then the SIMPSON_MAX_ITERATIONS_COUNT must be reduced by 1 to 63, since the stage method of the TrapezoidIntegrator has a maximum valid input argument of 64.

 ","openjdk version ""1.8.0_162""                                                                                                           
OpenJDK Runtime Environment (build 1.8.0_162-8u162-b12-0ubuntu0.16.04.2-b12)                                                          
OpenJDK 64-Bit Server VM (build 25.162-b12, mixed mode)    ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-05-02 11:07:07.956,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue May 08 15:51:53 UTC 2018,,,,,,0|i3t7pr:,9223372036854775807,,,,,,,,"02/May/18 11:07;erans;Thanks a lot for the report and thorough analysis.
The next step would be to write a unit test that displays the bug. And then a patch/PR to fix it.
I don't follow the argument about {{Incrementor}} but please note that it is _deprecated_ in the development version of the library: updates/fixes should be performed against the ""master"" branch in the code repository.","02/May/18 11:43;alexherbert;The comment about the Incrementor is for the new IntegerSequence.Incrementor not the deprecated org.apache.commons.math3.util.Incrementor.

I think the public void increment(int nTimes) method of IntegerSequence.Incrementor should be:
{code:java}
public void increment(int nTimes) throws MaxCountExceededException {
  if (nTimes <= 0) {
    throw new NotStrictlyPositiveException(nTimes);
  }

  // This is a change from: if (!canIncrement(0)) {
  if (!canIncrement(nTimes)) {
    maxCountCallback.trigger(maximalCount);
  }
  count += nTimes * increment;
}
{code}
Since I am new to this please allow some naive questions. I assumed that a core developer would be able to look at this and I had contributed enough. Are you saying that I have to provide the patch?","02/May/18 18:17;erans;I've created issue MATH-1460. Please have a look.
{quote}I assumed that a core developer would be able to look at this and I had contributed enough. Are you saying that I have to provide the patch?
{quote}
We are lacking time and expertise to support all the code in this library. We've started to split it in more manageable parts that can be maintained more easily by people not necessarily able to fix all the issues that keep coming for Commons Math and prevent timely releases.

See the new components [Commons RNG|http://commons.apache.org/rng], [Commons Numbers|http://commons.apache.org/numbers], [Commons Statistics|http://commons.apache.org/statistics], [Commons Geometry|http://commons.apache.org/geometry], ... that are at various stages of completion.

You are most welcome to contribute to e.g. a new module in ""Commons Numbers"" that could contain selected parts of the code currently in package {{org.apache.commons.math4.analysis}}.",03/May/18 10:45;alexherbert;I've branched the git repo and will create tests that the current SimpsonItegrator fails. I'll then fix it and submit a pull request for review.,"08/May/18 09:55;alexherbert;Changes submitted via the pull request:

[MATH-1458|https://github.com/apache/commons-math/pull/85]

 ","08/May/18 11:08;erans;Hi.
Thanks for the patch.
Sorry for the nit-pick, but please run
{noformat}
$ mvn site
{noformat}
and check that it did not introduce any CheckStyle errors (the generated site, with the reports, will be under the {{target/site}} directory).","08/May/18 12:32;alexherbert;Found 1 checkstyle error for trailing whitespace.

I was careful to never use a code formatting tool and followed the guideline here:

[CONTRIBUTING|https://github.com/aherbert/commons-math/blob/master/CONTRIBUTING.md]

I ran
{code:java}
mvn clean verify
{code}
but did not know about 
{code:java}
mvn site
{code}
It now all builds with no errors. ","08/May/18 15:51;erans;Merged (commit 36553ffab375518a8ce39b25a916f730775435a4 on ""master"").",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath.exp() generates java.lang.ArrayIndexOutOfBoundsException: length=1025; index=333726400,MATH-1457,13155698,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,,,R,R,27/Apr/18 12:50,27/Apr/18 12:52,07/Apr/19 20:38,,3.6.1,,,,,,,,,,0,,,,,,,,"Using Apache FastMath v3.6.1 in my Android project. The OS is Android Oreo 8.1. Got this exception:

{{java.lang.ArrayIndexOutOfBoundsException: length=1025; index=333726400}}
 {{        at org.apache.commons.math3.util.FastMath.exp(FastMath.java:936)}}
 {{        at org.apache.commons.math3.util.FastMath.exp(FastMath.java:864)}}

 

Noticed this for the first (and only) time. Most of the time the function works okay, so unable to reproduce it with some specific numbers.

 

Incriminated part of the code:

{{{color:#3f7f5f}/* Get the fractional part of x, find the greatest multiple of 2^-10 less than{color}{color:#3f7f5f} * x and look up the exp function of it.{color}{color:#3f7f5f} * fracPartA will have the upper 22 bits, fracPartB the lower 52 bits.{color}{color:#3f7f5f} */{color}}}

{{{color:#7f0055}final int {color}{color:#6a3e3e}intFrac {color}= ({color:#7f0055}int{color}) ((x - {color:#6a3e3e}intVal{color}) * {color:#0000ff}1024.0{color});}}
*_HERE ->>_*   {{{color:#7f0055}final double {color}{color:#6a3e3e}fracPartA {color}= ExpFracTable.{color:#0000c0}EXP_FRAC_TABLE_A{color}[{color:#6a3e3e}intFrac{color}];}}
 {{{color:#7f0055}final double {color}{color:#6a3e3e}fracPartB {color}= ExpFracTable.{color:#0000c0}EXP_FRAC_TABLE_B{color}[{color:#6a3e3e}intFrac{color}];}}
 {{ }}

 ",Android Oreo 8.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,2018-04-27 12:50:11.0,,,,,,0|i3t3s7:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mvn tests fail if JDK 9 is used,MATH-1455,13149295,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,,,mjw99,mjw99,31/Mar/18 11:11,31/Mar/18 11:58,07/Apr/19 20:38,,4.X,,,,,,,,,,0,,,,,,,,"This is essentially the JDK9 version of MATH-1156
{code:java}
// code 
Running org.apache.commons.math4.util.FastMathTest
FastMath does not implement: public static long java.lang.StrictMath.multiplyExact(long,int)
FastMath does not implement: public static double java.lang.StrictMath.fma(double,double,double)
FastMath does not implement: public static float java.lang.StrictMath.fma(float,float,float)
FastMath does not implement: public static long java.lang.StrictMath.multiplyFull(int,int)
FastMath does not implement: public static long java.lang.StrictMath.multiplyHigh(long,long)
FastMath does not implement: public static long java.lang.StrictMath.floorDiv(long,int)
FastMath does not implement: public static int java.lang.StrictMath.floorMod(long,int)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-31 11:58:04.433,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 31 11:58:04 UTC 2018,,,,,,0|i3s0jr:,9223372036854775807,,,,,,,,"31/Mar/18 11:58;erans;I was aware of that, but I didn't how to deal with it other than disabling the test, which will only hide the issue that {{FastMath}} is not a ""drop-in"" replacement of {{Math}} as the documentation states.

The alternative of implementing the missing methods gets us back to never addressed issues:
 * MATH-740
 * MATH-901
 * MATH-1113

AFAICT, the primary purpose of the (initial) {{FastMath}} code was accuracy. As it happens, several methods were also faster, but, as the above reports showed, the name was a bad choice.

First step would be to post to the ""dev"" ML, to see whether someone else agrees there is a fundamental issue, and is interested to fix it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Current github master HEAD fails to compile,MATH-1454,13149234,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mjw99,mjw99,30/Mar/18 20:11,31/Mar/18 00:52,07/Apr/19 20:38,31/Mar/18 00:52,4.0,,,,,,,,,,0,,,,,,,,"Current HEAD (c5512b) fails to compile:

 
{code:java}
// [ERROR] /home/mjw/workspace/commons-math/src/main/java/org/apache/commons/math4/complex/ComplexUtils.java:[140,23] method ofCartesian in class org.apache.commons.numbers.complex.Complex cannot be applied to given types;
  required: double,double
  found: double
  reason: actual and formal argument lists differ in length
[ERROR] /home/mjw/workspace/commons-math/src/main/java/org/apache/commons/math4/complex/ComplexUtils.java:[154,23] method ofCartesian in class org.apache.commons.numbers.complex.Complex cannot be applied to given types;
  required: double,double
  found: float
  reason: actual and formal argument lists differ in length
[ERROR] /home/mjw/workspace/commons-math/src/main/java/org/apache/commons/math4/complex/ComplexUtils.java:[439,31] method ofCartesian in class org.apache.commons.numbers.complex.Complex cannot be applied to given types;
  required: double,double
  found: double
  reason: actual and formal argument lists differ in length
[ERROR] /home/mjw/workspace/commons-math/src/main/java/org/apache/commons/math4/complex/ComplexUtils.java:[457,31] method ofCartesian in class org.apache.commons.numbers.complex.Complex cannot be applied to given types;
  required: double,double
  found: float
  reason: actual and formal argument lists differ in length
{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-31 00:52:58.372,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 31 00:52:58 UTC 2018,,,,,,0|i3s067:,9223372036854775807,,,,,,,,"31/Mar/18 00:52;erans;Thanks for the report: commit d9b0be1c9a8e9c0e34220d38276c5b449a06be54 fixed the problem.

Side note: Class {{ComplexUtils}} will be obsoleted by equivalent functionality in the [""Commons Numbers""|http://commons.apache.org/numbers] project.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mann-Whitney U Test returns maximum of U1 and U2,MATH-1453,13141758,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,,,nikos.katsip,nikos.katsip,01/Mar/18 10:46,30/Apr/18 19:03,07/Apr/19 20:38,,3.6.1,,,,,,,,,,0,,,,,,,,"Currently, I need to use Mann-Whitney U Test and I figured out that Apache Commons Math has it implemented. After consulting the [Wiki|https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test] presented in the Java Doc, it indicates that the U statistic of this test is the minimum among U1 and U2. However, when I look into Apache Commons Math {{MannWhitneyUTest.mannWhitneyU()}} method, it returns the maximum of U1 and U2. In fact, the code of this method is the following: 

 
{code:java}
public double mannWhitneyU(double[] x, double[] y) throws NullArgumentException, NoDataException {
  this.ensureDataConformance(x, y);
  double[] z = this.concatenateSamples(x, y);
  double[] ranks = this.naturalRanking.rank(z);
  double sumRankX = 0.0D;

  for(int i = 0; i < x.length; ++i) {
    sumRankX += ranks[i];
  }

  double U1 = sumRankX - (double)((long)x.length * (long)(x.length + 1) / 2L);
  double U2 = (double)((long)x.length * (long)y.length) - U1;
  return FastMath.max(U1, U2);
}
{code}
Also, in the Java Doc it is stated that the maximum value of U1 and U2 is returned.

 
My question is why Apache Commons returns the maximum of those two values, whereas all other sources I found online indicate returning the minimum? If this is not wrong, then shouldn't the Java Doc be updated to include a source that justifies that the maximum U should be returned.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-03-01 12:00:05.253,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 30 19:03:31 UTC 2018,,,,,,0|i3qqkf:,9223372036854775807,,,,,,,,"01/Mar/18 12:00;erans;Thanks for your interest in this project.
We are currently looking for contributors for refactoring the {{stat}} package of ""Commons Math"". A [new project|http://commons.apache.org/proper/commons-statistics/] has been set up.

For the particular issue which you raise, a unit test showing the problem is most welcome (e.g. comparing with the value computed by another implementation on identical input).  Thanks.","30/Apr/18 19:03;psteitz;The minimum value is what should be reported as the value of the statistic.  That is in fact what is used by the code to estimate p-values.  The p-value computation also suffers from some accuracy issues.  First, no continuity correction is applied when computing the normal approximation.  Second (as noted in the javadoc), nothing is done to adjust the variance in the presence of ties in the data.   The patch applied to fix [this issue|https://github.com/Hipparchus-Math/hipparchus/issues/38] in Hipparchus could be fairly easily backported to current [math] code.  The patch there also includes exact computation of p-values for very small samples.  Patches welcome there too, of course.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Infinite loop for CycleCrossover with duplicates,MATH-1451,13138344,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,elastxy,elastxy,13/Feb/18 21:41,15/Feb/18 09:38,07/Apr/19 20:38,,3.6.1,,,,,,,,,,0,,,,,,,,"I've found an infinite loop when recombining two integers Chromosomes admitting duplicates with CycleCrossover class.

For example, recombining example Chromosomes it works fine, but, if you substitute all ""8"" with ""7"", you run into an infinite loop while mating.

I attach the self-contained unit test (JUnit) for a better comprehension (issue a mvn clean install to run).

I added a little quick patch to fix, which I'm happy to provide, even could be less efficient than original.",Home PC,,,,,,,,,,,,,,,,,,,,13/Feb/18 21:40;elastxy;test-apache-math3.zip;https://issues.apache.org/jira/secure/attachment/12910452/test-apache-math3.zip,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2018-02-13 22:20:49.045,,,false,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 15 09:38:21 UTC 2018,,,,,,0|i3q5jz:,9223372036854775807,,,,,,,,"13/Feb/18 22:20;erans;Hello Gabriele.
Thanks for the report.
However, as you might have seen from recent activity on JIRA, or the ""Commons"" project ""dev"" ML, the monolithic ""Commons Math"" library is being phased out in favour of smaller, more focused, modular components:
* [Commons RNG|http://commons.apache.org/rng]
* [Commons Numbers|http://commons.apache.org/numbers]
* [Commons Statistics|http://commons.apache.org/proper/commons-statistics/]
* ...

Moreover, although being the last official release, v3.6.1 is obsolete because of the evolutions (bug fixes and refactoring) that happened on the ""master"" branch (geared towards v4.0) during the last 3 years.

This is to say that if you are a heavy user of the {{genetics}} package, and are able, and willing, to maintain it, you are welcome to signal it on the ""dev"" ML. The ensuing discussion would clarify whether a new ""Commons Genetics"" component would be acceptable to the project leaders.","14/Feb/18 20:01;elastxy;Hi Gilles, thank you for clarification, sounds good. Sent an e-mail to ""dev"" ML.","15/Feb/18 09:38;erans;bq.  Sent an e-mail to ""dev"" ML.

I haven't seen any post.  Did you subscribe first?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PolygonsSet sets incorrect value for last vertex in open loops,MATH-1450,13137673,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mattjuntunen,mattjuntunen,11/Feb/18 04:41,15/Feb/18 18:57,07/Apr/19 20:38,15/Feb/18 18:57,3.5,3.6,4.0,,,,,4.0,,,0,,,,,,,,"According to the documentation, for open infinite vertex loops returned by the PolygonsSet.getVertices() method, the last two points can be used to determine the direction of the last edge in the loop. However, the current code returns a point from the second-to-last edge. For example, the code below builds a box open on the top. It currently returns the vertex loop [null, \{0; 1}, \{0; 0}, \{1; 0}, \{1; 0}], where the last two vertices are the same point and cannot be used to determine the direction of the last edge. The returned vertex loop should be [null, \{0; 1}, \{0; 0}, \{1; 0}, \{1; 1}].
{code:java}
Cartesian2D v0 = new Cartesian2D(0, 1);
        Cartesian2D v1 = new Cartesian2D(0, 0);
        Cartesian2D v2 = new Cartesian2D(1, 0);
        Cartesian2D v3 = new Cartesian2D(1, 1);

        Line left = new Line(v0, v1, 1e-10);
        Line bottom = new Line(v1, v2, 1e-10);
        Line right = new Line(v2, v3, 1e-10);

        List<SubHyperplane<Euclidean2D>> boundaries = new ArrayList<>();
        boundaries.add(new SubLine(left, new IntervalsSet(left.toSubSpace(v0).getX(), left.toSubSpace(v1).getX(), 1e-10)));
        boundaries.add(new SubLine(bottom, new IntervalsSet(bottom.toSubSpace(v1).getX(), bottom.toSubSpace(v2).getX(), 1e-10)));
        boundaries.add(new SubLine(right, new IntervalsSet(right.toSubSpace(v2).getX(), right.toSubSpace(v3).getX(), 1e-10)));

        PolygonsSet polygon = new PolygonsSet(boundaries, 1e-10);

        polygon.getVertices();{code}
      

 

Pull Request: [https://github.com/apache/commons-math/pull/81]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-14 09:14:41.776,,,false,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 15 18:57:58 UTC 2018,,,,,,0|i3q1ev:,9223372036854775807,,,,,,,,"11/Feb/18 04:55;mattjuntunen;The pull request for this is very small. I have more tests around this issue in a branch for MATH-1437 but they all use some new test convenience methods I made. So, I thought I'd just submit the bare minimum here and then include the other tests separately to help with traceability and to avoid overly noisy commits.",14/Feb/18 00:59;mattjuntunen;I have the extra tests for this ready to go on my MATH-1437 branch. Would it be better to submit a single pull request for both of these together?,"14/Feb/18 09:14;erans;Fine.  While at it, please add braces to enclose the {{else}} branch. :)
Thanks.","15/Feb/18 01:53;mattjuntunen;I've updated the pull request listed above with the extra tests. I'm not sure what ""else"" branch you're referring to, though.","15/Feb/18 10:15;erans;Merged in ""master"".

bq. I'm not sure what ""else""

Nevermind, there are several other instances of
{code}
if (...) {
    ...
} else if (...) {
    ...
} else {
    ...
}
{code}
which I find difficult to read. I prefer pairs of {{if}} and {{else}}:
{code}
if (...) {
    ...
} else {
    if (...) {
        ...
    } else {
        ...
    }
}
{code}",15/Feb/18 18:57;erans;commit c965f1c7fca41baf313e2234c6328f4082fe9ab2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PolygonsSet.getBarycenter() returns incorrect value for empty region,MATH-1449,13136155,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mattjuntunen,mattjuntunen,04/Feb/18 19:13,07/Feb/18 00:01,07/Apr/19 20:38,07/Feb/18 00:01,3.6,4.0,,,,,,4.0,,,0,,,,,,,,"The o.a.c.m.geometry.euclidean.twod.PolygonsSet.getBarycenter() method returns the point (0, 0) when the region is empty. It should return (NaN, NaN) since the barycenter does not exist. This is the behavior of IntervalsSet and PolyhedronsSet.

 

Pull request: https://github.com/apache/commons-math/pull/80",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-04 23:51:46.647,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 07 00:01:15 UTC 2018,,,,,,0|i3ps27:,9223372036854775807,,,,,,,,"04/Feb/18 23:51;erans;In the check
{code:java}
Assert.assertEquals(Double.POSITIVE_INFINITY, poly.getSize(), 1e-10);
{code}
using a tolerance looks a bit strange.  Shouldn't it rather be
{code:java}
Assert.assertTrue(Double.isInfinite(poly.getSize()));
{code}
?

Also, statements such as
{code:java}
Assert.assertEquals(false, poly.isEmpty());
Assert.assertEquals(true, poly.isFull());
{code}
would read better as
{code:java}
Assert.assertFalse(poly.isEmpty());
Assert.assertTrue(poly.isFull());
{code}","05/Feb/18 01:43;mattjuntunen;Yes, I suppose it does look kind of strange with the tolerance. I didn't use Double.isInfinite() because I wanted to distinguish between positive and negative infinity. The tolerance value is just there to keep Assert happy. I could rewrite it as 

 
{code:java}
Assert.assertTrue(Double.POSITIVE_INFINITY == poly.getSize());
{code}
to make it clearer if needed.

Also, I disagree that the boolean statements read better with the assertTrue/False methods in this case. When asserting a method return value, I prefer to have the expected return value listed explicitly rather than encoded in the name of the assertion method. I think it stands out better that way. But, this is just a preference thing and really makes no difference otherwise.

Let me know if you'd like me to make either of these changes. Note that I'm pretty sure I used both of the syntaxes you mentioned above all over my last batch of unit test changes, so, if I do change these, I should probably update the other tests as well to match.","05/Feb/18 10:59;erans;bq. I didn't use Double.isInfinite() because I wanted to distinguish between positive and negative infinity.

I thought so.
Since these are unit tests, performance is not paramount, so I'd suggest to use 2 assertions:
{code}
Assert.assertTrue(poly.getSize() > 0);
Assert.assertTrue(Double.isInfinite(poly.getSize()));
{code}
as they state different kinds of expectations.

bq. I disagree that the boolean statements read better

I guess that it's a matter of taste.
But if there is a shorter method, if it is customarily used and there is nothing against it (except taste), it is better to follow the same convention everywhere. And change what does not, when spotted.

I agree that these are nit-picks but they are in order to avoid subsequent ""flip-flop"" changes by people with different tastes. ;) Thanks for your understanding.",06/Feb/18 02:04;mattjuntunen;Sounds good. I just updated the pull request with those changes. I'll include updates to other tests in future commits on MATH-1437.,"07/Feb/18 00:01;erans;Merged in commit cfe0502990a1e61d5ae7c744b119e8c330c37c0c (""master"" branch).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PolygonsSet does not handle intersecting infinite lines,MATH-1447,13135675,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mattjuntunen,mattjuntunen,02/Feb/18 03:55,02/Feb/18 16:54,07/Apr/19 20:38,02/Feb/18 16:54,3.5,3.6,4.0,,,,,4.0,,,0,,,,,,,,"When created from boundaries consisting of two intersecting infinite lines, PolygonsSet.getVertices() throws an IndexOutOfBoundsException.

{{Ex:}}
 {{Line line1 = new Line(new Cartesian2D(0, 0), new Cartesian2D(1, 1), 1e-10);}}
 {{Line line2 = new Line(new Cartesian2D(1, -1), new Cartesian2D(0, 0), 1e-10);}}

{{List<SubHyperplane<Euclidean2D>> boundaries = new ArrayList<>();}}
 {{boundaries.add(line1.wholeHyperplane());}}
 {{boundaries.add(line2.wholeHyperplane());}}

{{PolygonsSet poly = new PolygonsSet(boundaries, 1e-10);}}

{{poly.getVertices(); // throws exception}}

 

Pull request: https://github.com/apache/commons-math/pull/78",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-02-02 16:54:36.056,,,false,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 02 16:54:36 UTC 2018,,,,,,0|i3pp3j:,9223372036854775807,,,,,,,,02/Feb/18 16:54;erans;commit a37dcb93bed20ee302526473a9653f6cb5ae51a1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Passing ""this"" within constructor",MATH-1444,13134462,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,erans,erans,29/Jan/18 14:44,29/Jan/18 14:44,07/Apr/19 20:38,,,,,,,,,4.X,,,0,constructor,unsafe,,,,,,"In the constructor of {{DfpField}}, {{this}} is passed to the constructor of {{Dfp}} instances (which in turn calls a method on the partially constructed {{DfpField}} instance).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,2018-01-29 14:44:28.0,,,,,,0|i3phmn:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PolyhedronsSet.getSize() is incorrect for full space,MATH-1442,13133256,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mattjuntunen,mattjuntunen,24/Jan/18 02:54,26/Jan/18 00:04,07/Apr/19 20:38,26/Jan/18 00:04,3.5,3.6,4.0,,,,,4.0,,,0,,,,,,,,"The getSize() method in PolyhedronsSet returns 0 for instances representing the full space. It should return Double.POSITIVE_INFINITY.

{{Ex:}}

{{PolyhedronsSet poly = new PolyhedronsSet(1e-10);}}

{{poly.isFull(); // returns true}}

{{poly.getSize(); // returns 0.0}}

 

Pull request: https://github.com/apache/commons-math/pull/75",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-24 13:56:03.179,,,false,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 26 00:04:17 UTC 2018,,,,,,0|i3pa7j:,9223372036854775807,,,,,,,,"24/Jan/18 13:56;erans;Thanks for the improvements.

Could you please add Javadoc to all fields and methods?  Thanks.
",25/Jan/18 03:15;mattjuntunen;No problem. I added them on the same pull request.,26/Jan/18 00:04;erans;commit 3ea45970dbe94643fb57ce7713dc1c624526853b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IntervalsSet getBoundarySize() method fails when interval is a single point,MATH-1439,13132487,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mattjuntunen,mattjuntunen,21/Jan/18 05:42,21/Jan/18 13:30,07/Apr/19 20:38,21/Jan/18 13:30,4.0,,,,,,,4.0,,,0,,,,,,,,"After creating an IntervalsSet representing a single point, calls to getBoundarySize() fail with a MathInternalError. The exception is thrown from the o.a.c.m.geometry.partitioning.Characterization class, which does not expect the BSP tree nodes it visits to have hyperplanes that are similar to those of their parents. However, IntervalsSet can (and should be allowed to) create such a tree. I believe that the Characterization class should be updated to handle this situation gracefully.

 

Here is an example of the failing code:

{{IntervalsSet set = new IntervalsSet(1.0, 1.0, 1e-10);}}

{{set.getBoundarySize();}}

 

Pull request: https://github.com/apache/commons-math/pull/74",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-21 13:30:15.489,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 21 13:30:15 UTC 2018,,,,,,0|i3p5h3:,9223372036854775807,,,,,,,,"21/Jan/18 06:09;mattjuntunen;The approach I took was to treat sub-hyperplanes that have the same hyperplane as the bsp node they're being tested against as if they were on the minus side of the node hyperplane. This seems to solve the edge case I mention here (which has actually shown up quite a bit for me when working with large models), and I can't think of any cases where it would cause issues. I think this is especially true considering that the previous behavior was to throw an exception.","21/Jan/18 13:30;erans;commit 6d9bc1ade01040bb48ac01afe6b5198ea379f4e8 (""master"" branch).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PolygonsSet Infinite Lines and SubOrientedPoint Tolerance Issues,MATH-1436,13126131,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mattjuntunen,mattjuntunen,20/Dec/17 03:26,10/Jan/18 14:23,07/Apr/19 20:38,25/Dec/17 09:52,3.5,3.6,3.6.1,4.0,,,,4.0,,,0,,,,,,,,"These are two separate issues that I found while using the partitioning code from 3.5 to work with complex solid models. The issues are:
1. org.apache.commons.math[34].geometry.euclidean.oned.SubOrientedPoint uses a hardcoded tolerance of 1.0e-10 instead of the tolerance from the parent hyperplane. This causes issues when working with a tolerance other than the default.
2. org.apache.commons.math[34].geometry.euclidean.twod.PolygonsSet fails on infinite line segments. An IndexOutOfBoundsException is thrown when a PolygonsSet is created with a single infinite SubLine as a boundary and a NullPointerException is thrown when one is created with a mix of finite and finite boundaries.
I will be attaching a pull request shortly with fixes and unit tests.

UPDATE:
-Pull request for v4.0: [https://github.com/apache/commons-math/pull/70]-
-Pull request for v3.6.x: [https://github.com/apache/commons-math/pull/71]- (removed; no future releases planned for v3.x)

UPDATE [2017-12-23]:
Split initial pull request into two separate ones:
- SubOrientedPoint changes: [https://github.com/apache/commons-math/pull/72]
- PolygonsSet changes: [https://github.com/apache/commons-math/pull/73 ]",,,,,,,,,,,,,MATH-1432,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-12-21 15:15:43.926,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 25 09:52:20 UTC 2017,,,,,,0|i3o3sn:,9223372036854775807,,,,,,,,"21/Dec/17 15:15;erans;Hello.

Thank you for the report and fixes.

bq. These are two separate issues

Would you mind making two separate patches to help tracking of the changes? Thanks.
Also, as I'm not knowledgeable in this part of the codebase, could you perhaps write a more elaborate comment to describe the new statements (e.g. around line 850 in ""PolygonsSet.java"").

bq. Pull request for v3.6.x

No release based on the 3.x branch is planned anymore.
A few weeks ago, there was a discussion (on the ""dev"" ML, see [archive|http://markmail.org/message/75vuyhzblfadc5op]) to move the contents of package {{org.apache.commons.math4.geometry}} into a component of its own (i.e. ""Commons Geometry"") as part of a major overhaul towards modularizing ""Commons Math"" which, in the current circumstances, is not maintainable as a monolithic library.
Work is underway: components [""Commons RNG""|https://commons.apache.org/rng] and [""Commons Numbers""|https://commons.apache.org/numbers] have already been created.
As a user, your input is important; and you are most welcome to help getting there.","22/Dec/17 03:26;mattjuntunen;Hi. Thanks for the feedback. When you say to create two separate patches, do you mean to close this issue and create two new ones with separate pull requests or just to create two pull requests for this one issue?

That's good to know about the 3.x branch. I'll remove that pull request.

Reading the mailing list thread you posted was quite informative. That gave me a much better picture for the current state of the project. I'm using the geometry code from v3.6.1 quite a bit in my current project and I keep running up against issues and things I'd like to update (specifically in the partitioning package). So, I'd be very interested in helping out if this code is going to be refactored at all.","22/Dec/17 11:03;erans;bq. do you mean to close this issue and create two new ones with separate pull requests

For future work, one report per issue is the best option.

bq. just to create two pull requests for this one issue

That'll be fine but please make the log message (for each issue) a little more explicit than ""adding fixes"". ;)

bq. I'm using the geometry code from v3.6.1 quite a bit in my current project \[...\] So, I'd be very interested in helping out

Currently, you're probably the one with the most expertise with this part of the library; thanks a lot for the offer.","23/Dec/17 19:28;mattjuntunen;I split the pull request in two and added the new links in the issue description. There's also a few more unit tests and more comments. And better commit messages, too :-)

Let me know if this will work or not. Thanks.","25/Dec/17 09:52;erans;Merged in ""master"" (with tabs removed).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BaseMultiStartMultivariateOptimizer.doOptimize() swallows exceptions,MATH-1433,13106975,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,crowlogic,crowlogic,04/Oct/17 15:49,04/Oct/17 18:11,07/Apr/19 20:38,,3.6,,,,,,,,,,0,easyfix,,,,,,,"it took me a while to figure out that the random vector generator was throwing an exception that was being swallowed. that should never happen
","        // Multi-start loop.
        for (int i = 0; i < starts; i++) {
            // CHECKSTYLE: stop IllegalCatch
            try {
                // Decrease number of allowed evaluations.
                optimData[maxEvalIndex] = new MaxEval(maxEval - totalEvaluations);
                // New start value.
                double[] s = null;
                if (i == 0) {
                    s = startPoint;
                } else {
                    int attempts = 0;
                    while (s == null) {
                        if (attempts++ >= getMaxEvaluations()) {
                            throw new TooManyEvaluationsException(getMaxEvaluations());
                        }
                        s = generator.nextVector();
                        for (int k = 0; s != null && k < s.length; ++k) {
                            if ((min != null && s[k] < min[k]) || (max != null && s[k] > max[k])) {
                                // reject the vector
                                s = null;
                            }
                        }
                    }
                }
                optimData[initialGuessIndex] = new InitialGuess(s);
                // Optimize.
                final PAIR result = optimizer.optimize(optimData);
                store(result);
            } catch (RuntimeException mue) {
                lastException = mue;
            }
            // CHECKSTYLE: resume IllegalCatch

            totalEvaluations += optimizer.getEvaluations();
        }",,,,,,,,,,,,,,,,,,,,04/Oct/17 18:11;crowlogic;BaseMultiStartMultivariateOptimizerTest.java;https://issues.apache.org/jira/secure/attachment/12890405/BaseMultiStartMultivariateOptimizerTest.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-10-04 16:05:34.982,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 04 18:11:16 UTC 2017,,,,,,0|i3kvfb:,9223372036854775807,,,,,,,,"04/Oct/17 16:05;erans;Could you provide a fully working code that illustrates the issue (e.g. a ""Junit"" test)?

The exceptions are swallowed on the assumption that they could only be raised by a bad starting point.  You are right that it was sloppy to catch {{RuntimeException}} rather than more specific types.

But why is the RNG throwing an exception?  I'd guess that it should not happen...
","04/Oct/17 16:10;crowlogic;Sure thing.. I'll try to get to it soon. It was a bug in my random 
vector guess generating code that just took a while to track down. The 
code as is, just ignores it as a ""bad guess"" and ends up returning the

initial starting point results and only throws the exception if the 
initial starting point isn't there. I'm thinking something like an 
exception in the initial guess should go ahead and cause the entire 
optimize() call to fail since

one would think that the caller always wants to know something is messed 
up first.


",04/Oct/17 18:11;crowlogic;test case,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException when calling PolygonsSet.getSize(),MATH-1432,13098594,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,n,n,30/Aug/17 16:12,01/Jan/19 01:32,07/Apr/19 20:38,01/Jan/19 01:32,3.6.1,,,,,,,,,,0,bug,exception,,,,,,"Running the following code produces an IndexOutOfBoundsException:


    public static void main(String[] args) {
        List<Vector2D> vectors = new ArrayList<>();
        vectors.add(new Vector2D(1, 1));
        vectors.add(new Vector2D(1_189, 1));
        vectors.add(new Vector2D(1_697_165, 147));
        vectors.add(new Vector2D(1_592_444, 249_323));
        vectors.add(new Vector2D(248_665, 110_887));
        vectors.add(new Vector2D(37_142, 24_654));
        vectors.add(new Vector2D(10_093, 8_137));
        vectors.add(new Vector2D(966, 823));
        vectors.add(new Vector2D(25, 25));
        new MonotoneChain().generate(vectors).createRegion().getSize();
    }

Forgive the weird vector values!","Linux Fedora
Java 8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-01-10 14:22:40.435,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 01 01:32:39 UTC 2019,,,,,,0|i3jghz:,9223372036854775807,,,,,,,,"10/Jan/18 14:22;nkollar;This is fixed on master (see MATH-1436), and will be included in 4.0. It looks like no new release is planned on 3.6.x branch, I guess you'll have to upgrade to 4.0.","10/Jan/18 14:56;erans;bq. It looks like no new release is planned on 3.6.x branch, I guess you'll have to upgrade to 4.0.

There are/have been discussions on the ""dev"" ML towards moving the contents of package {{org.apache.commons.math4.geometry}} into its own component (i.e. ""Commons Geometry"").
Let us know if you'd be willing to help with the move (e.g. be a ""beta-tester"").
","10/Jan/18 16:28;nkollar;[~erans] what kind of help do you require from a ""beta-tester""? I'm not directly using commons math, but if I can fit it into my time, I'm happy to help.","10/Jan/18 17:18;erans;bq. kind of help

After creating a repository, the first thing would be to move (and adapt) the code and unit tests.
See e.g. what is being done in [""Commons Numbers""|https://git-wip-us.apache.org/repos/asf?p=commons-numbers.git] (also split of ""Commons Math"").
Reviewing the API would also be nice.
If not done already, please subscribe to the ""dev"" ML; it will be necessary to coordinate who does what.
Thanks!
",01/Jan/19 01:32;erans;The OP marked this issue as duplicating a resolved one.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EmpiricalDistribution cumulativeProbability can return NaN when evaluated within an empty bin.,MATH-1431,13098387,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,jlin61,jlin61,29/Aug/17 20:51,11/Jun/18 13:04,07/Apr/19 20:38,,3.6.1,,,,,,,,,,1,,,,,,,,"The NaN can be reproduced by the following program. Evaluating at x = 0.3, it appeared that getkernel returned a NormalDistribution with mean of NaN and standardDeviation of NaN.

{code:java}
    final int len = 240000;
    double[] data = new double[len];
    for (int i = 0; i < len / 2; ++i) {
        data[i] = 0;
    }
    for (int i = len / 2 + 1; i < len ; ++i) {
        data[i] = 1;
    }

    int binCnt = Math.max(1, data.length / 10);
    EmpiricalDistribution edist = new EmpiricalDistribution(binCnt);
    edist.load(data);
    double x = 0.3;
    double y = edist.cumulativeProbability(x);
    System.out.println(""y is "" + y);
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-10-15 20:54:01.213,,,false,,,,,Important,,,,,,,,,9223372036854775807,,,Sun Oct 15 20:54:01 UTC 2017,,,,,,0|i3jf7z:,9223372036854775807,,,,,,,,"15/Oct/17 20:54;psteitz;The diagnosis above is correct.  The error is in EmpiricalDistribution#getKernel, which checks for singleton or no-variance bins, but not empty bins.  The fix is to change the test in getKernel to also return constant distribution for empty bins.  This is fixed in Hipparchus here: https://github.com/Hipparchus-Math/hipparchus/commit/96fdfa07b56f51cd4c398f6e659a064f43a4178f
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OLSMultipleLinearRegression estimates different  residuals with different order of input,MATH-1428,13094053,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,butchild,butchild,11/Aug/17 09:13,11/Aug/17 11:25,07/Apr/19 20:38,,3.4.1,,,,,,,,,,0,ols,regression,residuals,,,,,"I have a regression job with  31 X  ,which 30 of them are dummys .
And the length of data is 800+ .
I'm using OLSMultipleLinearRegression to do regression.
I found if I change the order of the 800+ data, the residuals I got from  ols.estimateResiduals()
are differents ,and  the correlation of the two differet  rersiduals is near 100%,like 99.8%.

My data is below in Docs Text area.
The fields of each Column is :
sig,y,x1,x2,........xn
",win7  64bit  jdk1.8  intelljidea ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-08-11 11:25:12.984,,"600085.SH,-0.283,1.35,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600360.SH,-0.921,-0.535,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000738.SZ,1.287,-0.531,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002244.SZ,1.791,1.094,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600726.SH,-2.315,0.25,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600118.SH,-0.304,1.139,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600359.SH,0.835,-0.351,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002003.SZ,0.617,-0.676,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600038.SH,-1.05,0.394,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601008.SH,-0.108,-0.968,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600806.SH,-0.116,-0.287,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000429.SZ,-0.311,-0.05,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600199.SH,0.652,-0.24,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600439.SH,-0.216,0.215,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
000612.SZ,1.791,0.574,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002232.SZ,-0.877,-0.506,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000933.SZ,1.791,1.792,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600841.SH,1.791,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600887.SH,0.87,1.986,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600153.SH,1.367,1.338,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000016.SZ,-0.843,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002130.SZ,0.094,-0.878,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000831.SZ,-2.161,-1.042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000751.SZ,0.555,0.435,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000726.SZ,-0.344,0.433,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600702.SH,0.603,-0.668,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600051.SH,0.577,-1.185,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600622.SH,1.791,-0.469,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600818.SH,-0.994,-0.784,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600221.SH,1.791,1.951,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600863.SH,0.992,1.177,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600588.SH,-1.297,1.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000887.SZ,-0.08,0.216,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600177.SH,-1.21,2.063,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600693.SH,-0.661,-0.806,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600783.SH,-2.315,0.294,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
000417.SZ,-0.134,0.323,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002142.SZ,-0.189,2.214,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0
600141.SH,-0.976,-0.191,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600325.SH,-0.513,0.528,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002371.SZ,-0.884,-0.859,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600073.SH,0.0,-1.006,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600509.SH,-0.213,0.227,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600990.SH,1.791,-0.387,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600761.SH,1.791,-0.575,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002266.SZ,-0.87,-0.775,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000683.SZ,1.057,-0.119,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600496.SH,-0.621,-0.823,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000990.SZ,-0.471,-0.835,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600610.SH,-0.043,-0.391,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600736.SH,-0.241,-0.304,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600270.SH,1.791,0.156,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600108.SH,-0.038,0.539,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600175.SH,0.188,-0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600269.SH,-0.474,1.234,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000554.SZ,-1.277,-0.956,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600831.SH,-0.325,-0.212,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600061.SH,0.656,-0.529,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
000828.SZ,0.414,0.137,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000989.SZ,-0.289,-0.67,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002186.SZ,-0.269,-0.661,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600510.SH,-0.626,0.421,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000301.SZ,-0.863,0.141,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000099.SZ,-0.301,-0.81,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000026.SZ,0.301,-0.889,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
600143.SH,-0.528,0.955,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600609.SH,-2.315,-0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600369.SH,-1.125,2.182,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
600644.SH,-0.42,-1.34,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002107.SZ,-0.823,-1.347,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002140.SZ,-0.068,0.178,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000488.SZ,1.791,1.301,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
000520.SZ,-1.433,-1.042,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002025.SZ,-0.499,-0.734,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600429.SH,-2.315,-0.131,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600704.SH,0.211,0.265,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600323.SH,-0.507,-0.702,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600897.SH,-0.408,-0.335,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600396.SH,-0.691,-1.071,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000510.SZ,0.562,-0.434,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600578.SH,-0.455,-0.477,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002200.SZ,-2.315,-0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000671.SZ,1.791,-0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000636.SZ,1.791,0.149,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000911.SZ,1.458,-0.531,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000566.SZ,-0.609,-0.492,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600683.SH,-0.718,-0.388,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600026.SH,-0.032,2.456,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600292.SH,-0.429,-1.129,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600808.SH,0.981,2.211,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600131.SH,0.528,-1.372,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600208.SH,-0.131,2.066,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002152.SZ,-0.537,0.931,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000031.SZ,1.791,1.081,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002129.SZ,-2.315,-0.072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601099.SH,1.091,1.682,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
600748.SH,0.0,0.556,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600004.SH,-0.216,0.828,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000532.SZ,-1.273,-0.985,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600522.SH,-0.363,0.054,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
600614.SH,0.011,-0.846,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600855.SH,-0.536,-1.268,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002017.SZ,-0.689,-0.792,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
600775.SH,1.188,-0.289,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
600259.SH,1.791,0.28,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000959.SZ,-0.74,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600425.SH,-0.266,0.25,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000913.SZ,0.244,-0.982,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002258.SZ,-0.665,-0.676,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600970.SH,-0.224,1.298,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600586.SH,1.791,0.059,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600460.SH,1.791,0.164,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600133.SH,-0.084,-0.561,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600540.SH,-0.492,-1.063,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600386.SH,-0.667,-0.581,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600598.SH,-0.824,1.977,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000897.SZ,-0.26,0.282,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600626.SH,1.131,-1.167,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600517.SH,-0.591,0.996,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600602.SH,0.551,0.525,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002212.SZ,-0.96,-0.899,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002052.SZ,-2.315,-0.631,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002029.SZ,-0.254,0.554,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000718.SZ,-0.366,1.428,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600006.SH,1.791,0.727,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000609.SZ,-1.459,-1.471,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000960.SZ,0.904,1.264,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600379.SH,0.885,-0.821,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600192.SH,-0.922,-1.139,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600333.SH,0.779,-0.676,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000061.SZ,-0.186,1.143,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600121.SH,-0.817,-0.191,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600648.SH,1.791,1.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002156.SZ,1.791,-0.336,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000598.SZ,1.062,0.778,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000793.SZ,0.005,0.455,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600282.SH,1.791,0.046,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600879.SH,-0.595,0.623,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600636.SH,0.734,-1.216,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600529.SH,-0.318,-0.457,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600206.SH,0.486,-1.205,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600716.SH,0.487,-0.544,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600741.SH,-0.031,1.945,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600311.SH,-2.315,-0.001,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600449.SH,-1.589,-0.514,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600367.SH,0.769,-1.398,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000972.SZ,-2.302,-0.053,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600797.SH,0.315,-0.079,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600398.SH,-0.347,-1.026,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600843.SH,0.491,-0.644,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600260.SH,-0.516,-0.956,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
000811.SZ,1.791,-0.482,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000666.SZ,0.686,-0.472,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
600576.SH,0.254,-0.535,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000728.SZ,-0.739,2.171,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
002122.SZ,-0.521,1.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600530.SH,-0.43,-1.242,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600123.SH,-0.591,1.578,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600624.SH,-0.779,-1.103,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000006.SZ,-0.889,-0.644,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002154.SZ,-0.608,0.137,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000539.SZ,1.791,1.566,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000969.SZ,0.6,1.361,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600284.SH,-1.069,-0.752,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600190.SH,0.888,0.262,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600785.SH,-0.186,0.072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002248.SZ,0.5,-0.379,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600470.SH,1.431,-1.207,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600718.SH,-0.935,1.706,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600877.SH,-0.127,0.008,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000425.SZ,1.791,2.301,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002062.SZ,-0.549,-0.047,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600189.SH,0.917,-1.337,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000540.SZ,-0.176,0.222,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000970.SZ,1.791,0.238,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000783.SZ,-0.496,2.239,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
000937.SZ,-0.415,1.839,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
002236.SZ,0.983,-0.055,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000507.SZ,-0.135,-0.991,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600075.SH,1.568,-0.255,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600157.SH,1.791,-0.021,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
002094.SZ,0.504,-0.986,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600845.SH,-0.529,0.343,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600415.SH,-0.437,2.211,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600507.SH,1.791,-0.154,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600658.SH,0.866,-0.191,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600249.SH,-0.217,-0.951,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601179.SH,-0.362,2.263,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600612.SH,-0.244,0.391,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
002166.SZ,1.791,-0.954,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000877.SZ,-0.328,0.45,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600343.SH,-0.107,-0.775,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600811.SH,-0.849,0.654,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000155.SZ,-1.769,-1.158,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002271.SZ,-0.896,-0.449,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600867.SH,-0.134,-0.003,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000600.SZ,-1.126,-0.451,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002005.SZ,1.583,-0.026,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000925.SZ,-0.703,-0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002110.SZ,0.725,-0.469,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600063.SH,-0.519,-1.035,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000899.SZ,-0.695,-0.792,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002109.SZ,0.033,-0.988,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600388.SH,-0.751,-0.182,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600482.SH,1.791,0.224,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000761.SZ,-2.315,1.425,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
000903.SZ,-0.685,-0.508,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002050.SZ,0.487,0.152,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600216.SH,0.251,1.218,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000028.SZ,-0.31,0.609,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600459.SH,1.791,-0.983,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600377.SH,-0.129,2.502,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600171.SH,0.519,-0.303,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002032.SZ,-0.034,0.793,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600033.SH,-0.786,0.975,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600790.SH,-0.546,-0.339,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002273.SZ,1.067,-0.543,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000709.SZ,-1.051,2.269,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
000596.SZ,1.068,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
002078.SZ,0.838,0.874,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
600858.SH,-0.293,0.397,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600548.SH,-0.253,0.825,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600789.SH,1.791,-0.244,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000550.SZ,0.662,1.442,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600743.SH,-0.262,-0.038,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600686.SH,-0.703,-0.974,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000859.SZ,0.551,-1.222,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000068.SZ,0.234,-0.284,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000882.SZ,-0.651,-0.933,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000503.SZ,0.019,0.193,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600640.SH,0.524,-0.91,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000022.SZ,-0.11,0.412,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000916.SZ,-0.513,-0.373,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002123.SZ,-0.645,1.122,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600239.SH,-0.017,0.931,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002182.SZ,1.358,-1.323,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600708.SH,-0.414,-0.809,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600480.SH,0.409,-0.664,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600583.SH,-0.743,1.925,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
601003.SH,1.791,0.976,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
000756.SZ,-0.08,-0.873,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600079.SH,1.178,0.554,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600354.SH,0.354,-0.584,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600893.SH,-0.657,1.406,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601588.SH,-0.874,1.197,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000400.SZ,-0.214,1.221,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600469.SH,1.791,-0.59,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000938.SZ,-0.518,-1.115,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000963.SZ,-0.464,1.025,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002204.SZ,-0.527,-0.475,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600962.SH,-2.13,-1.569,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600824.SH,-0.389,-0.209,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000046.SZ,1.791,1.827,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600834.SH,-0.392,-0.104,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000768.SZ,0.648,2.297,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000159.SZ,-0.866,-0.035,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000825.SZ,-0.732,2.476,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600389.SH,-2.315,-1.735,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600664.SH,-0.43,2.338,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600777.SH,-2.315,-0.613,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600158.SH,-0.217,0.377,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600183.SH,1.705,0.556,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600240.SH,-0.29,-0.872,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002091.SZ,-0.252,0.435,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600320.SH,-1.304,2.392,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002261.SZ,-0.529,-0.633,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600376.SH,0.789,1.454,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000584.SZ,-1.101,-1.081,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600067.SH,1.791,-0.056,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600846.SH,0.526,-0.833,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600871.SH,0.5,2.407,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
002054.SZ,-0.164,-0.607,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600812.SH,1.791,1.042,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002238.SZ,-0.529,0.054,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600698.SH,-2.315,0.034,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600755.SH,-0.572,0.427,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000997.SZ,-0.479,0.008,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600008.SH,1.791,1.186,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600226.SH,-0.518,-0.982,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600639.SH,-0.294,0.368,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600089.SH,-0.41,2.471,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600467.SH,-0.295,-0.14,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600329.SH,-0.43,1.019,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000823.SZ,1.469,-0.128,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000525.SZ,-0.657,-0.504,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000677.SZ,0.681,-0.443,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600435.SH,-1.082,0.732,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002007.SZ,1.791,2.43,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600676.SH,0.303,-0.192,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000422.SZ,1.242,0.475,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000961.SZ,1.791,0.755,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002263.SZ,0.19,-0.7,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002192.SZ,0.517,-0.827,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600802.SH,-2.315,-1.364,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000559.SZ,-0.168,1.278,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600317.SH,-0.675,0.164,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600298.SH,0.014,0.759,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600252.SH,0.711,0.672,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600126.SH,1.791,-0.627,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
000410.SZ,1.477,0.187,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000572.SZ,1.791,-0.183,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002216.SZ,-0.322,-0.24,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600650.SH,-0.407,0.065,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000009.SZ,-0.349,0.807,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601666.SH,-0.896,2.343,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
000058.SZ,0.68,-0.459,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002042.SZ,1.791,-0.315,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600230.SH,0.511,-0.911,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600423.SH,1.791,-0.911,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002030.SZ,-0.383,-0.81,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000973.SZ,1.109,0.384,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600516.SH,-1.311,0.583,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600479.SH,-0.444,-0.113,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002019.SZ,-0.997,-0.829,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000707.SZ,1.199,-1.08,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600055.SH,-0.283,-0.699,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600330.SH,0.558,-0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002179.SZ,-0.546,-0.348,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000036.SZ,-1.138,-0.261,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600500.SH,0.523,1.348,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000608.SZ,1.791,-0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000537.SZ,1.791,-0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600787.SH,-0.238,-0.052,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002251.SZ,-0.507,0.081,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600202.SH,-0.745,-0.13,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000880.SZ,0.531,-0.524,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600881.SH,0.123,1.117,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002064.SZ,-0.786,0.143,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002081.SZ,-0.156,0.822,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600307.SH,1.791,1.66,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600380.SH,0.343,0.995,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000951.SZ,1.061,0.291,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600649.SH,-0.185,1.836,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000631.SZ,-1.129,0.213,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600836.SH,0.594,-1.075,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
600352.SH,-0.311,1.42,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600595.SH,1.791,1.312,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600021.SH,0.732,0.544,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000687.SZ,0.053,-0.853,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
000758.SZ,-0.377,0.51,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600765.SH,0.536,0.764,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600826.SH,-1.079,-0.312,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601607.SH,1.309,2.502,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000965.SZ,1.791,-0.379,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600339.SH,-0.087,-0.333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600723.SH,-0.421,-0.391,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600620.SH,-0.843,-1.262,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600288.SH,-0.625,-0.856,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600861.SH,-0.392,-1.167,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600185.SH,0.719,-0.477,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002206.SZ,-0.308,-0.619,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600770.SH,-0.401,0.475,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600059.SH,-0.432,0.205,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600082.SH,-0.764,-0.956,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000788.SZ,1.791,-0.566,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600408.SH,-2.315,-0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600327.SH,-0.123,-0.228,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000999.SZ,-0.199,2.337,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600838.SH,1.791,-0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000685.SZ,-0.66,0.73,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600654.SH,1.791,-0.604,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000088.SZ,-0.651,0.462,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002162.SZ,-2.272,-0.384,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600163.SH,0.185,-1.028,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000839.SZ,-0.479,1.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000661.SZ,0.937,0.034,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000850.SZ,-0.841,-0.341,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600546.SH,1.791,1.439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600747.SH,0.522,-0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600619.SH,0.943,-0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000511.SZ,-0.443,-0.465,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600804.SH,-1.023,1.185,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
000066.SZ,1.409,0.782,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600197.SH,-0.242,0.152,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600642.SH,-0.438,2.092,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002218.SZ,-0.149,-0.036,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000570.SZ,-0.619,-0.117,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002241.SZ,1.791,1.044,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600138.SH,-0.295,0.095,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600278.SH,-0.739,-0.989,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600851.SH,-0.779,0.1,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002088.SZ,-0.671,-0.653,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600161.SH,0.067,1.174,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600662.SH,-0.619,-0.127,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600779.SH,-0.369,0.658,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
000975.SZ,0.903,-0.904,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002022.SZ,-0.57,0.571,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600816.SH,-0.077,0.011,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
000592.SZ,-0.812,-0.967,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600303.SH,-0.591,-0.788,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000407.SZ,-0.775,-0.971,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000837.SZ,-0.444,-0.882,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600487.SH,0.246,-0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
600630.SH,0.302,-0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600195.SH,-0.387,0.845,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000616.SZ,-1.123,-0.546,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002056.SZ,1.791,0.308,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600200.SH,-0.322,-0.771,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000419.SZ,0.605,-0.974,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600069.SH,1.791,-0.191,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
600499.SH,0.176,1.223,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600382.SH,-0.476,-1.325,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600266.SH,0.005,0.536,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600780.SH,-0.426,-0.339,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002172.SZ,1.791,-0.529,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000513.SZ,-0.525,1.126,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000012.SZ,1.791,2.058,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600337.SH,0.202,-0.599,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
600173.SH,-0.922,-0.202,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000860.SZ,0.43,0.328,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600035.SH,-0.147,-0.215,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600828.SH,-0.189,-0.237,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600966.SH,-0.7,-0.755,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
600536.SH,-1.125,-0.148,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600106.SH,-0.508,-0.26,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600629.SH,1.791,-0.823,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000078.SZ,-0.76,0.427,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002137.SZ,0.885,-0.864,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002208.SZ,1.791,-0.911,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000778.SZ,-0.196,1.188,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600581.SH,1.74,0.427,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
000987.SZ,-0.397,0.524,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
002194.SZ,-0.707,0.511,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
600315.SH,-0.381,1.388,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600128.SH,-0.218,-1.188,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000501.SZ,-0.234,0.378,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000931.SZ,-2.315,-0.205,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002287.SZ,-0.738,0.449,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601001.SH,-0.862,2.259,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600895.SH,-0.389,1.276,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600652.SH,0.543,-1.193,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600151.SH,1.166,0.182,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002115.SZ,-0.492,-0.899,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
002187.SZ,-0.546,-0.433,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600062.SH,-0.466,1.517,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600383.SH,1.791,2.404,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000040.SZ,1.466,-1.51,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000521.SZ,1.791,-0.558,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000762.SZ,0.115,0.447,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
000715.SZ,-0.302,-0.685,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600428.SH,-0.773,0.85,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600703.SH,0.399,2.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600416.SH,0.689,0.078,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600657.SH,-0.771,0.782,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002267.SZ,-0.616,0.531,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600508.SH,-0.723,1.24,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
000682.SZ,-0.28,-0.231,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000807.SZ,0.577,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000968.SZ,-0.568,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
000876.SZ,-0.337,0.175,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600210.SH,1.077,0.439,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
600978.SH,-0.258,-0.117,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
000005.SZ,-2.315,-0.731,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002106.SZ,0.39,1.006,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600531.SH,0.937,-0.792,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600027.SH,-1.072,2.246,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600623.SH,1.791,1.096,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600096.SH,1.58,0.833,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000601.SZ,0.103,-0.569,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002073.SZ,-0.735,0.912,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600692.SH,-2.315,-1.165,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000830.SZ,0.236,-0.261,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600876.SH,-1.299,-0.544,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000428.SZ,-0.962,-0.232,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600039.SH,-0.17,-1.297,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002233.SZ,-0.178,-0.535,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600475.SH,-0.42,-0.595,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600750.SH,-0.203,0.846,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600129.SH,-0.488,-0.63,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000027.SZ,0.479,1.986,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002128.SZ,-0.367,2.096,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600565.SH,-1.051,-0.674,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000589.SZ,-0.437,-1.008,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600395.SH,-0.66,1.954,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
000533.SZ,1.791,0.433,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600611.SH,-0.727,1.211,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600829.SH,-0.468,0.427,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000910.SZ,-0.296,-0.638,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
600166.SH,0.512,1.449,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600497.SH,1.791,1.633,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002255.SZ,-0.56,0.025,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600107.SH,0.362,-0.882,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600291.SH,-2.315,-0.791,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
000737.SZ,-2.315,-1.299,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000819.SZ,-0.918,-0.968,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002048.SZ,1.791,0.133,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600438.SH,-2.315,-0.013,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600713.SH,-0.457,-0.631,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002153.SZ,-0.077,0.473,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600268.SH,1.791,-0.072,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600543.SH,-0.831,-0.859,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600130.SH,0.719,-0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
600312.SH,-1.057,0.661,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600198.SH,-2.315,0.196,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
600874.SH,-0.244,0.64,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600037.SH,-0.169,1.569,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000416.SZ,-1.289,-0.656,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
600759.SH,-0.33,-0.078,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600293.SH,1.791,-0.289,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
002097.SZ,0.04,-0.085,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600072.SH,-0.57,-0.384,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000852.SZ,-0.626,-0.762,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000625.SZ,1.791,2.024,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000900.SZ,-0.375,0.265,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600246.SH,-0.925,0.137,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600521.SH,-0.375,0.231,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600418.SH,1.791,0.565,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000725.SZ,0.27,2.436,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000565.SZ,-0.776,-1.467,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600682.SH,-0.305,-0.859,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000912.SZ,-1.239,-0.57,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600220.SH,-0.415,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000886.SZ,0.445,-0.072,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600725.SH,0.636,-0.339,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600132.SH,-0.484,1.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600660.SH,1.404,1.636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601009.SH,-0.137,2.041,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0
000042.SZ,1.791,-0.974,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600555.SH,1.791,-0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600830.SH,-0.316,-0.379,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
000829.SZ,1.16,1.137,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600060.SH,-0.465,1.009,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000659.SZ,0.184,-0.494,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
002185.SZ,1.791,-0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000543.SZ,0.216,-0.165,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600109.SH,-0.683,1.359,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
600590.SH,-0.767,0.124,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000713.SZ,0.769,-0.814,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600886.SH,-0.952,1.354,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002223.SZ,-0.424,0.706,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600655.SH,-0.468,1.781,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002333.SZ,-0.852,-0.748,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002163.SZ,-0.717,-0.307,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600737.SH,-0.025,0.941,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600120.SH,-0.116,-0.459,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600219.SH,-0.176,1.556,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600258.SH,-0.099,-0.518,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600533.SH,1.791,-0.247,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600589.SH,-0.812,-0.599,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600864.SH,-0.843,-0.423,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
000786.SZ,-0.361,0.192,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600782.SH,1.791,0.332,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600406.SH,1.791,2.023,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600621.SH,-0.806,-0.754,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600819.SH,1.403,-0.353,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000599.SZ,-1.267,-1.103,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600176.SH,0.103,0.198,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600694.SH,1.791,1.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600281.SH,-2.315,-0.631,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600528.SH,-0.303,1.134,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000426.SZ,-0.133,-0.478,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600717.SH,-0.181,1.354,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
300059.SZ,-0.494,0.597,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600236.SH,1.791,0.851,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600368.SH,0.07,-0.749,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600798.SH,0.934,-0.702,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000506.SZ,-0.675,-0.391,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000936.SZ,-0.571,-0.63,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000541.SZ,-0.421,1.218,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600110.SH,1.34,0.377,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600563.SH,0.754,-0.133,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600322.SH,1.627,-0.374,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600993.SH,-0.635,0.019,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002093.SZ,-0.488,-0.153,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
002041.SZ,0.417,0.761,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000667.SZ,-0.454,0.571,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600397.SH,1.175,-0.821,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600122.SH,-0.488,-0.091,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000100.SZ,1.598,1.047,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600168.SH,1.271,-0.91,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000729.SZ,-0.253,2.225,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
000060.SZ,1.791,1.995,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002155.SZ,1.791,0.839,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600729.SH,-0.289,0.493,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600436.SH,-0.302,0.04,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600866.SH,1.791,0.066,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
601888.SH,-0.942,1.491,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600191.SH,0.861,-0.96,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600820.SH,-0.313,0.129,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600637.SH,0.58,-0.37,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002063.SZ,-0.467,0.051,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600981.SH,-0.367,-1.009,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600064.SH,-0.52,-0.053,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600551.SH,-0.451,0.236,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000655.SZ,0.481,0.678,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
000759.SZ,-0.417,0.381,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000822.SZ,0.638,-0.183,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000717.SZ,0.74,0.095,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
002269.SZ,-1.251,1.876,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002028.SZ,-0.198,1.204,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000680.SZ,0.615,0.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002133.SZ,0.859,-1.026,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002167.SZ,-0.803,-0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601333.SH,-0.349,2.222,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000878.SZ,0.97,2.191,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600854.SH,-0.41,-0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600667.SH,-0.326,-0.991,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600271.SH,-0.299,1.429,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600098.SH,1.791,1.085,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600300.SH,-0.864,0.741,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
000553.SZ,-1.122,-0.273,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600809.SH,-0.018,1.699,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
601005.SH,-1.208,0.141,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600690.SH,1.671,2.132,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600146.SH,1.791,-0.893,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600983.SH,-0.268,0.052,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002225.SZ,-0.384,-0.098,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600100.SH,-0.357,2.176,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600868.SH,-1.618,-0.062,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600518.SH,-0.374,2.086,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601168.SH,-0.387,2.182,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600387.SH,-1.201,-0.986,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600707.SH,0.687,0.115,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600888.SH,0.172,0.037,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600458.SH,0.286,0.32,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000029.SZ,1.791,-0.379,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002051.SZ,-0.39,0.057,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600601.SH,0.57,0.568,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600261.SH,1.017,-0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600086.SH,0.558,-0.979,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
600017.SH,-0.208,0.559,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600309.SH,-0.305,2.45,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002083.SZ,-0.544,0.779,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000528.SZ,0.14,1.045,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600426.SH,-0.528,0.198,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600810.SH,0.529,-0.472,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600739.SH,-0.25,2.226,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600273.SH,-2.315,-0.169,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600971.SH,-0.128,0.719,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
000597.SZ,-0.809,-0.146,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600178.SH,0.352,-0.318,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000050.SZ,0.584,-0.739,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002004.SZ,-0.87,0.144,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000926.SZ,1.717,0.098,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600635.SH,-0.53,0.902,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000585.SZ,-0.295,-0.67,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002038.SZ,-0.561,1.01,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600597.SH,1.144,0.522,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600295.SH,1.791,0.892,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600366.SH,-1.142,0.057,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600238.SH,1.791,-0.661,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
002237.SZ,-0.802,0.205,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000690.SZ,-0.864,0.776,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600399.SH,-0.652,-1.132,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
000679.SZ,-0.442,-1.436,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002095.SZ,-0.112,-0.913,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600074.SH,-0.864,-1.309,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600844.SH,0.326,1.273,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000623.SZ,-0.109,1.61,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600007.SH,-0.729,0.765,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000518.SZ,0.643,-0.48,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600112.SH,-0.293,-0.399,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002203.SZ,-0.664,-0.399,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600575.SH,-1.189,-0.172,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600332.SH,0.21,0.735,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000062.SZ,1.041,-0.023,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000563.SZ,-0.786,-0.807,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
002016.SZ,-0.352,-0.725,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002121.SZ,-0.277,-0.122,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600680.SH,-0.413,-0.475,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
600052.SH,0.112,-0.448,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000888.SZ,1.791,-0.497,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600822.SH,-0.284,-0.561,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000402.SZ,-0.826,1.815,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002250.SZ,0.03,0.389,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600572.SH,0.123,-0.225,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600251.SH,1.791,-0.023,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600010.SH,-2.315,1.911,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
000630.SZ,-0.266,1.829,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000927.SZ,1.791,1.122,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002215.SZ,-0.644,0.135,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002158.SZ,1.791,-0.498,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600835.SH,-0.298,0.721,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002353.SZ,-0.293,0.683,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600009.SH,0.783,2.168,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600823.SH,0.605,0.989,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000573.SZ,-0.223,-1.181,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600525.SH,0.799,0.164,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600227.SH,-0.751,-0.353,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600468.SH,-0.219,0.163,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601117.SH,-0.654,1.83,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600961.SH,-1.1,-0.215,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600422.SH,0.195,-0.691,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002008.SZ,-1.317,0.491,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000767.SZ,0.496,-0.165,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600744.SH,0.542,-1.028,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600503.SH,-0.976,-0.488,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600549.SH,1.791,1.336,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600308.SH,-1.913,0.075,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
000780.SZ,-0.573,0.693,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600559.SH,-0.365,-0.838,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
000950.SZ,-1.316,-0.371,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600135.SH,0.867,-0.959,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
600410.SH,-0.401,0.057,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600078.SH,-0.904,-0.382,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601002.SH,0.246,-0.361,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600594.SH,-0.027,-0.128,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600481.SH,1.791,1.048,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600995.SH,-0.794,-0.93,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000514.SZ,-0.479,-0.056,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600022.SH,0.573,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600651.SH,-0.348,0.158,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600596.SH,-0.729,0.479,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002100.SZ,-0.45,-0.59,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600537.SH,-2.315,-0.146,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600628.SH,-0.51,0.107,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000652.SZ,-0.298,1.015,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600973.SH,-0.339,-1.372,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000607.SZ,-2.315,-0.927,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000949.SZ,1.32,-0.631,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600056.SH,-0.534,-0.422,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000423.SZ,-0.169,2.176,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600501.SH,-0.05,-0.908,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600801.SH,-0.016,0.262,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002249.SZ,-0.618,0.986,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002168.SZ,-0.615,-0.436,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600066.SH,0.515,0.391,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600742.SH,0.072,-0.804,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002080.SZ,1.791,-0.015,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600169.SH,-0.368,0.501,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000881.SZ,-0.132,-1.329,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002065.SZ,-0.439,0.885,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600880.SH,-0.447,0.998,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600604.SH,0.191,-0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000548.SZ,-0.719,-0.921,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000619.SZ,-0.482,-0.824,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000021.SZ,0.278,1.252,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000917.SZ,-0.809,0.27,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000800.SZ,0.809,2.239,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601918.SH,-0.604,1.818,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
000952.SZ,-0.552,-1.182,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600710.SH,1.791,-0.728,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600997.SH,-0.612,1.712,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
600020.SH,0.063,0.407,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000686.SZ,-0.686,1.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
000516.SZ,-0.464,-0.766,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600170.SH,0.086,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600491.SH,-0.701,-0.538,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000731.SZ,-0.844,-0.893,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002018.SZ,-0.73,-1.549,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002274.SZ,-0.502,-1.216,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600665.SH,0.65,-0.769,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600054.SH,-0.824,0.701,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600478.SH,-1.091,-0.607,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000708.SZ,1.791,-0.315,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600616.SH,-0.51,-0.183,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600859.SH,-0.348,1.256,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600287.SH,1.791,-0.939,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600562.SH,0.125,-0.983,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000905.SZ,1.03,-0.768,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002191.SZ,-0.287,0.239,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
002053.SZ,-0.598,-1.572,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000698.SZ,1.791,-0.128,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600125.SH,-0.011,1.353,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600400.SH,-0.064,-1.528,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000571.SZ,-0.333,-0.396,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0
000868.SZ,1.039,-0.928,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600160.SH,1.718,0.127,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600754.SH,-0.92,1.097,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000996.SZ,0.019,-0.056,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000059.SZ,-0.5,0.866,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600456.SH,-1.19,0.814,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600584.SH,1.147,0.614,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600391.SH,-1.203,-1.377,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600638.SH,-0.758,-0.215,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600720.SH,-0.626,0.002,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000939.SZ,-0.202,0.605,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600088.SH,0.078,-0.075,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600363.SH,-0.361,-0.596,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600825.SH,-0.718,0.791,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600103.SH,0.897,-0.644,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
000158.SZ,0.495,0.013,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600963.SH,0.89,-0.273,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0
002146.SZ,-0.595,1.243,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600550.SH,-0.885,2.182,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600776.SH,-0.173,0.117,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
600159.SH,-2.315,-0.579,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002069.SZ,1.791,1.064,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002092.SZ,1.791,1.439,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600677.SH,-0.71,-1.172,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000962.SZ,1.061,-0.139,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002262.SZ,-0.171,-0.119,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600316.SH,1.791,0.957,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000988.SZ,-0.338,0.256,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600987.SH,-0.313,-1.161,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600127.SH,1.574,-0.668,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600511.SH,-0.464,0.88,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600643.SH,0.264,0.186,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
600557.SH,-0.284,0.221,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600884.SH,-0.823,0.387,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000650.SZ,-0.388,0.227,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600351.SH,-0.065,0.049,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600592.SH,-0.769,-1.095,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000799.SZ,0.833,-0.728,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600277.SH,0.5,0.767,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600231.SH,1.148,-0.214,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
000089.SZ,-0.311,0.715,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002161.SZ,-1.161,-0.339,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000735.SZ,-0.685,0.081,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002138.SZ,0.118,-0.908,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600545.SH,-1.534,0.539,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600488.SH,-0.214,-0.483,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002252.SZ,-0.365,0.264,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000581.SZ,1.791,0.403,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600580.SH,-0.278,0.447,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600299.SH,0.495,-0.456,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600058.SH,1.791,1.552,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600201.SH,-0.129,-1.083,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000930.SZ,0.358,0.233,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000151.SZ,-0.279,-0.728,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000055.SZ,-0.454,-0.333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000627.SZ,1.791,0.218,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0
001696.SZ,-0.742,1.076,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600328.SH,-0.854,-1.092,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600896.SH,-0.806,-0.778,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000816.SZ,-1.096,-0.618,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002001.SZ,-0.357,1.304,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600523.SH,-0.411,-0.362,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002230.SZ,-0.317,0.085,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600724.SH,-0.533,0.664,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600673.SH,1.791,0.755,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600815.SH,-0.863,-0.169,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002023.SZ,-0.62,-0.801,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600186.SH,1.065,-0.217,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
000536.SZ,0.553,1.282,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600685.SH,-0.356,0.739,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002254.SZ,-0.896,-0.058,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000932.SZ,-0.138,0.914,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
600255.SH,1.01,-1.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600290.SH,-0.319,-0.771,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600839.SH,0.437,0.917,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600326.SH,-2.315,-0.211,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600409.SH,1.231,-0.102,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000998.SZ,0.203,-0.124,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002045.SZ,0.931,-0.229,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002183.SZ,-0.909,0.04,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600081.SH,1.791,-0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600582.SH,-0.56,1.362,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600223.SH,-0.85,0.233,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600894.SH,0.537,-0.477,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600653.SH,1.791,0.068,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600827.SH,-0.257,0.404,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600361.SH,-0.718,-0.729,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000861.SZ,-0.12,-0.023,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600535.SH,-0.299,1.481,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600105.SH,1.791,-0.812,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
000893.SZ,1.791,-0.168,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600432.SH,1.791,1.461,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000848.SZ,-0.278,0.317,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600862.SH,0.384,-0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000777.SZ,-0.302,-0.509,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002011.SZ,-0.704,0.233,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000617.SZ,-1.634,-1.121,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600805.SH,1.791,-0.293,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000090.SZ,0.985,-0.609,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600872.SH,-0.593,0.065,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0
600196.SH,-0.247,2.346,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600068.SH,-0.71,2.181,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600498.SH,-0.184,1.053,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
600570.SH,-0.582,0.733,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600641.SH,-0.236,-0.156,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000418.SZ,0.065,0.345,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000919.SZ,-0.875,-0.142,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600012.SH,-0.527,0.608,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600697.SH,-0.37,-0.628,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600267.SH,-0.573,1.295,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600569.SH,0.519,0.596,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
000755.SZ,0.32,-0.84,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600139.SH,-0.118,-0.042,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002242.SZ,-0.826,0.614,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601872.SH,-1.089,1.393,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600420.SH,-0.292,-0.258,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000826.SZ,-0.205,0.658,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002089.SZ,-0.359,-0.722,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
600162.SH,-2.315,-0.633,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002104.SZ,-0.407,0.17,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
600289.SH,0.206,-0.301,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0
000733.SZ,-0.265,-0.615,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
002220.SZ,1.791,-0.673,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600618.SH,1.791,0.264,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
601999.SH,-0.393,0.274,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
000920.SZ,0.168,-0.849,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
600117.SH,1.791,-0.322,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0
",false,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 11 11:25:12 UTC 2017,,,,,,0|i3ioqf:,9223372036854775807,,,,,,,,"11/Aug/17 11:25;erans;What result did you expect?
What do other libraries produce?

Also, please provide a _minimal_ working code (preferably a JUnit test) example.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unreachable statements in Complex.abs(),MATH-1427,13092055,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Won't Fix,,David Nickerson,David Nickerson,03/Aug/17 02:34,21/Jan/18 15:28,07/Apr/19 20:38,21/Jan/18 15:28,3.6.1,,,,,,,,,,0,easyfix,newbie,patch,,,,,"This return statement in Complex.abs() is unreachable:

{code:java}
if (FastMath.abs(real) < FastMath.abs(imaginary)) {
  if (imaginary == 0.0) {
    return FastMath.abs(real);
  }
{code}

If imaginary == 0, then there's no way that the preceding condition would be true. There are two similar inner 'if' statements that were accidentally switched. Returned values are still correct, but performance suffers.

The attached patch switches these back. Note that we're still protected from dividing by zero.
",,,,,,,,,,,,,,,,,,,,,03/Aug/17 02:35;David Nickerson;complex_abs.patch;https://issues.apache.org/jira/secure/attachment/12880153/complex_abs.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-08-03 11:06:28.783,,,false,,,,,Patch,,,,,,,,,9223372036854775807,,,Sun Jan 21 15:28:56 UTC 2018,,,,,,0|i3icov:,9223372036854775807,,,,,,,,"03/Aug/17 11:06;erans;The ""complex number"" functionality is being refactored within a new project: http://commons.apache.org/proper/commons-numbers/

You are most welcome to review the changes, currently performed within the [""complex-dev"" branch|https://git1-us-west.apache.org/repos/asf?p=commons-numbers.git;a=tree;h=0a01e2d0e7e6c6621cbf6b5c2c7da885c1691c07;hb=0a01e2d0e7e6c6621cbf6b5c2c7da885c1691c07] and to provide suggestions on the ""dev"" ML.

Please report issues at
https://issues.apache.org/jira/projects/NUMBERS
",03/Aug/17 17:19;David Nickerson;Bug report migrated to [https://issues.apache.org/jira/browse/NUMBERS-48],21/Jan/18 15:28;erans;See NUMBERS-48.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong eigen values computed by EigenDecomposition when the input matrix has large values,MATH-1424,13085127,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,Jerome834,Jerome834,06/Jul/17 08:07,06/Jul/17 11:13,07/Apr/19 20:38,,3.6.1,,,,,,,,,,0,easyfix,,,,,,,"The following code gives a wrong result:
RealMatrix m = [[10_000_000.0, -1_000_000.0],[-1_000_000.1, 20_000_000.0]]; // pseudo code
EigenDecomposition ed = new EigenDecomposition(m);
double[] eigenValues = ed.getRealEigenvalues();

Computed values: [1.57E13, 1.57E13].
Expected values: [1.0E7, 2.0E7]

The problem lies in method EigenDecomposition.transformToSchur(RealMatrix).
At line 758, the value matT[i+1][i] is checked against 0.0 within an EPSILON margin.
If the precision of the computation were perfect, matT[i+1][i] == 0.0 means that matT[i][i] is a solution of the characteristic polynomial of m. In the other case there are 2 complex solutions.
But due to imprecisions, this value can be different from 0.0 while m has only real solutions.
The else part assume that the solutions are complex, which is wrong in the provided example.
To correct it, you should resolve the 2 degree polynomial without assuming the solutions are complex (that is: test whether p*p + matT[i+1][i] * matT[i][i+1] is negative for 2 complex solutions, or positive or null for 2 real solutions).
You should also avoid testing values against something within epsilon margin, because this method is almost always wrong in some cases. At least, check with a margin that depends on the amplitude of the value (ex: margin = highest absolute value of the matrix * EPSILON); this is still wrong but problems will occur less often.

The problem occurs when the input matrix has large values because matT has values of magnitude E7. MatT[1, 0] is really low (E-10) and you can not expect a better precision due to the large values on the diagonal.
The test within EPSILON margin fails, which does not occurs when the input matrix has lowest values.
Testing the code with m2 = m / pow(2, 20) will work, because matT[1, 0] is now low enough.",JDK 7.51 64 bits on Windows 7.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-07-06 10:33:34.869,,,false,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 06 11:13:18 UTC 2017,,,,,,0|i3h6hb:,9223372036854775807,,,,,,,,"06/Jul/17 10:33;erans;Thanks for the report.
Could you prepare a patch, with unit test, against the current development version (""master"" branch of the repository)?
","06/Jul/17 11:04;Jerome834;Yes, but not before September (too much workload now).","06/Jul/17 11:13;erans;Fine, since it's unlikely that we'll be in a position to consider a release of ""Commons Math"" before we finalize the initial release of [""Commons Numbers""|http://commons.apache.org/proper/commons-numbers/modules.html].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SingularMatrixException on Non-Square Matrix,MATH-1423,13083518,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,,,cmwarre,cmwarre,29/Jun/17 17:06,29/Jun/17 20:22,07/Apr/19 20:38,,3.5,,,,,,,,,,0,OLSMutlipleRegression,SingularMatrixException,,,,,,"I'm trying to implement an OLSMultipleLinearRegression class in the apache commons math java library and I keep getting a ""SingularMatrixException"". This is confusing to me because my data isn't even square (60 x 2160) which I thought was a requirement for a Singular Matrix.

I've played with the data by pruning rows off and adding them back on, and found differing numbers of rows that will work/fail with this dataset.
Also, I've checked my matrix for columns or rows that are full of zeros as suggested in this post:

Using Apache Library for OLS Regression : Matrix is singular exception

Is there something else with this library that I don't understand? Is there a way I can make this more robust or predict a singular array beforehand?",Oracle JDK 1.8.121,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-06-29 18:09:53.705,,,false,,,,,Patch,,,,,,,,,9223372036854775807,,,Thu Jun 29 20:22:47 UTC 2017,,,,,,0|i3gwlb:,9223372036854775807,,,,,,,,"29/Jun/17 18:09;psteitz;I don't think there is a bug here - more a question for the user list and maybe documentation improvement.  In order for there to be a unique solution to the OLS parameter estimation problem, the columns of the X matrix have to be linearly independent and there must be at least as many rows as there are columns.   if your dimensions above are (row, column), there will be no solution because 60 observations are not sufficient to estimate a model with 2160 independent variables).  It is the XX' matrix that ends up singular in this case, which is what ultimately triggers the SingularMatrixException.

If it is 2160 rows and 60 columns, that will work as long as the columns are linearly independent. If one of your variables is (very close to) a linear combination of some subset of the others, you will end up with a SingularMatrixException.  Check to make sure that all of the columns are distinct and that none is just a multiple of another.

The javadoc should advertise SME and maybe explain this or provide a link to a reference on OLS.

","29/Jun/17 20:22;cmwarre;Hi Phil,

Sorry I actually meant 60 attributes and 2160 observations.  Where is the user group???  Some more documentation on some of that would be nice as well though.  

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How accurate is FastMath javadoc?,MATH-1422,13082199,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,leventov,leventov,23/Jun/17 18:51,24/Jun/17 01:58,07/Apr/19 20:38,,,,,,,,,,,,0,,,,,,,,"Javadoc to FastMath states that it is a ""faster, more accurate alternative to java.lang.Math"" since at least 2012: https://github.com/apache/commons-math/blob/696be68b5dc8872a818a24d8a377158314b54e31/src/main/java/org/apache/commons/math3/util/FastMath.java

How accurate is this assertion in 2017, Java 8?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-06-23 19:52:22.854,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 24 01:58:17 UTC 2017,,,,,,0|i3gogn:,9223372036854775807,,,,,,,,"23/Jun/17 19:52;erans;Generally speaking it has never been true; see those still open issues:
* MATH-740
* MATH-901
* MATH-1113

Those benchmarks were performed with homegrown micro-benchmark codes (see {{PerfTestUtils}} class in the ""test"" part of the code repository).
It is a good idea to create benchmarks based on JMH. Contributions welcome.",24/Jun/17 00:06;leventov;[~erans] What about precision? Does FastMath yield more precise outputs than java.lang.Math?,"24/Jun/17 01:58;erans;It was certainly the other main goal.
See the Javadoc of the {{FastMath}} class which mentioning a best effort to reach a 0.5 ulp precision, and the unit tests that validate the claim.
I've suggested a rename to {{AccurateMath}}, since the only benchmarks we have do not seem to support the ""Fast"" qualifier...
However I indeed do not know for sure whether there is still an advantage to using {{FastMath}}: see also
* MATH-1114
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WilsonConfidenceInterval returning negative values,MATH-1421,13081335,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mathijs,mathijs,21/Jun/17 07:29,22/Jun/17 01:21,07/Apr/19 20:38,22/Jun/17 01:21,3.6.1,,,,,,,4.0,,,0,,,,,,,,"Wilson confidence intervals sometimes return negative values as the lower bound, e.g.
IntervalUtils.getWilsonScoreInterval(19436, 0, 0.95).getLowerBound() returns -1.3549849074815073E-20

This is causing assertion fails in our code, which we have fixed it now by wrapping the confidence score bounds in Math.max(0, Math.min(1, bound)) until the method doesn't return wrong results anymore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-06-22 01:21:33.607,,,false,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 22 01:21:33 UTC 2017,,,,,,0|i3gj5b:,9223372036854775807,,,,,,,,"22/Jun/17 01:21;erans;Thanks for the report.

As of commit 612a04d6b0ff8fa3060d5e943f4f72968ea71700 the reported failure does not happen anymore.
Please have a look, and reopen this issue if you have other case that still produce a negative value.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid usage of exception in PolynomialSplineFunction,MATH-1419,13080065,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,hangpark,hangpark,15/Jun/17 11:55,15/Jun/17 13:28,07/Apr/19 20:38,15/Jun/17 12:24,3.6.1,,,,,,,4.0,,,0,easyfix,,,,,,,"In PolynomialSplineFunction constructor, it tests whether length of knots is smaller than 2 or not. If <2, it throws NumberIsTooSmallException like below:
{code:java}
if (knots.length < 2) {
    throw new NumberIsTooSmallException(LocalizedFormats.NOT_ENOUGH_POINTS_IN_SPLINE_PARTITION,
                                                               2, knots.length, false);
        }
{code}

But definition of above exception has parameters of the form:
{code:java}
/**
 * Construct the exception with a specific context.
 *
 * @param specific Specific context pattern.
 * @param wrong Value that is smaller than the minimum.
 * @param min Minimum.
 * @param boundIsAllowed Whether {@code min} is included in the allowed range.
 */
public NumberIsTooSmallException(Localizable specific,
                                 Number wrong,
                                 Number min,
                                 boolean boundIsAllowed) {
    super(specific, wrong, min);

    this.min = min;
    this.boundIsAllowed = boundIsAllowed;
}
{code}

h3. In my opinion, *2, knots.length, false* should be *knots.length, 2, true*

since 2 is the minimum value and knots.length is the wrong value in this case. Moreover, boolean should be set by true because 2 is also acceptable.",,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-06-15 12:24:41.82,,,false,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 15 13:28:39 UTC 2017,,,,,,0|i3gbbj:,9223372036854775807,,,,,,,,"15/Jun/17 12:24;erans;Changed in commit 1b53f09c3a9dcd64dd281c1955b062fc28999366
Thanks for the report and fix!
","15/Jun/17 12:39;hangpark;[~erans] Oh, I just made a PR right before.. at [https://github.com/apache/commons-math/pull/62], duplicated.

But, you should also change order of string format position in English and French! I made it, so check above PR. Thanks!","15/Jun/17 13:28;erans;Merged in commit 777af155a678286614d261887790352b43fa7c2a
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
weird result in RRQR decomposition.,MATH-1417,13070345,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,lecomtje,lecomtje,09/May/17 08:56,10/May/17 09:54,07/Apr/19 20:38,,3.6.1,,,,,,,,,,0,,,,,,,,"When using RRQRDecomposition on rank deficient matrix, results are wrong.

double[][] Xi = {
            {0, 0, 0, 0, 0, 0, 0, 0, 0},
            {0, 1, 0, 0, 0, 0, 0, 0, 0},
            {0, 0, 1, 0, 0, 0, 0, 0, 0},
            {0, 0, 1, 0, 0, 0, 0, 0, 0},
            {0, 0, 1, 0, 0, 0, 0, 0, 0},
            {0, 0, 0, 1, 0, 0, 0, 0, 0},
            {0, 0, 0, 0, 0, 0, 1, 0, 0},
            {0, 0, 0, 0, 0, 0, 0, 0, 0},};

With this matrix, i obtain: 

rank 6

R:
|1,000 0,000 0,000 0,000 0,000 0,000 0,000 0,000|
|0,000 1,000 1,000 1,000 0,000 0,000 0,000 0,000|
|0,000 0,000 0,000 0,000 0,000 0,000 0,000 0,000|
|0,000 0,000 0,000 0,000 1,000 0,000 0,000 0,000|
|0,000 0,000 0,000 0,000 0,000 0,000 0,000 0,000|
|0,000 0,000 0,000 0,000 0,000 1,000 0,000 0,000|
|0,000 0,000 0,000 0,000 0,000 0,000 0,000 0,000|
|0,000 0,000 0,000 0,000 0,000 0,000 0,000 0,000|
|0,000 0,000 0,000 0,000 0,000 0,000 0,000 0,000|

Q:
|0,000 0,000 1,000 0,000 0,000 0,000 0,000 0,000 0,000|
|1,000 0,000 0,000 0,000 0,000 0,000 0,000 0,000 0,000|
|0,000 1,000 0,000 0,000 0,000 0,000 0,000 0,000 0,000|
|0,000 0,000 0,000 1,000 0,000 0,000 0,000 0,000 0,000|
|0,000 0,000 0,000 0,000 1,000 0,000 0,000 0,000 0,000|
|0,000 0,000 0,000 0,000 0,000 0,000 1,000 0,000 0,000|
|0,000 0,000 0,000 0,000 0,000 1,000 0,000 0,000 0,000|
|0,000 0,000 0,000 0,000 0,000 0,000 0,000 1,000 0,000|
|0,000 0,000 0,000 0,000 0,000 0,000 0,000 0,000 1,000|

Where Scipy (lapack) or ejml gives me:
rank 4
Type = dense real , numRows = 9 , numCols = 8
-1,000   0,000   0,000   0,000   0,000   0,000   0,000   0,000  
 0,000  -1,000   0,000   0,000  -1,000   0,000  -1,000   0,000  
 0,000   0,000  -1,000   0,000   0,000   0,000   0,000   0,000  
 0,000   0,000   0,000  -1,000   0,000   0,000   0,000   0,000  
 0,000   0,000   0,000   0,000   0,000   0,000   0,000   0,000  
 0,000   0,000   0,000   0,000   0,000   0,000   0,000   0,000  
 0,000   0,000   0,000   0,000   0,000   0,000   0,000   0,000  
 0,000   0,000   0,000   0,000   0,000   0,000   0,000   0,000  
 0,000   0,000   0,000   0,000   0,000   0,000   0,000   0,000  

Type = dense real , numRows = 9 , numCols = 9
 0,000   0,000   0,000   0,000   0,000   0,000   1,000   0,000   0,000  
-1,000   0,000   0,000   0,000   0,000   0,000   0,000   0,000   0,000  
 0,000  -1,000   0,000   0,000   0,000   0,000   0,000   0,000   0,000  
 0,000   0,000  -1,000   0,000   0,000   0,000   0,000   0,000   0,000  
 0,000   0,000   0,000   0,000   1,000   0,000   0,000   0,000   0,000  
 0,000   0,000   0,000   0,000   0,000   1,000   0,000   0,000   0,000  
 0,000   0,000   0,000  -1,000   0,000   0,000   0,000   0,000   0,000  
 0,000   0,000   0,000   0,000   0,000   0,000   0,000   1,000   0,000  
 0,000   0,000   0,000   0,000   0,000   0,000   0,000   0,000   1,000  

That are the results i expect.



 ","linux RH6
netbeans 8.2 
java 1.8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-05-09 10:52:01.626,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed May 10 09:54:35 UTC 2017,,,,,,0|i3ep1j:,9223372036854775807,,,,,,,,"09/May/17 10:52;erans;Thanks for the report!

Are you willing to dig into the code in order to locate the bug?","09/May/17 12:00;lecomtje;Hi Gilles,

Why not ? 
Is common math easily compiled once i checked it out  from git ?

Thanks.

Jef

","09/May/17 12:06;erans;bq. Is common math easily compiled once i checked it out from git ?

Asumimg that you have ""maven"" installed, it should just be a matter of running this command:
{noformat}
mvn compile
{noformat}
","09/May/17 14:15;lecomtje;Ok, i got the source.
I'm not a QR specialist. I took a look at the code and I've found what gave me that weird result.

In performHouseholderReflection:(RRQRDecomposition.java:111), just change
for (int j = 0; j < qrt[i].length; j++) {
(Norm is computed against the whole column )
By
for (int j = minor; j < qrt[i].length; j++) {
(Norm is computed under the current pivot )

 In performHouseholderReflection:(QRDecomposition.java:139-143) the norm is calculated again using  the column starting from minor pivot, 
then in my case, I have a column whose values are before the diagonal ( then before the pivot ). This gives a ""full norm"" that is not null but that is null beside pivot.
This column is given for computing a reflector but this calculous failed ( a == 0) because ""restricted norm"" is null.

If norms are computed always the same manner, the pivoting is ok and the result is ok in my test case.

Further testing should be done but maybe it can be a quick fix.

Hope this can help.

I'm also surprised to see a test (a!=0) in QR Decomposition with double values.
","09/May/17 14:41;erans;Have you checked out the git ""master"" branch?
With your example (and no change to the source), I obtain 4 as the rank value.","09/May/17 15:56;lecomtje;Sorry, but i am searching a nullspace and so i work with Xi transposed, not Xi by itself. And with this transposed matrix I should also obtain rank 4.
I didn't test master branch but the code seems the same.
","09/May/17 19:35;erans;Got it. :)
Please check that commit ed1ce82d822ffe185875b7b7d38352f20171c096 fixed the problem.
","10/May/17 09:32;lecomtje;Ok, it's ok for me.

I use the current stable release 3.6.1 of apache math in an industrial product.
How should I proceed to take advantage of this fix ?
","10/May/17 09:54;erans;Side-note: When you add a comment, please disable quoting of previous comments.

bq. How should I proceed to take advantage of this fix ?

It depends on your requirements.
Best is to start a thread on the ""dev"" ML.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add generics to Frequency,MATH-1413,13064299,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,kinow,kinow,kinow,16/Apr/17 22:31,10/May/17 08:09,07/Apr/19 20:38,10/May/17 08:08,,,,,,,,4.0,,,0,generics,,,,,,,"The Frequency class has no generic types, but creates an internal TreeMap, with keys with the type `Comparator<?>`. Then whenever you add or get values from the TreeMap, it will throw CCE if you pass a key value with a different type from the others stored in the map.

{code}
Frequency f = new Frequency();
// Okay
f.addValue(""Ola"");
// Key 12, type Integer, does not match String type. CCE, wrapped in a Math exception
f.addValue(12);
{code}

And even a simpler example without Math classes to help outlining the issue.

{code}
TreeMap<Integer, String> map = new TreeMap<>();
int key = 1;
map.put(key, ""One"");
// Okay
map.get(100);
// TreeMap will try to call 1.1#compareTo against each Key it contains, which can result
// in a CCE here.
map.get(1.1);
{code}

It is not clear how we could compare the key type to the given v value. Checking if the map is not empty, and then checking the type of the first key doesn't sound very elegant (and not sure if it would be a valid solution).

A better solution could be to add generics to the class, breaking the current contract (i.e. not allowing three types, boxing to longs, etc), but on the bright side, it won't use CCE as a normal workflow, nor need suppress warnings annotations any more.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,Patch,,,,,,,,,9223372036854775807,,,Wed May 10 08:09:15 UTC 2017,,,,,,0|i3dobz:,9223372036854775807,,,,,,,,"23/Apr/17 01:24;kinow;Any objections for this? Otherwise I will review the code once more, and prepare to merge in the next days.","10/May/17 08:09;kinow;Fixed. Checked checkstyle, PMD, Findbugs, all looking good (had to adjust some Javadoc during merge). Resolving the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kolmogorov-Smirnov fixTies can set minDelta too small for jiggler to have significant effect,MATH-1405,13047102,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,dfinkel,dfinkel,28/Feb/17 21:38,19/Mar/17 17:53,07/Apr/19 20:38,19/Mar/17 17:53,3.6.1,,,,,,,,,,0,,,,,,,,"For samples that do not exceed LARGE_SAMPLE_PRODUCT with their product and relatively large values, a minDelta can be calculated in fixTies() that is too small to have any effect on the ""tied"" values. This results in a MathInternalError, as the jiggling with the ineffective minDelta fails to fix the ties.

The following arrays exhibit this behavior when run with kolmogorovSmirnovTest(x, y) in 3.6.1

x = [1.3750969645841487, 1.0845460746754014, 1.3693352427126644, 1.329688765445783, 1.3392109491039106, 1.3532766470312723, 1.3187287426697727, 1.386273031970554, 1.3416950149276097, 1.0510872606482404, 1.3532766470312723, 1.3075923871137798, 1.3862730319705543, 1.3814421433922548, 1.0527927570919202, 1.3847314864464313, 1.319362658529506, 1.3579238253227275, 1.2455452272301641, 1.329688765445783, 1.3827781646781876, 1.0755168081687903, 1.2566273460024566, 1.3099622795250825, 1.357440924560318, 1.3519397370266515, 1.0927347979524134, 1.3566357346921618, 1.238800036669969, 1.2931730628634528, 1.048463407884969, 1.3779471642491719, 1.2978533797116658, 1.376230881554943, 1.166901202345226, 1.3690425182006263, 1.166901202345226, 1.2953476417603207, 1.0827945761165951, 1.2942406680885112, 1.224414840377028, 1.3910905417259205, 1.303231085263425, 1.348635183816037, 1.3750969645841487, 1.049648651501274, 1.3119534979602083, 1.0446033225080773, 1.0494686631294756, 1.3862026705844126, 1.2719496963348844, 1.3489938748102903, 1.3780468374004164, 1.3884878389662338, 1.3352682241994538, 1.3348722240568909, 1.3921944407986777, 1.0476833161122294, 1.0845460746754008, 1.344165352323966, 1.298548179079665, 1.1979240079667628, 1.3539078973394736, 1.3187287426697725, 1.082794576116595, 1.3779471642491719, 1.3771347858434184, 1.3921944407986777, 1.193793081523992, 1.362050393265006, 1.076638744462226, 1.3551174562135766, 1.3393693468578751, 1.2470361076952952, 1.3696023478216113, 1.3750969645841487, 1.2964734722088322, 1.2953476417603207, 1.2470361076952952, 1.382661263313539, 1.3862026705844126, 1.3771240109822156, 1.25443884328785, 1.3136690818105938, 1.3853832858443051, 1.3486351838160378, 1.348026557887345, 1.0604869883721861, 1.3352682241994536, 1.3480480718535308, 1.3363233390543028, 1.154658436584056, 1.3921944407986775, 1.1979240079667626, 1.3620503932650059, 1.0881358731694244, 1.369042518200626, 1.3532766470312723, 1.2890012831575908, 1.3735565244300663]

and

y = [1.1262991662205104, 1.3136690818105938, 1.0446033225080773, 1.3551174562135764, 1.3032310852634252, 1.3806258468851462, 1.2270612333345983, 1.2719496963348844, 1.3601566259413194, 1.3756888280688913, 1.3475322202511097, 1.1937930815239919, 1.0510872606482404, 1.3441653523239654, 1.359738761905118, 1.3382152957887032, 1.0766387444622263, 1.1937930815239919, 1.0820779503060238, 1.1448104521200428, 1.3853832858443051, 1.28757746537949, 1.298548179079665, 1.067255392172351, 1.3168701741293156, 1.3910905417259205, 1.2908594990421354, 1.3750969645841487, 1.329688765445783, 1.386649365275275, 1.285486511663053, 1.2566273460024566, 1.323664826995234, 1.3862730319705538, 1.049346328049449]

which produce minDelta = 1.11022302462516E-016",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-02-28 23:39:06.704,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 07 12:26:26 UTC 2017,,,,,,0|i3ar5z:,9223372036854775807,,,,,,,,"28/Feb/17 21:55;dfinkel;A temporary solution I've found is, in fixTies(), pulling the RealDistribution Object creation inside the do-while loop, and doubling the minDelta with every iteration until it's large enough:

{code}
do {
            LOGGER.warn(""using minDelta: {}"", minDelta);
            // Add jitter using a fixed seed (so same arguments always give same results),
            // low-initialization-overhead generator
            final RealDistribution dist = new UniformRealDistribution(new JDKRandomGenerator(100), -minDelta, minDelta);

            jitter(x, dist);
            jitter(y, dist);
            ties = hasTies(x, y);
            ct++;
            minDelta *= 2;
        } while (ties && ct < 1000);
{code}","28/Feb/17 23:39;erans;Thanks a lot for the report.

Could you create patches against the ""master"" branch? I.e.
* set up the example as a Junit test case (and check that it fails with the development version of the library),
* insert your proposed fix (see below)

By ""temporary solution"", do you mean that there could be problem in adopting it?

I'd suggest to instantiate the RNG outside the loop, as follows (see http://commons.apache.org/rng and the distribution class in the development version of Commons Math)
{code}
import org.apache.commons.rng.UniformRandomProvider;
import org.apache.commons.rng.simple.RandomSource;
// ...
final UniformRandomProvider rng = RandomSource.create(RandomSource.TWO_CMRES);
do {
      final RealDistribution.Sampler sampler =
          new UniformRealDistribution(-minDelta, minDelta).createSampler(rng);
      jitter(x, sampler);
      jitter(y, sampler);
      ties = hasTies(x, y);
      ct++;
      minDelta *= 2;
} while (ties && ct < 1000);
{code}

Since {{jitter}} is private and used for this single purpose, it would probably be better, performance-wise, to modify it to use the RNG directly; its signature would thus become:
{code}
private static void jitter(double[] data, UniformRandomProvider rng, double delta) {
    for (int i = 0; i < data.length; i++) {
        final double d = delta * (2 * rng.nextDouble() - 1);
        data[i] += d;
    }
}
{code}
","01/Mar/17 17:42;dfinkel;By ""temporary solution"" I only meant it as a workaround until an optimal solution would be introduced in the next release. I'm working on a patch right now. Should have a PR up soon with your proposed solution.",01/Mar/17 20:29;dfinkel;PR created https://github.com/apache/commons-math/pull/55,"02/Mar/17 14:26;erans;Thanks.
Changes applied to ""master"" in commit 98055455265c04cf7f7a354bdc7aeac428730c5d

However, I believe that this issue cannot be resolved as such.
Indeed, the bound for the ""ct"" variable seems much too large (as it implies that the jitter value could grow up to minDelta * 2^1000).","02/Mar/17 14:43;dfinkel;Fortunately, it is inside of a do-while, so it should exit as soon as the minDelta gets large enough.
What maximum value of ""ct"" do you think would be best? 100? I have personally tested data that requires at least 8 loops before a significant minDelta is reached, so I wouldn't want to make the value too small.

Also, you wrote ""Changes applied to ""master"" in commit 98055455265c04cf7f7a354bdc7aeac428730c5d"" but I'm not sure what that means. Github is telling me ""master"" hasn't been updated in 6 days, I couldn't find the commit you referenced with a search, and the commit I submitted for review is ""18f181ada7826542725fa4a9460307d606695b5d""","02/Mar/17 15:16;erans;bq. Fortunately, it is inside of a do-while, so it should exit as soon as the minDelta gets large enough.

Sure, but it looks wrong for the code to hint that 1000 might be necessary.

bq. I have personally tested data that requires at least 8 loops before a significant minDelta is reached, so I wouldn't want to make the value too small.

Sure.
Could you make up a unit test that requires that many loops?

bq. you wrote ""Changes applied to ""master""

Yes, I forgot to ""push""! Sorry.","03/Mar/17 17:02;dfinkel;After further testing I've found a few more edge cases and determined the maximum possible value for the max iterations to be 2048, as opposed to 1000, since the exponent can be expressed in 11 bits. Here is a test I created that shows this.
{code}
    @Test
    public void testTwoSampleWithManyTiesAndExtremeValues_ManyAttempts() {
        final double[] largeX = {
                Double.MAX_VALUE, Double.MAX_VALUE,
                1e40, 1e40, 2e40, 2e40, 
                1e30, 2e30, 3e30, 4e30,
                5e10, 6e10, 7e10, 8e10};
        
        final double[] smallY = {
                Double.MIN_VALUE, (2 * Double.MIN_VALUE), 
                1e-40, 1e-40, 2e-40, 2e-40,
                1e-30, 2e-30, 3e-30, 4e-30,
                5e-10, 6e-10, 7e-10, 8e-10};
        
        // these values result in an initial calculated minDelta of 4.9E-324 (Double.MIN_VALUE),
        // but after 2046 iterations, the delta increases to 1.9958403095347198E292

        final KolmogorovSmirnovTest test = new KolmogorovSmirnovTest();
        Assert.assertEquals(0.63548496, test.kolmogorovSmirnovTest(largeX, smallY), 1e-6);
    }
{code}

There is also separate bug that can occur if there are two instances of Double.NaN, which are treated as a tie but can never be resolved through ""jiggling"" with a minDelta. I would suggest throwing a descriptive Exception if a NaN is passed into the KS test, instead of waiting for the loop to run through its iterations and throw a MathInternalError.

The last bug I found occurs if there are ties in the data and the minDelta is equal to Double.MIN_VALUE. At line 1154 in fixTies(), we divide the minDelta by 2, which in this case results in a new minDelta of 0... My method of iteratively doubling the minDelta becomes no longer viable. However, this can easily be resolved by adding an extra line right after:
{code}
 minDelta = Math.max(minDelta, Double.MIN_VALUE);
{code}","04/Mar/17 00:38;erans;bq. after 2046 iterations, the delta increases to 1.9958403095347198E292

So it looks to me that the code will then produce nonsense (changing the data so that it has nothing to do with the original).

bq. largeX ... smallY

Is there any chance that these two sequences come from the same distribution?

bq. Assert.assertEquals(0.63548496, test.kolmogorovSmirnovTest(largeX, smallY), 1e-6);

Where does the expected value come from?

bq. The last bug I found occurs if there are ties in the data and the minDelta is equal to Double.MIN_VALUE.

Agreed.
Could you set up a separate unit test for such a case?","04/Mar/17 13:58;erans;Could you please review the following suggested changes to {{hasTies}}, {{fixTies}} and {{jitter}}?

{code}
private static boolean hasTies(double[] x, double[] y) {
   final double[] values = MathArrays.unique(MathArrays.concatenate(x, y));
   if (values.length == x.length + y.length) {
       return false;  // There are no ties 
   }

   return true;
}
{code}

{code}
private static void fixTies(double[] x, double[] y) {
    if (hasTies(x, y)) {
        // Add jitter using a fixed seed (so same arguments always give same results),
        // low-initialization-overhead generator. 
        final UniformRandomProvider rng = RandomSource.create(RandomSource.TWO_CMRES, 7654321);

        // It is theoretically possible that jitter does not break ties, so repeat
        // until all ties are gone.  Bound the loop and throw MIE if bound is exceeded.
        int ct = 0;
        boolean ties = true;
        do {
            jitter(x, rng, 10);
            jitter(y, rng, 10);
            ties = hasTies(x, y);
            ++ct;
        } while (ties && ct < 10);
        if (ties) {
            throw new MathInternalError(); // Should never happen.
        }
    }
}
{code}

{code}
private static void jitter(double[] data,
                           UniformRandomProvider rng,
                           int ulp) {
    for (int i = 0; i < data.length; i++) {
        final int rand = rng.nextInt(ulp * 2) - ulp;
        data[i] += rand * Math.ulp(data[i]);
    }
}
{code}
","06/Mar/17 14:43;dfinkel;I like this solution a lot; I didn't even know a Math.ulp() method existed, but that makes a lot more sense for this use case, and significantly reduces the lines of code.

What are your thoughts on throwing an exception in the case of two NaNs appearing in the data, as no amount of jittering could resolve that tie, and a MathInternalError wouldn't be descriptive enough?

Also, why switch the RandomSource from JDK to TWO_CMRES?","07/Mar/17 12:26;erans;bq. I like this solution a lot

Great. :)

All proposed changes are now in ""master"" (commit b0b23c179ac55334f760aa29fce262c73a909268).

bq. Also, why switch the RandomSource from JDK to TWO_CMRES?

See [Commons RNG|http://commons.apache.org/proper/commons-rng/userguide/rng.html#a4._Performance]: {{RandomSource.JDK}} is only for reference purpose, but any of the other alternatives is better, performance-wise and/or quality-wise.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Test failure which occur with LC_ALL=en_US.UTF8, but not with LC_ALL=de_DE.UTF-8",MATH-1404,13046894,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Bug,,krichter,krichter,28/Feb/17 08:27,12/Jun/18 06:37,07/Apr/19 20:38,11/Jun/18 13:10,,,,,,,,,,,0,,,,,,,," Specifying `LC_ALL=en_US.UTF-8` causes test failures like `RandomUtilsDataGeneratorJDKSecureRandomTest>RandomUtilsDataGeneratorAbstractTest.testNextLongNegativeRange:99->RandomUtilsDataGeneratorAbstractTest.checkNextLongUniform:130 Chisquare test failed p-value = 0.0082266579945659 chisquare statistic = 25.304000000000006.` which don't seem to occur with `LC_ALL=de_DE.UTF-8`, see https://travis-ci.org/krichter722/commons-math/jobs/205896548 for details",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-02-28 12:28:03.114,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 12 06:37:23 UTC 2018,,,,,,0|i3apvz:,9223372036854775807,,,,,,,,"28/Feb/17 12:28;erans;This test can fail with probability 0.01 even if the distribution of the random numbers is uniform.

If you run the test several times, you should get successes and failures that are not correlated to the local settings.
",11/Jun/18 13:10;erans;No feedback: Resolving under the assumption that the previous comment was the appropriate explanation.,"11/Jun/18 13:37;krichter;I can't check because of 

{code}
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /mnt/data/sources/commons-math/src/main/java/org/apache/commons/math4/analysis/solvers/LaguerreSolver.java:[20,42] cannot find symbol
  symbol:   class ComplexUtils
  location: package org.apache.commons.numbers.complex
[ERROR] /mnt/data/sources/commons-math/src/main/java/org/apache/commons/math4/analysis/solvers/LaguerreSolver.java:[150,29] cannot find symbol
  symbol:   variable ComplexUtils
  location: class org.apache.commons.math4.analysis.solvers.LaguerreSolver
[ERROR] /mnt/data/sources/commons-math/src/main/java/org/apache/commons/math4/analysis/solvers/LaguerreSolver.java:[196,39] cannot find symbol
  symbol:   variable ComplexUtils
  location: class org.apache.commons.math4.analysis.solvers.LaguerreSolver
[ERROR] /mnt/data/sources/commons-math/src/main/java/org/apache/commons/math4/analysis/solvers/LaguerreSolver.java:[226,36] cannot find symbol
  symbol:   variable ComplexUtils
  location: class org.apache.commons.math4.analysis.solvers.LaguerreSolver
[INFO] 4 errors
{code}

(probably worth a separate report).",11/Jun/18 21:35;erans;Commit e37de249b1d2a7f9ea6e99332ff05c116006a5bf should fix the compilation error.,12/Jun/18 06:37;krichter;The issue no longer occurs (tested with the compilation error fix).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collinearity test: QR Decomposition rank incorrect (SVD ok),MATH-1403,13045702,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,hmf,hmf,23/Feb/17 18:24,10/Apr/17 14:45,07/Apr/19 20:38,,3.6.1,,,,,,,4.0,,,0,,,,,,,,"Hello,

I am aware that such a question have been asked before but I cannot seem to solve this issue for a very simple example. The closest example I have is:

https://issues.apache.org/jira/browse/MATH-1100

from which I could not get an answer.

I am trying to copy an algorithm from R's Caret package that identifies collinear columns of a matrix [1]. I am assuming a ""long"" matrix and and am using the trivial example from the reference above. However I cannot get this to work because the QR's rank result is incorrect.

I have the following example:

import org.apache.commons.math3.linear.RealMatrix;
import org.apache.commons.math3.linear.RRQRDecomposition;
import org.apache.commons.math3.linear.Array2DRowRealMatrix;
import org.apache.commons.math3.linear.SingularValueDecomposition ;

public class QRIssue {

  public static void main(String[] args) {

    double[][] am = new double[5][];
    double[] c1 = new double[] {1.0, 1.0, 1.0, 1.0, 1.0, 1.0} ;
    double[] c2 = new double[] {1.0, 1.0, 1.0, 0.0, 0.0, 0.0} ;
    double[] c3 = new double[] {0.0, 0.0, 0.0, 1.0, 1.0, 1.0} ;
    double[] c4 = new double[] {1.0, 0.0, 0.0, 1.0, 0.0, 0.0 } ;
    double[] c6 = new double[] {0.0, 0.0, 1.0, 0.0, 0.0, 1.0 } ;

    am[0] = c1 ;
    am[1] = c2 ;
    am[2] = c3 ;
    am[3] = c4 ;
    am[4] = c6 ;

    Double threshold = 1e-1;

    Array2DRowRealMatrix m = new Array2DRowRealMatrix( am, false )  ; // use array, don't copy
    RRQRDecomposition qr = new RRQRDecomposition( m,  threshold) ;
    RealMatrix r = qr.getR() ;
    int numColumns = r.getColumnDimension() ;
    int rank = qr.getRank( threshold ) ;
    System.out.println(""QR rank: "" + rank) ;
    System.out.println(""QR is singular: "" + !qr.getSolver().isNonSingular()) ;
    System.out.println(""QR is singular: "" + (numColumns == rank) ) ;

    SingularValueDecomposition sv2 = new org.apache.commons.math3.linear.SingularValueDecomposition(m);
    System.out.println(""SVD rank: "" + sv2.getRank()) ;
    }
}


For SVD I get a rank of 4 which is correct (columns 0,1,2 are collinear : c0 = c1 + c2). But for QR I get 5. I have tried several thresholds with no success. For several subsets of the columns above (example only 0,1,2 I get the correct answer). What am I doing wrong?

TIA,
Hugo F.


1. https://topepo.github.io/caret/pre-processing.html#lindep

","Linux ubuntu
JDK 8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-02-24 13:41:08.506,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 27 14:23:59 UTC 2017,,,,,,0|i3aixj:,9223372036854775807,,,,,,,,"24/Feb/17 13:41;erans;It looks wrong indeed.
The Javadoc mentions ""When a large fall in norm is seen, the rank is returned"" which seems fairly unhelpful in order to select an appropriate threshold value.

As human resources have become scarce for the Commons Math project, you are most welcome to look at the code in order to find the bug.
I've slightly modified your example (transformed into a unit test):
{code}
    @Test
    public void testMath1403() {
        final double delta = 1e-7; // Test fails when delta <= 1e-8.
        final double[][] m = {
            {1, 1, 1, 1 + delta, 1, 1},
            {1, 1, 1, delta, 0, 0},
            {0, 0, 0, 1, 1, 1},
            {1, 0, 0, 1, 0, 0},
            {0, 0, 1, 0, 0, 1}
        };

        final RRQRDecomposition qr = new RRQRDecomposition(new Array2DRowRealMatrix(m));
        final double dropThreshold = 1e-7; // Test fails when dropThreshold <= 1e-8.
        Assert.assertEquals(4, qr.getRank(dropThreshold));
    }
{code}
It hints at a numerical problem...
","27/Feb/17 11:50;hmf;Hello Gilles,

Thanks for the feedback. Unfortunately I am not knowledgeable enough to tackle this task.

Finally, I confirmed that the original R code uses the BLAS library. Its implementation
is also a rank revealing QR decomposition. What I find interesting is that the rank value
is obtained after the decomposition and no explicit function is called. So these 
don't seem to be implementations of the same algorithm. 

As I said, I don't know much about numerical methods. However, if someone can
point me to a simple description of an algorithm I could try and debug it. 

Thanks","27/Feb/17 14:15;erans;bq. Unfortunately I am not knowledgeable enough to tackle this task.

It could start by finding out a reference algorithm (either in a scientific textbook or paper) or another code that implements the functionality, and figure out where the key differences are).
Unfortunately the Javadoc is out-of-sync since it refers to Jama having this same algo, whereas it [hasn't|http://math.nist.gov/javanumerics/jama/doc/].
","27/Feb/17 14:23;erans;Jama's documentation for says:
{noformat}
public int rank()

    Matrix rank

    Returns:
        effective numerical rank, obtained from SVD.
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect calculation at BigFraction's doubleValue and floatValue,MATH-1402,13041676,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Won't Fix,,sushakov,sushakov,09/Feb/17 11:56,19/Mar/17 17:48,07/Apr/19 20:38,19/Mar/17 17:48,3.6.1,4.0,,,,,,,,,0,,,,,,,,"h1. Problem
* BigFraction#doubleValue produces incorrect result ""Infinity"" when numerator's value exceeds primitive double capacity, but denominator not
* Same relates to floatValue",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-02-09 12:28:04.883,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 19 17:48:54 UTC 2017,,,,,,0|i39uc7:,9223372036854775807,,,,,,,,"09/Feb/17 12:11;sushakov;PR opened at github:
* https://github.com/apache/commons-math/pull/50
* https://github.com/apache/commons-math/pull/51","09/Feb/17 12:28;erans;The ""fraction"" functionality has just been moved to the (new) ""Commons Numbers"" project:
https://github.com/apache/commons-numbers

Please move this issue to the corresponding JIRA project:
https://issues.apache.org/jira/browse/NUMBERS

Thanks for the report!
","19/Mar/17 17:48;erans;Code is deprecated (to be replaced by module {{commons-numbers-fraction}} in new ""Commons Numbers"" project.
Issue has been [duplicated|https://issues.apache.org/jira/browse/NUMBERS-15] and fixed there.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception at IntervalUtils.getClopperPearsonInterval,MATH-1401,13037603,Bug,In Progress,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,kinow,art.g,art.g,25/Jan/17 07:38,09/May/17 12:30,07/Apr/19 20:38,,3.6.1,,,,,,,4.0,,,0,,,,,,,,"IntervalUtils.getClopperPearsonInterval throws an exception when number of successes equals to zero or number of successes = number of trials.

IntervalUtils.getClopperPearsonInterval(1, 0, 0.95) or IntervalUtils.getClopperPearsonInterval(1, 1, 0.95) throws org.apache.commons.math3.exception.NotStrictlyPositiveException despite that its input parameters are valid. 



 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-02-01 15:28:27.089,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue May 09 11:59:45 UTC 2017,,,,,,0|i3965z:,9223372036854775807,,,,,,,,"01/Feb/17 15:28;erans;I seem to recall that it might be fixed in the current ""master"" branch.
Can you try it?  Thanks.
","03/May/17 07:11;kinow;With the latest code from master today (dff1a0953d97d46290750a46d01be1e1519ae698):

{code}
ConfidenceInterval ci = IntervalUtils.getClopperPearsonInterval(1, 0, 0.95);
{code}

Throws:

{noformat}
Exception in thread ""main"" org.apache.commons.math4.exception.MathIllegalArgumentException: lower bound (0) must be strictly less than upper bound (0)
	at org.apache.commons.math4.stat.interval.ConfidenceInterval.checkParameters(ConfidenceInterval.java:103)
	at org.apache.commons.math4.stat.interval.ConfidenceInterval.<init>(ConfidenceInterval.java:57)
	at org.apache.commons.math4.stat.interval.ClopperPearsonInterval.createInterval(ClopperPearsonInterval.java:56)
	at org.apache.commons.math4.stat.interval.IntervalUtils.getClopperPearsonInterval(IntervalUtils.java:104)
{noformat}

And:

{code}
ConfidenceInterval ci = IntervalUtils.getClopperPearsonInterval(1, 1, 0.95);
{code}

Throws:

{noformat}
Exception in thread ""main"" org.apache.commons.math4.exception.NotStrictlyPositiveException: degrees of freedom (0)
	at org.apache.commons.math4.distribution.FDistribution.<init>(FDistribution.java:85)
	at org.apache.commons.math4.distribution.FDistribution.<init>(FDistribution.java:63)
	at org.apache.commons.math4.stat.interval.ClopperPearsonInterval.createInterval(ClopperPearsonInterval.java:49)
	at org.apache.commons.math4.stat.interval.IntervalUtils.getClopperPearsonInterval(IntervalUtils.java:104)
{noformat}","03/May/17 07:40;kinow;And in R, the [PropCIs|https://artax.karlin.mff.cuni.cz/r-help/library/PropCIs/html/exactci.html] library gives:

{code}
library(PropCIs)
exactci(1, 1, 0.95)

data:  

95 percent confidence interval:
 0.025 1.000
{code}

But must admit that I'm abstracting away the fact that there are two functions for calculating clopper pearson interval (exactci and midPci). I have some links from the PropCIs R vignette (as tutorials are called in R), a [Wikipedia page I found|https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Clopper-Pearson_interval], and another [R related web page|http://www.stat.cmu.edu/~mciollar/resources/clopper-pearson.html].

The Java code seems to be correctly checking to avoid division by zero and NaN errors. However, looking at the R implementation, it seems that we could simply add an extra case for when the denominator is zero. Though, instead of simply copying the code from other libraries, I intend to read these links before fixing the code, in order to learn what are clopper pearson intervals. If anyone beats me fixing the code, that'll be all right as well :-)","09/May/17 10:27;kinow;To make it easier, will start naming the two cases we have. Case #1 is the one I had some time to study today, and is when **number of successes is equals zero**. And Case #2 is where **number of successes is equal to the number of trials"".

The exception is raised [here|https://github.com/apache/commons-math/blob/20403f09bfe6f06626cd1253042b848e38f038fd/src/main/java/org/apache/commons/math4/stat/interval/ConfidenceInterval.java#L101]:

{code}
# class: ConfidenceInterval
    /**
     * Verifies that (lower, upper) is a valid non-empty interval and confidence
     * is strictly between 0 and 1.
     *
     * @param lower lower endpoint
     * @param upper upper endpoint
     * @param confidence confidence level
     */
    private void checkParameters(double lower, double upper, double confidence) {
        if (lower >= upper) { <---------------------- HERE, both are 0 <=> 0
            throw new MathIllegalArgumentException(LocalizedFormats.LOWER_BOUND_NOT_BELOW_UPPER_BOUND, lower, upper);
        }
        if (confidence <= 0 || confidence >= 1) {
            throw new MathIllegalArgumentException(LocalizedFormats.OUT_OF_BOUNDS_CONFIDENCE_LEVEL, confidence, 0, 1);
        }
}
{code}

The code is checking if the confidence interval lower and upper bounds are different. The [R code|https://artax.karlin.mff.cuni.cz/r-help/library/PropCIs/html/exactci.html] handles three cases, when the number of successes is zero, when the number of successes and trials is the same, and the case where both previous constraints are not true.

Our code in ClopperPearsonInterval checks if the number of successes is greater than 0 only. Then returns a ConfidenceInterval object.

My current guess is that by understanding why the R code is treating these three cases, we may be able to confirm if it makes sense adding some if/else for these constraints in our Java code too.

","09/May/17 10:34;kinow;Continuing on case #1, [this PDF|http://www.lexjansen.com/phuse/2013/sp/SP05.pdf] mentions that ""The lower bound is set to 0 when x = 0, and the upp
er bound is set to 1 when x = n. "". These are exact variable names in the R library, and this is exactly what is happening.

There is still the matter that the calculation is different when x >0 and x != n.","09/May/17 10:53;kinow;Found a better paper explaining it (A. Boomsma, ""Confidence Intervals for a Binomial Proportion"" link: http://www.ppsw.rug.nl/~boomsma/confbin.pdf).

{quote}
For x = 0, the lower limit r1 = 0, because the upper limit ru satisfies the equality (1 - ru) ^n = alpha / 2, from which it follows that ru = 1 - (alpha / 2) ^ 1/n.

For x = n, the upper limit ru = 1, because the lower limit satisfies r = alpha / 2, which makes r = (alpha / 2) ^ 1/n.
{quote}

Which matches exactly with the R implementation. I will update the code, and run some codes for this Case #1. In case it works, will report back here and focus on Case #2 (which could be automatically fixed by fixing Case#1 I think...).","09/May/17 11:59;kinow;Here are the results with R PropCIs:

{noformat}
data:  

95 percent confidence interval:
 0.000 0.975

> exactci(n=1, x=0, 0.95)



data:  

95 percent confidence interval:
 0.000 0.975

> exactci(n=1, x=1, 0.95)



data:  

95 percent confidence interval:
 0.025 1.000

> 
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Rotation(double[][], double) constructs inverse quaternion due to bug in mat2quat",MATH-1400,13032903,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,rhuitl,rhuitl,08/Jan/17 20:09,03/May/17 09:58,07/Apr/19 20:38,,3.6.1,,,,,,,4.0,,,0,,,,,,,,"Constructing a Rotation object from a rotation matrix and extracting the quaternion gives the *inverse* of the expected quaternion.

E.g. something like this:
{code}
Rotation rot = new Rotation(Array2DRowRealMatrix(matrixData).getData())
Quaternion q = new Quaternion(rot.getQ0(), rot.getQ1(), rot.getQ2(), rot.getQ3());
{code}

results in q being the inverse of what is expected.

I tracked this down to Rotation#mat2quat(final double[][]) which seems to access the matrix elements as if they were stored transposed. E.g. compare with Quat4f#set(Matrix3f):

{code:title=Rotation.java}
quat[1] = inv * (ort[1][2] - ort[2][1]);
quat[2] = inv * (ort[2][0] - ort[0][2]);
quat[3] = inv * (ort[0][1] - ort[1][0]); // <-- m01 - m10
{code}

{code:title=Quat4f.java}
this.x = (m1.m21 - m1.m12) * ww;
this.y = (m1.m02 - m1.m20) * ww;
this.z = (m1.m10 - m1.m01) * ww; // <-- m10 - m01
{code}

I compared the result from Commons Math with JavaFX, JavaX Vecmath and NumPy + http://www.lfd.uci.edu/~gohlke/code/transformations.py.html. All but Commons Math agree on the result.

You can find my test program here: http://pastebin.com/jxwFi9mt
It prints the following output (Python results added manually):

{noformat}
[ 0.7  0.0  0.0 -0.7] (Commons Math)
[ 0.7  0.0  0.0  0.7] (JavaFX)
[ 0.7  0.0  0.0  0.7] (JavaX Vecmath)
[ 0.7  0.0  0.0  0.7] (NumPy + transformations.py)

[-0.2  1.0  0.0  0.0] (Commons Math)
[ 0.2  1.0  0.0  0.0] (JavaFX)
[ 0.2  1.0  0.0  0.0] (JavaX Vecmath)
[ 0.2  1.0  0.0  0.0] (NumPy + transformations.py)

[ 0.2  0.0  1.0  0.0] (Commons Math)
[ 0.2  0.0 -1.0  0.0] (JavaFX)
[ 0.2  0.0 -1.0  0.0] (JavaX Vecmath)
[-0.2  0.0  1.0  0.0] (NumPy + transformations.py)

[-0.2  0.0  0.0  1.0] (Commons Math)
[ 0.2  0.0  0.0  1.0] (JavaFX)
[ 0.2  0.0  0.0  1.0] (JavaX Vecmath)
[ 0.2  0.0  0.0  1.0] (NumPy + transformations.py)
{noformat}

The other constructor using mat2quat() is probably also affected although I did not verify this.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-01-10 00:17:26.093,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed May 03 09:58:16 UTC 2017,,,,,,0|i38eyv:,9223372036854775807,,,,,,,,"10/Jan/17 00:17;erans;Thanks for the thorough report.

Unfortunately, the Commons Math project is in dire need of maintainers.
For a few months, we've been (very slowly) moving some of the functionality over to new, smaller, components (""random number generation"" was completed, ""complex"" is under way).
The ""geometry"" package (from the ""master"" branch in the source code repository) is another very good candidate, as it is almost self-contained (few dependencies on other parts of the library).

Help is most welcome.
Please join the ""dev"" ML of the ""Commons"" project for more information.",12/Apr/17 09:53;erans;This bug report should be copied to [NUMBERS|https://issues.apache.org/jira/browse/NUMBERS] since the the {{Quaternion}} has been moved to the {{commons-numbers-quaternion}} module.,"03/May/17 07:45;kinow;[~erans] should we move the issue, or leave this one here as Won't Fix, and open a new one under NUMBERS for it??? I can do either way tomorrow, just let me know which way you prefer (I normally move issues, but I recall we doing something else some time ago... for an issue with reciprocal method?)","03/May/17 09:58;erans;Issue was copied to NUMBERS, but the discussion there indicates that MATH may actually be the right place for this functionality.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"test failure due to ""CMAESOptimizerTest.testMaximize:155->doTest:510 expected:<1.0> but was:<0.7153950105848449>""",MATH-1399,13032795,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,krichter,krichter,07/Jan/17 13:09,08/Jan/17 00:40,07/Apr/19 20:38,,3.4.1,,,,,,,,,,0,,,,,,,,"`mvn test` randomly fails due to `CMAESOptimizerTest.testMaximize:155->doTest:510 expected:<1.0> but was:<0.7153950105848449>`, see attached log for details

experienced with MATH_3_4_1_RC1-718-g75b98fa",travis-ci.org,,,,,,,,,,,,,,,,,,,,07/Jan/17 13:11;krichter;travis-ci.org.log.tar.xz;https://issues.apache.org/jira/secure/attachment/12846183/travis-ci.org.log.tar.xz,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2017-01-08 00:40:06.666,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 08 00:40:06 UTC 2017,,,,,,0|i38eav:,9223372036854775807,,,,,,,,"08/Jan/17 00:40;erans;Please try version 3.6.1.
Hopefully, this problem is fixed in the latest version of the library.

If not, it is unfortunately unlikely to be anytime soon: the Commons Math project is in dire need of maintainers. 
For a few months, we've been (very slowly) moving some of the functionality over to new, smaller, components (""random number generation"" was completed, ""complex"" is under way).
The ""optim"" package (from the ""master"" branch in the source code repository) is another good candidate.

Help is most welcome.
Please join the ""dev"" ML of the ""Commons"" project for more information.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Complex.ZERO.pow(2.0) is NaN,MATH-1397,13022247,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Won't Do,ericbarnhill,maweki,maweki,21/Nov/16 17:16,01/May/17 23:41,07/Apr/19 20:38,01/May/17 23:41,3.6.1,,,,,,,4.0,,,1,,,,,,,,"```
package complextest;

import org.apache.commons.math3.complex.Complex;

public class T {
	public static void main(String[] args) {
		System.out.println(Complex.ZERO.pow(2.0));
	}
}
```

This is the code and the readout is `(NaN, NaN)`. This surely isn't right. For one, it should actually be zero (https://www.wolframalpha.com/input/?i=(0%2B0i)%5E2) and second of all, the documentation doesn't state that anything could go wrong from a Complex number that has no NaNs and Infs.

The other definition states that it doesn't work when the base is Zero, but it surely should. This strange corner case destroys any naive implementation of stuff wrt the mandelbrot set.

It would be nice to not have to implement this exception myself.","Linux, Java1.7/Java1.8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-01-02 16:35:34.774,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon May 01 23:41:57 UTC 2017,,,,,,0|i36l87:,9223372036854775807,,,,,,,,"02/Jan/17 16:35;RayDeCampo;Added a pull request to implement the case of 0 raised to a real positive number:

https://github.com/apache/commons-math/pull/47
","02/Jan/17 23:31;erans;Hello Mario and Raymond.

Having complex numbers (and related functionality) in their own component was on the roadmap; an effort led by Eric Barnhill (to whom I've just assigned this issue).
If you'd like to help move the code out of Commons Math, please post to the ""dev"" ML.

Thanks for the report.
","27/Apr/17 07:14;wilbur;I am not sure what the status of this issue is (apparently has been unsolved for years), but this is an URGENT PROBLEM and should be fixed soon!

I told my students to use a ""proven"" libary instread of doing their own implementation of 'Complex' and the first thing they run across is this bug. What should I tell them now? Surprisingly, the (NaN, NaN) outcome is intended (even checked in a test case!), although I do not know of any other environment with a similar behaviour.

Why is the pow() based on the log() in the first place? Wouldn't it be simpler to perform exponentiation in polar form, without the 0-singularity?

--Wilhelm","27/Apr/17 12:41;erans;bq. status of this issue

It has been fixed in the new [""Commons Numbers"" project|http://commons.apache.org/proper/commons-numbers] (see NUMBERS-4).
The {{complex}} package will not be part of the next release of Commons Math.

bq. I told my students to use a ""proven"" library

I totally agree.

bq. first thing they run across is this bug.

I'm very sorry.
It shows that even after many years, there is still room for improving supposedly mature code.

bq.  What should I tell them now?

Please tell them that they are most welcome to test and review the code, to provide comments and suggestions, and participate in the development by filing bug reports and fixing bugs.

""Commons Numbers"" is being actively worked on right now.
I hope that we are able to produce the first official release in the coming weeks.

The new (non-official) artefacts for the ""complex"" functionality can downloaded from the [snapshot repository|https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-numbers-complex/1.0-SNAPSHOT/].

bq. I do not know of any other environment with a similar behaviour.

You are quite right.
Eric Barnhill is leading the refactoring, with the explicit goal to make the behaviour conform to the ISO standard.
I'm sure that he'll welcome your help. If not done already, please subscribe to the ""dev"" ML in order to discuss the features which you expect from an implementation of the concept of ""complex numbers"".

bq. Wouldn't it be simpler to perform exponentiation in polar form

I certainly agree.
I had a, perhaps naive, proposal aimed at ensuring that the most effective algorithm would be used for a given computation: see https://issues.apache.org/jira/browse/NUMBERS-10
Please let us know what you think.","27/Apr/17 14:10;wilbur;Hello Gilles, thanks for your enlightening comments. I'd be happy to help if I can (i.e., add anything to the outstanding experts' work). I have seen the recent patch for the pow() method by Raymond and wondered if there was an accessible (and complete) GIT repo for the ""Commons Numbers"" project. I tried to build the subproject but could not find the associated parent project (POM).

--Wilhelm","27/Apr/17 14:31;erans;Did you proceed as indicated [here|http://commons.apache.org/proper/commons-numbers/source-repository.html]?

From the top directory, running
{noformat}
mvn clean package
{noformat}
should create all the artefacts (to be found in each module's ""target"" subdirectory).
","27/Apr/17 16:26;wilbur;Thanks - no, I had looked in the wrong place at [""Source code repository (current)"" | http://git-wip-us.apache.org/repos/asf/commons-rng.git] and did not realize the other menu entry further down.

I was able to clone and build the complete repo (with command-line mvn) successfully. However, importing into Eclipse gave me a couple of errors (apparently related to the maven-antrun-plugin and checkstyle) which I could not resolve, e.g. (for every pom.xml on line 23):

{noformat}
Plugin execution not covered by lifecycle configuration: org.apache.maven.plugins:maven-antrun-plugin:1.7:run (execution: javadoc.resources, phase: generate-sources)
pom.xml	/commons-numbers-complex	line 23
{noformat}

Any hints appreciated...","27/Apr/17 16:41;ericbarnhill;Welcome Wilhelm!

I actually found it so fragile, to integrate Eclipse and Maven with a Git project, that I gave up Eclipse and learned how to code Java using personalized Vim. I realize this is not much of a hint.

I am currently working to conform Complex to the ISO C standard. If you know the standard you know there are a great many behaviors to check, and Java only does some of them inherently, so it has turned into a pretty large project. You should feel free to fix any behaviors you don't like on your own branch and I will integrate them.

As for using the polar representation for certain operations, I certainly have no objection in principle. The ISO standard is defined completely in terms of real and imaginary, including a great many equivalence relationships that I need to test. Thinking about how to re-define every branch cut in polar coordinates is more than I can commit to and would be prone to error. Also, libraries that provide useful trig formulas like Complex.js provide them all in terms of real and imaginary and reconstructing these in polar would also be prone to error.

 cpow() is covered in G.6.4.1 and actually has no branch cuts so you should go ahead; however you can see from G.6.3.2 that we could not do clog() the same way.
","27/Apr/17 20:21;wilbur;Hello Eric, 

thanks for this update. I have used Eclipse for all my projects over many years with good success. However, I agree that Maven support is brittle, so I have been considering to switch to IntelliJ as an alternative for quite a while now. As a first test, I tried it on the 'commons-number' project and it importet without any glitch. Looks good!

Thanks for the invitation to participate in this project, I'll be happy to look into it and see where I can help. Note however that, while I do have a CS background, I am not a mathematician (hobbyist at best) ...

--Wilhelm","01/May/17 23:41;erans;Issue moved to ""Commons Numbers"", and fixed there.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Overflows in ""UniformIntegerDistribution""",MATH-1396,13021524,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,erans,erans,17/Nov/16 17:24,23/Nov/16 12:56,07/Apr/19 20:38,23/Nov/16 12:56,3.6.1,,,,,,,4.0,,,0,,,,,,,,"In {{o.a.c.m.distribution.UniformIntegerDistribution}}, several methods will compute an invalid result when the {{lower}} and {{upper}} bounds are such that
{noformat}
upper - lower
{noformat}
overflows.

Affected methods:
* {{probability}}
* {{cumulativeProbability}}
* {{getNumericalVariance}}

Method
* {{getNumericalMean}}

will return an invalid result when
{noformat}
upper + lower
{noformat}
overflows.

A possible fix is to define instances variables
{noformat}
upperPlusLower = (double) upper + (double) lower;
upperMinusLower = (double) upper - (double) lower;
{noformat}
and use them instead of the respective integer operations in the above methods.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 23 12:56:27 UTC 2016,,,,,,0|i36grj:,9223372036854775807,,,,,,,,23/Nov/16 12:56;erans;commit af1b5872ab8355acea3197522ddf94972b3c8386,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MathParseException when parsing fractions on Android,MATH-1395,13021340,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,brianzable,brianzable,17/Nov/16 03:02,18/Apr/17 15:23,07/Apr/19 20:38,,,,,,,,,4.0,,,0,,,,,,,,"I'm seeing a strange issue when trying to work with the Fraction classes on Android. In my tests, which are standard JUnit tests, the following code performs flawlessly;

{code:java}
String amount = ""1 1/2"";
ProperFractionFormat ff = new ProperFractionFormat();
Fraction f = ff.parse(amount.trim());
{code}

However, at run time in an Android application, the same code throws an Exception:
{noformat}
                   Caused by: org.apache.commons.math3.exception.MathParseException: illegal state: string ""1 1/2"" unparseable (from position 3) as an object of type org.apache.commons.math3.fraction.Fraction
                      at org.apache.commons.math3.fraction.FractionFormat.parse(FractionFormat.java:199)
{noformat}

I am testing on a Nexus 5X emulator with API level 23 for reference.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,2016-11-17 03:02:22.0,,,,,,0|i36fmn:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OLS/GLS Regression sufficient data test is overly aggressive when there is no intercept term,MATH-1392,13020308,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,psteitz,psteitz,12/Nov/16 22:05,12/Apr/17 00:01,07/Apr/19 20:38,,3.6,3.6.1,,,,,,4.0,,,0,,,,,,,,"See https://github.com/Hipparchus-Math/hipparchus/issues/13
for full description and patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,2016-11-12 22:05:08.0,,,,,,0|i3699b:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default number of iterations in integrators,MATH-1388,13008865,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,,,erans,erans,30/Sep/16 12:53,30/Sep/16 12:53,07/Apr/19 20:38,,,,,,,,,4.0,,,0,,,,,,,,"In {{TrapezoidIntegrator}} and {{MidPointIntegrator}}, the default value for the number of iterations is set to 64.

At iteration 64, the number of points would be 2^63, whereas the [largest positive long value is 2^63 - 1|https://docs.oracle.com/javase/7/docs/api/java/lang/Long.html#MAX_VALUE].

Even if the default is set to 63, is such a huge value reasonable?

The default value should be defined as a ""private"" field.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,2016-09-30 12:53:24.0,,,,,,0|i34atr:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Travis CI does not work in master branch,MATH-1387,13006260,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,chtompki,chtompki,chtompki,20/Sep/16 18:26,09/Oct/16 20:39,07/Apr/19 20:38,09/Oct/16 20:38,,,,,,,,4.0,,20/Sep/16 00:00,0,,,,,,,,The travis-ci & coveralls build do not work in the master branch,github,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-09-21 15:07:16.0,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sun Oct 09 20:39:02 UTC 2016,,,,,,0|i33urz:,9223372036854775807,,,,,,,,"20/Sep/16 18:39;chtompki;For Java 1.7 the following is the exception trace:
{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project commons-math4: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test failed: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
[ERROR] Command was /bin/sh -c cd /home/travis/build/chtompki/commons-math && /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.7.5.201505241946/org.jacoco.agent-0.7.5.201505241946-runtime.jar=destfile=/home/travis/build/chtompki/commons-math/target/jacoco.exec -jar /home/travis/build/chtompki/commons-math/target/surefire/surefirebooter9184690604659772326.jar /home/travis/build/chtompki/commons-math/target/surefire/surefire4166513052699975176tmp /home/travis/build/chtompki/commons-math/target/surefire/surefire_06594136187034826821tmp
{code}","20/Sep/16 18:40;chtompki;For Java 1.8 the following is the exception trace:
{code}
Results :
Failed tests: 
  RandomUtilsDataGeneratorJDKSecureRandomTest>RandomUtilsDataGeneratorAbstractTest.testNextLongPositiveRange:104->RandomUtilsDataGeneratorAbstractTest.checkNextLongUniform:127 Chisquare test failed p-value = 0.001668143358028784 chisquare statistic = 28.214000000000006. 
value	expected	observed
1	0.09		110
2	0.09		63
3	0.09		94
4	0.09		86
5	0.09		97
6	0.09		86
7	0.09		87
8	0.09		78
9	0.09		123
10	0.09		81
11	0.09		95
This test can fail randomly due to sampling error with probability 0.01.
  RandomUtilsDataGeneratorJDKSecureRandomTest>RandomUtilsDataGeneratorAbstractTest.testNextUniformUniformNegativeToPositiveBounds:251->RandomUtilsDataGeneratorAbstractTest.checkNextUniformUniform:300 Chisquare test failed p-value = 0.009193629173217643 chisquare statistic = 13.469999999999999. 
value	expected	observed
1	0.2		214
2	0.2		224
3	0.2		208
4	0.2		197
5	0.2		157
This test can fail randomly due to sampling error with probability 0.01.
Tests run: 2693, Failures: 2, Errors: 0, Skipped: 30
{code}","21/Sep/16 15:07;erans;The latter is not related to either Travis or the JVM version.
It is due to the feature of ""SecureRandom"" of producing a different sequence every time the test is run.
Sometimes the sequence is not uniform enough according to the statistics criterion.  Just run the build again.
",24/Sep/16 12:38;chtompki;Him...that makes me feel a little uneasy. I would think we would want the build to be deterministic.,"24/Sep/16 12:53;chtompki;It's beginning to feel like we're running up against either thread count or memory utilization and the JVM is somehow crashing...I'm currently playing around with thread count in my fork of master:

https://travis-ci.org/chtompki/commons-math/builds","24/Sep/16 14:12;erans;bq.  I would think we would want the build to be deterministic.

From [Javadoc for {{SecureRandom}} | https://docs.oracle.com/javase/7/docs/api/java/security/SecureRandom.html]: ""SecureRandom must produce non-deterministic output.""

A way to avoid some of the build failures is to use the custom ""RetryRunner"".
See e.g. {{src/test/java/org/apache/commons/math4/ml/neuralnet/sofm/KohonenTrainingTaskTest.java}}
","25/Sep/16 01:31;chtompki;Hm...curious. That opens an interesting unit testing problem to solve generally: how to write a deterministic unit test for something that necessarily is non-deterministic in nature. That's a really good one that'll probably have me thinking all this week.

Regardless, on the other build failure it seems that it could be failing because of some jacoco/surefire incompatibility, but I haven't pin pointed anything there...planning on continuing to chip away at it.","08/Oct/16 12:06;chtompki;The debug trace looks like
{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project commons-math4: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test failed: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
[ERROR] Command was /bin/sh -c cd /home/travis/build/chtompki/commons-math && /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.7.5.201505241946/org.jacoco.agent-0.7.5.201505241946-runtime.jar=destfile=/home/travis/build/chtompki/commons-math/target/jacoco.exec -jar /home/travis/build/chtompki/commons-math/target/surefire/surefirebooter2170756884795153381.jar /home/travis/build/chtompki/commons-math/target/surefire/surefire6900190985092741351tmp /home/travis/build/chtompki/commons-math/target/surefire/surefire_01703874856244866896tmp
[ERROR] -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project commons-math4: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test failed: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Command was /bin/sh -c cd /home/travis/build/chtompki/commons-math && /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.7.5.201505241946/org.jacoco.agent-0.7.5.201505241946-runtime.jar=destfile=/home/travis/build/chtompki/commons-math/target/jacoco.exec -jar /home/travis/build/chtompki/commons-math/target/surefire/surefirebooter2170756884795153381.jar /home/travis/build/chtompki/commons-math/target/surefire/surefire6900190985092741351tmp /home/travis/build/chtompki/commons-math/target/surefire/surefire_01703874856244866896tmp
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:224)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.PluginExecutionException: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test failed: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Command was /bin/sh -c cd /home/travis/build/chtompki/commons-math && /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.7.5.201505241946/org.jacoco.agent-0.7.5.201505241946-runtime.jar=destfile=/home/travis/build/chtompki/commons-math/target/jacoco.exec -jar /home/travis/build/chtompki/commons-math/target/surefire/surefirebooter2170756884795153381.jar /home/travis/build/chtompki/commons-math/target/surefire/surefire6900190985092741351tmp /home/travis/build/chtompki/commons-math/target/surefire/surefire_01703874856244866896tmp
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:143)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
	... 19 more
Caused by: java.lang.RuntimeException: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?
Command was /bin/sh -c cd /home/travis/build/chtompki/commons-math && /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java -javaagent:/home/travis/.m2/repository/org/jacoco/org.jacoco.agent/0.7.5.201505241946/org.jacoco.agent-0.7.5.201505241946-runtime.jar=destfile=/home/travis/build/chtompki/commons-math/target/jacoco.exec -jar /home/travis/build/chtompki/commons-math/target/surefire/surefirebooter2170756884795153381.jar /home/travis/build/chtompki/commons-math/target/surefire/surefire6900190985092741351tmp /home/travis/build/chtompki/commons-math/target/surefire/surefire_01703874856244866896tmp
	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:515)
	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:380)
	at org.apache.maven.plugin.surefire.booterclient.ForkStarter.run(ForkStarter.java:167)
	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeProvider(AbstractSurefireMojo.java:990)
	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.executeAfterPreconditionsChecked(AbstractSurefireMojo.java:824)
	at org.apache.maven.plugin.surefire.AbstractSurefireMojo.execute(AbstractSurefireMojo.java:722)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
	... 20 more
[ERROR] 
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
{code}",08/Oct/16 12:07;chtompki;This also looks a lot like: https://issues.apache.org/jira/browse/SUREFIRE-1193,"08/Oct/16 14:01;erans;Then perhaps you could ask there how to get around the problem...
Thanks.
",08/Oct/16 15:07;chtompki;That's my plan. I just wanted to write it down. ,09/Oct/16 20:39;chtompki;Resolved with: https://github.com/apache/commons-math/pull/44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.OutOfMemoryError: GC overhead limit exceeded,MATH-1385,13005838,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,cauchyriemann,cauchyriemann,19/Sep/16 07:40,26/Apr/17 23:05,07/Apr/19 20:38,,,,,,,,,4.0,,,0,,,,,,,,"I'm writing code for shorttime fourier transform , but jvm return this error Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.commons.math3.transform.TransformUtils.createComplexArray(TransformUtils.java:138)
	at org.apache.commons.math3.transform.FastFourierTransformer.transform(FastFourierTransformer.java:376)

Seems that your code everytime that compute fft , causes the execution of gc.","Windows 10, jdk 8, Netbeans 8.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-09-19 12:08:45.778,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 26 23:05:50 UTC 2017,,,,,,0|i33s6n:,9223372036854775807,,,,,,,,"19/Sep/16 12:08;erans;Thanks for the report.

Could you try the development version and set up a JUnit test?
JAR is available here: [https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math4/4.0-SNAPSHOT]
","26/Apr/17 13:00;chtompki;The issue at hand here is that we have no upper limits on the size of the array returned by {{TransformUtils.createComplexArray}}, and this causes the potential for memory profile limitations. 

I wonder if we could somehow perform the transform in portions as opposed to doing a complete copy of the input data here: https://github.com/apache/commons-math/blob/MATH_3_6_1/src/main/java/org/apache/commons/math3/transform/FastFourierTransformer.java#L370-L372

or minimally not make a second copy of the data in the {{TransformUtils}}.","26/Apr/17 23:05;erans;Please start a thread on the ""dev"" ML.
I recall that there was a discussion about whether we should just drop the FFT code (IIRC due to its being much less efficient than alternatives).
Eric probably has further to say about this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HypergeometricDistribution logProbability() returns NaN for edge cases,MATH-1384,13002817,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Duplicate,,armanbilge,armanbilge,06/Sep/16 02:26,06/Sep/16 06:48,07/Apr/19 20:38,06/Sep/16 06:48,3.0,4.0,,,,,,4.0,,,0,,,,,,,,"For certain edge cases, HypergeometricDistribution.logProbability() will return NaN.

To compute the hypergeometric log probability, three binomial log probabilities are computed and then combined accordingly. The implementation is essentially the same as in BinomialDistribution.logProbability() and uses the SaddlePointExpansion. However, the Binomial implementation includes an extra check for the edge case of 0 trials which the HyperGeometric lacks.

An example call which fails is:
new HypergeometricDistribution(null, 11, 0, 1).logProbability(0)
which returns NaN instead of 0.0.
Note that
new HypergeometricDistribution(null, 10, 0, 1).logProbability(0)
returns 0 as expected.

Possible fixes:
1. Check for the edge cases and return appropriate values. This would make the code somewhat more complex.
2. Instead of duplicating the implementation use BinomialDistribution.logProbability(). This is much simpler/more readable but will reduce performance as each call to BinomialDistribution.logProbability() makes redundant checks of validity of input parameters etc.

I am happy to submit a PR at the GitHub repo implementing either 1 or 2 with the necessary tests.",,,,,,,,,,,,,MATH-1356,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-09-06 06:24:20.259,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 06 06:47:40 UTC 2016,,,,,,0|i339jr:,9223372036854775807,,,,,,,,"06/Sep/16 06:24;erans;This seems to be the same issue as MATH-1356.
",06/Sep/16 06:31;armanbilge;True; sorry for the duplicate. Where is the commit with the fix?,"06/Sep/16 06:47;armanbilge;Found it; thanks.
0880a21c56cec1a2442b5123c3845bfc99e83a7f",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BinomialTest P-value > 1,MATH-1381,12989690,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,kinow,wangqiang8511,wangqiang8511,15/Jul/16 06:28,09/May/17 03:08,07/Apr/19 20:38,09/May/17 03:08,,,,,,,,4.0,,,1,,,,,,,,"When I use the Binomial Test, I got p-value > 1 for two sided check.

Example:
(new BinomialTest()).binomialTest(200, 200, 0.9950429, AlternativeHypothesis.TWO_SIDED) == 1.3701357550780435

In my case, if the expected p-value is 1 (calculated by package in other language, scipy in this case), the p-value returned could be > 1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-08-31 03:38:59.47,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon May 08 12:26:28 UTC 2017,,,,,,0|i310lb:,9223372036854775807,,,,,,,,31/Aug/16 03:38;kexinxie;I have a fix in https://github.com/apache/commons-math/pull/43,"31/Aug/16 08:10;erans;Having just looked at your proposed, I can't be sure that it fixes a potential bug in the computation.
The wrong value being so much larger than the expected 1, it seems quite possible that wrong results
could also be lower than 1, which your fix wouldn't fix.

At first sight, it would be useful to test a whole range of values against the results provided by another implementation.","31/Aug/16 19:33;kexinxie;Hi [~erans], thanks for looking at the PR. I agree with you that this does seems like it's a dirty fix and mask a potential bug in the computation.

However, the main problem here is that there is one corner case that the current algorithm did not consider. Which is that if the probability is large enough and the success is the same as the number of trials and both numbers are small enough, it will cause the {{criticalValueLow}} to rise too quickly and be the same as {{criticalValueHigh}}. The if condition in L138 is suppose to check the symmetry case when {{pLow == pHigh}}, but is not for the case when {{criticalValueLow == criticalValueHigh}}. At that point the probability will always jump to above 1, but it should really be 1 because {{criticalLow}} is the same as {{criticalHigh}} already (maybe I should return 1 there?).

It may seem like a dirty fix, but I have checked against results in R, and Python's scipy equivalent, and they produce the same value. I implemented this way because it actually works in handling this boundary condition, and it's the least change to the original implementation. Note that Python's scipy also uses a similar approach to deal with estimated value rising above 1 https://github.com/scipy/scipy/blob/v0.14.0/scipy/stats/morestats.py#L1661

I've also updated the PR with more exhaustive test cases (with results cross checked with R and scipy), please have a look again. Also I think the current implementation is correct as explained above, but I'm happy to change the estimation algorithm if that's required.","07/May/17 08:47;kinow;Agree with [~erans] on spending some more time reviewing the current implementation.

Today I spent some time reviewing distributions, binomial, and tests. Then looked at our BinomialTest, but still couldn't tell what was wrong. Then looked at other Java implementations, until stumbled across [hipparchus|https://github.com/Hipparchus-Math/hipparchus]. Their BinomialTest implementation has an extra if, to check when both extreme values are equals.

https://github.com/Hipparchus-Math/hipparchus/blob/ff26617bd14472fa84d70e8efe66c1177f952145/hipparchus-stat/src/main/java/org/hipparchus/stat/inference/BinomialTest.java#L131

The comment next to it says

{quote}One side can't move{quote}

And from our Javadocs:

{quote}The lower value is added to the p-Value (if both values are equal, both are added). Then we continue with the next extreme value, until we added the value for the actual observed sample.{quote}

Nothing obvious for me, but with that check, we get precisely 1.0 as p-value for the example reported in this issue.

Added the pull request here: https://github.com/apache/commons-math/pull/59

But probably worth combining with the tests from https://github.com/apache/commons-math/pull/43.

Hope that helps
Bruno","07/May/17 11:26;erans;Hi Bruno.

Thanks for your help.
But it's so sad that you had to spend time looking for clues about an [issue that was solved 9 months ago in the CM fork|https://github.com/Hipparchus-Math/hipparchus/issues/9]. :(","07/May/17 13:12;kinow;Ooohh, makes sense, should have thought about looking at the issues. Nice catch [~erans]. [~kexinxie], let's merge our pull requests? I think mine has the solution applied to that fork project. Yours has more test cases.

Let me know if you prefer to update your pull request or mine in the next days. Feel free to copy the code changed and simply apply to yours if you'd like ;-)

Then we merge it and fix this issue.

Cheers
Bruno","07/May/17 16:49;kexinxie;Thanks, [~kinow]! I've updated https://github.com/apache/commons-math/pull/43 to contain your changes.",08/May/17 08:18;kinow;Pull request looks good. Planning to merge the PR tomorrow. WDYT [~erans]?,"08/May/17 12:26;erans;Please do! Thanks to both for your work on this.

Refactoring/modularizing the {{o.a.c.m.stat}} package would be a worthy goal IMHO.
A lot of work... ;)

Unless I'm mistaken, the bulk of the original CM code was comprised of the statistical utilities. It should have been named ""Commons Stat"" (to avoid open-ended extensions).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoessInterpolator can calculate median residual for robustness iterations incorrectly,MATH-1380,12983847,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,,,rwaj,rwaj,27/Jun/16 16:53,18/Apr/17 15:32,07/Apr/19 20:38,,3.6.1,,,,,,,4.0,,,0,,,,,,,,"When LoessInterpolator.smooth determines the median residual to use in weighting points when performing robustness iterations its calculation is only strictly correct for odd numbers of points. For even numbers it should take the mean of the central two residuals. While this may not generally make a large difference it hinders comparison with other implementations (such as R loess) for testing.

Patch and additional tests available.",,,,,,,,,,,,,,,,,,,,,30/Jun/16 13:22;rwaj;MATH-1380-loess-residual-median.patch;https://issues.apache.org/jira/secure/attachment/12815456/MATH-1380-loess-residual-median.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-06-28 10:43:57.032,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 28 12:45:06 UTC 2016,,,,,,0|i3073j:,9223372036854775807,,,,,,,,"28/Jun/16 10:43;erans;Thanks for the report.

bq. Patch available with additional tests.

Please attach them to this page.
","28/Jun/16 11:18;erans;Could you make this patch against the development version?
It is in branch ""develop"" of the git repository.
Thanks.
","28/Jun/16 12:45;erans;Same comment as for MATH-1379.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoessInterpolator can update bandwidth interval incorrectly,MATH-1379,12983827,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,rwaj,rwaj,27/Jun/16 15:43,11/Apr/17 23:59,07/Apr/19 20:38,,3.6.1,,,,,,,4.0,,,0,,,,,,,,"LoessInterpolator.updateBandwidthInterval assumes that the bandwidth interval cannot step by more than one index value for each call. This may not be true if the xvals are unevenly spaced (or are so after points with zero weight are omitted).

Patch available with additional tests.",,,,,,,,,,,,,,,,,,,,,30/Jun/16 13:22;rwaj;MATH-1379-loess-interval-update.patch;https://issues.apache.org/jira/secure/attachment/12815455/MATH-1379-loess-interval-update.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-06-28 10:43:35.936,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 28 14:57:09 UTC 2016,,,,,,0|i306z3:,9223372036854775807,,,,,,,,"28/Jun/16 10:43;erans;Thanks for the report.

bq. Patch available with additional tests.

Please attach them to this page.
","28/Jun/16 11:18;erans;Could you make this patch against the development version?
It is in branch ""develop"" of the git repository.
Thanks.
","28/Jun/16 12:44;erans;Thanks for the update.
Sorry I hadn't looked further, but I find it confusing that the patch refers to MATH-296 (in comments and method names), rather than this report.
It's fine to use the same test case but the issue being different, a code reviewer should be directed to here.
",28/Jun/16 14:33;rwaj;Is there a convention for or against using issue ids in test method names?,"28/Jun/16 14:57;erans;It is fine to use an issue id, but the id should be the proper one: this report is MATH-1379 so a related test method would be {{testMath1379}}, not {{testMath296}}.

Note however that test methods are better named according to what they are supposed to check, without necessarily referring to a JIRA report.
The latter is used as a last resort.

For example, this gives full information (what is checked, and that the unit resulted from a JIRA report):
{code}
    // Cf. MATH-1379
    @Test
    public void testUpdateBandwidthIntervalWithUnevenXSpacing() {
      // ...
    }
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexOptimizer.doOptimize(): Wrong Iteration Number (0) Passed to Convergence Checker,MATH-1376,12977856,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,thomasWeise,thomasWeise,11/Jun/16 12:38,11/Jun/16 19:31,07/Apr/19 20:38,11/Jun/16 19:31,3.6.1,,,,,,,4.0,,,0,Convergence,Iterations,SimplexOptimizer,,,,,"The convergence checker used in method doOptimize() of SimplexOptimizer always receives 0 as iteration counter. This can very easily be fixed. Check this out:

Original (with added comments):

{code}
int iteration = 0; // XXXXXXXXX set to zero and never update
        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();
        while (true) {
            if (getIterations() > 0) {
                boolean converged = true;
                for (int i = 0; i < simplex.getSize(); i++) {
                    PointValuePair prev = previous[i];
                    converged = converged && // XXXXXXXXX ouch below
                        checker.converged(iteration, prev, simplex.getPoint(i));
                }
                if (converged) {
                    // We have found an optimum.
                    return simplex.getPoint(0);
                }
            }
{code}

should be (with added comments)

{code}
int iteration = 0;
        final ConvergenceChecker<PointValuePair> checker = getConvergenceChecker();
        while (true) {
            iteration = getIterations(); // XXXXXXXX CHANGE 1
            if (iteration > 0) {  // XXXXXXXX CHANGE 2
                boolean converged = true;
                for (int i = 0; i < simplex.getSize(); i++) {
                    PointValuePair prev = previous[i];
                    converged = converged &&
                        checker.converged(iteration, prev, simplex.getPoint(i));
                }
                if (converged) {
                    // We have found an optimum.
                    return simplex.getPoint(0);
                }
            }
{code}",,1200,1200,,0%,1200,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-06-11 19:31:43.399,,,false,,,,,Patch,,,,,,,,,9223372036854775807,,,Sat Jun 11 19:31:43 UTC 2016,,,,,,0|i2zbwn:,9223372036854775807,,,,,,,,"11/Jun/16 19:31;erans;Fix applied in branch ""develop"".
Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BOBYQAOptimizer Seems to Sometimes Enter Endless Loop,MATH-1375,12977081,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,,,thomasWeise,thomasWeise,09/Jun/16 02:59,18/Apr/17 15:47,07/Apr/19 20:38,,3.6.1,,,,,,,4.0,,,0,,,,,,,,"I am using BOBYQAOptimizer to solve some numerical problems related to nonlinear function fitting. BOBYQAOptimizer is provided with close-to-optimal solutions which it is supposed to refine. In some cases, BOBYQAOptimizer seems to enter an endless loop, or at least an extremely long loop. The problem is almost impossible to reproduce as it occurs maybe once every 1000 runs.

From what I can see with the debugger, the source of the problem is probably method trsbox which is called by bobyqb. In trsbox, some values of a vector (sorry, forgot which one) grow extremely large (>=1e250). Either way, I noticed that both mentioned methods feature a for(;; ) loop.

Now that algorithm looks quite mathematical to me and seemingly has been translated from FORTRAN or something. I think fixing and finding mathematical issues might be complicated (see also the caveats reported in the release notes) and overall, the algorithm is working.

How about you also count the iterations of the for(;; ) loops in bobyqb and trsbox and throw an exception if they exceed some limit? In the easiest case, instead of for(;; ) you can do something like

{code}
for(int maxRemainingSteps=100; (--maxRemainingSteps)>=0;) {
...
}
   throw new MaxCountExceededException(100);
// or TooManyEvaluationsException(100);
// or MathIllegalStateException(LocalizedFormats.SIMPLE_MESSAGE, ""Huh?"");
{code}

Since the original for loops are always left via ""return"", that would already do the trick. Or you could use an Incrementor object for this purpose. Either way, I think with the very simple fix above, you would prevent endless loops, add only a tiny bit of very easy-to-understand code, and would not break the algorithm contract, since such exceptions could be thrown sometimes even without the fix.

In summary: BOBYQAOptimizer needs some work. Fixing the issue I observed properly (i.e., by fixing the special cases causing it) is probably very complex and is probably not feasible. Preventing it, however, seems to be rather easy, as I have shown above.","Java 8 JDK, OpenJDK, Ubuntu",14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-06-09 10:24:33.339,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 16 21:00:27 UTC 2016,,,,,,0|i2z74n:,9223372036854775807,,,,,,,,"09/Jun/16 10:24;erans;I've linked related issues.

The first one seems to be the same problem which you report; perhaps the second one is the cause of it.  Could you please look at it, and confirm whether it can have an impact?

If so, you are most welcome to provide a patch.

Side questions:
* Did you try other optimizers?
* Did you try another implementation of BOBYQA?","11/Jun/16 11:37;thomasWeise;Hi. Yes, MATH-1282 is definitely related, probably the same problem.

I am using BOBYQA in combination with other optimizers. I found that in my particular problem, Levenberg-Marquardt + Nelder/Mead + BOBYQA in combination often give the best performance/runtime results.

I did not test other implementations of BOBYQA. I see where you are getting at: This problem seems to only occur rarely, it might actually already be present in the original algorithm.

From this perspective, my suggested fix may be the best option for the time being: Limiting the loops to a reasonable, generous maximum number of iterations. The main issue is this: The problem does not occur often, the other issue confirms that. If the problem occurs, this is an endless loop. So for a productive system, this is a real show stopper. Since the problem does not occur often, it would even be acceptable if BOBYQA would run quite a bit longer, as long as it does not run forever. Thus the idea with the generous upper limit. trsbox could simply return null is the limit is exceeded, which could be checked in the calling method, which could then simply return immediately as well. One could even allow the user to specify the limit number.

The resulting changes in code would be minimal and they would not violate the contract of the methods. BOBYQA cannot guarantee to find the global optimum anyway. If in some very rare cases it would be terminated ""early"" after quite some time while it was actually not in an endless loop, it could return the best-so-far solution, which would then probably not be very different from what it would return anyway.","11/Jun/16 14:55;garydgregory;The loop count would need to be configurable IMO.

An alternative would be for the loops to check once in a while if the current thread has been interrupted.

Having an SME look over the sctual algorithm would be best of course.","13/Jun/16 02:06;thomasWeise;I agree, it should be configurable, with a reasonably-large default: One could run several tests on existing problems (Maybe there already are some unit tests or something?), check the largest number of iterations ever encountered in the problematic methods, then multiply with 1000. This would be a good default maximum loop count. Users would be extremely unlikely to experience any difference in performance and results. (And even if they would, the changes should be extremely small. And even if they were not, this would still be within the contract of the methods, as result quality and discovery of optima is not guaranteed anywhere.)

Regarding the second option, checking for interruption: I think that one would be less desirable. So far, two issues have reported/confirmed that bug. Thus, if one would build a productive system, currently, BOBYQA would not be an option, as it might hang - and nobody knows why. So the issue should be fixed. If we would fix it with checking for interruption, this would mean that whenever we use BOBYQA, we need to start another thread, wait for, say, 10 minutes, and then interrupt. This would be a completely different scenario from what is done so far and also completely different from how the other algorithms behave. The required changes in code that uses or would use BOBYQA would be much more severe than with the maximum loop counter option.

From this perspective, it would - in my opinion - even be more feasible to provide a ""maximum runtime"" in ms and have methods check System.currentTimeMillis() from time to time. Such an option may actually even be better than having a maximum iteration limit, as it currently exists. Reason: Most algorithms support an iteration limit, but the runtime per iteration may differ extremely from algorithm to algorithm. The users currently cannot really know how much runtime an algorithm will take if they say ""at most 1000 iterations"", for instance. However, I think what most users actually want is to limit the runtime - so why not do it in a natural way, by letting them specify a time limit in ms? Hm ... that would actually be a feature request... ","16/Nov/16 16:11;Burton;Have you tried reducing the stopping trust region radius?  I have encountered optimization problems where BOBYQA consistently hit the evaluation limit with a stopping trust region radius of 1e-8, but finished in a reasonable time with 1e-6.","16/Nov/16 21:00;thomasWeise;If I remember correctly, the two loops do not check any evaluation limit. This bug is _not_ about the optimization failing with bad results or throwing an exception. It is about the algorithm going into an endless loop. A loop from which it will never return. And such a loop must be impossible in any productive environment.

In a productive environment, you often cannot control the exact features of the optimization problem you want to solve. It could be a problem which normally can be solved well with certain setups but the same problem may also have instances with a configuration where that is not possible.

Thus my suggestion to either limit the iteration numbers of these loops or to throw an exception if they iterate too long. Fixing this issue will probably just cost 30 minutes of programming. And the suggested changes will have no impact in scenarios where the algorithm works well (or at least does not loop forever). Only in the border cases where it loops forever, they will kick in.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KMeansPlusPlusClusterer unable to converge having repeatable points in input dataset,MATH-1374,12975073,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,C0rWin,C0rWin,C0rWin,02/Jun/16 07:13,12/Apr/17 00:02,07/Apr/19 20:38,,,,,,,,,4.0,,,0,,,,,,,,"If the input list size of {{Clusterable}} is greater than parameter {{k}} while has less unique points than {{k}}, the algorithm will fail to converge, tested w/ different EmptyClusterStrategy options, here is the example of default one: 

{code}
   @Test
    public void testNumberOfRequestedClustersSameAsInputSize() {

        final RandomVectorGenerator rng = new UncorrelatedRandomVectorGenerator(10,
                new GaussianRandomGenerator(RandomSource.create(RandomSource.MT)));

        List<DoublePoint> points = new ArrayList<>();

        for (int i = 0; i < 10; i++) {
            final DoublePoint point = new DoublePoint(rng.nextVector());
            for (int j = 0; j < 3; j++) {
                points.add(point);
            }
        }

        final KMeansPlusPlusClusterer<DoublePoint> clusterer = new KMeansPlusPlusClusterer<>(12);
        clusterer.cluster(points);
    }
{code}",,,,,,,,,,,,,,,,,,,,,02/Jun/16 07:44;C0rWin;MATH-1374.patch;https://issues.apache.org/jira/secure/attachment/12807644/MATH-1374.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 02 07:45:46 UTC 2016,,,,,,0|i2yvhj:,9223372036854775807,,,,,,,,"02/Jun/16 07:19;C0rWin;Reporting this bug here, since I'm not 100% sure what is the desirable or correct solution to fix it, for example {{matlab}} and {{octave}} implementation of {{kmeans}} dealing with such situation, by randomly selecting repeated points into new clusters. Also I'm not sure whenever this is a real life scenario one should take care off. I've found it accidentally running some tests on my data.",02/Jun/16 07:44;C0rWin;Proposed fix which deals w/ the problem.,02/Jun/16 07:45;C0rWin;https://github.com/apache/commons-math/pull/37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In LogNormalDistribution.java, it appears shape & scale are reversed/mis-labelled.",MATH-1373,12974992,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,,,kgierach,kgierach,02/Jun/16 00:27,21/Jan/18 15:50,07/Apr/19 20:38,,3.6.1,,,,,,,4.0,,,0,,,,,,,,"When I compute the logshape and log scale based on the formulas on wikipedia's lognormal distribution page that use empirical mean and variance, I found that the getNumericalMean() method was not returning the empirical mean.

However, upon just trying to reverse the shape and scale parameters in the constructor proved to fix the problem, and the object then returns the correct empirical mean.

",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,18/Jun/16 00:02;kgierach;MATH-1373.patch;https://issues.apache.org/jira/secure/attachment/12811492/MATH-1373.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-06-08 23:06:55.766,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 21 15:50:33 UTC 2018,,,,,,0|i2yuzj:,9223372036854775807,,,,,,,,"08/Jun/16 23:06;erans;Since there are many tests for this class, I wonder what the problem is.
Is it a documentation bug?

Would you provide a patch that would fix the issue?
","17/Jun/16 17:51;kgierach;The problem is that in the method shape & scale are simply reversed:
@Override
    public double getNumericalMean() {
        double s = shape;
        return FastMath.exp(scale + (s * s / 2));
    }

It should be:    FastMath.exp( shape + (scale * scale / 2 ) );
In any case here is a unit test code snippet that illustrates the problem:

        var defaultScale = 1.0; // aka variance
        var mean = 12.2;
        var meanSquared = mean * mean
        // compute sigma (log scale) parameter of the lognormal distribution.
        // according to formulas on Wikipedia
        var logScale = 
          Math.sqrt( 
                      Math.log( 1.0 + (defaultScale / meanSquared) )
                    )
        
        var logShape = Math.log( mean / 
                                 Math.sqrt( 1.0 + ( defaultScale / meanSquared ) ) 
                               )

        println( ""verifyLogNormalParms(): initializing with: scale/shape="" + logScale + "", "" + logShape )
        
        // parameter order according to api docs: scale      shape/location
        // here, parameters are reversed, and produce the correct result
        var dist = new LogNormalDistribution( logShape, logScale );
        
        var numMean = dist.getNumericalMean()
        
        if( Math.abs( numMean - mean ) > 0.01 ) {
          println( ""verifyLogNormalParms(): mean is NOT OK: "" + numMean  )
          assertTrue( false )
        }
        else {
          println( ""verifyLogNormalParms(): mean is OK: "" + numMean  )
        }","17/Jun/16 20:40;kgierach;I will come up with a patch based on github:  http://git-wip-us.apache.org/repos/asf/commons-math.git
Also, it appears the parameters are reversed in getNumericalVariance() as well.

Sound OK?

Thanks,
Karl",18/Jun/16 00:02;kgierach;this patch includes a revision to unit test which calcuates all expected shape and scale parameters based on the mean and variance of the unscaled data.,"19/Jun/16 04:22;brentworden;I am not completely convinced this change is correct.

Referencing on of the citations, http://mathworld.wolfram.com/LogNormalDistribution.html, the density function is parameterized the same way LogNormalDistribution is parameterized with
* MathWorld's M being equivalent to Commons Math's scale
* MathWorld's S being equivalent to Commons Math's shape

The distribution mean according to MathWorld is Exp(M + S^2 / 2) which corresponds to Exp(scale + shape^2 / 2) and is how it is coded in LogNormalDistribution.

Likewise, MathWorld states the distribution variance Exp(S^2 + 2 M) * (Exp(S^2 - 1) which is Exp(shape^2 + 2 scale) * (Exp(shape^2 - 1).  Again, this matches the implementation.

Furthermore, generating a large sample from the distribution results in sample means and variances that are pretty close to the population values returned from the getNumericalMean and getNumericalVariance methods.  Here is the code I am using to make that claim:

{code}
    @Test
    public void testMeanAndVariance() {
        LogNormalDistribution dist = new LogNormalDistribution(5.375, 1.125);
        double[] x = new double[100000];
        for (int i = 0; i < x.length; ++i) {
            x[i] = dist.inverseCumulativeProbability(Math.random());
        }
        double actualMean = new Mean().evaluate(x);
        double actualVariance = new Variance().evaluate(x);

        double expectedMean = dist.getNumericalMean();
        double expectedVariance = dist.getNumericalVariance();

        System.out.println(String.format(""Mean: %f vs %f (actual vs expected)"", actualMean, expectedMean));
        System.out.println(String.format(""Variance: %f vs %f (actual vs expected)"", actualVariance, expectedVariance));
    }
{code}
","19/Jun/16 17:36;erans;I think that it's a documentation issue.

{quote}
* MathWorld's M being equivalent to Commons Math's scale
* MathWorld's S being equivalent to Commons Math's shape
{quote}

In [MathWorld|http://mathworld.wolfram.com/LogNormalDistribution.html], {{M}} and {{S}} above are respectively named {{mu}} and {{sigma}}.
But CM uses the names {{scale}} and {{shape}} (in that order), whereas [Wikipedia|https://en.wikipedia.org/wiki/Log-normal_distribution#Location_and_scale] refers to them as {{location}} and {{scale}} (in that order).

IIUC, CM uses the [NIST convention |http://www.itl.nist.gov/div898/handbook/eda/section3/eda3669.htm] (where {{sigma}} is referred to as {{shape}}) but the Javadoc links to sites that use other conventions.

The API  would be less confusing if we'd use {{meanLog}} (a.k.a. {{mu}} or {{scale}}) and {{standardDeviationLog}} (a.k.a. {{sigma}} or {{shape}}).

Do you agree?
","21/Jan/18 15:50;erans;Discussion should be moved to the issue tracker of the new ""Commons Statistics"" component (see STATISTICS-2).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
equalsIncludingNaN not producing the expected result,MATH-1368,12973470,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Done,,enietzel,enietzel,27/May/16 17:48,01/Jun/16 14:55,07/Apr/19 20:38,01/Jun/16 14:55,2.2,,,,,,,,,,0,,,,,,,,"I wouldn't expect this result given these 2 values?

x=-1.0, y=4.0
x to long bits = -4616189618054758400
y to long bits = 4616189618054758400
x equals y = false
MathUtils.equalsIncludingNaN(x, y) = true
MathUtils.equals(x, y) = false

It appears the lexicographic ordering logic is producing this, other numbers are fine. Notice the value produced for -1.0 and 4.0 by Double.doubleToLongBits().",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-05-28 02:21:50.639,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 01 14:55:28 UTC 2016,,,,,,0|i2ylyn:,9223372036854775807,,,,,,,,"28/May/16 02:21;erans;Version 2.2 is very old.
The methods you refer to do not exist in more recent releases; the equivalent functionality is implemented in class {{Precision}}.","31/May/16 22:57;enietzel;Just to circle back, I tested this with commons-math3 3.6.1 and it performs appropriately.
{code}
x=-1.0, y=4.0
Double.doubleToLongBits(x) = -4616189618054758400
Double.doubleToLongBits(y) = 4616189618054758400
x.equals(y) = false
Precision.equalsIncludingNaN(x, y) = false
Precision.equals(x, y) = false
{code}","01/Jun/16 14:55;erans;Thanks for the feedback.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DBSCAN Implementation does not count the seed point itself as part of its neighbors count,MATH-1367,12971742,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,amolsingh,amolsingh,21/May/16 05:18,16/Aug/18 04:03,07/Apr/19 20:38,,3.6.1,,,,,,,4.0,,,1,,,,,,,,"The DSCAN paper describes the eps-neighborhood of a point as 

https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf (Page 2)
Definition 1: (Eps-neighborhood of a point) The Eps-neighborhood of a point p, denoted by NEps(p), is defined by NEps(p) = {q ∈ D | dist(p,q)< Eps} 

in other words for all q points that are a member of database D whose distance from p is less that Eps should be classified as a neighbor. This should include the point itself. 

The implementation however has a reference check to the point itself and does not add it to its neighbors list.

private List<T> getNeighbors(final T point, final Collection<T> points) {
        final List<T> neighbors = new ArrayList<T>();
        for (final T neighbor : points) {
            if (point != neighbor && distance(neighbor, point) <= eps) {
                neighbors.add(neighbor);
            }
        }
        return neighbors;
    } 

""point != neighbor ""  check should be removed here. Keeping this check effectively is raising the minPts count by 1. Other third party QuadTree backed DBSCAN implementations consider the center point in its neighbor count E.g. bmw-carit library. 

If this is infact by design, the check should use value equality instead of reference equality. T extends Clusterable<T> , the client should be able to define this behavior. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-05-21 08:27:01.928,,,false,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 16 04:03:27 UTC 2018,,,,,,0|i2ybbb:,9223372036854775807,,,,,,,,"21/May/16 08:27;erans;Hi Amol.

Thanks for your report.

From reading the problem description it is not obvious to figure out whether your proposed fix won't have any unwanted side-effects (e.g. it could be that the ""getNeighbors"" was not meant to exactly match the definition in the reference you cite; maybe you are totally right, I'm just guessing since I never looked at that code...).

Could you provide a unit test showing that it is indeed a bug (i.e. a case where the best solution cannot be recovered unless the fix is applied)?
","21/May/16 16:17;amolsingh;Hi Giles, 

Thanks for the quick response. 

Here's a simple breaking test, you should be able to add this to DBSCANClustererTest.java and run it. 

 @Test public void testBreakingCase() {
        final DoublePoint[] points = { new DoublePoint(new double[] { 21.345289965479466, 32.149537670215295 }),
                new DoublePoint(new double[] { 42.02226837841131, 19.69611946377732 }),
                new DoublePoint(new double[] { 41.930019221367985, 21.954397109962457 }),
                new DoublePoint(new double[] { 42.87345060400816, 18.796775009724836 }) };

        final DBSCANClusterer<DoublePoint> clusterer = new DBSCANClusterer<DoublePoint>(3, 3);
        List<Cluster<DoublePoint>> clusters = clusterer.cluster(Arrays.asList(points));
        Assert.assertEquals(1, clusters.size());
        final List<DoublePoint> clusterOne = Arrays.asList(points[1], points[2], points[3]);
        Assert.assertTrue(clusters.get(0).getPoints().containsAll(clusterOne));
    }

Now, if you set minPts=2 or change the code to remove the reference check of the point to itself in getNeighbors() this will pass. 

Let me know if you agree this is a bug. I'm happy to submit a patch. This change does change the behavior for existing users, the algorithm will produce more clusters / shape of clusters may change. Nonetheless I feel this would be the correct interpretation of the DBSCAN paper, and it seems consistent with other third party libraries I've evaluated.","24/May/16 12:57;erans;bq. Let me know if you agree this is a bug.

I don't know. :(

If you are positive that the algorithm was not correctly implemented, please do submit a patch with code comments that explain why the change was necessary.
It would also be nice to set up a unit test showing a practical case that this implementation agrees with the result computed by another implementation.

Thanks.","24/May/16 14:33;amolsingh;Okay, I'll do that. 

https://en.wikipedia.org/wiki/DBSCAN Not the most reliable source but if you look at the pseudocode, thats how others have interpreted this algorithm as well. 
{quote}
regionQuery(P, eps)
   return all points within P's eps-neighborhood (including P)
{quote}

I'll submit a patch. ","12/Jan/18 08:50;jiaopaner_cn;Hello,Amol
    I quite agree with your opinion .I also found this problem when I use the DBSCAN algorithm.The version I used is apache-commons -math-3.6.1.
So the problem still exists,and I also found other problems in addition to the above problem.The time complexity of the algorithm is always n^2 when I test.Besides that,there are many no necessary operations in the algorithm implementation.BTW,why only DoublePoint, no StringPoint, so I can't use Levenshtein distance to search the adjacency points.I'll create a detailed bug report .",16/Aug/18 04:03;backkom;I agree with you ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""NaturalRankingTest"" broken?",MATH-1361,12969776,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,erans,erans,14/May/16 17:23,23/Oct/17 18:13,07/Apr/19 20:38,,3.6.1,,,,,,,4.0,,,0,test,,,,,,,"As reported on the [""dev"" ML|http://markmail.org/message/2oqkvx33arjj34lz], the test method
{noformat}
testNaNsFixedTiesRandom()
{noformat}
reports failure for more than 96 out of 100 seed values.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2017-10-23 18:13:46.275,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 23 18:13:46 UTC 2017,,,,,,0|i2xz6v:,9223372036854775807,,,,,,,,"23/Oct/17 18:13;psteitz;This is the result of sloppy test implementation (by me).  The TiesRandom strategy assigns ranks randomly among the applicable ranks, so the expected values are in fact stochastic.  To eliminate seed dependency, the expected values for ties need to be allowed to vary over the possible ranks.  One way to do that is the fix that I applied to Hipparchus for this issue [https://github.com/Hipparchus-Math/hipparchus/commit/9cc199a7c7b2051a02bd1db060caf43a00748f2b].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HypergeometricDistribution probability give NaN result,MATH-1356,12958453,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tlacroix,tlacroix,13/Apr/16 12:27,06/Sep/16 06:48,07/Apr/19 20:38,06/May/16 21:31,3.6.1,,,,,,,4.0,,,0,,,,,,,,"Hi,
Unless I am mistaken the HypergeometricDistribution probability method returns NaN for the following cases :

HypergeometricDistribution hgd = new HypergeometricDistribution(11,11,1);
double probIT = hgd.probability(1);

HypergeometricDistribution hgd = new HypergeometricDistribution(11,11,11);
double probIT = hgd.probability(11);

I think it should return 1.0
Thanks,
Thomas",Windows,,,,,,,,,,,,,,,,,,,,14/Apr/16 10:07;erans;MATH-1356.patch;https://issues.apache.org/jira/secure/attachment/12798698/MATH-1356.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-04-13 14:47:00.616,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 19 11:28:36 UTC 2016,,,,,,0|i2w21b:,9223372036854775807,,,,,,,,"13/Apr/16 14:47;erans;Bug(s) indeed:
* failing to check precondition(s) that lead to computing ""0 * infinity"", and/or
* failing to give the correct result in partcular cases (that throw off the implementation)

The NaN is created in {{SaddleExpansion.logBinomialProbability}} and {{SaddleExpansion.getDeviancePart}}.
Both perform a computation like the above.

Case
{noformat}
double probIT = hgd.probability(0);
{noformat}
also produces NaN.
","14/Apr/16 10:07;erans;I propose to commit the attached patch in branch ""feature-MATH-1158"" (since it contains modified versions, w.r.t. ""develop"" of the files in package {{o.a.c.math4.distribution}}).","19/Apr/16 11:28;erans;Commit pushed to branch ""feature-MATH-1158"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""solverAbsoluteAccuracy"" in ""NormalDistribution""",MATH-1352,12953890,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Won't Fix,,erans,erans,28/Mar/16 02:47,21/Jan/18 16:12,07/Apr/19 20:38,21/Jan/18 16:12,,,,,,,,,,,0,,,,,,,,"{{NormalDistribution}} has a constructor that sets a ""solverAbsoluteAccuracy"" field.
This field is never used since the class overrides method ""inverseCumulativeProbability"".

This constructor is called in ""PoissonDistribution"" and ""EmpiricalDistribution"" (i.e. explicitly passing a value that is useless).
",,,,,,,,,,,,,,,,,MATH-1345,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 21 16:12:09 UTC 2018,,,,,,0|i2v9wv:,9223372036854775807,,,,,,,,"28/Mar/16 23:06;erans;Same issue in:
* {{CauchyDistribution}}
* {{WeibullDistribution}}
* {{TriangularDistribution}}
* {{ExponentialDistribution}}


","21/Jan/18 16:12;erans;Those distribution classes have been moved to [""Commons Statistics""|https://git1-us-west.apache.org/repos/asf?p=commons-statistics.git;a=tree;f=commons-statistics-distribution/src/main/java/commons/statistics/distribution;hb=HEAD], where the problematic constructors have been removed (cf. STATISTICS-2).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Wrong equation in Javadoc of Percentile.EstimationType, LEGACY, variable estimation",MATH-1347,12951984,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,,,richard.brunauer@salzburgresearch.at,richard.brunauer@salzburgresearch.at,21/Mar/16 10:16,22/Apr/17 14:20,07/Apr/19 20:38,,3.6,,,,,,,4.0,,,0,documentation,,,,,,,"The Javadoc entry for Enum Percentile.EstimationType LAGACY uses a wrong equation for variable estimate. 

Explaination in version 3.0, 3.6 source code and 3.6 semantics do an interpolation between two values. The quation for variable estimation has only a ceiling function. This does not match. ",Windows 7,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-03-21 10:43:35.053,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 21 11:59:28 UTC 2016,,,,,,0|i2uy5b:,9223372036854775807,,,,,,,,21/Mar/16 10:43;erans;Could you please provide a unit test in order to clarify the issue?,"21/Mar/16 11:59;richard.brunauer@salzburgresearch.at;No, I can’t provide a junit test. The mistake is in Javadoc and not in Java code. The results are, for my opinion, correct. 

I guess that the explanation is correct but the equation does not represent the explanation’s semantics. Explanation and expression are semantically not equal! 

Explaination in Percentile for the default/legacy case: “Else let lower be the element in position floor(pos) in the array and let upper be the next element in the array. Return lower + d * (upper - lower)”

Equation in Percentile.EstimationType: x[ceil(h - 1/2)]

I would expect the same equation for variable “estimate” as in R_6. Furthermore, the Percentile.EstimationType “R_6” and “LAGACY” is almost a code copy (compare method index()).Is this intended? 

To see the mistake:
https://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/stat/descriptive/rank/Percentile.EstimationType.html
section: Enum Constant Detail
enum: LEGACY 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ODE failure with close events,MATH-1342,12950877,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,evanward1,evanward1,16/Mar/16 15:42,23/Apr/16 00:32,07/Apr/19 20:38,23/Apr/16 00:31,3.6,,,,,,,3.6.1,,,0,,,,,,,,ODE integration crashes if two events are closer to each other than the root finder's tolerance.,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-04-23 00:32:32.907,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 23 00:32:32 UTC 2016,,,,,,0|i2urs7:,9223372036854775807,,,,,,,,16/Mar/16 20:23;evanward1;fixed in 7a8dc00,17/Mar/16 12:59;evanward1;fixed in 5238b00 on 3.X,23/Apr/16 00:32;erans;Code was released in CM v3.6.1.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""RandomDataGenerator"" is brittle",MATH-1341,12949857,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,erans,erans,14/Mar/16 11:45,29/May/16 21:06,07/Apr/19 20:38,29/May/16 21:06,,,,,,,,4.0,,,0,API,cleanup,deprecation,thread-safety,,,,"Class {{RandomDataGenerator}} can easily be misused as it advertizes a method to access its internal RNG (which is _not_ thread-safe).

The class is also a mixed bag of ""data generators"" that are either ""secure"" or not.
Moreover it uses the ""lazy initialization"" pattern (for the RNG instance) solely because of this duality; otherwise users that need one or the other form of data generation will obviously always use the RNG since all data generation methods need it.
This entails also a performance hit (albeit tiny) as each call checks whether the RNG has been initialized already.
The clean solution would be to separate the two types of data generation (secure vs not) into different classes.
",,,,,,,,,,,,,,,,,,,,,13/May/16 13:26;erans;RandomUtils.java;https://issues.apache.org/jira/secure/attachment/12803859/RandomUtils.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-05-13 18:10:48.699,,,false,,,,,,,,,,,,,,9223372036854775807,,,Fri May 20 13:35:09 UTC 2016,,,,,,0|i2ulif:,9223372036854775807,,,,,,,,"19/Mar/16 23:25;erans;Another performance hit: For each sampling, a new distribution object is created.
This will be compounded with the new sampling API (MATH-1158) where a {{Sampler}} object will also be created.

If a user needs sampling from a distribution, is there any advantage to use {{RandomDataGenerator}} (over instantiating the appropriate {{Sampler}} instance)?
Even if an application might have some use of the syntactic sugar which this class provides, is there any advantage for CM to provide an alternative sampling API that is not efficient?
","13/May/16 13:26;erans;I attached a new {{RandomUtils}} class which I propose as a replacement for {{RandomDataGenerator}}.

Having removed all methods that are _trivial_ syntactic sugar for the sampler API in {{o.a.c.m.distribution}}, the number of lines in {{RandomUtils}} is ~60% of the old {{RandomDataGenerator}}.

The ""nextSecureXxx"" methods were also removed as most were identical to their ""non-secure"" counterpart, except for the underlying RNG (secure vs not). In {{RandomUtils}}, secure versions are thus ""automatically"" achieved by passing a secure RNG to the class's constructor.

Methods {{nextHexString}} and {{nextSecureHexString}} were different in that the latter uses a {{MessageDigest}} object in its processing.
The two codes were merged in a single method and a boolean argument selects one or the other behaviour.

{{RandomUtils}} also obsoletes class {{RandomGeneratorFactory}} whose functionality is replaced by the following factory method:
{noformat}
public static UniformRandomProvider asUniformRandomProvider(final Random rng) {
  // ....
}
{noformat}

Old ""features"" intentionally left out:
* Lazy initialization (if you don't need the RNG, you don't need an instance of this class)
* Reseeding of the RNG (if necessary - although a bad idea - the caller can hold a reference to the {{Random}} object being wrapped, and call ""setSeed"" on it)


Please have a look, and let me know of any functionality I may have missed to port to the new class.
",13/May/16 18:10;luc;Seems good to me.,"15/May/16 23:31;erans;Code available in branch ""feature-MATH-1341"".","17/May/16 14:24;erans;Code merge into branch ""develop"".","20/May/16 13:35;erans;Class {{RandomDataGenerator}} removed in branch ""task-MATH-1366"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Does ""EmpiricalDistribution"" use ""RandomDataGenerator""?",MATH-1338,12949639,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Blocker,Fixed,,erans,erans,13/Mar/16 01:04,22/Apr/16 23:31,07/Apr/19 20:38,22/Apr/16 23:31,,,,,,,,4.0,,,0,api,cleanup,exception,,,,,"The class {{EmpiricalDistribution}} (in package ""o.a.c.m.random"") holds an instance field (""randomData"") of type {{RandomDataGenerator}}.
The documentation of this field indicates that it is only used for sampling, and indeed the field is only accessed in order to fetch the RNG object stored in it. 
It is unclear why {{RandomDataGenerator}} was needed to hold a RNG since the same RNG could have been (in the current design) readily available in the base class.

Sampling is performed in method {{getNextValue()}} (which in turn calls {{sample()}} in the parent class).
{{getNextValue()}} performs a precondition check before calling {{sample()}} and raises a CM specific {{MathIllegalStateException}}; but the inherited {{sample()}} method can be still called directly on the instance, and in that case, if the (same) condition is not fulfilled, a standard {{NullPointerException}} will be thrown instead.

In line with MATH-1158, the sampling functionality should be accessed through the {{RealDistribution.Sampler}} interface, and {{getNextValue()}} is to be removed as it duplicates the common API (i.e. the {{sample()}} method).

Since the RNG is then passed to the {{createSampler}} factory method, the ""randomData"" field becomes obsolete.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-03-22 14:36:57.062,,,false,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 22 23:31:05 UTC 2016,,,,,,0|i2uk5r:,9223372036854775807,,,,,,,,"14/Mar/16 02:01;erans;{{EmpiricalDistribution}} contains two methods
* public void reseedRandomGenerator(long seed)
* public void reSeed(long seed)

that do the same thing.
","22/Mar/16 14:36;chtompki;Seems like the thing to do here is to {{@Depricated}} to the method no longer needed and have it call the desired method for the time being. Does that seem correct?

Also it seems like the method, of the two in question from your comment, to deprecate is {{reSeed(long seed)}} simply because it isn't inherited and is from an earlier version of the API. ","22/Mar/16 15:42;erans;Actually, resolving MATH-1158 will make _both_ obsolete.
","25/Mar/16 14:25;chtompki;In line with MATH-1158, would you continue the pattern in both {{AbstractIntegerDistribution}} and {{AbstractMultivariateRealDistribution}}? It seems reasonable to at least implement analogs to the Sampler in those locations as well. ","25/Mar/16 15:21;erans;{{RealDistribution}} and {{IntegerDistribution}} are taken care of in branch ""feature-MATH-1158"" which I've just pushed to the project's repository.
You are most welcome to check the changes.

{{MultivariateRealDistribution}} is not done yet.
Would you be willing to provide a patch?
","25/Mar/16 15:33;erans;Issue is handled in branch ""feature-MATH-1158"".",25/Mar/16 17:37;chtompki;I see. I just read {{doc/development/development.howto.txt}} and now see how the branching strategy works. I'll spend some time reading over the MATH-1158 branch to the work there for future reference. Thanks for the patience with my naivety here.,"29/Mar/16 09:50;erans;Obsolete methods removed in branch ""feature-MATH-1158"".","22/Apr/16 23:31;erans;Code merged into ""develop"" branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MullerSolver returns value that is out of bounds,MATH-1333,12947879,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,cpmeister,cpmeister,08/Mar/16 01:43,12/Apr/17 00:00,07/Apr/19 20:38,,3.6,,,,,,,4.0,,,0,,,,,,,,"Not sure what went wrong here but in a bracket of [20,100] I got back -19.9!",,,,,,,,,,,,,,,,,,,,,08/Mar/16 02:08;cpmeister;BrokenMuller.java;https://issues.apache.org/jira/secure/attachment/12791897/BrokenMuller.java,09/Mar/16 19:23;cpmeister;BrokenMuller2.java;https://issues.apache.org/jira/secure/attachment/12792321/BrokenMuller2.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2016-03-09 15:59:53.269,,,false,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 11 23:12:00 UTC 2016,,,,,,0|i2ub8f:,9223372036854775807,,,,,,,,"09/Mar/16 15:59;erans;Confirmed.
Thanks for the report.

I've created a branch ""feature-MATH-1333"" on the repository with your example as a unit test (that is currently failing).

The problem is already present at line 161, where a comment indicates that ""x"" will be between ""x0"" and ""x2"" but a print shows that it is not:
{noformat}
x=64.25602375943637 x0=66.68753568924203 x2=100.04173804515072
{noformat}

Class {{MullerSolver2}} produces the expected answer.
 
The documentation (of these two classes) is unclear as to whether the code is strictly implementing the algorithm from the mentioned reference or if there is an adaptation.  The latter seems plausible from the comment:
{noformat}
Muller's original method would have function evaluation at complex point.
Since our f(x) is real, we have to find ways to avoid that.
{noformat}

In {{MullerSolver}}:
{noformat}
            // Muller's method employs quadratic interpolation through
            // x0, x1, x2 and x is the zero of the interpolating parabola.
{noformat}
In {{MullerSolver2}}:
{noformat}
            // quadratic interpolation through x0, x1, x2
{noformat}

Hence, at first sight, the interpolation step seems to be the same.
If the code duplication is confirmed, we should fix that first (and it will probably also fix the original problem).
","09/Mar/16 18:36;cpmeister;I would like to note that I have found the exact same out of bounds issue with MullerSolver2, but with a different test case than the one I provided. I'll see if I can provide a failing test case for MullerSover2 so you can add it to your unit tests as well.",09/Mar/16 19:23;cpmeister;I just attached BrokenMuller2.java which shows the same out of bounds issue in MullerSolver2.,"09/Mar/16 22:21;erans;Thanks.  Unit test added to the ""feature-MATH-1333"" branch"".","10/Mar/16 17:57;erans;Hi Connor.

You are most welcome to provide a patch for fixing the issue.
I'd suggest to first create a class (and unit tests) for performing the quadratic interpolation step involved in both classes.

Thanks in advance for your help. :)

And you are welcome to ask questions here or on the ML if you are unsure of how to proceed...
","11/Mar/16 03:10;cpmeister;So it looks like MullerSolver was failing due to the solution being right next to one of the the bracketing values.
The strict test for sequential ordering of x0,x1, and x2 causes problems when x1==x2.
I have a fix for that particular problem in MullerSolver but the bounds issue for MullerSolver2 is an entirely different issue.
After analysis it looks like MullerSolver2's lack of bracketing backfires on it spectacularly as shown in BrokenMuller2.java.
According to the documentation: 
{noformat}
Because the interval may not be bracketing, the bisection alternative is 
not applicable here. However in practice our treatment usually works 
well, especially near real zeroes where the imaginary part of the complex 
approximation is often negligible.
{noformat}
The keywords are _usually works well_. It seems that MullerSolver2 has known limitations to it's algorithm and IMO it looks like a bunch of bandaids applied to a flawed design. I have no problems with adding yet another bandaid but I just want to know your opinion on the subject and the history behind that class.

PS. I'm not trying to come off as rude I am just deeply concerned.","11/Mar/16 11:36;erans;bq.  usually works well

;)
It can happen that an algorithm is not appropriate for certain cases.
MATH-631 might be an interesting (but long!) read, as this could be a similar issue: an implementation's weakness is not detected at the right place, leading to a result that is in blatant contradiction with the documentation.

bq. I have no problems with adding yet another bandaid

A relatively important point is to be clear about whether the classes implement a *reference algorithm*. The more so if an ""accommodation"" is sometimes the cause for failing in totally unexpected ways.

Whether the implementations are ""standard"" or not, if it is possible to detect the root cause of the problem (and prevent such silly behaviour as you have reported), then IMO it should be appropriately dealt with (raising an exception that states the limitation of the algorithm, or put the search back on track).

bq. I'm not trying to come off as rude I am just deeply concerned.

You can be, I agree.  But hopefully, there will be a lesson drawn from MATH-631...
Could you please post a message to the ""dev"" ML?
","11/Mar/16 23:12;cpmeister;Well MATH-163 was certainly an interesting read. :)
I posted to the ML as you requested hopefully that should allow more people to be part of the discussion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RealVector.Entry is private, cannot be used outside class",MATH-1329,12944700,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,ziqizhang,ziqizhang,25/Feb/16 23:02,01/Jun/18 10:39,07/Apr/19 20:38,,3.6,,,,,,,4.0,,,2,,,,,,,,"RealVector class has two methods that return an iterator to allow iterating through the entries in the vector:

RealVector.sparseIterator()
RealVector.iterator()

Both return Iterator<RealVector.Entry>

However, RealVector.Entry is a private class defined within RealVector. As a result, it is not possible to use the returned iterator to go through the values outside the class RealVector.


",Any,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2018-06-01 10:39:31.132,,,false,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 01 10:39:31 UTC 2018,,,,,,0|i2ts67:,9223372036854775807,,,,,,,,01/Jun/18 10:39;MightyDash;Same problem for OpenMapRealVector,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Negative integer seed for MersenneTwister is not working.,MATH-1322,12937857,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,kawajoe,kawajoe,09/Feb/16 13:13,23/Apr/16 01:01,07/Apr/19 20:38,23/Apr/16 01:01,3.6,,,,,,,,,,0,patch,,,,,,,"MersenneTwister.setSeed(int seed) has a bug.

If seed is negative, longMT also becomes negative.
First 32 bits are filled by 0xFFFFFFFF.
But it should be positive. First 32 bits should be filled by 0x00000000.

Ex) Integer -1 is 0xffffffff. 
Long -1 is 0xffffffffffffffff.
Long 0xffffffff is 4294967295.

I created simple patch. 



",,,,,,,,,,,,,,,,,,,,,09/Feb/16 13:13;kawajoe;MersenneTwister.patch;https://issues.apache.org/jira/secure/attachment/12787064/MersenneTwister.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-02-09 13:45:26.292,,,false,,,,,Patch,,,,,,,,,9223372036854775807,,,Sat Apr 23 01:01:23 UTC 2016,,,,,,0|i2slzj:,9223372036854775807,,,,,,,,09/Feb/16 13:13;kawajoe;Patch to fix bug.,"09/Feb/16 13:45;erans;bq. If seed is negative, longMT also becomes negative.

I'd rather expect that both types result in the same value.
Why is it a problem?

The documentation does not seem to imply any constraint on the argument.

Do I understand correctly that if your patch were applied, each of the following calls would result in the generator being in a different state:
{code}
rng.setSeed(-1);
rng.setSeed((long) -1);
{code}
?","09/Feb/16 16:03;kawajoe;My patch is for setSeed(int seed), there is no problem in setSeed(long seed).
Actually, I compared apache MersenneTwiter with C++ mt19937 which is included in random.
And apache MersenneTwister generated different random value from C++ mt19937 only if seed is negative.

The reason for the difference is that longMT is 0xffffffffffffffff.

{code:title=MersenneTwister.java|borderStyle=solid}
    public void setSeed(int seed) {
        // we use a long masked by 0xffffffffL as a poor man unsigned int
        long longMT = seed;

// longMT is 0xffffffffffffffff, but it is 0x00000000ffffffff in C++ or other language implementation.

        // NB: unlike original C code, we are working with java longs, the cast below makes masking unnecessary
        mt[0]= (int) longMT;
        for (mti = 1; mti < N; ++mti) {
            // See Knuth TAOCP Vol2. 3rd Ed. P.106 for multiplier.
            // initializer from the 2002-01-09 C version by Makoto Matsumoto
            longMT = (1812433253l * (longMT ^ (longMT >> 30)) + mti) & 0xffffffffL;
            mt[mti]= (int) longMT;
        }

        clear(); // Clear normal deviate cache
    }
{code}




Followings are the samples.

{code:title=Sample.java|borderStyle=solid}
MersenneTwister rng = new MersenneTwister();
rng.setSeed((int) -1);
// Without patch, ""-931718226"" is output.
// After applying patch, ""419326371"" is output.
System.out.println(rng.nextInt());
{code}


{code:title=CppSample.cpp|borderStyle=solid}
#include <iostream>
#include <random>

int main()
{
  std::mt19937 mt(0xFFFFFFFF);
  // 419326371 is displayed
  std::cout << mt() << std::endl;

  std::mt19937 mt1(-1);
  // 419326371 is displayed
  std::cout << mt() << std::endl;
}
{code}
","09/Feb/16 16:21;erans;With the code you wrote above:
{code}
MersenneTwister rng = new MersenneTwister();
rng.setSeed((int) -1);
// Without patch, ""-931718226"" is output.
// After applying patch, ""419326371"" is output.
System.out.println(rng.nextInt());
{code}
what is the output of
{code}
rng.setSeed((long) -1);
{code}
# in the current  code and
# with the patch applied

?
","09/Feb/16 16:33;kawajoe;There is no impact to setSeed(long seed).

{code:borderStyle=solid}
MersenneTwister rng = new MersenneTwister();
rng.setSeed((long) -1);
// 1. Without patch, ""93740670"" is displayed.
// 2. After applying patch, ""93740670"" is displayed. Same value.
System.out.println(rng.nextInt());
{code}

","09/Feb/16 17:36;erans;bq. After applying patch, ""93740670"" is displayed. Same value.

What worries me (even as your patch is applied) is that the same seed (namely -1) results in different states, whether it happens to be cast to an ""int"":
{code}
rng.setSeed((int) -1);
int n = rng.nextInt(); // n == 419326371
{code}
or cast to a ""long"":
{code}
rng.setSeed((long) -1);
int n = rng.nextInt(); // n == 93740670
{code}

Do you see what I mean?
This is one of the (IMO) deficiencies which issue MATH-1319 aims at making disappear (by defining a _unique_ way to set the seed).
In this case, IIUC, the problem is having overloaded {{setSeed}} to take an {{int}} argument although to be faithful to the original code, it must be interpreted as the low-order bits of a {{long}}.

Since the principle of specifying a seed is intended for reproducing a sequence, bluntly changing the code is not an option.

I'd rather imagine that {{setSeed(int)}} must be deprecated, and a conversion method must be provided so that {{setSeed(convertToLong((int) seed))}} results in the same state as the current {{setSeed((int) seed)}}.","09/Feb/16 21:41;tn;+1 for the patch. It does the same as the stdc++11 impl, boost and the original c version of Mersenne Twister.",23/Apr/16 01:01;erans;This class is superseded by the new API provided in package {{o.a.c.m.rng}} which you are welcome to test.  See MATH-1335.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexOptimizer returns wrong values,MATH-1321,12937840,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Blocker,Not A Problem,,pavol.loffay,pavol.loffay,09/Feb/16 11:36,09/Feb/16 15:57,07/Apr/19 20:38,09/Feb/16 15:57,3.6,,,,,,,,,,0,,,,,,,,"Hello I'm trying to minimize MSE of double exponential smoothing and get optimal parameters alpha and beta. 
https://www.otexts.org/fpp/7/2

During the minimization the output shows values of alpha and beta which differs from alpha, and beta returned from SimplexOptimizer.optimize().

{code:title=output|borderStyle=solid}
...
Nelder MSE = 0.007226473669598979, alpha=0.896283, beta=0.228161
Nelder MSE = 0.0069843320509952005, alpha=0.913694, beta=0.190210 # returned minimum
Nelder MSE = 0.008577342645261695, alpha=0.931617, beta=0.131448
Nelder MSE = 0.00743296945818598, alpha=0.918018, beta=0.166808
Nelder MSE = 0.007818891499431175, alpha=0.936768, beta=0.136053
Nelder MSE = 0.007293932014855209, alpha=0.927010, beta=0.155973
Nelder MSE = 0.007319455298330941, alpha=0.923120, beta=0.178180
Nelder MSE = 0.007110221641945739, alpha=0.921873, beta=0.175281
Nelder MSE = 0.007271067724068611, alpha=0.907713, beta=0.212689
Nelder MSE = 0.007084561548618076, alpha=0.912928, beta=0.197226
Nelder MSE = 0.007072487763137581, alpha=0.903911, beta=0.213540

Nelder -> key = [2.3595947265625, -1.44864501953125], fce minimum= 0.00698433205099520050
{code}

{code:title=Test.java|borderStyle=solid}

@Test
    public void testOptimization() throws IOException {

        int maxEval = 1000;
        int maxIter = 1000;

        // Nelder-Mead Simplex
        SimplexOptimizer nelderSimplexOptimizer = new SimplexOptimizer(0.0001, 0.0001);
        PointValuePair nelderResult = nelderSimplexOptimizer.optimize(
                GoalType.MINIMIZE, new MaxIter(maxIter), new MaxEval(maxEval),
                new InitialGuess(new double[]{0.4, 0.1}), new ObjectiveFunction(optimizationFn(""Nelder"")),
                new NelderMeadSimplex(2));

 System.out.format(""\nNelder (%d eval) -> key = %s, fce minimum= %.20f"", nelderSimplexOptimizer.getEvaluations(),
                Arrays.toString(nelderResult.getKey()), nelderResult.getValue());
}

private MultivariateFunction optimizationFn(String algorithm) {

        final List<DataPoint> testData = metricData.subList(0, 50);
        // func for minimization
        MultivariateFunction multivariateFunction = point -> {

            double alpha = point[0];
            double beta = point[1];

            DoubleExponentialSmoothing doubleExponentialSmoothing = new DoubleExponentialSmoothing(alpha, beta);
            AccuracyStatistics accuracyStatistics = doubleExponentialSmoothing.init(testData);

            System.out.format(""%s MSE = %s, alpha=%f, beta=%f\n"", algorithm, accuracyStatistics.getMse(), alpha, beta);
            return accuracyStatistics.getMse();
        };
        MultivariateFunctionMappingAdapter multivariateFunctionMappingAdapter =
                new MultivariateFunctionMappingAdapter(multivariateFunction,
                        new double[]{0.0, 0.0}, new double[]{1, 1});

        return multivariateFunctionMappingAdapter;
    }
{code}



","Fedora 23,
java version ""1.8.0_51""
Java(TM) SE Runtime Environment (build 1.8.0_51-b16)
Java HotSpot(TM) 64-Bit Server VM (build 25.51-b03, mixed mode)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-02-09 12:17:16.474,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 09 15:57:37 UTC 2016,,,,,,0|i2slvr:,9223372036854775807,,,,,,,,09/Feb/16 12:06;pavol.loffay;[~tn] Could you please look at it? ,"09/Feb/16 12:17;erans;In the table above, you refer to the ""returned minimum"".  Do I understand correctly that you imply that *that* value is not correct?  If so, how does one know that it is wrong?","09/Feb/16 12:27;pavol.loffay;Should *SimplexOptimizer.optimize() *return the optimal parameters alpha and beta which are passed as arguments to *MultivariateFunction.value(double[] point)*? 

In my example method *optimize* returns optimal MSE which is  *0.00698433205099520050*  but *key = \[2.3595947265625, -1.44864501953125\]* is not equal to alpha/beta from the debug output of MultivariateFunction.

Then what does the key returned from *optimize* mean ?","09/Feb/16 13:24;erans;bq. \[2.3595947265625, -1.44864501953125\] is not equal to alpha/beta from the debug output

I had missed the point, sorry.

The ""getKey()"", or preferrably ""getPoint()"" since it is more to the point :), should indeed  return the optimal value.

But as you use a {{MultivariateFunctionMappingAdapter}} wrapper, it is expected that the debug output, which shows values of the original function, is not the same as the values of the function that is actually being optimized.
In order to retrieve the values in the original range, you must perform the [inverse transformation|http://commons.apache.org/proper/commons-math/javadocs/api-3.6/org/apache/commons/math3/optim/nonlinear/scalar/MultivariateFunctionMappingAdapter.html]:
{code}
double[] actual = optimizationFn.unboundedToBounded(nelderResult.getKey());
{code}
which hopefully will return the values shown in the output (modulo the possible loss of accuracy that could result from the conversions).","09/Feb/16 15:52;pavol.loffay;Thanks, this is the right solution.",09/Feb/16 15:57;erans;Thanks for the feedback.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultiKMeansPlusPlusClusterer buggy for alternative evaluators,MATH-1315,12929024,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,thorstenschaefer,thorstenschaefer,10/Jan/16 18:18,12/Apr/17 00:11,07/Apr/19 20:38,,3.5,,,,,,,4.0,,,0,,,,,,,,"I just looked into the source code for the MultiKMeansPlusPlusClusterer and realized that it would return null in case of an alternative evaluator that favors bigger values instead of smaller ones:
The basic idea of the clustering method is that we perform n clusterings and choose the best result. The decision what's the best result is performed by the evaluator, which by default assumes smaller values are better. 
According to the documentation, we can also provide a different evaluator, which for instance would decide that bigger values are better, but given we initialize the best value with Double.POSITIVE_INFINITY in method MultiKMeansPlusPlusClusterer.cluster(Collection<T>), we would never find a ""better"" result and thus always return null. ",,,,,,,,,,,,,,,,,,,,,07/May/16 20:29;erans;MATH-1315.patch;https://issues.apache.org/jira/secure/attachment/12802841/MATH-1315.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2016-01-10 19:46:39.843,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue May 17 16:46:07 UTC 2016,,,,,,0|i2r3sn:,9223372036854775807,,,,,,,,"10/Jan/16 19:46;tn;Indeed, you are right this has to be fixed.",06/May/16 21:59;erans;Do you agree that the attached patch would solve this issue?,07/May/16 17:43;thorstenschaefer;I think you uploaded the wrong patch. ,"07/May/16 20:29;erans;Indeed, sorry; here is the right one!","17/May/16 16:46;erans;Hi Thorsten.

Can you contribute a unit test to ensure that the fix is working?  Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Wrong tolerance in some unit tests of ""RandomGeneratorAbstractTest""",MATH-1313,12929009,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,erans,erans,erans,10/Jan/16 15:18,11/Jan/16 11:10,07/Apr/19 20:38,11/Jan/16 11:10,,,,,,,,4.0,,,0,unit-test,,,,,,,"I doubt that the mean check in the unit test below is ever going to trigger an assertion failure...

{noformat}
    @Test
    public void testDoubleDirect() {
        SummaryStatistics sample = new SummaryStatistics();
        final int N = 10000;
        for (int i = 0; i < N; ++i) {
            sample.addValue(generator.nextDouble());
        }
        Assert.assertEquals(""Note: This test will fail randomly about 1 in 100 times."",
                0.5, sample.getMean(), FastMath.sqrt(N/12.0) * 2.576);
        Assert.assertEquals(1.0 / (2.0 * FastMath.sqrt(3.0)),
                     sample.getStandardDeviation(), 0.01);
    }

{noformat}

And similar in ""testFloatDirect()"".

I propose the following replacement:
{noformat}
    @Test
    public void testDoubleDirect() {
        SummaryStatistics sample = new SummaryStatistics();
        final int N = 100000;
        for (int i = 0; i < N; ++i) {
            sample.addValue(generator.nextDouble());
        }
        assertUniformInUnitInterval(sample, 0.99);
    }
{noformat}

where ""assertUniformInUnitInterval"" is defined as:
   {noformat}
    /**                                                                                                                                                                                    
     * Check that the sample follows a uniform distribution on the {@code [0, 1)} interval.                                                                                                
     *                                                                                                                                                                                     
     * @param sample Data summary.                                                                                                                                                         
     * @param confidenceIntervalLevel Confidence level. Must be in {@code (0, 1)} interval.                                                                                                
     */
    private void assertUniformInUnitInterval(SummaryStatistics sample,
                                             double confidenceIntervalLevel) {
        final int numSamples = (int) sample.getN();
        final double mean = sample.getMean();
        final double stddev = sample.getStandardDeviation() / FastMath.sqrt(numSamples);
        final TDistribution t = new TDistribution(numSamples - 1);
        final double criticalValue = t.inverseCumulativeProbability(1 - 0.5 * (1 - confidenceIntervalLevel));
        final double tol = stddev * criticalValue;
        Assert.assertEquals(""mean="" + mean + "" tol="" + tol + "" (note: This test will fail randomly about "" +
                            (100 * (1 - confidenceIntervalLevel)) + "" in 100 times)."",
                            0.5, mean, tol);
        Assert.assertEquals(FastMath.sqrt(1d / 12), sample.getStandardDeviation(), 0.01);
    }
{noformat}

Please correct if this new test is not what was intended.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-01-10 16:37:42.086,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 11 11:10:31 UTC 2016,,,,,,0|i2r3pb:,9223372036854775807,,,,,,,,"10/Jan/16 16:37;luc;Testing uniformity seems better than what we currently have, so +1 for changing the test.","10/Jan/16 21:02;psteitz;The current test is definitely weak and the proposal above is better, but it could be made better still by looking not just at the mean, but the distribution itself.  I suspect the test was implemented before we had ChiSquare or Kolmogorov-Smirnov tests available, either of which could be used to evaluate conformity of the distribution.   The most direct would be to do a one-sample KS test with a Uniform distribution instance as the RealDistribution.  If the p-value returned is small, say less than .001, you fail the test.  Alternatively, you could set up equal-sized bins and do a ChiSquare test with expected bin counts all equal to the sample size / number of bins (this is what RealDistributionAbstractTest#testSampling does).","10/Jan/16 23:54;erans;bq. Testing uniformity seems better than what we currently have

I thought that the current was testing uniformity (unless out of bad luck, the sample follows another distribution that also happens to have a 0.5 mean).
The problem with the current tolerance is that it is wrong: it is proportional to sqrt(N) instead of being proportional to sqrt(1/N).",11/Jan/16 01:31;psteitz;The current test checks only the mean and standard deviation of the sample.  It does not actually test that the deviates follow a uniform distribution.  I think it was likely implemented before we had tests that could test that the sample deviates actually follow the distribution.  That is what the various tests of distribution samplers do.  That is what we should do here.,"11/Jan/16 02:28;erans;This issue is about a *bug* in the current test code.

In the description, I'm asking for confirmation about
# whether it is a bug or not,
# whether the proposed fix does what was _originally_ intended (i.e. test that, if the distribution is uniform, then the mean must be 0.5 within some confidence interval).

That the unit test can be _changed_ to do something else should be discussed in another report.
","11/Jan/16 05:01;psteitz;The test *should* be changed to do something else.  The ""improvement"" effectively performs a t-test for equality of the mean to the expected mean and a non-statistical test of the standard deviation.  It does not test anything other than systematic bias of the mean and variance close (no actual statistical test) to expected.  The use of a t-test is questionable since we know (hope, actually) that the distribution being sampled is *not* Gaussian, but that is not the point.  The point is we should be testing that the values are in fact uniformly distributed, which the above does little to confirm.  I see now that testNextFloat correctly does a chisquare test.  I  recommend that we drop this test and the one for floats and create one for doubles that does what testNextFloat does, which actually does confirm uniformity.  Now that we have KS tests, it might actually be even better to use that for the double test, which would give us two kinds of actual statistical tests of distribution conformance.","11/Jan/16 11:10;erans;Commit 2df0a3be66f6fa1067d976d7a05500beff7eda53 fixes the bug in a way similar to what is done for sigma in the same unit test.

New issue was opened to eventually change the tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve accuracy and performance of 2-sample Kolmogorov-Smirnov test,MATH-1310,12925144,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,31/Dec/15 20:03,25/Jan/16 20:27,07/Apr/19 20:38,31/Dec/15 22:22,3.5,,,,,,,3.6,,,0,,,,,,,,"As of 3.5, the exactP method used to compute exact  p-values for 2-sample Kolmogorov-Smirnov tests is very slow, as it is based on a naive implementation that enumarates all n-m partitions of the combined sample.  As a result, its use is not recommended for problems where the product of the two sample sizes exceeds 100 and the kolmogorovSmirnovTest method uses it only for samples in this range.  To handle sample size products between 100 and 10000, where the asymptotic KS distribution can be used, this method currently uses Monte Carlo simulation.  Convergence is poor for many problem instances, resulting in inaccurate results.

To eliminate the need for the Monte Carlo simulation and increase the performance of exactP itself, a faster exactP implementation should be added.  This can be implemented by unwinding the recursive functions defined in Chapter 5, table 5.2 in:

Wilcox, Rand. 2012. Introduction to Robust Estimation and Hypothesis Testing, Chapter 5, 3rd Ed. Academic Press.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-01-25 20:27:49.129,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:49 UTC 2016,,,,,,0|i2qfvj:,9223372036854775807,,,,,,,,"31/Dec/15 22:22;psteitz;Fixed in dcd8015fa (master) and 2983ff81b (3_X).

The functions C and N defined by Wilcox have been implemented with recursive definitions unwound.  Using N, exactP has been reimplemented and is now fast enough to extend the small sample range to cover up to 10000 sample product.  This allowed monteCarloP to be eliminated from use by the ksTest default implementation.  That function has been deprecated.",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RNG: public ""setSeed"" method should not be called from inside the constructor",MATH-1309,12924424,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Won't Fix,erans,erans,erans,27/Dec/15 23:10,04/Feb/16 01:04,07/Apr/19 20:38,04/Feb/16 01:04,3.5,,,,,,,,,,0,,,,,,,,"It is dangerous to call an overridable method from inside a constructor.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-12-28 19:41:24.689,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 03 13:42:01 UTC 2016,,,,,,0|i2qbtr:,9223372036854775807,,,,,,,,"28/Dec/15 17:29;erans;Created private methods ""setSeedInternal"" in commit 4fc5b3402c58d6a4b317bf23b896ea91d22af6fe (master).",28/Dec/15 19:41;rosti.bsd;Why it is dangerous?,29/Dec/15 00:58;erans;http://stackoverflow.com/questions/3404301/whats-wrong-with-overridable-method-calls-in-constructors,31/Dec/15 00:37;erans;Commit 2fcfce303989ae14b5b51f4c9fc92e97bc540ba8 fixes some missed occurrences.,03/Feb/16 13:42;erans;Commits were reverted.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rotation.getAngles produces wrong angles for Cardan RotationOrders,MATH-1303,12923304,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,jboyerIST,jboyerIST,20/Dec/15 20:15,25/Jan/16 20:27,07/Apr/19 20:38,26/Dec/15 20:35,3.5,,,,,,,3.6,,,0,,,,,,,,"See MATH-1302 first. 

Because unit tests for getAngles relies upon the Rotation constructor described in MATH-1302 to work and the constructor does not actually work, getAngles is passing tests when it actually shouldn't.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-12-26 20:35:11.131,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:47 UTC 2016,,,,,,0|i2q4zj:,9223372036854775807,,,,,,,,"26/Dec/15 20:35;luc;The issue has been fixed in git repository (both MATH_3_X branch and master branch).

The getAngles method know accepts both vector operator or frame transform convention. The first convention was the one used op to 3.5, and
explains the order of the angles. The second convention is new in 3.6 and
corresponds to the order you requested.",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rotation constructor with RotationOrder and angles produces wrong rotation,MATH-1302,12923303,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,jboyerIST,jboyerIST,20/Dec/15 20:13,25/Jan/16 20:27,07/Apr/19 20:38,27/Dec/15 12:12,3.5,,,,,,,3.6,,,0,,,,,,,,"Rotation constructor taking (RotationOrder, double, double, double) has the local variable ""composed"" set to an incorrect rotation because the use of r1 and r3 are swapped.",,10800,10800,,0%,10800,10800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-12-21 09:22:32.184,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:55 UTC 2016,,,,,,0|i2q4zb:,9223372036854775807,,,,,,,,"21/Dec/15 09:22;luc;No. The order is the right one.

What you present is a very well known consequence of a convention that is already explained at length in the Javadoc.
You can look at the javadoc of the Rotation(Vector axis, double angle) constructor.
We *know* some people do not like this convention.

The problem is that a rotation can be considered either as a vector operator that moves vectors with respect to a fixed
reference frame, or it can be seen as a frame conversion operator that moves frames while vectors are kept fixed.
Suppose that for example we simply state: rotation r is a 90 degrees rotation around the Z axis. Using the first
convention (fixed frame, moving vectors), the image of vector with coordinates (1, 2, 3) would be vector (-2, 1, 3).
This means that the vector rotates counterclockwise. Using the second convention (the frames rotates), then
the image of vector with coordinates (1, 2, 3) would be vector (2, -1, 3) because the frame rotates counterclockwise,
so the fixed vector appears to rotate in the opposite direction.

Apache Commons Math uses the first convention, because it is focused on representing a vectorial operator.

The other convention, which Apache Commons Math does not use, is more often encountered in the following
fields of applications: frames transforms (typically 3D scenes modelization) or attitude in space flight dynamics.
When people work in these fields (and in fact I do work in space flight dynamics and attitude), then one as to
be aware of the different conventions and as to think that the angle alpha that Apache Commons Math expect
is a perfectly well defined angle that is simply the opposite of the one I have at hand. So when I build my
rotation, I simply have to pass -alpha, and when I retrieve the angle using getAngle, I have to change its
sign after retrieval.

As this topic comes back from time to time, we *may* add an enumerate for specifying the convention in the
two constructors involved (one axis and one angle or one rotation order and three angles) and the symmetrical
getters. However, simply changing from one convention to the other without any hint for the user to which
convention is used is not gonna happen. There are no reasons why the other convention is right and the
current convention is wrong. They are simply that: conventions. They are relevant in different fields of
applications.

What would you think about we add an enumerate, which could be for example VECTOR_OPERATOR_CONVENTION
and FRAMES_CONVERSION_CONVENTION?","21/Dec/15 16:58;jboyerIST;While I like the idea of offering an enumeration that depicts the convention a user may want, a slightly simpler solution using the named constructor idiom might be better. This way, there will be no performance sacrifice while offering the two conventions to users.

Your comments about the conventions makes sense. Even if you do not decide to provide multiple conventions to users, it would be beneficial to update the documentation for the constructor Rotation(RotationOrder, double, double, double) to mention that convention. It also might help to document the convention in the applyTo and applyInverseTo methods.
","26/Dec/15 20:33;luc;The issue has been fixed in git repository (both MATH_3_X branch and master branch).

I have kept the enumerate approach. It allows user to select the convention at runtime. It can be used for example in some intermediate libraries that
cannot know by themselves the conventions the user wants to select.","27/Dec/15 07:17;luc;The applyTo(Rotation) method should also expose the rotation convention.

Currently, the method imposes vector operator convention.","27/Dec/15 12:12;luc;Two methods, compose and composeInverse have been added.
They both allow specifying rotation convention.",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BitsStreamGenerator#nextBytes(byte[]) is wrong,MATH-1300,12923189,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,rosti.bsd,rosti.bsd,19/Dec/15 01:57,25/Jan/16 20:28,07/Apr/19 20:38,31/Dec/15 23:21,3.5,,,,,,,3.6,,,0,,,,,,,,"Sequential calls to the BitsStreamGenerator#nextBytes(byte[]) must generate the same sequence of bytes, no matter by chunks of which size it was divided. This is also how java.util.Random#nextBytes(byte[]) works.

When nextBytes(byte[]) is called with a bytes array of length multiple of 4 it makes one unneeded call to next(int) method. This is wrong and produces an inconsistent behavior of classes like MersenneTwister.

I made a new implementation of the BitsStreamGenerator#nextBytes(byte[]) see attached code.",,,,,,,,,,,,,,,,,MATH-1304,,,,19/Dec/15 02:03;rosti.bsd;MersenneTwister2.java;https://issues.apache.org/jira/secure/attachment/12778648/MersenneTwister2.java,19/Dec/15 02:03;rosti.bsd;TestMersenneTwister.java;https://issues.apache.org/jira/secure/attachment/12778649/TestMersenneTwister.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-12-19 15:50:58.77,,,false,,,,,Important,Patch,,,,,,,,9223372036854775807,,,Mon Jan 25 20:28:00 UTC 2016,,,,,,0|i2q49z:,9223372036854775807,,,,,,,,"19/Dec/15 02:03;rosti.bsd;Attaching the code.

New BitsStreamGenerator#nextBytes(byte[]) implementation, made as an overriden method of MersenneTwister. Copy this nextBytes(byte[]) implementation into the BitsStreamGenerator code.

The second file is a Unit test that demonstrates the wrong and the right nextBytes(byte[]) behaviors.","19/Dec/15 15:50;erans;Thanks for pointing out that problem.

bq. Sequential calls to \[...\] nextBytes must generate the same sequence of bytes, no matter by chunks of which size it was divided.

Attempting to figure out how general the claim is, I've implemented a more general unit test based on your suggestion:

{code}
    private void checkNexBytesChunks(int chunkSize,
                                     int numChunks) {
        final RandomGenerator rg = makeGenerator();
        final long seed = 1234567L;

        final byte[] b1 = new byte[chunkSize * numChunks];
        final byte[] b2 = new byte[chunkSize];

        // Generate the chunks in a single call.                                                                                                                                                                              
        rg.setSeed(seed);
        rg.nextBytes(b1);

        // Reset.                                                                                                                                                                                                             
        rg.setSeed(seed);
        // Generate the chunks in consecutive calls.                                                                                                                                                                          
        for (int i = 0; i < numChunks; i++) {
            rg.nextBytes(b2);
        }

        // Store last 128 bytes chunk of b1 into b3.                                                                                                                                                                          
        final byte[] b3 = new byte[chunkSize];
        System.arraycopy(b1, b1.length - b3.length, b3, 0, b3.length);

        // Sequence of calls must be the same.                                                                                                                                                                                
        Assert.assertArrayEquals(""chunkSize="" + chunkSize + "" numChunks="" + numChunks,
                                 b2, b3);
    }
{code}

The original CM code always fails it, as you observed.
In your example test case (chunkSize=128 and numChunks=8), your fix makes the test pass.

However, it fails whenever the size of the array (argument to ""nextBytes"") is not a multiple of 4.
That is, the ""chunkSize"" does matter.

And ""nextBytes"" in the JDK's {{Random}} class also fails the test when the array's size is not a multiple of 4.

So there are several issues:
# Do you have a reference that the behaviour _must_ be as your described?
# Is the requested behaviour supposed to work only when the array's size is a multiple of 4?  If so, should we add some note about it in the documentation?
# Is there a way to implement ""nextBytes"" in {{BitsStreamGenerator}} so that the property holds for any size?  And if so, should we consider  making the change (given that {{Random}} does not work that way)?
","19/Dec/15 16:12;erans;Independently of the above discussion, I propose to replace the current code of ""nextBytes"" with the following:
{code}
   public void nextBytes(byte[] bytes) {
        final int mask = 0xff;
        final int numBytesChunk = 4;
        final int shift = 8;
        final int numBits = numBytesChunk * shift;

        int index = 0;
        int remainingBytes = bytes.length;
        while (remainingBytes >= numBytesChunk) {
            final int random = next(numBits);
            for (int j = 0; j < numBytesChunk; j++) {
                bytes[index++] = (byte) ((random >> (j * shift)) & mask);
                --remainingBytes;
            }
        }

        if (remainingBytes > 0) {
            final int random = next(remainingBytes * shift);
            for (int j = 0; j < remainingBytes; j++) {
                bytes[index++] = (byte) ((random >> (j * shift)) & mask);
            }
        }
    }
{code}
which I find much more legible (no unrolled loop, no repeated hard-coded constants, explicit variables names).
It however changes the semantics in the part that fills the ""remainingBytes"", where the argument to ""next"" requests only the required number of bits, thus potentially (?) changing the bytes sequence.  All the CM tests still pass, and perhaps there should be a unit test that assert the expected semantics.
","19/Dec/15 17:44;rosti.bsd;My first statement about the same bytes sequence generated by differently sized chunks was too optimistic. Indeed even java.util.Random doesn't guarantee this. But neither java.util.Random nor org.spaceroots.mantissa.random.MersenneTwister make unneeded calls to the nextInt() method (that just calls next(32)). Obviously the same bytes sequence guarantee can't be done without a significant performance degradation.

Also MersenneTwister#next() always generates int but return only asked number of bits.
{code:java}        return y >>> (32 - bits);{code}

BTW the spaceroots's implementation of MersenneTwister was the reference to me. You can download it (including source code) from www.spaceroots.org/downloads.html in their mantissa library.

I still propose to change the nextBytes() code to my version because of the performance. I tested the performance of the four implementations and my implementation is better. I did it by running following test code:
{code:java}
	@Test
	public void test4() {
		long start;
		long end;
		int iterations = 20000000;
		int chunkSize = 123;

		org.spaceroots.mantissa.random.MersenneTwister referenceMt = new org.spaceroots.mantissa.random.MersenneTwister();
		org.apache.commons.math3.random.MersenneTwister cmMt = new org.apache.commons.math3.random.MersenneTwister();
		MersenneTwister2 mt2 = new MersenneTwister2();
		MersenneTwister3 mt3 = new MersenneTwister3();
		byte[] buf = new byte[chunkSize];

		referenceMt.setSeed(1234567L);
		cmMt.setSeed(1234567L);
		mt2.setSeed(1234567L);
		mt3.setSeed(1234567L);

		start = System.currentTimeMillis();
		for (int i = 0; i < iterations; i++) {
			referenceMt.nextBytes(buf);
		}
		end = System.currentTimeMillis();
		System.err.printf(""Spaceroots MersenneTwister %d iterations:\t%8d ms.\n"", iterations, (end - start));

		start = System.currentTimeMillis();
		for (int i = 0; i < iterations; i++) {
			cmMt.nextBytes(buf);
		}
		end = System.currentTimeMillis();
		System.err.printf(""CM 3.5 MersenneTwister %d iterations:\t%8d ms.\n"", iterations, (end - start));

		start = System.currentTimeMillis();
		for (int i = 0; i < iterations; i++) {
			mt2.nextBytes(buf);
		}
		end = System.currentTimeMillis();
		System.err.printf(""Rostislav MersenneTwister %d iterations:\t%8d ms.\n"", iterations, (end - start));

		start = System.currentTimeMillis();
		for (int i = 0; i < iterations; i++) {
			mt3.nextBytes(buf);
		}
		end = System.currentTimeMillis();
		System.err.printf(""Gilles MersenneTwister %d iterations:\t%8d ms.\n"", iterations, (end - start));
	}
{code}
On my (pretty old) computer (Pentium 4 Prescott2M 3.2GHz, 2GB RAM, JDK 7u80 32-bit) I got following results (two runs):
{code}
Spaceroots MersenneTwister 20000000 iterations:	   22937 ms.
CM 3.5 MersenneTwister 20000000 iterations:	   17532 ms.
Rostislav MersenneTwister 20000000 iterations:	   15812 ms.
Gilles MersenneTwister 20000000 iterations:	   24235 ms.
{code}
{code}
Spaceroots MersenneTwister 20000000 iterations:	   27937 ms.
CM 3.5 MersenneTwister 20000000 iterations:	   16735 ms.
Rostislav MersenneTwister 20000000 iterations:	   15547 ms.
Gilles MersenneTwister 20000000 iterations:	   23953 ms.
{code}","19/Dec/15 17:57;erans;bq. BTW the spaceroots's implementation of MersenneTwister was the reference to me.

If you found a discrepancy between CM and Mantissa, we should leave the creator of Mantissa and the main CM developer to discuss that between themselves. :D
","19/Dec/15 19:00;rosti.bsd;According to the code comments both the CM and the Mantissa implementations of MersenneTwister are based on the same code ""developed by Makoto Matsumoto and Takuji Nishimura during 1996-1997"". The main difference between them is that Mantissa implementation extends the standard java.util.Random and uses JDK's nextBytes() method while the CM implementation extends its own BitsStreamGenerator with its own nextBytes() method implementation. So the reference behavior for the nextBytes() method must be the java.util.Random since it is a part of the JDK and so it is the standard.

BTW I  looked at my code again and found that & 0xff operations are not needed. Narrowing Primitive Conversion just discards all unneded bits above the byte, so we don't need to do it by ourself before int to byte type casting.
http://docs.oracle.com/javase/specs/jls/se7/html/jls-5.html#jls-5.1.3

So I propose this new code of the BitsStreamGenerator#nextBytes() method with even a little bit better performance
{code:java}
	@Override
	public void nextBytes(byte[] bytes) {
		int random;
		int index = 0;
		for (int max = bytes.length & 0x7ffffffc; index < max;) {
			random = next(32);
			bytes[index++] = (byte) random;
			bytes[index++] = (byte) (random >>> 8);
			bytes[index++] = (byte) (random >>> 16);
			bytes[index++] = (byte) (random >>> 24);
		}

		if (index < bytes.length) {
			random = next(32);
			while (true) {
				bytes[index++] = (byte) random;
				if (index < bytes.length) {
					random >>>= 8;
				} else {
					break;
				}
			}
		}
	}
{code}","19/Dec/15 19:13;erans;Two runs from me: ;)
{noformat}
nextBytes (calls per timed block: 200000, timed blocks: 100, time unit: ms)
       name      time/call      std error total time      ratio      difference
  Rostislav 2.27207249e-04 5.12523201e-05 4.5441e+03 1.0000e+00  0.00000000e+00
     Gilles 2.41729594e-04 3.13763057e-05 4.8346e+03 1.0639e+00  2.90446893e+02
CommonsMath 2.25267632e-04 3.46659491e-05 4.5054e+03 9.9146e-01 -3.87923460e+01
{noformat}

{noformat}
nextBytes (calls per timed block: 200000, timed blocks: 100, time unit: ms)
       name      time/call      std error total time      ratio      difference
  Rostislav 2.32682559e-04 3.84982763e-05 4.6537e+03 1.0000e+00  0.00000000e+00
     Gilles 2.49178627e-04 3.92662253e-05 4.9836e+03 1.0709e+00  3.29921360e+02
CommonsMath 2.26983378e-04 2.72215448e-05 4.5397e+03 9.7551e-01 -1.13983632e+02
{noformat}
I'm still ~7% slower than you, but that's far from the ~50% which your benchmark indicates.
In your benchmark, your code is faster that the current CM code; but in mine, it's the other way around...","19/Dec/15 20:12;rosti.bsd;We definitly have a different hardware and most likely different operating system and JDK.

I've run my benchmarks several times on JDK 7u80 32-bit with different chunk sizes and numbers of iterations; the results were always similar to what I posted above. Anyway even in your benchmarks your code is ~7% slower, that is +1 voting for my code to be committed :-)","19/Dec/15 23:30;erans;Personally, I'm always in favour of cleaining up in the sense to make the code safer and more understandable, before trying to grab a few milliseconds in a micro-benchmark.
If we want to focus on performance, it would be better to have a realistic use-case (e.g. estimate how much time is spent in ""nextBytes"" relative to the usage of the generated numbers).

I think that we need to determine how to resolve the original issue, and perhaps open another report for the performance improvement.
","20/Dec/15 00:28;psteitz;I am not sure that I buy the fact that this is a bug.  We don't advertise this invariant and I don't see the need to constrain implementations to satisfy it.  Unless I am misunderstanding, we *do* provide seed-consistency with constant output buffer size and that is what practical implementations should depend on.  Is there a practical use case that requires the invariant asked for in this issue?","20/Dec/15 09:55;tn;The Random class does not guarantee that for any chunk size the same sequence is generated. In fact it always gets the next 32 bits and uses as much as needed.

This means that for chunk sizes that are not multiples of 4, the test from Gilles will also fail.

I do see a problem in the nextBytes implementation in BitStreamGenerator, as there are unnecessary calls to next(int) in case the chunk size is a multiple of 4. I think the proposed patches could be further improved into something like that:

{code}
    public void nextBytes(byte[] bytes) {
        final int len = bytes.length;
        for (int i = 0; i < len;) {
            int random = nextInt();
            int n = Math.min(len - i, 4);
            while (n-- > 0) {
                bytes[i++] = (byte) random;
                random >>= 8;
            }
        }
    }
{code}","20/Dec/15 12:07;erans;bq. there are unnecessary calls to next(int) in case the chunk size is a multiple of 4

Indeed, that's what Rostislav proposed to solve in the first place.
Depending on the answers to the questions in my first comment, we could make this as a new feature request.
Or we can explicitly document that consecutive calls to nextBytes won't provide the same sequence as single call (as per the above unit test).
The feature will work when size is a multiple of 4 (just with the fix that removes the additional call when not necessary).

Then there was the issue (or not) of performance.

{noformat}
nextBytes (calls per timed block: 200000, timed blocks: 100, time unit: ms)
       name      time/call      std error total time      ratio      difference
CommonsMath 1.21513910e-04 3.20776342e-05 2.4303e+03 1.0000e+00  0.00000000e+00
  Rostislav 1.23101061e-04 2.58350393e-05 2.4620e+03 1.0131e+00  3.17430180e+01
     Thomas 2.14572528e-04 2.55932836e-05 4.2915e+03 1.7658e+00  1.86117237e+03
    Gilles1 1.28583021e-04 1.04224889e-05 2.5717e+03 1.0582e+00  1.41382220e+02
    Gilles2 1.21604685e-04 9.00060879e-06 2.4321e+03 1.0007e+00  1.81551100e+00
{noformat}

{noformat}
nextBytes (calls per timed block: 200000, timed blocks: 100, time unit: ms)
       name      time/call      std error total time      ratio      difference
CommonsMath 1.24257845e-04 2.47466019e-05 2.4852e+03 1.0000e+00  0.00000000e+00
  Rostislav 1.27903613e-04 2.89947446e-05 2.5581e+03 1.0293e+00  7.29153600e+01
     Thomas 2.23244881e-04 4.19456869e-05 4.4649e+03 1.7966e+00  1.97974071e+03
    Gilles1 1.34765909e-04 2.56119543e-05 2.6953e+03 1.0846e+00  2.10161271e+02
    Gilles2 1.28621420e-04 2.35835928e-05 2.5724e+03 1.0351e+00  8.72714880e+01
{noformat}

""Gilles1"" is my above proposal (minus the unnecessary mask operation).
""Gilles2"" is a variant of Rostislav's code but using static variables rather than hard-coded numbers.

Based on this not really reliable kind of benchmarks, my position is that we should strive to make the code
# not use hard-coded numbers
# self-documenting
# simple
# more documented (not at the Javadoc level but explaining the statements)
# safe (calling an overrideable method in a constructor is not - see e.g. {{MersenneTwister}})
# thread-safe
# fast

Thomas' version is the simplest but the performance obviously (?) suffers.
Rostislav's is (often) the fastest, but it is (much) harder to understand, relatively to the small number of lines.
","20/Dec/15 12:21;luc;In fact, the code from Apache Commons Math come from the mantissa project.
Mantissa is obsolete, it was a single person project (myself). I joined
the Apache Commons project to continue the work there. The complete Mantissa
library was donated to Apache at that time and merged. The Apache version of
this code is therefore the  most up to date one.","20/Dec/15 12:34;luc;My position would be that even if having values independent of chunk size is a nice features,
it is not worth degrading either understandability or performances for it. So just advertising
the fact this property does *not* hold would be fine to me.

I'm also not sure losing much time with Mersenne twister is worth it. This generator was the
best one a few years ago but has been superseded with the WELL family of generators
almost 10 years ago (see http://www.iro.umontreal.ca/~panneton/WELLRNG.html, where
a mink to the reference paper can be found). According to the paper, the Mersenne twister
suffers fro a lack of chaos at the start (i.e. the first few millions generations) that the WELL
generators fix.","20/Dec/15 13:12;erans;But do you agree to remove the unnecessary call to ""next(32)""? Or is there a standard reference (such that the RNG must reproduce the same sequence as that one for the same seed)?

IMO, there is much that could be done to improve the understandability of the code (starting with the simple rule: no hard-coded numbers...).

However, if there are algorithms that are superseded, I of course agree that it's not worth spending any time on them.
Perhaps it is now a good opportunity to indicate in the Javadoc which RNGs are definitely obsolete (and deprecate them?) and which are recommended, so that uninformed people would not start investigating outdated code...
","20/Dec/15 13:21;luc;If it is unnecessary, sure it can be removed.

Indicating that the WELL generators are considered more up to date than the Mersenne Twister would be nice too.","20/Dec/15 15:10;erans;Minimal change committed as 1d635088f697178660b6e1c9a89d2b7d3bbe2d29 in ""master"" branch (4.0).

A unit test shows that the property referred to in the description of this issue passes if the array size is a multiple of 4.
Another test is ""@Ignore""d as a reminder of the limitation.  Can be removed if deemed useless.

We have yet to decide whether to upgrade the Javadoc to mention the feature.

Since the behaviour has changed, perhaps this should not be backported to 3.x (?).

For improving the code (e.g. performance), another report should be created.
","20/Dec/15 18:07;rosti.bsd;WELL generators in CM inherite the same nextBytes() of BitsStreamGenerator. They extend an AbstractWell that extends the BitsStreamGenerator class. The discussed issue of the BitsStreamGenerator#nextBytes() relates to all BitsStreamGenerator descendants. I used the MersenneTwister class just for the demonstration.

After looking at the Gilles commit I've a few questions:
https://git1-us-west.apache.org/repos/asf?p=commons-math.git;a=commitdiff;h=1d635088f697178660b6e1c9a89d2b7d3bbe2d29

1. Why did you do the change so minimal with many unneded operations still in the code instead of taking the code I've proposed? I'm talking about the unneded & 0xff operations, not optimal index incrementation and cases where the last shift right 8 bits isn't needed. If you think my code is hard to understand you may add comments into it. In my opinion this is very simple code. Anyway I think the performance is important.

2. I've just noticed the AbstractRandomGenerator has its own implementation of the nextBytes() method. Why does it need a differently implemented nextBytes()? And why that implementation is so strange? After Gilles commit it's even stranger.

before commit:
{code:java}
     @Override
     public void nextBytes(byte[] bytes) {
         int bytesOut = 0;
         while (bytesOut < bytes.length) {
           int randInt = nextInt();
           for (int i = 0; i < 3; i++) {
               if ( i > 0) {
                  randInt >>= 8;
               }
               bytes[bytesOut++] = (byte) randInt;
               if (bytesOut == bytes.length) {
                   return;
               }
           }
         }
     }
{code}
after commit:
{code:java}
     @Override
     public void nextBytes(byte[] bytes) {
         int bytesOut = 0;
         while (bytesOut < bytes.length) {
             int randInt = nextInt();
             for (int i = 0; i < 3; i++) {
                 if (i > 0) {
                     randInt >>= 8;
                 }
             }
             if (bytesOut < bytes.length) {
                 bytes[bytesOut++] = (byte) randInt;
                 if (bytesOut == bytes.length) {
                     return;
                 }
             }
         }
     }
{code}
The original version before commit is not optimized but this is not the only issue. It uses only three bytes of the random int, doesn't it? And after Gilles commit it uses only one byte of the random int, making many unneeded actions around. Both versions of the AbstractRandomGenerator need more calls to nextInt() than java.util.Random. Both versions look as a bug.

In my opinion both the BitsStreamGenerator and the AbstractRandomGenerator should use the same nextBytes() code that I proposed above.","20/Dec/15 23:15;erans;bq. WELL generators in CM inherite the same nextBytes() of BitsStreamGenerator. They extend an AbstractWell that extends the BitsStreamGenerator class. The discussed issue of the BitsStreamGenerator#nextBytes() relates to all BitsStreamGenerator descendants. I used the MersenneTwister class just for the demonstration.

Sure.
My comment about deprecating the {{MersenneTwister}} class was not meant to imply that the redundant call should not be removed.

bq. 1. Why did you do the change so minimal with many unneded operations still in the code instead of taking the code I've proposed?

Because we generally prefer one commit per issue.
This issue (MATH-1300) was about a redundant call that prevented the property which you expected.
Fixing that first does not prevent further changes.

bq. I'm talking about the unneded & 0xff operations,

I'm going to do that in another commit.
Usually this should also require another report, but I'll just refer to the discussion here in the commit message.

bq. not optimal index incrementation and cases where the last shift right 8 bits isn't needed.

See above discussion.
Please open another issue, as it is not related to your original statement (IMHO: ""wrong != suboptimal"").

bq. If you think my code is hard to understand you may add comments into it. In my opinion this is very simple code.

The ""for"" statement in your code is not easy to understand (for a ""for"" statement, that is).
If I'm not mistaken, a CM unwritten rule for code is ""No hard-coded numbers"".  So I would not just commit your code as is.

bq. Anyway I think the performance is important.

Agreed, just not at all cost, IMO.
Maintenance is also a parameter to take into account, especially if the gain of less clear code is quite small (and subject to erratic variations between HW and JVM).
This does not mean that I prefer to leave it at that; just it should also be another issue.

bq. 2. I've just noticed the AbstractRandomGenerator has its own implementation of the nextBytes() method. Why does it need a differently implemented nextBytes()?

It doesn't; I just grouped it in the same commit because it is the same problem, to be fixed in the most obvious way before further changing the code (another CM rule).

bq. And why that implementation is so strange? After Gilles commit it's even stranger.

Sorry, my mistake; I'll fix ASAP.  Thanks for the review.  I hope I'll get it right in the next commit.

bq. In my opinion both the BitsStreamGenerator and the AbstractRandomGenerator should use the same nextBytes() code

Agreed.
But they are not in the same hierarchy. It's a pity.  If it should be the same code, then the code should be shared.  Suggestions on how to achieve that are welcome.
","21/Dec/15 00:04;ole;bq. Or is there a standard reference (such that the RNG must reproduce the same sequence as that one for the same seed)?

It's nice to know that running the same simulation with the same seed always produces the same result.  Matlab's documentation also states that the seed is used to produce a predictable sequence of numbers.
http://www.mathworks.com/help/matlab/ref/rng.html?requestedDomain=www.mathworks.com","21/Dec/15 00:42;erans;bq. It's nice to know that running the same simulation with the same seed always produces the same result. 

As Phil noted, CM abides by this.
The question was whether _different_ implementations should produce the exact same sequence.
In this case, if the reference also contained the sometimes redundant call, then fixing it in CM would consequently make its sequences differ from the reference.
","21/Dec/15 00:53;rosti.bsd;I made a new Jira MATH-1305 ticket with my code of nextBytes() with better performance.
I've improved its readability and added a few comments. Please review.",31/Dec/15 23:21;erans;Backport in c9c252bf26165e7fafd093cd892af35b23aa8f3f,25/Jan/16 20:28;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,
multistep integrator start failure triggers NPE,MATH-1297,12921671,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,14/Dec/15 14:09,25/Jan/16 20:27,07/Apr/19 20:38,27/Dec/15 12:43,3.5,,,,,,,3.6,,,0,,,,,,,,"Multistep ODE integrators like Adams-Bashforth and Adams-Moulton require a starter procedure.
If the starter integrator is not configured properly, it will not create the necessary number of initial points and the multistep integrator will not be initialized correctly. This results in NullPointErException when the scaling array is referenced later on.

The following test case (with an intentionally wrong starter configuration) shows the problem.

{code}
@Test
public void testStartFailure() {

     TestProblem1 pb = new TestProblem1();
      double minStep = 0.0001 * (pb.getFinalTime() - pb.getInitialTime());
      double maxStep = pb.getFinalTime() - pb.getInitialTime();
      double scalAbsoluteTolerance = 1.0e-6;
      double scalRelativeTolerance = 1.0e-7;

      MultistepIntegrator integ =
          new AdamsBashforthIntegrator(4, minStep, maxStep,
                                                            scalAbsoluteTolerance,
                                                            scalRelativeTolerance);
      integ.setStarterIntegrator(new DormandPrince853Integrator(0.2 * (pb.getFinalTime() - pb.getInitialTime()),
                                                                pb.getFinalTime() - pb.getInitialTime(),
                                                                0.1, 0.1));
      TestProblemHandler handler = new TestProblemHandler(pb, integ);
      integ.addStepHandler(handler);
      integ.integrate(pb,
                             pb.getInitialTime(), pb.getInitialState(),
                             pb.getFinalTime(), new double[pb.getDimension()]);

    }
{code}

Failure to start the integrator should be detected and an appropriate exception should be triggered.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:58 UTC 2016,,,,,,0|i2pux3:,9223372036854775807,,,,,,,,"27/Dec/15 12:43;luc;Fixed in git repository, both in MATH_3_X and master branches.",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DescriptiveStatistics return geometric mean as 0 when product of values is zero, expected to return NaN",MATH-1296,12917527,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,anmol,anmol,02/Dec/15 09:50,03/Dec/15 03:43,07/Apr/19 20:38,03/Dec/15 03:43,3.4.1,,,,,,,,,,0,easyfix,,,,,,,"	@Test
	public void test() {
		DescriptiveStatistics stats = new DescriptiveStatistics();
		stats.addValue(1);
		stats.addValue(2);
		stats.addValue(4);
		System.out.println(stats.getGeometricMean()); //prints 2

		stats.addValue(0);
		System.out.println(stats.getGeometricMean()); //prints 0, expected NaN as per the documentation
}

The class in consideration is: org.apache.commons.math3.stat.descriptive.DescriptiveStatistics
",,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-12-02 12:02:59.221,,,false,,,,,Important,Patch,,,,,,,,9223372036854775807,,,Thu Dec 03 03:43:00 UTC 2015,,,,,,0|i2p5wf:,9223372036854775807,,,,,,,,"02/Dec/15 12:02;erans;The documentation is wrong (in {{DescriptiveStatistics}} and in {{GeometricMean}} classes).
When one of the values is zero, the [geometric mean|http://mathworld.wolfram.com/GeometricMean.html] should be 0.
","02/Dec/15 12:25;anmol;MS Excel returns not a number too when any value is zero in the set. 

Documentation is inconsistent at a couple of other places too:

1) getKustosis() returns NaN when <= 3 values are added, but documentation says it will return 0
2) getSkewness() returns NaN when <=2 values are added, but documentation says it will return 0","02/Dec/15 20:48;Otmar Ertl;Gilles, I agree, it is the documentation that needs to be fixed. Geometric mean should be 0.

By the way, I have checked the documentations for Kurtosis.java and Skewness.java. The documentation for Kurtosis.java already describes that NaN is returned for less than 4 values. The documentation of Skewness.java also needs to be improved, the corresponding information is hidden in the documentation of its evaluate() method.","02/Dec/15 20:58;psteitz;Gilles, Otmar - yes, that is correct.  Tests and code are correct, but the javadoc needs to be fixed.  I am working on this.

Anmol - thanks for the report!","03/Dec/15 03:43;psteitz;Fixed in 
b2627dacc (master)
c23e9fb1a (MATH_3_X)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NonLinearConjugateGradientOptimizer and BracketFinder TooManyEvaluationsException,MATH-1295,12917293,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,luke.lindsay,luke.lindsay,01/Dec/15 15:56,25/Jan/16 20:27,07/Apr/19 20:38,02/Dec/15 14:58,3.5,,,,,,,3.6,4.0,,0,,,,,,,,"I am getting the exception below when using NonLinearConjugateGradientOptimizer.  

 org.apache.commons.math3.exception.TooManyEvaluationsException: illegal state: maximal count (50) exceeded: evaluations
	at org.apache.commons.math3.optim.univariate.BracketFinder.eval(BracketFinder.java:287)
	at org.apache.commons.math3.optim.univariate.BracketFinder.search(BracketFinder.java:181)
	at org.apache.commons.math3.optim.nonlinear.scalar.LineSearch.search(LineSearch.java:127)
	at org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizer.doOptimize(NonLinearConjugateGradientOptimizer.java:283)
	at org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizer.doOptimize(NonLinearConjugateGradientOptimizer.java:47)
	at org.apache.commons.math3.optim.BaseOptimizer.optimize(BaseOptimizer.java:154)
	at org.apache.commons.math3.optim.BaseMultivariateOptimizer.optimize(BaseMultivariateOptimizer.java:66)
	at org.apache.commons.math3.optim.nonlinear.scalar.MultivariateOptimizer.optimize(MultivariateOptimizer.java:64)
	at org.apache.commons.math3.optim.nonlinear.scalar.GradientMultivariateOptimizer.optimize(GradientMultivariateOptimizer.java:74)
	at org.apache.commons.math3.optim.nonlinear.scalar.gradient.NonLinearConjugateGradientOptimizer.optimize(NonLinearConjugateGradientOptimizer.java:245)


NonLinearConjugateGradientOptimizer calls the no argument constructor of BracketFinder which defaults its max evaluations to 50.  I tried changing the source code of BracketFinder so that the default max evaluations is 200 and since making the change have not encountered the problem.  I was wondering if BracketFinder could have its default max evaluations increased or if NonLinearConjugateGradientOptimizer could set a higher max evaluations when it constructs a BracketFinder.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-12-02 14:58:12.042,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:47 UTC 2016,,,,,,0|i2p4gf:,9223372036854775807,,,,,,,,"02/Dec/15 14:58;erans;Default increased to 500.

Commits:
26ad6ac83721fd90e35fe4db4613685b5857fed9 (MATH_3_X)
34646ec9b52192a71e52ffc09cf7fefdd506c48c (master)
",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data race PolynomialUtils::buildPolynomial,MATH-1294,12915180,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,fifol,fifol,22/Nov/15 18:38,25/Jan/16 20:27,07/Apr/19 20:38,23/Nov/15 22:20,4.0,,,,,,,3.6,4.0,,0,easyfix,performance,,,,,,"If you run PolynomialUtilsTest methods concurrently there will occur problem with ComparisonFailure due to incorrect building of coefficient list. https://github.com/apache/commons-math/blob/master/src/main/java/org/apache/commons/math4/analysis/polynomials/PolynomialsUtils.java#L368 should be in synchronized block. Explanation: polynomial of given degree can be created by other thread  and when primary thread access synchronized block, there is already created coefficients for this degree, therefore no coefficients should be added to coefficients list.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-11-23 22:20:08.572,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:53 UTC 2016,,,,,,0|i2orf3:,9223372036854775807,,,,,,,,"23/Nov/15 22:20;tn;Thanks for the report. It has been fixed in the following commits:

 * 3.6: fe23c9b04
 * 4.0: 487ac1980

The synchronization is now also on the coefficient list itself (which must be non-null), and thus there should be a slight performance improvement when creating different polynomials in multiple threads.",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect calculation in org.apache.commons.math3.complex.Complex#multiply,MATH-1289,12913160,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,hsq125,hsq125,15/Nov/15 16:39,15/Nov/15 18:43,07/Apr/19 20:38,15/Nov/15 18:41,3.5,,,,,,,,,,0,easyfix,,,,,,,"Hello guys!

If think there is a bug in the org.apache.commons.math3.complex.Complex#multiply method

Line 536 & 537 below are not correct
{code:title=Complex.java|borderStyle=solid}
return createComplex(real * factor.real - imaginary * factor.imaginary, real * factor.imaginary + imaginary * factor.real);
{code}
It should be
{code:title=Complex.java|borderStyle=solid}
return createComplex(real * factor.real + imaginary * factor.imaginary, imaginary * factor.real-real * factor.imaginary);
{code}
Can you confirm that? Or do I miss something?
Thanks a lot (also, for this excellent library).

Best,
Octave","Linux 3.16.0-44-generic #59-Ubuntu SMP Tue Jul 7 02:07:39 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

java version ""1.8.0_45""
Java(TM) SE Runtime Environment (build 1.8.0_45-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-11-15 18:41:52.975,,,false,,,,,Important,,,,,,,,,9223372036854775807,,,Sun Nov 15 18:41:52 UTC 2015,,,,,,0|i2oezr:,9223372036854775807,,,,,,,,"15/Nov/15 18:41;luc;The expressions used in the library are correct.

With your expressions, computing I^2 would give +1 instead of -1. and multiplication would not be commutative (for example I * 1 would be the opposite of 1 * I).

Remember that in the complex class, the imaginary field is a *real* value holding the multiplicative factor to be applied to I (the square root of -1). In other words z = real + I * imaginary.

So
{noformat}
  z1 * z2 = (r1 + I * i1) * (r2 + I * i2)
          = (r1 * r2 + I * r1 * i2 + I * i1 * r2 + I * I * i1 * i2)
          = (r1 * r2 + I * I * i1 * i2) + I * (r1 * i2 + i1 * r2)
          = (r1 * r2 - i1 * i2) + I * (r1 * i2 + i1 * r2)
{noformat}
which is exactly the expression in the library.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vector is-not-a Point,MATH-1284,12907640,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,RayDeCampo,roman.werpachowski,roman.werpachowski,24/Oct/15 12:35,13/May/17 12:53,07/Apr/19 20:38,13/May/17 12:53,3.5,,,,,,,4.0,,,0,,,,,,,,"The class hierarchy for geometry claims that Vector is-a Point: https://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/geometry/Point.html

This is mathematically incorrect, see e.g. http://math.stackexchange.com/a/645827

Just because they share the same numerical representation, Point and Vector shouldn't be crammed into a common class hierarchy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-10-24 20:48:50.377,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat May 13 12:53:31 UTC 2017,,,,,,0|i2nh53:,9223372036854775807,,,,,,,,"24/Oct/15 20:48;erans;I think that {{Point}} here is just meant to be a base class of {{Vector}}.
It is perhaps imprecise, mathematically. 
Does a ""point"" actually need a ""distance"" to exist?  If ""distance"" is defined in this class, then isn't it equivalent to a ""vector""?
Perhaps Luc could clarify his choice of terms within the Javadoc.
What would you suggest?
","24/Oct/15 20:52;roman.werpachowski;My point (no pun intended) is that Points and Vectors behave differently under translation T:

T(point) != point

T(vector) = vector

So if someone starts coding symmetry operations using this classes, they're in a bind.","24/Oct/15 21:09;erans;I don't follow...

Is there a problem with the current usage of {{Point}} in CM?
Or do you point :) out that, the name ""Point"" being taken by this class, CM won't be able to provide an implementation of some geometry concepts using the expected terminology?

Could you provide a concrete example?
Do you intend to provide such an extension to CM?
","24/Oct/15 21:13;roman.werpachowski;I'm just saying that this setup is obviously wrong for any mathematician. I understand that the intention of these classes is to model geometrical concepts. If this is not the case, then you can ignore my ticket.","24/Oct/15 22:08;erans;bq. obviously wrong for any mathematician

Well, It's not obvious to me, nor to anyone who might have looked at this CM code since version 3.3.

bq. I understand that the intention of these classes is to model geometrical concepts.

I would think so too.
The discussion link which you give above is not authoritative.  Care to share a math reference?
",24/Oct/15 22:12;roman.werpachowski;https://www.encyclopediaofmath.org/index.php/Vector,"24/Oct/15 22:48;erans;Defining ""vector"" in terms of two ""point""s makes it obvious, indeed.  Unless... the first point is the ""origin"".
I think that it is important to show what will go wrong when we wish to implement some geometrical concepts.

Perhaps it would be useful to start a discussion on the ""dev"" ML.
A proposal would be to just remove the {{Point}} interface, if what is currently implemented in CM only needs the ""vector"" concept.
","25/Oct/15 09:20;luc;I agree that vector is not a point.
This issue was discussed (shortly) on the developers list on May 22nd 2011.
You can look here <http://mail-archives.apache.org/mod_mbox/commons-dev/201105.mbox/browser> for
the thread or here <http://commons.markmail.org/search/list:org.apache.common.dev+%22affine+and+vector+spaces%22>.

One argument for merging was that the difference was too subtle and merging the two concepts was quite common.
The other argument was that our interface is not restricted to Euclidean space but also encompasses spherical
geometry (we do use it for BSP trees on the 1-sphere and on the 2-sphere for example).

So we decided to stick to a single representation and in the Vector Javadoc we state:

 This interface represents a generic vector in a vectorial space or a point in an affine space.

This implies users are free to decide by themselves what is the semantic of the object they use.

The Transform interface defined in the lower partitioning package needs to know about the semantics.
Therefore, it specifically states it represents a transform in *affine* space. On the other hand, the
Rotation class in the euclidean.threed package is an operator acting in a vector space, and it does
*not* implement the aforementioned Transform interface.

I would indeed be glad to separate the two concepts, but has to think really a lot about it
in spherical geometry.",12/Apr/17 11:08;chtompki;[~erans] Should this issue remain open? It feels closable.,"12/Apr/17 11:19;erans;I don't know. It's not quite clear that there isn't anything to fix.

This is the kind of issues that IMHO calls for separation of concerns (i.e. separate components for separate domains of expertise).","22/Apr/17 22:45;RayDeCampo;I agree with the issue as reported; from a mathematical POV a vector is not a point.

From a CS POV I am having trouble seeing the value that having the interface {{Vector}} extending the interface {{Point}} brings.  In fact, by having {{Vector}} extend {{Point}}, one ends up with a method in your {{Vector}} implementation where one is ostensibly calculating the ""distance from a vector to a point"".  This isn't really satisfying to me from a mathematical or CS perspective.  The implementations of the {{Point}} {{distance()}} method in {{Vector}} implementations all immediately cast the {{Point}} to a {{Vector}} implementation.

I will prepare a branch with a proposal for resolving this issue.
","25/Apr/17 23:09;RayDeCampo;Please take a look at the {{feature-MATH-1284}} branch.

First, I made it so that {{Vector}} no longer extends {{Point}}.  Then I added the appropriate methods from the {{Point}} interface to the {{Vector}} interface.

The only implementations which represented problems were the {{Vector?D}} classes.  I noticed that the existing code dealing with these implementations relied pretty heavily on casting back and forth between {{Point}} and {{Vector}}, so the most prudent thing seemed to be supplying a class which implements both.

I decided on {{Coordinates?D}} for the ""new"" classes (these are really just {{Vector?D}} renamed).  Here I am using the fact that point a vector and a point in finite dimensional Euclidean space can be represented by a set of coordinates.  (Keeping the classes with {{Vector?D}} name would feel like we hadn't really addressed the issue.)

If it is a problem that the {{Vector?D}} classes are just dropped we could introduce them as an intermediate interface (or even as a concrete class which {{Coordinates?D}} extends but that feels less satisfying).

Having {{Coordinates?D}} implement both interfaces led to some method calls being ambiguously defined.  Here I just removed one of the methods.  I am thinking now it would have been better just to create an implementation accepting the {{Coordinate?D}} class.

In any case, there would be more work in terms of cleaning up and documentation, this is not meant to be a finished product but a basis for discussion.
","26/Apr/17 00:28;erans;I replied to the commit on the ML, asking whether ""Coordinates?D"" actually meant _Cartesian_ coordinates in the code.","06/May/17 15:11;RayDeCampo;OK, I have made the change to {{Cartesian?D}}.  I have also restored the {{Vector?D}} as abstract classes which {{Cartesian?D}} extend - I thought that would be easier on existing client code.

I also made an attempt to update the javadoc and supporting files.

Please take another look at the {{feature-MATH-1284}} branch for review.",13/May/17 12:53;RayDeCampo;Resolution applied in commit 7a59c0af26177cf69e702eaac85471e54762f664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gamma function computation,MATH-1283,12907019,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,jnd77,jnd77,22/Oct/15 08:40,25/Jan/16 20:27,07/Apr/19 20:38,22/Oct/15 20:34,3.5,4.0,,,,,,3.6,4.0,,0,,,,,,,,"In the gamma method, when handling the case ""absX > 20"", the computation of gammaAbs should replace ""x"" (see code below with x in bold) by ""absX"".
For large negative values of x, the function returns with the wrong sign.

final double gammaAbs = SQRT_TWO_PI / *x* *
                                     FastMath.pow(y, absX + 0.5) *
                                     FastMath.exp(-y) * lanczos(absX);",All,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-10-22 20:34:05.115,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:56 UTC 2016,,,,,,0|i2ndbj:,9223372036854775807,,,,,,,,"22/Oct/15 20:34;tn;Fixed in the following commits:

 * 3.6: 40f35da56
 * 4.0: 9e0c5ad4b

Thanks for the report and suggested fix!",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BOBYQA trsbox infinite loop,MATH-1282,12906321,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,Tillmann Gaida,Tillmann Gaida,20/Oct/15 17:41,18/Apr/17 15:17,07/Apr/19 20:38,,3.2,,,,,,,4.0,,,0,,,,,,,,"Apparently the BOBYQA optimizer can get stuck in the trsbox method. This happened to us out of the blue after millions of successful optimizations.

Unfortunately our target function has too many dependencies, so I can't provide a unit test for the full optimization. I have however created a unit test which isolates the call to trsbox: http://pastebin.com/wYPVS3SC

I took a quick look at the code, but it looks like one can't understand a thing without having read the original paper. Can someone help?",,,,,,,,,,,,,,,,,MATH-1375,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-10-21 12:52:03.555,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 27 20:51:31 UTC 2015,,,,,,0|i2n91z:,9223372036854775807,,,,,,,,"21/Oct/15 12:52;erans;Hi.

If you look at the linked issues, you'll see that previous attempts to improve the port of this code did not quite succeed. :(
The warning in the [release notes|http://commons.apache.org/proper/commons-math/changes-report.html] has been standing through 8 releases...
","27/Oct/15 20:51;tn;Thanks for providing an isolated test case, but I fear that this will not be enough to track down the problem. Investigating the test case shows that the gradientAtTrustRegionCenter variable has numbers of very large magnitude (10e100 and larger). Subsequent calculations involving this parameters result in Infinity and NaN, which might be the final cause for the infinite loop.

Looking at the unit tests, I could not observe similar large values for the gradient. Would it be possible for you to trace the evolution of this parameter?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DescriptiveStatistics methods return NaN for operations on arrays with single items,MATH-1280,12903012,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,mgaspar,mgaspar,07/Oct/15 14:29,25/Jan/16 20:27,07/Apr/19 20:38,07/Oct/15 19:31,3.4,,,,,,,3.6,4.0,,0,,,,,,,,"1. Create a DescriptiveStatistics object passing an array with only one value
2. Call getMean() method
3. Call getStandardDeviation() method
4. Both return NaN",,,,,,,,,,,,,MATH-1252,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-10-07 15:26:45.047,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:54 UTC 2016,,,,,,0|i2moxz:,9223372036854775807,,,,,,,,"07/Oct/15 15:26;erans;I cannot reproduce this behaviour on the development code (for upcoming versions 3.6 and 4.0).

Please let us know whether the bug exists on version 3.5 (current supported version):
  http://commons.apache.org/proper/commons-math/download_math.cgi
",07/Oct/15 15:56;mgaspar;Version 3.5 is also affected. I will download version 3.6 and test it.,07/Oct/15 19:31;tn;This is a duplicate of the referenced issue and has already been fixed.,"07/Oct/15 19:37;mgaspar;Thanks, Thomas. ",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect Kendall Tau calc due to data type mistmatch,MATH-1277,12888682,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,marcr1230,marcr1230,18/Sep/15 21:39,25/Jan/16 20:27,07/Apr/19 20:38,20/Sep/15 08:15,3.5,,,,,,,3.6,4.0,,0,correlation,Kendall,tau,,,,,"The Kendall Tau calculation returns a number from -1.0 to 1.0

due to a mixing of ints and longs, a mistake occurs on large size columns (arrays) passed to the function. an array size of > 50350 triggers the condition in my case - although it may be data dependent

the ver 3.5 library returns 2.6 as a result (outside of the defined range of Kendall Tau)

with the cast to long below - the result reutns to its expected value


commons.math3.stat.correlation.KendallsCorrelation.correlation


here's the sample code I used:
I added the cast to long of swaps in the 

			int swaps = 1077126315;
			 final long numPairs = sum(50350 - 1);
			    long tiedXPairs = 0;
		        long tiedXYPairs = 0;
		        long tiedYPairs = 0;
		        
		  final long concordantMinusDiscordant = numPairs - tiedXPairs - tiedYPairs + tiedXYPairs - 2 * (long) swaps;
	        final double nonTiedPairsMultiplied = 1.6e18;
	        double myTest = concordantMinusDiscordant / FastMath.sqrt(nonTiedPairsMultiplied);
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-09-20 08:15:21.383,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:57 UTC 2016,,,,,,0|i2l52v:,9223372036854775807,,,,,,,,"20/Sep/15 08:15;Otmar Ertl;Thanks for reporting! Fixed bug in following commits:
* 81ce1b183aa4fb56e7710ebe0274740c268118fa (3.6)
* fb0078159d2463da149de54018fca79a9447153e (4.0)",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EventHandler and StepHandler interfaces do not provide derivatives,MATH-1275,12873093,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,luc,luc,16/Sep/15 20:12,16/Sep/15 20:12,07/Apr/19 20:38,,3.5,,,,,,,4.0,,,0,,,,,,,,"The EventHandler and StepHandler interfaces allow passing the current state vector to user code, but not its derivatives. This is a pity because the integrator does know these derivatives as they are the basis from which everything else is computed. The data is lying around, just not passed to user.

This is a design issue and as it affects user interfaces, it cannot be fixed before 4.0. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,2015-09-16 20:12:41.0,,,,,,0|i2k9pb:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FastMath.pow(double, long) enters an infinite loop with Long.MIN_VALUE",MATH-1272,12862964,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,Qualtagh,Qualtagh,10/Sep/15 06:07,25/Jan/16 20:27,07/Apr/19 20:38,10/Sep/15 09:15,3.5,4.0,,,,,,3.6,4.0,,0,patch,,,,,,,"FastMath.pow(double, long) enters an infinite loop with Long.MIN_VALUE. It cannot be negated, so unsigned shift (>>>) is required instead of a signed one (>>).",,,,,,,,,,,,,,,,,,,,,10/Sep/15 06:08;Qualtagh;MATH-1272.patch;https://issues.apache.org/jira/secure/attachment/12755072/MATH-1272.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-09-10 09:15:23.918,,,false,,,,,Patch,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:51 UTC 2016,,,,,,0|i2jzcn:,9223372036854775807,,,,,,,,"10/Sep/15 09:15;luc;Fixed in git repository (commit d93c95d for master branch, commit 252a013 for MATH_3_X branch).

Thanks for the report and for the patch!",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath.exp may return NaN for non-NaN arguments,MATH-1269,12862092,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,Otmar Ertl,Otmar Ertl,06/Sep/15 11:49,25/Jan/16 20:27,07/Apr/19 20:38,05/Nov/15 20:30,4.0,,,,,,,3.6,4.0,,0,,,,,,,,"I have observed that FastMath.exp(709.8125) returns NaN. However, the exponential function must never return NaN (if the argument is not NaN). The result must always be non-negative or positive infinity.",,,,,,,,,,,,,,,,,,,,,23/Oct/15 19:34;Otmar Ertl;MATH-1269-fix-tempC-infinity.patch;https://issues.apache.org/jira/secure/attachment/12768389/MATH-1269-fix-tempC-infinity.patch,10/Sep/15 09:54;sebb@apache.org;MATH-1269.patch;https://issues.apache.org/jira/secure/attachment/12755095/MATH-1269.patch,09/Sep/15 11:21;erans;MATH-1269.patch;https://issues.apache.org/jira/secure/attachment/12754885/MATH-1269.patch,22/Oct/15 21:21;tn;MATH-1269_fix_z.patch;https://issues.apache.org/jira/secure/attachment/12768140/MATH-1269_fix_z.patch,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2015-09-09 11:21:42.832,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:55 UTC 2016,,,,,,0|i2ju1r:,9223372036854775807,,,,,,,,09/Sep/15 11:21;erans;Any objection to the attached trivial patch?,"09/Sep/15 16:52;Otmar Ertl;I have an objection. The patch changes the result of FastMath.exp(709.0001) from 8.219229343394329E307 to Double.POSITIVE_INFINITY. Although this kind of error is less harmful than that caused by the bug, I would prefer a more rigorous solution.","09/Sep/15 18:26;sebb@apache.org;I agree that the solution could be improved, as it artificially restricts the range of possible values.

If the result of the calculation is NaN (and the input was not NaN) can't you just change the NaN to Infinity?",09/Sep/15 19:22;Otmar Ertl;In my opinion simply changing NaNs to infinity can only be a short-term solution. Such a transformation at the end of this function is like admitting that we do not really know how our code works. This is not acceptable for such an essential function as the exponential function.,"09/Sep/15 23:27;sebb@apache.org;It looks like the problem is that the function works fine up to the maximum that can be represented in a double.

It agrees with j.l.Math up to 709.782712893384 (0x1.62e42fefa39efp9) and for many numbers after that point it shows Infinity also in agreement with j.l.Math.
However eventually it starts returning NaN instead of Infinity, whereas j.l.Math continues to return Infinity.

I think there are a couple of options here:
1) choose a number between 709 and 710 which is correctly calculated in the Infinite range and use that as the max
2) convert NaN to Infinity as appropriate.

In either case it would be a good idea to add a test to show that Math and FastMath agree for doubles which are around the max value that can be calculated. Ditto for expm1().
","10/Sep/15 00:30;erans;Why _choose_ a number?
Didn't you determine it precisely above?

Isn't just a matter of changing the test from
{code}
if (intVal > 709)
{code}
to
{code}
if (x > 709.782712893384)
{code}
?","10/Sep/15 00:38;sebb@apache.org;[Didn't you just choose a number above? ...]

But yes, one possible code fix is as you suggest, along with a unit test to show that the number has been properly determined.","10/Sep/15 05:00;Qualtagh;https://github.com/apache/commons-math/pull/14

    If (intVal > 709 || intVal == 709 && x >= 709.782712893384)

I had the same thoughts)).
Test included.","10/Sep/15 08:27;sebb@apache.org;Thanks. 
However I'm not sure it is necessary to check intVal at all - why not just check x?","10/Sep/15 09:23;sebb@apache.org;One should be able to use {{if (x > Math.log(Double.MAX_VALUE))}}
(Of course the value should be a static constant)",10/Sep/15 09:54;sebb@apache.org;Suggested fix - there was already a private constant with the correct value in the FastMath class,"10/Sep/15 19:06;Otmar Ertl;+1 for 2nd patch, I have run the test over the range (MAX_LONG - 100000000, MAX_LONG + 100000000), I think that should be enough to assume that there is a clean transition from finite to infinite.","22/Oct/15 21:21;tn;I propose another patch for this issue.
The polynomial expansion for z = exp(epsilon) - 1.0 is actually returning wrong results in case epsilon = 0.

The patch handles this case and further fixes the calculation of (1+z)(tempA+tempB) in this case (as z is 0, we only have to add tempA and tempB).

This would fix your case and other potential cases in the range [0, 709] for which the epsilon is also 0.",23/Oct/15 19:34;Otmar Ertl;One more proposal. The problem is the calculation of (1+z)(tempA+tempB) if tempA is infinite. In this case tempC is also infinite. If z < 0 at the same time we get NaN for tempC*z + tempB + tempA. The proposed patch returns infinity for the case tempC is positive infinite. In all other cases tempC*z cannot be negative infinite and the evaluation of tempC*z + tempB + tempA is not a problem even if tempA or tempB are positive infinite.,"04/Nov/15 21:51;tn;your latest patch should be more performant for the same result, thus I would be in favor to apply it.","05/Nov/15 20:29;Otmar Ertl;fixed
3.6: 8aecb842d32464a98eaab722da84902f8126957b
4.0: a94ff90ab6cd2d92ccb2eb1fd7913b4e5256f02b",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
split and side methods may be inconsistent in BSP trees,MATH-1266,12861849,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,04/Sep/15 13:55,25/Jan/16 20:27,07/Apr/19 20:38,06/Sep/15 18:24,3.5,,,,,,,3.6,4.0,,0,,,,,,,,"In BSP trees, there are two related methods dealing with the relative position of a sub-hyperplane and an hyperplane: side and split.

sub.side(hyperplane) returns an enumerate (PLUS, MINUS, BOTH, HYPER) telling the relative position of the syb-hyperplane with respect to the hyperplane.

sub.split(hyperplane) splits the sub-hyperplane in two parts, one on the plus side of the hyperplane and one on the minus side of the hyperplane.

These methods should be consistent, i.e. when side returns BOTH, then split should return two non-null parts. This fails in the following case:

{code}
    @Test
    public void testSideSplitConsistency() {

        double tolerance = 1.0e-6;
        Circle hyperplane = new Circle(new Vector3D(9.738804529764676E-5, -0.6772824575010357, -0.7357230887208355),
                                       tolerance);
        SubCircle sub = new SubCircle(new Circle(new Vector3D(2.1793884139073498E-4, 0.9790647032675541, -0.20354915700704285),
                                                 tolerance),
                                      new ArcsSet(4.7121441684170700, 4.7125386635004760, tolerance));
        SplitSubHyperplane<Sphere2D> split = sub.split(hyperplane);
        Assert.assertNotNull(split.getMinus());
        Assert.assertNull(split.getPlus());
        Assert.assertEquals(Side.MINUS, sub.side(hyperplane));

    }
{code}

Here, side returns BOTH but the plus part is null. This is due to the plus
side being smaller than the tolerance (1.0e-6 here) and filtered out in the split methods whereas it is not filtered out in the side method, which has a slightly different algorithm. So instead of returning BOTH, side should return MINUS as it should filter out the too small plus part.

In fact, it is only one particular case, the same could occur in other spaces (Euclidean or Spherical, and on various dimensions).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:49 UTC 2016,,,,,,0|i2jsl3:,9223372036854775807,,,,,,,,"06/Sep/15 18:24;luc;Fixed in git repository (commit 2091cfb for branch master, commit 50c5eae for branch MATH_3_X)",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Heap overflow in org.apache.commons.math4.special.BesselJ,MATH-1262,12858643,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,,,neilwalkinshaw,neilwalkinshaw,25/Aug/15 14:49,12/Apr/17 11:02,07/Apr/19 20:38,,4.0,,,,,,,4.0,,,0,newbie,performance,,,,,,"This test case:
{code:java}
import org.apache.commons.math4.special.BesselJ;

public class BesselJHeapBug {
    public static void main(String[] args) {
        BesselJ.value(1182054491, 3.589635306407856E-8D);
    }
}
{code}

throws the following exception:

{code}
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
	at org.apache.commons.math4.special.BesselJ.rjBesl(BesselJ.java:247)
	at org.apache.commons.math4.special.BesselJ.value(BesselJ.java:161)
	at BesselJHeapBug.main(BesselJHeapBug.java:5)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-08-25 15:47:41.576,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 08 15:02:39 UTC 2015,,,,,,0|i2jcf3:,9223372036854775807,,,,,,,,"25/Aug/15 15:47;Otmar Ertl;The current implementation allocates a double array with size equal to order+1. Hence, the OutOfMemoryError is not very surprising in this case with order equal to 1182054491. To fix this, we would need to implement a more space efficient algorithm like the one used in boost, see https://github.com/mirror/boost/blob/master/boost/math/special_functions/detail/bessel_jn.hpp.","30/Aug/15 18:57;Otmar Ertl;-This [paper|http://www.cl.cam.ac.uk/~jrh13/papers/bessel.pdf] could be interesting.- 

EDIT:
The paper states that the approach could be extended to moderate orders. However, I doubt that the method could handle large orders such as 1182054491 as given in the test case.","06/Sep/15 14:02;Otmar Ertl;The paper [C.Schwartz, Numerical Calculation of Bessel Functions, 2012|http://arxiv.org/pdf/1209.1547] demonstrates an interesting approach to calculate Bessel functions. Unfortunately, it does not contain an algorithm that can be implemented straightforwardly.","08/Sep/15 09:58;neilwalkinshaw;Hi Otmar,

I suppose that the best pragmatic solution in the interim might be to declare an Exception that is thrown for numbers beyond a given limit?","08/Sep/15 10:14;sebb@apache.org;It's not going to be possible in general to determine what limit to use. What works on one system may fail on another, and vice versa.
We don't want to artificially limit what users can do just in order to avoid heap overflow.

I suggest we update the Javadoc to note that the implementation can require a lot of memory (and maybe give some examples).",08/Sep/15 15:02;Otmar Ertl;+1 for updating Javadoc and adding a note that memory needs are proportional to the order.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Overflow checks in Fraction multiply(int) / divide(int),MATH-1261,12857523,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,osamu.ikeuchi,osamu.ikeuchi,20/Aug/15 12:47,25/Jan/16 20:28,07/Apr/19 20:38,20/Aug/15 22:31,,,,,,,,3.6,4.0,,0,,,,,,,,"The member methods multiply(int) / divide(int) in the class org.apache.commons.math3.fraction.Fraction do not have overflow checks.

{code:java}
return new Fraction(numerator * i, denominator);
{code}

should be

{code:java}
return new Fraction(ArithmeticUtils.mulAndCheck(numerator, i), denominator);
{code}

or, considering the case gcd(i, denominator) > 1,

{code:java}
return multiply(new Fraction(i));
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-08-20 22:31:02.733,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:28:01 UTC 2016,,,,,,0|i2j5mv:,9223372036854775807,,,,,,,,"20/Aug/15 22:31;erans;Fixed (using your second suggestion) in the following commits:
0820703df043ca5df06fe808bc2998296e7bbcc5 (3.6)
4c4b3e2e32ddae35e4c1a6ffce1cd2b2eafa958b (4.0)

Thanks for the report and fix!
",20/Aug/15 22:57;osamu.ikeuchi;Hi Gilles. Thank you very much for your quick reply and fixing.,25/Jan/16 20:28;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Negative max in class Incrementor,MATH-1259,12857492,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,gjahanagirova,gjahanagirova,20/Aug/15 09:39,25/Jan/16 20:27,07/Apr/19 20:38,30/Aug/15 16:42,,,,,,,,3.6,4.0,,0,,,,,,,,"In class Incrementor in the org.apache.commons.math4.util package it is possible to pass a negative number for the max variable. Given that the count variable is initialised to 0 by default, it means that canIncrement() method will always return false. 

I wonder whether it is an acceptable behaviour, given that documentation says that Incrementor is a utility that increments a counter until a maximum is reached. And in the case of negative max this maximum is not reachable at all. 

",,,,,,,,,,,,,,,,,MATH-913,,,,28/Aug/15 02:09;erans;IntegerSequence.java;https://issues.apache.org/jira/secure/attachment/12752918/IntegerSequence.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-08-20 11:22:41.819,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:57 UTC 2016,,,,,,0|i2j5fz:,9223372036854775807,,,,,,,,"20/Aug/15 11:22;erans;The concept was of a ""counter"" rather than an ""incrementor""; so the name is a bit misleading.
I can see that it could become an incrementor (where one would specify initial and final values, rather than count number).

But please raise this issue on the ""dev"" ML to get more opinions on whether this change is needed (preferably with a practical use-case), or whether it would suffice to forbid negative counts (by throwing an exception).
","25/Aug/15 14:42;gjahanagirova;Hi!

Can you, please, provide the link to ""dev"" ML?",25/Aug/15 22:19;erans;http://commons.apache.org/proper/commons-math/mail-lists.html,"28/Aug/15 02:09;erans;To avoid compatibility issues, I propose to implement the correct functionality in a new class (see attached file).
Please review.

The old {{Incrementor}} would be deprecated in 3.6 and removed in 4.0.
",30/Aug/15 14:27;erans;New class introduced in commit 818533e92bfb1c142ae4ce5be18ab92d1ae17025.,"30/Aug/15 14:32;erans;Fixed in 4.0
Yet to be backported to 3.6.
","30/Aug/15 16:42;erans;Commit b54dbfb6efa65e75bfd5f7c1a1322795eb94cd0f (3.6)
",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compute() method in classes in org.apache.commons.math4.ml.distance package,MATH-1258,12857202,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,gjahanagirova,gjahanagirova,19/Aug/15 12:51,25/Jan/16 20:28,07/Apr/19 20:38,20/Aug/15 20:02,,,,,,,,3.6,4.0,,0,,,,,,,,"Hi!

There are five classes CanberraDistance, ChebyshevDistance, EarthMoversDistance, EuclideanDistance and ManhattanDistance in org.apache.commons.math4.ml.distance package, which compute different types of distances. Each of them contains method compute(double[] a, double[] b) that accepts two double arrays as variables.

However, if the lengths of array a is greater than the length of array b, the method compute() in all the five classes produces java.lang.ArrayIndexOutOfBoundsException.

For example,
         private void test0() {
           CanberraDistance distance = new CanberraDistance();
    
            final double[] a = { 1, 2, 3, 4, 9, 4 };
            final double[] b = { -5, -6, 7, 4, 3 };
            distance.compute(a, b);
       }",,,,,,,,,,,,,,,,,,,,,19/Aug/15 17:49;Otmar Ertl;patch.diff;https://issues.apache.org/jira/secure/attachment/12751309/patch.diff,20/Aug/15 00:58;erans;pre-MATH-1258.patch;https://issues.apache.org/jira/secure/attachment/12751383/pre-MATH-1258.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-08-19 13:08:26.008,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:28:01 UTC 2016,,,,,,0|i2j3ov:,9223372036854775807,,,,,,,,19/Aug/15 13:08;tn;Closing this as invalid as the javadoc of DistanceMeasure#compute clearly states that the provided arrays need to have the same dimension.,"19/Aug/15 13:17;gjahanagirova;Hi!

Well, it is good that documentation states that, but why not to add a simple check into the code which will throw an exception with a meaningful message?","19/Aug/15 17:49;Otmar Ertl;I also do not like throwing an ArrayIndexOutOfBoundsException due to 2 reasons:

* The current behavior is asymmetric. distance(a,b) succeeds while distance(b, a) fails with an exception, if a and b have different lengths. Since distance functions are known to be symmetric, their behavior should be symmetric too, even in case of invalid parameters.
* The element-by-element functions in MathArrays throw a DimensionMismatchException instead of an ArrayIndexOutOfBoundsException. For the sake of consistency, I think throwing a DimensionMismatchException is more appropriate for distance functions as well.

Please see attached patch.
","20/Aug/15 00:58;erans;Then, I'd propose that the patch I've attached be applied first.
It adds a ""checkEqualLength(double[], double[])"" utility to {{MathArrays}}.
","20/Aug/15 05:27;tn;No objections against the patches, they certainly make things more consistent.","20/Aug/15 11:10;erans;My patch was applied in the following commits:
9cb16d5b1e02ad41fe8481cb7e74fa57da5f08e7 (3.6)
f70741c9b216d3836d047c542847920d4778cf9d (4.0)
","20/Aug/15 16:05;Otmar Ertl;Committed my patch:
7934bfea106206d2840ba062eef105001601588a (3.6)
5ca0a1c3564d35293a5ecf03e5f32e6f0f6f445c (4.0)",20/Aug/15 18:30;Otmar Ertl;I have just added the missing entries to changes.xml. I think we can close this issue now. It seems that I do not have the required rights to do that. ,"20/Aug/15 19:59;erans;I've just added you in the ""committers"" list (in JIRA).  Could you try again to resolve the issue?
",20/Aug/15 20:04;Otmar Ertl;Thanks! It worked now.,25/Jan/16 20:28;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NormalDistribution.cumulativeProbability() suffers from cancellation,MATH-1257,12857050,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,murphyw,murphyw,18/Aug/15 22:31,25/Jan/16 20:27,07/Apr/19 20:38,19/Aug/15 21:16,3.5,,,,,,,3.6,4.0,,0,,,,,,,,"I see the following around line 194:
{noformat}
        return 0.5 * (1 + Erf.erf(dev / (standardDeviation * SQRT2)));
{noformat}

When erf() returns a very small value, this cancels in the addition with the ""1.0"" which leads to poor precision in the results.

I would suggest changing this line to read more like:
{noformat}
return 0.5 * Erf.erfc( -dev / standardDeviation * SQRT2 );
{noformat} 

Should you want some test cases for ""extreme values"" (one might argue that within 10 standard deviations isn't all that extreme) then you can check the following: http://www.jstatsoft.org/v52/i07/ then look in the v52i07-xls.zip at replication-01-distribution-standard-normal.xls

I think you will also find that evaluation of expressions such as {noformat}NormalDistribution( 0, 1 ).cumulativeProbability( -10.0 );{noformat}
are pretty far off.",,,,,,,,,,,,,,,,,,,,,19/Aug/15 16:23;erans;MATH-1257.patch;https://issues.apache.org/jira/secure/attachment/12751298/MATH-1257.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-08-19 11:54:46.328,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:54 UTC 2016,,,,,,0|i2j2rr:,9223372036854775807,,,,,,,,"19/Aug/15 11:54;erans;bq. return 0.5 * Erf.erfc( -dev / standardDeviation * SQRT2 );

Using either the current code or
{code}
return 0.5 * Erf.erfc(-dev / (standardDeviation * SQRT2));
{code}
the unit tests pass.

bq. \[...\] test cases for ""extreme values"" \[...\]

A patch with a test case that shows the problem would be welcome.

","19/Aug/15 15:35;murphyw;I'm not sure where your test cases are in your src, or how one runs them. A suitable test is to see if 
{noformat}NormalDistribution( 0, 1 ).cumulativeProbability( -10.0 );{noformat}

is evaluated to 0.0 (I think incorrect) or like 7.61985E-24 (I think close/approximately correct). 

This should really be reviewed by someone sharper at math than me, so even were I to know where to find the tests and add one as a patch, this needs someone more familiar with the problem domain to assure this is the right move. It looks good to me, however, this is insufficient assurance.","19/Aug/15 16:15;erans;Well, you found a list of reference values; do you mean that you don't trust them?

Otherwise, the unit test would amount to put them in an array of pairs (x, N_cumulative\(x\)) and compare the reference values with the value computed by the library...
","19/Aug/15 16:23;erans;I've added a test case for x=-10 and the above reference value.

CM's current code does not pass the test.

Your suggested change make it pass, so unless there is an objection, I'll apply the attached patch.
","19/Aug/15 17:37;murphyw;My doubts are based mostly on my not being a statistician. My suggested change was researched on wikipedia and wolfram. 

Glad that you like the mod, I would be in favor of applying this patch. Thanks!","19/Aug/15 18:07;Otmar Ertl;+1 for the patch, it eliminates one source of error. For values much smaller than -1 the regularizedGammaQ function evaluates to a very small positive values. The computation was like 0.5*((regularizedGammaQ - 1) + 1). With the patch the computation is now 0.5*regularizedGammaQ, obviously improving accuracy.","19/Aug/15 21:16;erans;Change applied in the following commits:
03178c8b15f1b522b98ded0f83cfb0e79f5ec4d3 (4.0)
49a9e6e8742cb6e05abf0219705338d95f732e12 (3.6)
",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Interval class upper and lower check,MATH-1256,12856561,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,gjahanagirova,gjahanagirova,17/Aug/15 09:25,25/Jan/16 20:28,07/Apr/19 20:38,17/Aug/15 11:32,,,,,,,,3.6,4.0,,0,,,,,,,,"In class Interval, which is in the package org.apache.commons.math4.geometry.euclidean.oned it is possible to pass the value for variable upper  less than the value of variable lower, which is logically incorrect and  also causes the method getSize() to return negative value.

For example:

 @Test
  public void test1()  throws Throwable  {
      Interval interval0 = new Interval(0.0, (-1.0));
      double double0 = interval0.getSize();
      assertEquals((-1.0), double0, 0.01D);
  }

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-08-17 11:32:29.762,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:28:00 UTC 2016,,,,,,0|i2izvj:,9223372036854775807,,,,,,,,"17/Aug/15 11:32;erans;Thanks for the report.

Boundary check added in the following commits:
41f297809965523fcd021bef20b304b3584d9b4f (4.0)
eb8727f9c64f286d44bbcc1d19f96408ea5a385c (3.6)
","17/Aug/15 12:33;gjahanagirova;Hi! Thanks for your reply!

Can you, please, share the code of your boundary check? Do you throw an exception when the upper is less than lower?

Best regards,
Gunel.","17/Aug/15 13:03;erans;Hi.

bq. Do you throw an exception \[...\]

Yes.

I had started a thread on the project's ML (in case the issue had non-obvious implications):
  http://markmail.org/thread/62bpxwifl2lftvg5

You can browse through the modifications here:
  https://git1-us-west.apache.org/repos/asf?p=commons-math.git
",25/Jan/16 20:28;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Bug in ""o.a.c.m.ml.neuralnet.sofm.KohonenUpdateAction""",MATH-1255,12855969,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,erans,erans,13/Aug/15 21:14,25/Jan/16 20:27,07/Apr/19 20:38,13/Aug/15 21:35,3.5,,,,,,,3.6,4.0,,0,,,,,,,,"In method ""update"", the standard deviation of the ""Gaussian"" function is set to ""1 / n"" instead of ""n"" where n is the current neighbourhood size.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-01-25 20:27:48.077,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:48 UTC 2016,,,,,,0|i2iw8v:,9223372036854775807,,,,,,,,"13/Aug/15 21:35;erans;4f73871cf40b6dd05c8872d39246c67f798ed915 (4.0)
5c3988cb0414bb013464f8a12741fde37546cde3 (3.6)
",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test CorrelatedRandomVectorGeneratorTest is flaky,MATH-1254,12853694,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,Otmar Ertl,Otmar Ertl,09/Aug/15 15:19,25/Jan/16 20:27,07/Apr/19 20:38,24/Nov/15 20:58,4.0,,,,,,,3.6,4.0,,0,,,,,,,,CorrelatedRandomVectorGeneratorTest.testSampleWithZeroCovariance sometimes fails. The failure can be reproduced by setting the seed of the JDKRandomGenerator in createSampler equal to 628. ,,,,,,,,,,,,,,,,,,,,,09/Aug/15 15:23;Otmar Ertl;seed628.patch;https://issues.apache.org/jira/secure/attachment/12749471/seed628.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-11-23 22:34:33.402,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:48 UTC 2016,,,,,,0|i2iibj:,9223372036854775807,,,,,,,,"23/Nov/15 22:34;tn;Luc has already changed the test to use a fixed seed.

Increasing the number of samples also improves the robustness of the test.","24/Nov/15 20:58;Otmar Ertl;4.0: ad12d97cbb4c35017d55c4105174671f1c38b36a
3.6: 635c35be7446c55740765bb67b8bfa21f1c93de5",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skewness could get more precision from slightly reordered code.,MATH-1253,12851234,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,,,murphyw,murphyw,03/Aug/15 22:32,12/Apr/17 11:02,07/Apr/19 20:38,,3.5,,,,,,,4.0,,,0,,,,,,,,"In Skewness.java, approx line 180, there is code like:

{noformat}
            double accum3 = 0.0;
            for (int i = begin; i < begin + length; i++) {
                final double d = values[i] - m;
                accum3 += d * d * d;
            }
            accum3 /= variance * FastMath.sqrt(variance);
{noformat}

If the division was moved into the for loop, accum3 would be less likely to overflow to Infinity (or -Infinity). This might allow computation to return a result in a case such as:

{noformat}
double[] numArray = { 1.234E11, 1.234E51, 1.234E101, 1.234E151 };

Skewness    skew = new Skewness();
double    sk = skew.evaluate( numArray );
{noformat}

Currently, this returns NaN, but I'd prefer it returned approx 1.154700538379252.

The change I'm proposing would have the code instead read like:
{noformat}
            double accum3 = 0.0;
            double divisor = variance * FastMath.sqrt(variance);

            for (int i = begin; i < begin + length; i++) {
                final double d = values[i] - m;
                accum3 += d * d * d / divisor;
            }
{noformat}

Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-08-08 14:41:18.957,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 24 19:41:33 UTC 2015,,,,,,0|i2iaf3:,9223372036854775807,,,,,,,,"08/Aug/15 14:41;Otmar Ertl;The given example is problematic anyway, because the numbers (1.234E11, 1.234E51, 1.234E101, 1.234E151) are many orders of magnitude apart, for which cancellation effects are a major issue when using floating point types. I can reproduce the NaN result for values that have either large or small exponents, for example (1.234E148, 1.234E149, 1.234E150, 1.234E151) and (1.234E-148, 1.234E-149, 1.234E-150, 1.234E-151). The proposed code does not avoid this over-/underflows either, because the division is performed after the two multiplications which cause the over-/underflow. However, following code would work:
{code}
            final double variance = (accum - (accum2 * accum2 / length)) / (length - 1);
            final double stdDevInverse = 1d / FastMath.sqrt(variance);

            double accum3 = 0.0;
            for (int i = begin; i < begin + length; i++) {
                final double d = (values[i] - m) * stdDevInverse;
                accum3 += d * d * d;
            }
{code}
(Here the inverse is precomputed, in order to avoid additional divisions which are likely to be more expensive than multiplications.) Nevertheless, I am not sure, If we really should fix that. The variance calculation is also prone to over-/underflows for numbers greater than sqrt(Double.MAX_VALUE) and smaller than sqrt(Double.MIN_VALUE), respectively. So the fix would ""slightly"" extend the valid input range from (cbrt(Double.MIN_VALUE), cbrt(Double.MAX_VALUE)) to (sqrt(Double.MIN_VALUE), sqrt(Double.MAX_VALUE)) at the expense of an additional multiplication within the loop. Of course, if there was also an accurate variance calculation method, that avoids over-/underflows for values outside of (sqrt(Double.MIN_VALUE), sqrt(Double.MAX_VALUE)), this fix would make much more sense to me.","18/Aug/15 22:35;murphyw;I agree with Otmar's comments entirely.

I do not yet know a better suggestion. Even a slight extension of the range of correct results seems to me to be an improvement. I would still prefer going forwards with these mods as suggested, but a true fix as outlined by Otmar would be hugely better. Does someone else know enough numerical analysis stuff to provide a mostly-correct approximation over the range of doubles? It is sadly beyond me as of today.
","23/Nov/15 23:02;tn;just ftr: I did some tests with octave and scipy, and both return NaN or overflow for the input.","24/Nov/15 10:47;erans;Independently of extending the valid range, wouldn't Otmar's proposal also give a more accurate result (than the current code), especially when the number of samples is large?
","24/Nov/15 19:41;Otmar Ertl;Provided that there is no overflow, I do not think that my proposal is more accurate. The relative error of a sum of floating point numbers is the same after scaling the numbers by the same factor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResizableDoubleArray does not work with double array of size 1,MATH-1252,12848722,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,johnbay,johnbay,23/Jul/15 22:24,25/Jan/16 20:28,07/Apr/19 20:38,24/Jul/15 00:38,3.4,3.4.1,3.5,,,,,3.6,4.0,,0,,,,,,,,"When attempting to create a ResizableDoubleArray with an array of a single value (e.g. {4.0}), the constructor creates an internal array with 16 entries that are all 0.0

Bug looks like it might be on line 414 of ResizableDoubleArray.java:

        if (data != null && data.length > 1) {
",,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-07-24 00:38:29.22,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:28:02 UTC 2016,,,,,,0|i2hv7j:,9223372036854775807,,,,,,,,"24/Jul/15 00:38;psteitz;Many thanks for reporting this and pinpointing the cause.

Fixed in 
3.x:  9f148d41e0cb5839e8680bd3b4c4bc21510e444b
master: 09fe956a62e19c160d0093f8fecf254c2bb6f0cb",25/Jan/16 20:28;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Wrong ""number of calls"" in ""KohonenUpdateAction""",MATH-1251,12846185,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,erans,erans,19/Jul/15 21:07,25/Jan/16 20:27,07/Apr/19 20:38,20/Jul/15 13:51,3.5,,,,,,,3.6,4.0,,0,,,,,,,,"In class {{KohonenUpdateAction}} (package {{o.a.c.m.ml.neuralnet.sofm}}), the method {{getNumberOfCalls}} is off by 1 due to counter being initialized to -1.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-01-25 20:27:58.103,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:58 UTC 2016,,,,,,0|i2hfpz:,9223372036854775807,,,,,,,,20/Jul/15 13:51;erans;commit 9c545d44a4a703c88d417a6fa43298a80ee67735,25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlockRealMatrix BLOCK_SIZE value change,MATH-1249,12844593,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Invalid,,albert_triv,albert_triv,13/Jul/15 09:31,20/Jul/15 13:53,07/Apr/19 20:38,20/Jul/15 13:53,,,,,,,,,,,0,,,,,,,,"Changing the value of the filed BlockRealMatrix.BLOCK_SIZE takes to a java.lang.ArrayIndexOutOfBoundsException in the method multiply(); The 
thest is the following (aftre recompiling without the FINAL keyword for BlockRealMatrix.BLOCK_SIZE):

public void testBlockRealMatrix() throws Exception{
		logger.debug(""testBlockRealMatrix"");
		
		long seed = 12345L;
		Random random = new Random(seed);
		int rows = 100;
		int cols = 100;
		double[][] data = new double[rows][cols];
		for(int i=0; i<rows; i++){
			for(int j=0; j<cols; j++){
				data[i][j] = random.nextDouble();
			}
		}
		
		BlockRealMatrix m1 = new BlockRealMatrix(data);
		BlockRealMatrix m2 = new BlockRealMatrix(data);
		
		BlockRealMatrix.BLOCK_SIZE = 52;
		long t0 = System.currentTimeMillis();
		m2.multiply(m1);
		logger.debug(""time : "" + (System.currentTimeMillis() - t0));
		
		BlockRealMatrix.BLOCK_SIZE = 26;
		t0 = System.currentTimeMillis();
		m2.multiply(m1);
		logger.debug(""time : "" + (System.currentTimeMillis() - t0));
		
		BlockRealMatrix.BLOCK_SIZE = 104;
		t0 = System.currentTimeMillis();
		m2.multiply(m1);
		logger.debug(""time : "" + (System.currentTimeMillis() - t0));
	}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-07-13 12:20:39.521,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 13 12:20:39 UTC 2015,,,,,,0|i2h65b:,9223372036854775807,,,,,,,,"13/Jul/15 12:20;erans;bq. \[...\] aftre recompiling without the FINAL keyword for BlockRealMatrix.BLOCK_SIZE) \[...\]

So, *you* introduced the bug...
""final"" is there for a good reason: it _cannot_ be changed after construction!

Discussion and questions about usage or design should go through the project's MLs:
  http://commons.apache.org/proper/commons-math/mail-lists.html
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kolmogorov-Smirnov exactP gives incorrect p-values for some D-statistics,MATH-1247,12842759,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Duplicate,,psteitz,psteitz,06/Jul/15 00:45,06/Jul/15 00:51,07/Apr/19 20:38,06/Jul/15 00:51,3.4.1,,,,,,,3.5,4.0,,0,,,,,,,,"The exactP method in KolmogorovSmirnovTest, which is used by default for small samples in 2-sample tests, can give slightly incorrect p-values in some cases.  The reason for this is that p(D > d) is computed by examining all m-n partitions and counting the number of partitions that give D values larger than the observed value.  D values are not rounded, so some values that are mathematically identical to the observed value compare less than or greater than it.  This results in small errors in the reported p-values.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,2015-07-06 00:45:47.0,,,,,,0|i2gv6n:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kolmogorov-Smirnov 2-sample test does not correctly handle ties,MATH-1246,12842758,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,06/Jul/15 00:45,25/Jan/16 20:28,07/Apr/19 20:38,31/Dec/15 19:30,,,,,,,,3.6,,,0,,,,,,,,"For small samples, KolmogorovSmirnovTest(double[], double[]) computes the distribution of a D-statistic for m-n sets with no ties.  No warning or special handling is delivered in the presence of ties.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-09-09 20:10:45.955,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:28:02 UTC 2016,,,,,,0|i2gv6f:,9223372036854775807,,,,,,,,"07/Jul/15 03:57;psteitz;I think the current implementation can be fixed as follows.  If we move to a faster implementation, the strategy below may not work.

What exactP does now is to exhaustively compute all possible D-statistics for all m-set / n-set partitions of m+n and simply tally the number that exceed (strict) or are as large as (not strict) the observed D.  If there are ties in the data, it is not correct to look at partitions of m+n, since not all partitions of an m+n set with duplicates are distinct and the set of possible D values is different in the presence of ties.  I think we can correctly handle ties in the data if we compute and tally D statistics based on a combined multi-set sample with duplicates in the positions corresponding to what is observed in the data.  For example, suppose that the two samples are x = [0, 3, 6, 9, 9, 10] and y = [1, 3, 4, 8, 11].  then the multi-set universe is  U = [0, 1, 3, 3, 4, 6, 8, 9, 9, 10, 11].  As before, we generate partitions of 11 into a 6-set and a 5-set, but instead of computing the D-statistics on the subsets of 11, we use indexes into U instead.  So if a generated split is mSet = [0, 2, 3, 7, 8, 9], nSet = [1, 4, 5, 6, 10], we compute D for [0, 3, 3, 9, 9, 10] and [1, 4, 6, 8, 11].  The rationale here is that the p-value is the probability that if U is split randomly into a 5-set and a 6-set, the D-value exceeds the observed d.","09/Sep/15 01:04;psteitz;Pushed changes to exactP per outline above in ce98d00852e21ce34d8d247db7f6be138967b559.

The same problem applies to monteCarloP.  It is not obvious to me how to make the necessary changes to the improved implementation there to accommodate ties.  I suspect it is not possible, so unless others have better ideas, I will add back a modified version of the earlier naive implementation that samples the multi-set.","09/Sep/15 20:10;Otmar Ertl;I am thinking of another way to treat ties:

The probability that two values sampled from a continuous distribution are equal is equal to 0. One of them is always greater than the other. However, represented as doubles we cannot distinguish them. Therefore, the best what we can do is to treat both cases equally likely. For example, if we have x = (0, 3, 5) and y = (5, 6, 7) we get two different values for the observed D-statistic. If we assume value 5 in x to be smaller than that in y, we would get D=1. Otherwise, we would get D=2/3, both with probability 0.5. In the general case, we can determine a discrete distribution describing all possible values of the observed D-statistics. Finally, we calculate the p-value for each of those possible values and calculate the weighted average which we take as the final p-value.

Does this make sense? If yes, I think there is a way to adapt the new Monte Carlo approach.","10/Sep/15 00:27;psteitz;Otmar - that is a cool idea.  So for a given sample with ties, the D statistic would be the average of the D's you could make by differently ordering the tied values, right?  That is an alternative to what I did.  I wonder how different the p-values would end up.   I think they will definitely be different, but I wonder by how much.  Note also things get a little complicated when you have more than two of the same value, which we have to assume could happen.

How would you modify the current Monte Carlo approach to do this?

I tried to find references to how others handle this; but unfortunately most packages just say you can't compute exact p values in the presence of ties.  Given that we have two, defensible and different definitions of how the p-value should be defined, I can see why.  I will look some more to see if I can find some math stat references to help us settle on the right definition.

Thanks for looking into this.","10/Sep/15 20:29;Otmar Ertl;The Monte Carlo approach can be modified by simultaneously sampling D. Here is an outline how this sampling  could be achieved:
# First determine set of points P = (p_i) for which equal values exist in both samples.
# Determine maximum difference of CDFs over all values not included in P
# Determine for each point p_i if it is possible at all to get a CDF difference that is larger than the calculated maximum. If not, those points can be excluded from P. Otherwise, remember the difference of the CDF d_i just before that point and the number of equal values in both samples n_i and m_i, respectively.
# Within each Monte Carlo iteration, generate for each point p_i a random ordering of the n_i and m_i equal values (using a function similar to fillBooleanArrayRandomlyWithFixedNumberTrueValues). Determine the maximum differences of the CDFs at all points p_i using the random ordering and d_i, and take the maximum of them and the maximum calculated in 2) which gives us the sampled (observed) D-statistic that is finally compared to curD.

Anyway, we should find the right definition first before implementing anything.","13/Sep/15 23:36;psteitz;I have done some research and I am convinced that the definition based on the empirical distributions as given is correct.  In other words

1.  The statistic that we should use is that given by comparing the empirical distributions with ties contributing the mass that they do.  This is the Kolmogorov metric that is part of the definition of the test.  Distributions with point masses should be allowed and empirical distributions based on data including repeated values should be taken as presented by the data.
2.  The correct definition of p-value (with or without ties) is the probability that when an n-set and m-set are randomly selected from the combined dataset the associated D-value is greater than (resp greater than or equal to) the observed D value (with ties included).  Equivalently, it is the probability that when group assignment is done randomly, the resulting empirical distributions are separated by Kolmogorov distance as large as the observed D.

This is supported theoretically in [1], recommended in [2] and implemented in the R-package ks.boot, which the R community recommends when ties are present in the data.

The current small sample, exactP method computes the probability defined above by actually enumerating all n-m splits and agrees with tabulated data and R for samples with no ties.  As explained above, in the presence of ties, exact computation requires that the partition enumeration be over the actual combined data (including the ties).  The fix committed in ce98d00852e21ce34d8d247db7f6be138967b559 does that, so I think it is correct.  I will run some comparisons with ks.boot to check consistency / find errors in the implementation.

Happily, in [2] I found a much more efficient way to compute exactP in the no-ties case.  Unfortunately, I can't find [2] or the algorithm presented freely available anywhere.  I am going to try to implement it and once that is done, we can likely use Monte Carlo only for moderate size samples with ties (since the faster algorithm should work for the non-tied case up to the level where the asymptotic approximation is fine - this is basically what R does).  I think in any case, our Monte Carlo implementation should use the combined sample semantics (as in [1]), which means in the presence of ties, it will have to use the multi-set as sampling universe.

[1] Abadie, Alberto. 2002.  ""Bootstrap Tests for Distributional Treatment Effects in Instrumental Variable Models.'' Journal of the American Statistical Association, 97:457 (March) 284-292.  Currently available online at http://hks.harvard.edu/fs/aabadie/dtep.pdf

[2] Wilcox, Rand. 2012. ??Introduction to Robust Estimation and Hypothesis Testing??, 3rd Ed. Academic Press.","14/Sep/15 19:35;Otmar Ertl;I still have doubts:
# There is a difference, if there are ties within one sample, or if the same value exists at least once in both samples. In the first case the D-statistic is well defined. In the latter case the D-statistic is undefined. For example, if x = (1, 3, 3, 5) and y = (2, 4, 4, 6) D = 0.5. On the other hand, if  x = (1, 3, 3, 5) and y = (2, 3, 3, 6) the D-statistic could be any value  between 0.25 and 0.75. The current implementation returns the minimum (0.25 in this case), but this seems to be a quite arbitrary choice. Furthermore, the implementation does not distinguish between these two cases (see hasTies() method).
# If the current implementation of exactP() follows the definition you described, I do not really understand why the following two statements return different values:
{code}
new KolmogorovSmirnovTest().exactP(new double[]{0.9, 1.0, 1.1}, new double[]{0.0, 0.0}, false)
new KolmogorovSmirnovTest().exactP(new double[]{1.0, 1.0, 1.0}, new double[]{0.0, 0.0}, false)
{code}
The D-statistic is well-defined and equal to 1 in both cases.
# \[1\] describes estimating the p-Value using bootstrapping. I am not sure, if an exact definition can be derived from there, since bootstrapping in general is not a consistent  estimation method.","15/Sep/15 01:51;psteitz;Thanks again, Otmar for looking carefully at this.  It is a little painful to try to do this in JIRA comments, but since we started here and it will be best to keep the comments together, I will try to respond to each of your points above.

# I must be missing something here.  In the case x = (1, 3, 3, 5) and y = (2, 3, 3, 6), I don't see how there is ambiguity in the D statistic, which looks correct to me at .25.  The D statistic is the maximum difference in the empirical distributions.  In this case, the max is .25, attained at two domain values: 1 and 5.
# The D statistics are the same, but the empirical distributions and underlying datasets are different.  The p-value depends on both the D-statistic and the empirical distributions.  When there are no ties, D_n,m is has the same distribution regardless of the underlying sample data.  When ties are present, the distribution is still discrete, but it depends on the number and location of the ties.
# What is proven is that bootstrapping gives asymptotically correct results.  The bootstrapping is over the combined dataset, including ties (as ks.boot does).  Exact computation using enumeration of all possible splits will give the same result as what will be expected from bootstrapping.

It could be that we are not agreeing on the core definition of what the p-value is supposed to be.  To me, ties in the data just add mass to the empirical distributions where they fall and the 2-sample test is really just assessing the null hypothesis that the distributions represent draws from the same underlying population distribution.   The common underlying distribution under the null hypothesis is best represented by the pooled data.  This is the interpretation that [1] appears to agree with, and [2] (sadly not free) as well.  ","15/Sep/15 13:59;Otmar Ertl;After some research I have the feeling we are discussing how to define zero divided by zero. There are at least two methods to calculate a reasonable p-value in the presence of ties:
# The method you have proposed which seems to be also known as permutation method. Averaging only over some permutations and averaging over all possible permutations correspond to the bootstrap method and the current exactP() implementation, respectively.
# Another method is to add some jitter to the sampled values to break ties. (This google search https://www.google.com/?gfe_rd=cr&ei=qCL4VaKvNIWI8QfLibD4Bg&gws_rd=cr&fg=1#q=jitter+kolmogorov+smirnov immediately gives you a couple of references.) This method corresponds to the method I have proposed. Adding small random values to ties to get a strict ordering corresponds to choosing any random ordering. Averaging over all possible orderings would also lead to a well-defined p-value.

Maybe, the user should be able to choose the method how to resolve ties?
","15/Sep/15 23:37;psteitz;I don't think there is really a question about the definition of the p-value - I can't find any reference that does not confirm it to be what I described above.  And it is well-defined in the presence of ties - just messy to compute, as the distribution of D depends on not just n and m but the location of the ties.  The permutation method does correspond to ks.boot and what I would propose for the monte carlo impl in the presence of ties.  The method currently implemented for exactP(x,y,b) computes p-values based on full enumeration of the underlying sample space sampled by ks.boot (resp. the permutation method).

I understand, though, the inefficiency of doing full enumeration and the convenience of working with D statistics that depend only on n and m.  So I think it may be best to do as you suggest and make the behavior in the presence of ties configurable.  I like the idea of introducing a (public or protected) jitter method that just randomly perturbs combined samples with ties.  Then if you configure ties handling to use jitter, the implementation just applies the jitter and uses the (not yet implemented) fast method for exact computation without ties and the current monteCarlo implementation (that does not handle ties) for monteCarloP.  [An interesting theorem to prove is that the expected p-value computed using random jitter in the presence of ties equals the true p-value equals the expectation of the permutation method (the last part is what [1] shows)].  Once we have the fast no-ties method implemented, we may be able to dispense with the version of monteCarloP that does not handle ties, as the fast, exact method should be usable up to the sample sizes where the K-S distribution based method is OK (and more accurate).

Assuming you are OK with this, I will proceed with a) the fast implementation of the no ties exactP b) a version of monteCarloP that basically does what ks.boot does (resamples the combined dataset with ties included).  

If we agree on this approach we need to decide
# How configuration should work
# How (if at all) we signal to the user that there are ties in the data
# What the default behavior should be","16/Sep/15 05:48;Otmar Ertl;The p-value is the probability that the observed KS-statistic is smaller than the KS-statistic that I get if two random samples of same sizes are drawn from the underlying distribution. In the no-ties case this value can be calculated exactly without knowing the underlying distribution. In case of ties, the p-value cannot be calculated exactly. There are different approaches how to calculate some approximation of the p-value for the tie-case:
* Approximation of the underlying distribution by the observed data, which definitely makes sense for bootstrapping where the sample sizes are usually large. However, in our case the underlying distribution is estimated from small sample sizes, since this is the domain for the exactP method. Therefore, I doubt that the calculate p-value deserves the label ""exact"" in this case.
* Assumption that orderings of observed equal values are equally likely, which of course is also an approximation.

I still do not understand why the first approach should be the true one.","16/Sep/15 18:49;psteitz;I could be wrong on this and I am OK with reverting the current exactP ties handling code and replacing with the random jitter approach.  I still think the exact p can in fact be computed with ties present; but to do so you have to view the combined sample as the empirical distribution representing the (combined) population.   You make a good point above about that being dubious for small samples.   I will continue to research this, but given lack of consensus, I will remove the implementation from the code.

So let's see if we can agree on 
# Add non-naive exactP to handle no ties small sample.  Extend it to n * m = 10000 as default behavior (this is the cut that R uses).  Beyond this point, use the K-S distribution, so we no longer need MonteCarloP for moderate size samples.
# Implement jitter method and use this by default in the small sample case to break ties.  Until we  have eliminated the need for MonteCarloP as a default, use jitter to break ties for moderate sample sizes and use monteCarloP as is post-jitter.

Optionally, implement a ks.boot-like monteCarloP that works with tied data.


","16/Sep/15 19:26;Otmar Ertl;Sounds ok to me, due to its slow convergence avoiding Monte Carlo as much as possible is generally a good idea.","09/Nov/15 21:14;psteitz;I did some more extensive testing against R's ks.boot and found significant differences from the code in ce98d00852e21ce34d8d247db7f6be138967b559.  I have determined the reason why the results are different and that my initial approach was incorrect.  The difference is due to the fact that ks.boot samples ""with replacement"" from the combined empirical distribution while my approach constrains the n-m split to be a split that can be achieved using the combined dataset.  I interpreted the p-value to be essentially the same as in the no ties case - what is the probability that when the combined set of values is split into an n-set and an m-set, the KS statistic is greater than or equal to what we observe in the data.  The theoretical development in [1] and the implementation in ks.boot define the p-value to be the probability that when an m-set and n-set are drawn independently from the combined empirical distribution, the p-value exceeds what we see in the data.  This is not the same and when there are a lot of ties the estimates diverge.  Apologies for being a little dense on this.","22/Nov/15 18:42;psteitz;Flawed exactP implementation removed in 
fd37b5dd02bbce93f6f4fceb6bc3e6aa4641c5a7 (master)
3d055c620c7132b0fbe61ffeba87fd9f2391826f (MATH_3_X)","22/Nov/15 19:46;psteitz;Added boostrap method (like ks.boot) in
7851a3e2bf24c58eea66da70fa42a32f03532620 (master)
fbc327e9e30093acdc0fc325b1719cae4ea8bac1 (MATH_3_X)",25/Jan/16 20:28;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kolmogorov-Smirnov exactP gives incorrect p-values for some D-statistics,MATH-1245,12842757,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,psteitz,psteitz,06/Jul/15 00:42,25/Jan/16 20:27,07/Apr/19 20:38,10/Jul/15 19:34,3.4.1,3.5,,,,,,3.6,4.0,,0,,,,,,,,"The exactP method in KolmogorovSmirnovTest, which is used by default for small samples in 2-sample tests, can give slightly incorrect p-values in some cases.  The reason for this is that p(D > d) is computed by examining all m-n partitions and counting the number of partitions that give D values larger than the observed value.  D values are not rounded, so some values that are mathematically identical to the observed value compare less than or greater than it.  This results in small errors in the reported p-values.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-01-25 20:27:50.256,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:50 UTC 2016,,,,,,0|i2gv67:,9223372036854775807,,,,,,,,"10/Jul/15 19:34;psteitz;Fixed in master 32d33210a92b1197a6c5a07f19aa25426af72723
3.x 7a6aa92c8ac46059f7ca9d76d7da6b710df901aa",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Digamma calculation produces SOE on NaN argument,MATH-1241,12840198,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,dievsky,dievsky,24/Jun/15 12:49,25/Jan/16 20:27,07/Apr/19 20:38,24/Jun/15 13:43,3.5,,,,,,,3.6,4.0,,0,,,,,,,,"Digamma doesn't work particularly well with NaNs.

How to reproduce: call Gamma.digamma(Double.NaN)

Expected outcome: returns NaN or throws a meaningful exception

Real outcome: crashes with StackOverflowException, as digamma enters infinite recursion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-06-24 13:43:21.405,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:50 UTC 2016,,,,,,0|i2gfn3:,9223372036854775807,,,,,,,,"24/Jun/15 13:43;tn;Fixed in the following commits:

 * 4.0: 471e6b078a7891aea99b77f200e828
 * 3.6: 229232829c6d8741138decf27c4909

Now the Gamma.digamma and trigamma methods will propagate the input argument if it is not a real value.

Thanks for the report!",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoBracketingException send with valid brackets,MATH-1238,12839402,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Incomplete,,charles.s,charles.s,21/Jun/15 11:49,11/Sep/15 17:51,07/Apr/19 20:38,11/Sep/15 17:51,3.5,,,,,,,,,,0,,,,,,,,"The brent solver sometimes send a NoBracketingException with valid brackets : example for a beta distribution with large Beta (285) and alpha around 15
Exception in thread ""main"" org.apache.commons.math3.exception.NoBracketingException: function values at endpoints do not have different signs, endpoints: [0, 1], values: [-0.15, 0.85]

the exception is actually caused by the yinitial which is Nan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-06-21 17:55:42.335,,,false,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 11 17:51:46 UTC 2015,,,,,,0|i2gb8f:,9223372036854775807,,,,,,,,"21/Jun/15 17:55;erans;This behaviour stems from the particular semantics of NaN (all comparisons with NaN being ""false"").
The library lets NaN values propagate... until some code gives up.

The error message might indeed be misleading but not more than what would have been reported later on (i.e. in this case, if the ""brent"" method were passed a NaN as one of its argument).
","27/Jun/15 10:17;erans;Is it the ""automatically-computed"" or a ""user-defined"" initial value whose function evaluation is NaN?
In the former case, the library might try to generate another start value.
Could you provide a unit test that shows the problem?
","10/Sep/15 18:21;Axion004;I have the same issue through the following test program

import java.util.TreeSet;
import org.apache.commons.math3.analysis.UnivariateFunction;
import org.apache.commons.math3.analysis.solvers.*;

public class TestBrent {

    public static void main(String[] args) {
        BrentSolver test2 = new BrentSolver(1E-10);
        UnivariateFunction function = (double x) -> Math.sin(x);

        double EPSILON = 1e-6;
        TreeSet<Double> set = new TreeSet<>();
        for (int i = 1; i <= 500; i++) {
            set.add(test2.solve(1000, function, i, i+1));
        }

        for (Double s : set) {
            if (s > 0) {
                System.out.println(s);
            }
        }
    }
}






run:
Exception in thread ""main"" org.apache.commons.math3.exception.NoBracketingException: function values at endpoints do not have different signs, endpoints: [1, 2], values: [0.841, 0.909]
	at org.apache.commons.math3.analysis.solvers.BrentSolver.doSolve(BrentSolver.java:133)
	at org.apache.commons.math3.analysis.solvers.BaseAbstractUnivariateSolver.solve(BaseAbstractUnivariateSolver.java:199)
	at org.apache.commons.math3.analysis.solvers.BaseAbstractUnivariateSolver.solve(BaseAbstractUnivariateSolver.java:204)
	at TestBrent.main(TestBrent.java:15)
Java Result: 1
BUILD SUCCESSFUL (total time: 0 seconds)
","11/Sep/15 00:53;erans;bq. I have the same issue

I don't think so.
Your ""NoBracketingException"" occurs because the bracket is _not_ valid.
Moreover, there is no root of the sine function between 1 and 2.
","11/Sep/15 01:54;Axion004;Correct, I did not see my mistake. I will need to review the documentation more carefully. Thank You for pointing this out.","11/Sep/15 17:50;erans;You are welcome.

For usage questions, it is best to post to the [user ML|http://commons.apache.org/proper/commons-math/mail-lists.html].",11/Sep/15 17:51;erans;No reply from the OP.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Doc bug in floorMod(),MATH-1237,12839247,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,kenahoo,kenahoo,19/Jun/15 21:44,25/Jan/16 20:27,07/Apr/19 20:38,19/Oct/15 20:04,,,,,,,,3.6,4.0,,0,,,,,,,,"The docs for {{floorDiv()}} and {{floorMod()}} (both the {{int}} & {{long}} methods) say things like:

Finds q such that a = q b + r with 0 <= r < b *if b > 0* and b < r <= 0 *if b > 0*.

That latter clause is probably supposed to be ""if b < 0"" - this should be clarified since (AFAICT) the whole point of these functions is to change the behavior from native ops when {{b}} is negative, or maybe when {{a*b}} is negative, but I'm not really sure which.

As an aside, it would be great to add similar methods when {{a}} is a float/double, because Java's mod/div operators support float/double types too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-10-19 20:04:18.896,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:52 UTC 2016,,,,,,0|i2gab3:,9223372036854775807,,,,,,,,"19/Oct/15 20:04;tn;Fixed in the following commits:

 * 3.6: 0dd621687ddc1b0c254db14743e78cdd6315ae6b
 * 4.0: 435384cf132ab161bfedce0a3be96bccfb11f0b6

Thanks for the report!",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Impossible NotStrictlyPositiveException after getStandardDeviation(),MATH-1234,12838473,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,Sklavit,Sklavit,17/Jun/15 13:32,19/Jun/15 21:42,07/Apr/19 20:38,19/Jun/15 08:21,3.2,,,,,,,3.5,,,0,,,,,,,,"org.apache.commons.math3.random.EmpiricalDistribution.density() calls 
EmpiricalDistribution.getKernel which calls
bStats.getStandardDeviation() -- bStats is SummaryStatistics
and return result caused NotStrictlyPositiveException: standard deviation (0)
in new NormalDistribution(randomData.getRandomGenerator(),
                bStats.getMean(), bStats.getStandardDeviation(),
                NormalDistribution.DEFAULT_INVERSE_ABSOLUTE_ACCURACY);

As I understand, it shouldn't be so by contract of SummaryStatistics.","Windows 7, Java 1.8",,,,,,,,,,,,MATH-1203,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-06-18 22:39:29.42,,,false,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 08:21:42 UTC 2015,,,,,,0|i2g5lr:,9223372036854775807,,,,,,,,"18/Jun/15 22:39;erans;What do you mean by ""impossible""?  That it should not occur, or that it is not advertised in the javadoc?
Could  you please upload a unit test that shows the problem?

Did you try version 3.5 of Commons Math?
",19/Jun/15 08:21;tn;This has already been reported and fixed in MATH-1203.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncommon wilcoxon signed-rank p-values,MATH-1233,12837832,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,chtompki,icarocd,icarocd,15/Jun/15 13:05,30/Apr/18 19:17,07/Apr/19 20:38,,3.5,,,,,,,4.0,,,0,,,,,,,,"This implementation in WilcoxonSignedRankTest looks weird. For equal vectors, the correct pValue should be 1, because it is the probability of the vectors to come from same population.
On the opposite, this implementation returns ~0 for equal vectors. So we need to analyze the returned pValue > significanceLevel to reject H0 hypothesis, while in R and many others tools we perform the opposite: pValue <= significanceLevel gives us an argument to reject null hypothesis.",,,,,,,,,,,,,,,,,,,,,15/Jun/15 23:56;psteitz;MATH-1233-test.patch;https://issues.apache.org/jira/secure/attachment/12739730/MATH-1233-test.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-06-15 22:48:59.045,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 30 19:17:37 UTC 2018,,,,,,0|i2g1pj:,9223372036854775807,,,,,,,,"15/Jun/15 22:48;psteitz;Thanks for reporting this.  The intent of the code is to provide standard p-values, so this must indicate a bug in the implementation.",15/Jun/15 23:56;psteitz;Unit test illustrating bug.,"17/Jun/15 03:38;psteitz;At least one bug is that calculateDifferences does not discard pairs with 0 difference.  It may be best, actually, to do as R does for identical input arrays: return 0 for the statistic an NaN (or throw) for the p-value.","17/Jun/15 11:24;icarocd;I think it is fine to produce NaN for identical vectors. In fact, in apache commons math's source I noticed the procedure differ from the original formulation in some ways (discarding ties as you mentioned is an example).
Two good baselines to take into account are wilcox.test from R, and scipy.stats.wilcoxon from Python.","19/Oct/15 21:12;tn;The referenced wikipedia article explains the algorithm differently than it is implemented.
In our implementation, zero values are not discarded, but we calculate the signed rank as max of W+ and W-. I did not yet find a reference to this, but this subsequently leads to errors when calculating the p-value.

scipy allows 3 different zero handling strategies, see here http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.wilcoxon.html","30/Apr/17 18:36;chtompki;After some reading here, the assumptions on the data given are:

# Data are paired and come from the same population.
# Each pair is chosen randomly and independently.
# The data are measured at least on an ordinal scale (i.e., they cannot be nominal).

I wonder if the two input vectors are the same, then is the consumer not violating 3? I generally agree here that the same vector should be treated in its own way. I would think that we may want to throw an exception. The only question then becomes performance in nature, in that, is doing array equality at the beginning of the procedure valuable enough that we are willing to do it every time despite the _O( n )_ performance hit? Or do we simply document the fact that we'll not give reliable results when the vectors are the same.","05/May/17 18:25;chtompki;When we consider scipy, our results don't even match up with theirs on the same input, generally. This somewhat concerns me that our algorithm is generally not correct.","05/May/17 22:44;erans;Was it the case too with the last official release (v3.6.1)?
","05/May/17 23:29;icarocd;one suggestion is to just to copy the code from scipy, including the tests, then port it to java.","06/May/17 12:50;chtompki;Gilles - yes

Icaro - that was indeed my thought as well.

I'm hoping to sort this out this week.",08/May/17 14:02;chtompki;Gilles - do you think we should deprecate the old method or simply change the signature because we're going from 3.X to 4.X? It feels like that could be abrupt with out the forewarning that deprecation provides.,"08/May/17 18:01;erans;Several things will have changed or disappeared ""abruptly"" when 4.0 will be released. :)
The release notes should contain the context to help people with upgrading their code.","30/Apr/18 19:17;psteitz;A first attempt at a fix for this was implemented in [this issue|https://github.com/Hipparchus-Math/hipparchus/issues/37] in Hipparchus.  Current code there discards tied pairs, which would result in an exception if all pairs are tied.  I agree that it should be configurable how ties are handled and the scipy alternatives make sense.  Hipparchus currently implements the simplest one, what scipy calls ""wilcox.""  Another problem with the current [math] implementation is that the continuity correction is not applied correctly.  That was fixed in the Hipparchus patch, which should backport easily.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UknownParameterException message prints {0} instead of parameter name,MATH-1232,12837115,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,luc,luc,11/Jun/15 11:45,25/Jan/16 20:27,07/Apr/19 20:38,11/Jun/15 12:12,3.5,,,,,,,3.6,4.0,,0,,,,,,,,"The constructor for UnknownParameterException stores the
parameter name internally but does not forward it to the base class which creates the error message.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:59 UTC 2016,,,,,,0|i2fxdb:,9223372036854775807,,,,,,,,"11/Jun/15 12:12;luc;Fixed in git repository, both in master branch and MATH_3_X branch.",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MicrosphereInterpolator: Unnecessery restriction in constructor,MATH-1231,12836814,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,erans,erans,erans,10/Jun/15 11:59,11/Jun/15 22:13,07/Apr/19 20:38,11/Jun/15 22:13,3.5,,,,,,,4.0,,,0,,,,,,,,"In {{o.a.c.m.analysis.interpolation.MicrosphereInterpolator}}, the constructor requires that the ""exponent"" be an integer, whereas the algorithm has no such restriction.
",,,,,,,,,,,,,,,,,,,,,10/Jun/15 12:02;erans;MATH-1231.patch;https://issues.apache.org/jira/secure/attachment/12738806/MATH-1231.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 22:13:37 UTC 2015,,,,,,0|i2fvk7:,9223372036854775807,,,,,,,,"10/Jun/15 12:02;erans;Any objection to my applying the just uploaded patch?
",11/Jun/15 22:13;erans;commit 2990f6caad0db0f7c7a2df22d65f1031ed9e33e1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver returning wrong answer from optimize,MATH-1230,12836566,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,koppdk,koppdk,09/Jun/15 15:07,09/Jun/15 18:49,07/Apr/19 20:38,09/Jun/15 18:49,3.5,,,,,,,4.0,,,0,,,,,,,,"SimplexSolver fails for the following linear program:

min 2x1 +15x2 +18x3

Subject to

  -x1 +2x2  -6x3 <=-10
            x2  +2x3 <= 6
   2x1      +10x3 <= 19
    -x1  +x2       <= -2
    x1,x2,x3 >= 0

Solution should be
x1 = 7
x2 = 0
x3 = 1/2
Objective function = 23

Instead, it is returning
x1 = 9.5
x2 = 1/8
x3 = 0
Objective function = 20.875

Constraint number 1 is violated by this answer",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-06-09 15:22:58.323,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 18:49:44 UTC 2015,,,,,,0|i2fu2n:,9223372036854775807,,,,,,,,"09/Jun/15 15:13;koppdk;This test case should be added to SimplexSolverTest.java

    @Test
    public void testMath1230() {
        //  min 2x1 +15x2 +18x3
        //  Subject to
        //  -x1 +2x2  -6x3 <=-10
        //        x2  +2x3 <= 6
        //  2x1      +10x3 <= 19
        //  -x1  +x2       <= -2
        //   x1,x2,x3 >= 0

        LinearObjectiveFunction f =
          new LinearObjectiveFunction(new double[] { 2, 15, 18 }, 0);
        Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
        constraints.add(new LinearConstraint(new double[] { -1, 2  -6 }, Relationship.LEQ, -10));
        constraints.add(new LinearConstraint(new double[] {  0, 1,  2 }, Relationship.LEQ, 6));
        constraints.add(new LinearConstraint(new double[] {  2, 0, 10 }, Relationship.LEQ, 19));
        constraints.add(new LinearConstraint(new double[] { -1, 1,  0 }, Relationship.LEQ, -2));

        SimplexSolver solver = new SimplexSolver();
        PointValuePair solution = solver.optimize(f,
                                                  new LinearConstraintSet(constraints),
                                                  new NonNegativeConstraint(true),
                                                  PivotSelectionRule.BLAND);

        Assert.assertEquals(7, solution.getPoint()[0], 0d);
        Assert.assertEquals(0, solution.getPoint()[1], 0d);
        Assert.assertEquals(0.5, solution.getPoint()[2], 0d);
        Assert.assertEquals(23, solution.getValue(), 0d);
    }

","09/Jun/15 15:22;tn;You have a typo here:

{quote}
\{ -1, 2 -6 \} 
{quote}

it should be

{code}
{-1, 2, -6}
{code}

then the solver returns the correct solution.","09/Jun/15 15:50;koppdk;Ooops. I feel sheepish.

Thanks.",09/Jun/15 15:51;koppdk;I had a typo in my attempt at making an array,09/Jun/15 15:51;koppdk;Submitter error. Sorry about that.,"09/Jun/15 17:52;tn;Other people had the same problem before if I remember correctly.

In fact we should add a safe-guard mechanism to ensure that all constraints have the same coefficient dimension.","09/Jun/15 18:49;tn;Fixed in commit 96eb80efe1da48f37846aa899260aa0c84b15944.
The optimize method will now throw a DimensionMismatchException if the dimension of the objective function does not match the dimension of the constraints.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ResizableDoubleArray: Wrong ""initialCapacity""",MATH-1229,12833991,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,erans,erans,erans,30/May/15 16:25,25/Jan/16 20:27,07/Apr/19 20:38,30/May/15 17:27,3.5,,,,,,,3.6,,,0,,,,,,,,"In {{o.a.c.m.util.ResizableDoubleArray}}, in the constructor
{code}
public ResizableDoubleArray(double[] initialArray) {
    this(DEFAULT_INITIAL_CAPACITY,
            DEFAULT_EXPANSION_FACTOR,
            DEFAULT_CONTRACTION_DELTA + DEFAULT_EXPANSION_FACTOR,
            DEFAULT_EXPANSION_MODE,
            initialArray);
}
{code}
the initial capacity should be set to the length on the input, and not to the hard-coded default.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2016-01-25 20:27:48.371,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:48 UTC 2016,,,,,,0|i2feyn:,9223372036854775807,,,,,,,,"30/May/15 17:27;erans;commit 8be87e032a8c05622148357f30bdca3c614a669f
",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
First Moment is not public,MATH-1228,12833341,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,fabianlange,fabianlange,28/May/15 07:58,14/May/16 18:02,07/Apr/19 20:38,,3.5,,,,,,,4.0,,,2,,,,,,,,"Hi there,
is there a specific reason FirstMoment is not public?
I want to calculate a mean over a List<Double>, for that I want to loop over that list and invoke Mean.increment(listValue), however this is slower than it could be. Every increment call makes a check to incMoment.
I could avoid that if I use a FirstMoment directly, but I cannot create an instance, because it is protected.

Also this effectively means that the public constructor Mean(FirstMoment) is not usable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-09-13 08:08:04.866,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat May 07 17:16:50 UTC 2016,,,,,,0|i2fb1b:,9223372036854775807,,,,,,,,"13/Sep/15 08:08;jolkdarr;I agree that the class and its subclasses should be public.
Subclasses redundantly implements Serializable interface. That should be fixed as well.","13/Sep/15 10:05;erans;bq. this effectively means that the public constructor Mean(FirstMoment) is not usable. 

I'd tend to agree that this looks like an internal inconsistency.

However, if {{FirstMoment}} was intended as an internal support class, while {{Mean}} would be the corresponding public API, it is not good to just make {{FirstMoment}} public, as this would create a redundancy in the library.

I think that the performance problem would be alleviated significantly if the field ""incMoment"" would be *final*.
Currently, the method {{copy(Mean,Mean)}} prevents such a change.
IMHO it should be fixed by removing that method, as it is itself redundant with the copy constructor {{Mean(Mean)}}.

If I'm not mistaken, the code is from a time when using ""new"" was to be avoided, leading to many such ""manual"" optimizations, that nowadays prove harmful.

I'd suggest that your raise the issue on the ""dev"" ML (as it concerns a design decision, rather than a bug fix).","06/May/16 22:05;erans;Was this discussed on the ML?
Conclusion should be reported here.","07/May/16 07:30;fabianlange;I have not taken it to the ML. for me it is a clear API bug that public API contains private classes. In fact this is a violation of the osgi contract, because the package is exported:

Export-Package: org.apache.commons.math3.stat.descriptive.moment;version=""3.6.1""

if you would use the felix bundle plugin it would print: Warning: The exported package org.apache.commons.math3.stat.descriptive.moment contains references to non-public classes.","07/May/16 10:50;erans;bq. a clear API bug

Good reason to start the discussion on the ""dev"" ML.  A better design must be proposed, and agreed on, there.
Thanks for taking this further.
And there are other (loosely) related issues, e.g.  MATH-1281.
","07/May/16 11:21;erans;Also please note that the same design is used throughout, as {{ThirdMoment}} and {{FourthMoment}} are also ""package-private"".  Oddly, {{SecondMoment}} is public.
Having all of them public would just be a workaround; we should not leave the underlying design issues unanswered. Now is a good time to fix things, e.g. also get rid of all these protected fields (which is a goal in itself: MATH-758).","07/May/16 17:16;erans;I've started a case against the whole {{(First|Second|Third|Fourth)Moment}} hierarchy on the ""dev"" ML.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception thrown in ode for a pair of close events,MATH-1226,12830980,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,luc,luc,19/May/15 11:11,25/Jan/16 20:27,07/Apr/19 20:38,19/May/15 11:29,3.5,,,,,,,3.6,4.0,,0,,,,,,,,"When two discrete events occur closer to each other than the convergence threshold used for locating them, this sometimes triggers a NumberIsTooLargeException.

The exception happens because the EventState class think the second event is simply a numerical artifact (a repetition of the already triggerred first event) and tries to skip past it. If there are no other event in the same step later on, one interval boundary finally reach step end and the interval bounds are reversed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:46 UTC 2016,,,,,,0|i2ex6f:,9223372036854775807,,,,,,,,"19/May/15 11:29;luc;Fixed in git repository, as of commit c44bfe0 for master branch and commit 777273e for 3.X branch.",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ODE tutorial documentation not up to date,MATH-1225,12830185,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,bgodard,bgodard,15/May/15 14:43,25/Jan/16 20:27,07/Apr/19 20:38,17/May/15 16:18,3.5,,,,,,,3.6,4.0,,0,,,,,,,,"The tutorial on ODE is not up to date
http://commons.apache.org/proper/commons-math/userguide/ode.html
in particular in what concerns the usage of Parametrized ODE and jacobian providers.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-05-17 16:18:21.554,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:59 UTC 2016,,,,,,0|i2esbr:,9223372036854775807,,,,,,,,"17/May/15 16:18;luc;The userguide has been updated in git repository (both master branch and 3.x branch). The online site has also been updated.

Thanks for the report.",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerExceptions not documented in some classes,MATH-1224,12828708,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,Telcontar,Telcontar,11/May/15 00:43,25/Jan/16 20:27,07/Apr/19 20:38,19/May/15 11:47,3.3,3.5,,,,,,3.6,4.0,,0,documentation,easyfix,easytest,newbie,,,,"In general, the need to initialize newly constructed objects with more data is now documented, but we have found two cases where a NullPointerException is thrown because of missing data.

The documentation should be updated to reflect this. This is similar to issues report in MATH-1116 but concerns classes that are not going to be deprecated (as far as we can tell).

I have previously posted this as a new comment on issue 1116, but that comment has not elicited any response. As the original issue is one year old, I post this bug as a new issue.

Below is the code that produces the two cases:

org.apache.commons.math3.ode.nonstiff.HighamHall54Integrator var1 = new org.apache.commons.math3.ode.nonstiff.HighamHall54Integrator(0.0d, 0.0d, 0.0d, 0.0d);
double[] var2 = new double[] { 0.0d };
var1.computeDerivatives(0.0d, var2, var2); // NPE

new org.apache.commons.math3.stat.correlation.SpearmansCorrelation().getCorrelationMatrix(); // NPE

","Mac OS X, Java 6-8",2400,2400,,0%,2400,2400,,,,,,,,,,,,,,11/May/15 00:44;Telcontar;Report6.java;https://issues.apache.org/jira/secure/attachment/12731837/Report6.java,11/May/15 00:44;Telcontar;Report7.java;https://issues.apache.org/jira/secure/attachment/12731838/Report7.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-05-13 19:27:59.03,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:55 UTC 2016,,,,,,0|i2ejf3:,9223372036854775807,,,,,,,,11/May/15 00:44;Telcontar;Self-contained unit test to reproduce NullPointerException on HighamHall54Integrator (the need to initialize it further is not documented yet).,11/May/15 00:44;Telcontar;Self-contained unit test to reproduce NullPointerException on SpearmansCorrelation (the need to initialize it further is not documented yet).,"13/May/15 19:27;psteitz;Fixed for Spearman's 
3.x branch: fbf6259e0fd4fc85ff55ff7a496b40f98cca43f0
master: 83c61da2c90548f2ddf48e164e8ab14b388e1d0c",19/May/15 11:47;luc;Fixed in git repository for the remaining part (ODE package).,25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong splitting of huge double numbers,MATH-1223,12828013,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,luc,luc,07/May/15 13:20,25/Jan/16 20:27,07/Apr/19 20:38,07/May/15 13:42,3.5,,,,,,,3.6,4.0,,0,,,,,,,,"In both MathArrays and FastMath, some computations on double are performed by firt splitting double numbers in two numbers with about 26 bits.

This splitting fails when the numbers are huge, even if they are still representable and not infinite (the limit is about 1.0e300, eight orders of magnitude below infinity).

This can be seen by computing for example
{code}
FastMath.pow(FastMath.scalb(1.0, 500), 4);
{code}

The result is NaN whereas it should be +infinity.

or by modifying test MathArraysTest.testLinearCombination1 and scaling down first array elements by FastMath.scalb(a[i], -971) and scaling up the second array elements by FastMath.scalb(b[i], +971), which should not change the results. Here the result is a loss of precision because a safety check in MathArrays.linearCombination falls back to naive implementation if the high accuracy algorithm fails.

The reason for the wrong splitting is an overflow when computing
{code}
        final int splitFactor = 0x8000001;
        final double cd       = splitFactor * d; // <--- overflow
{code}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:47 UTC 2016,,,,,,0|i2ef7b:,9223372036854775807,,,,,,,,"07/May/15 13:42;luc;Fixed in git repository, with commit e4b3ac8 in the master branch (4.X) and with commit 9e1b0ac in the MATH_3_X branch.",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve performance of ZipfDistribution by caching the nth harmonic,MATH-1221,12826535,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tn,tn,01/May/15 12:07,01/May/15 12:13,07/Apr/19 20:38,01/May/15 12:13,,,,,,,,4.0,,,0,,,,,,,,The performance of pdf / cdf can be improved by caching the nth harmonic which is only dependeny on the distribution parameters.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 12:13:52 UTC 2015,,,,,,0|i2e6h3:,9223372036854775807,,,,,,,,"01/May/15 12:13;tn;Fixed in commit bd5afc0b5a977c168cf046e3244477b48fa72c5d.

Not backported to 3.X to avoid serialization issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SolutionCallback has incorrect class javadoc,MATH-1214,12819893,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,psteitz,psteitz,10/Apr/15 00:09,18/Apr/15 09:28,07/Apr/19 20:38,10/Apr/15 01:32,3.3,3.4,3.4.1,,,,,3.5,4.0,,0,,,,,,,,The class javadoc for o.a.c.m.optim.linear.SolutionCallback does not describe the class.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-04-18 09:28:25.105,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 09:28:25 UTC 2015,,,,,,0|i2d2lz:,9223372036854775807,,,,,,,,"10/Apr/15 01:32;psteitz;Fixed in
Branch: refs/heads/master
Commit: 6cd693a42256186fb3fd394ab040fa2132fac8d1
Branch: refs/heads/MATH_3_X
Commit: 6a24cf473c12dde2eeb584803d9e0f15ad061b00",18/Apr/15 09:28;luc;Closing resolved issue as 3.5 has been released.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LaguerreSolver complex solve methods do not allow maxEval limit,MATH-1213,12819892,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,psteitz,psteitz,10/Apr/15 00:08,11/Sep/15 18:00,07/Apr/19 20:38,,3.4.1,,,,,,,4.0,,,0,,,,,,,,"The methods solveAllComplex and solveComplex in the LaguerreSolver should allow the user to specify a maximum number of function evaluations, similarly to the real solvers.  Currently, the calls to setup have Integer.MAX_VALUE hard-coded.  Additional versions of these methods that take a maxEval parameter should be added.",,,,,,,,,,,,,MATH-1215,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-04-10 10:27:30.834,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 11 04:27:56 UTC 2015,,,,,,0|i2d2lr:,9223372036854775807,,,,,,,,"10/Apr/15 10:27;erans;bq. Additional versions of these methods [...] should be added.

I recall that some discussion occurred about that class, and how it did not fit into the design of the package.
IIRC, the idea was to move those methods to somewhere else (but it had to wait for a major release, and we didn't...).

It seems that the fluent API would be again a good way to configure all parameters.

Furthermore, we could create a ""ComplexSolver"" builder with the effect that the ""solve"" method would return a ""Complex[]"".
",10/Apr/15 12:06;psteitz;Sounds great for 4.0.  Any objections to my just adding the methods that take maxEval to 3.x branch?,"10/Apr/15 12:26;erans;bq. Any objections to my just adding the methods that take maxEval to 3.x branch?

No.
Do you mean, then, that the trunk would still be missing those methods?
I'm fine with that. 
But, then, could you perhaps indicate in the 3.x branch that the methods are deprecated?
I could do the same for ""trunk"" and open a JIRA request for the redesign.","10/Apr/15 15:11;psteitz;Its probably best to just move forward with the refactoring for trunk (i.e. no need to add the new methods).  I don't think the new methods in the 3.x branch need to be introduced as deprecated, as the refactoring is still in design and the 3.x branch is going to be maintained in any case.",10/Apr/15 16:00;erans;Issue created to keep track of ideas that have been floated around.,"11/Apr/15 04:27;psteitz;Fixed for 3.x in
Branch: refs/heads/MATH_3_X
Commit: 89a0c4b22154326b7598d68b0bc45e3fbb9f3e2f

Leaving open so we do not forget to provide a way to do this in the 4.0 branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RandomDataGenerator: Inconsistent Javadoc or missing ""extends""/""implements""",MATH-1212,12819658,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,erans,erans,09/Apr/15 13:08,10/Apr/15 19:42,07/Apr/19 20:38,10/Apr/15 19:11,3.4.1,,,,,,,4.0,,,0,documentation,,,,,,,There numerous \{@inheritDoc\} tags in that class although it does not extend a class or implement an interface that contains corresponding methods.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-04-10 19:11:22.803,,,false,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 10 19:42:09 UTC 2015,,,,,,0|i2d13j:,9223372036854775807,,,,,,,,"10/Apr/15 19:11;luc;Fixed in git repository (commit: 8937821).

I'm not sure this really affected 3.4.1 as the class did implement the RandomData interface which did define the corresponding methods (hence the {@inheritDoc}). The interface was deprecated in 3.X and removed in 4.0, so the {@inheritDoc} is now wrong.

The fix was simply to copy back the relevant part from the interface in 3.X and put it in the class in 4.0.

Thanks for the report, it was a nice catch.","10/Apr/15 19:42;tn;I missed this one when removing the deprecated RandomData class.
It does not affect 3.4.X thus this ticket should be deleted and removed from the changelog as it does not affect a released version of commons-math.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line",MATH-1211,12819160,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,michaelzimm,michaelzimm,08/Apr/15 02:25,18/Apr/15 09:28,07/Apr/19 20:38,09/Apr/15 20:37,,,,,,,,3.5,,,0,,,,,,,,"
I constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.

However, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point ""behind"" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.

I think the dot product above should always be positive. If multiple intersections exist along a ""whole"" line then the first one in ""front"" of the line's origin should be returned. This makes ray tracing with a PolyhedronsSet possible.
",,,,,,,,,,,,,,,,,,,,,08/Apr/15 15:44;michaelzimm;IntersectionTest.java;https://issues.apache.org/jira/secure/attachment/12723935/IntersectionTest.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-04-08 07:10:19.771,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 09:28:24 UTC 2015,,,,,,0|i2cy33:,9223372036854775807,,,,,,,,"08/Apr/15 02:48;michaelzimm;It might be enough to incorporate a direction check just after line 365 in the recurseFirstIntersection method of PolyhedronsSet, something like:

final Vector3D hit3D = plane.intersection(line);
if (hit3D != null && hit3D.subtract(point).dotProduct(line.getDirection()>0) {
  ...
}

The problem is that the Plane.intersection method does not check for directionality of the line, thus the PolyhedronsSet.firstIntersection method simply returns the first face that intersects the ""whole"" line rather than the first face intersecting the semi-infinite ""front"" of the line.","08/Apr/15 07:10;luc;Hi Mike,

Could you attach a self-contained test to reproduce the error?","08/Apr/15 15:46;michaelzimm;I'm using commons-math-3.4.  The attached code generates a triangulated cube and runs some interior checks and ray intersection tests.  I think the intersection test should always return a face in ""front"" of the ray but sometimes the PolyhedronsSet.findIntersection method is picking up faces ""behind"" the ray direction.","09/Apr/15 20:37;luc;Your analysis was perfectly correct and your solution was good. I only made
a minor change so it is easier to understand.

Fixed in git repository. Beware the fix is currently only in the MATH_3_X branch, which is not the master branch. It will be included in the upcoming 3.5 release.

Thanks for the report and the fix.","10/Apr/15 01:26;michaelzimm;Hi Luc,

Sorry if my initial explanation was a bit convoluted.  :)


But thanks so much for applying the fix, and glad I could help.

-- Mike





",18/Apr/15 09:28;luc;Closing resolved issue as 3.5 has been released.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation in PoissonDistribution.sample() has dead link,MATH-1209,12781252,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,jcogilvie,jcogilvie,11/Mar/15 17:09,18/Apr/15 09:28,07/Apr/19 20:38,11/Mar/15 21:15,3.4.1,,,,,,,3.5,4.0,,0,,,,,,,,The link in the javadoc at http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/distribution/PoissonDistribution.html for the sample() method points to http://irmi.epfl.ch/cmos/Pmmi/interactive/rng7.htm.  This link is dead.  I found it on the internet archive at https://web.archive.org/web/20090909055517/http://irmi.epfl.ch/cmos/Pmmi/interactive/rng7.htm.  The documentation should be updated or maybe Apache should mirror the page.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-03-11 21:15:01.628,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 09:28:24 UTC 2015,,,,,,0|i26nav:,9223372036854775807,,,,,,,,"11/Mar/15 21:15;tn;Fixed:

 * master: bfb3cf8bba6aab198c4644e236bbc9f3807111d0
 * MATH_3_X: 502335307709b17707d5c28707d668544c85d839

I have found another link at the original website.
Thanks for the report!",18/Apr/15 09:28;luc;Closing resolved issue as 3.5 has been released.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EmpiricalDistribution cumulativeProbability can return NaN when evaluated within a constant bin,MATH-1208,12780371,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,09/Mar/15 00:10,18/Apr/15 09:28,07/Apr/19 20:38,10/Mar/15 00:25,3.4.1,,,,,,,3.5,4.0,,0,,,,,,,,"If x belongs to a bin with no variance or to which a ConstantRealDistribution kernel has been assigned, cumulativeProbability(x) can return NaN.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-04-18 09:28:25.402,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 09:28:25 UTC 2015,,,,,,0|i26hzb:,9223372036854775807,,,,,,,,"09/Mar/15 02:40;psteitz;Fixed in 
master: b148046add84f6c5bd19aeceb10c1a0fcf2e1283
3.x: d41364faa4f73fc885dcc9e99a838e8fa4c95998

Fix was to eliminate interpolation within bin for bins having constant kernel distributions assigned.  For any x within a bin with constant kernel, cumulativeProbability(\x) is now the sum of the bin masses below the bin containing x plus the full mass of the bin to which x belongs.","09/Mar/15 13:51;psteitz;I think the fix I committed in master: b148046add84f6c5bd19aeceb10c1a0fcf2e1283 is incorrect.  When a constant kernel is explicitly set, behavior should pass the original version of testKernelOverrideConstant (with within-bin values less than the point mass not contributing the mass of the bin to the cumulative probability).   The special handling in cumulativeProbability should be
{code}
if (x < kernel.getNumericalMean()) {
    return pBminus;
 } else {
     return pBminus + pB;
 }
{code}
","10/Mar/15 00:25;psteitz;Updated fix applied in
master: ce2badf02e266a84e2485f1793f13bf722c29306
3.x: 13abd04b4633293153034e0348c5dc09ac8bd703",18/Apr/15 09:28;luc;Closing resolved issue as 3.5 has been released.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KohonenUpdateActionTest sometimes fails,MATH-1207,12780348,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,tn,tn,08/Mar/15 21:29,26/Mar/16 23:40,07/Apr/19 20:38,28/Aug/15 14:06,,,,,,,,3.6.1,4.0,,0,,,,,,,,"The KohonenUpdateActionTest sometimes failes with the following output:

{noformat}
KohonenUpdateActionTest
    testUpdate :
  java.lang.AssertionError
  java.lang.AssertionError: expected:<0.0> but was:<5.551115123125783E-17>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:494)
	at org.junit.Assert.assertEquals(Assert.java:592)
	at org.apache.commons.math4.ml.neuralnet.sofm.KohonenUpdateActionTest.testUpdate(KohonenUpdateActionTest.java:90)
{noformat}

Investigate why this is happening and fix the test accordingly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-03-08 23:48:48.381,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 08 23:48:48 UTC 2015,,,,,,0|i26huf:,9223372036854775807,,,,,,,,"08/Mar/15 23:48;erans;Could be related to numerical accuracy.
Tolerance increased in commit 86eb3a2fa74988e4ccf3aae7f4b77edb84555320
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cost in least-squares fitting,MATH-1206,12777885,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,erans,erans,26/Feb/15 13:55,09/Apr/15 12:39,07/Apr/19 20:38,09/Apr/15 12:39,3.3,3.4,3.4.1,,,,,4.0,,,0,,,,,,,,"In {{org.apache.commons.math4.fitting.leastsquares.AbstractEvaluation}}, the value returned by the ""getCost"" method is not consistent with the definition of ""cost"" in a least-squares problem: It is the sum of the squares of the residuals, but the method returns the square-root of that quantity.",,,,,,,,,,,,,,,,,,,,,26/Feb/15 15:47;erans;MATH-1206.patch;https://issues.apache.org/jira/secure/attachment/12701100/MATH-1206.patch,26/Feb/15 14:52;erans;MATH-1206.patch;https://issues.apache.org/jira/secure/attachment/12701078/MATH-1206.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 12:39:52 UTC 2015,,,,,,0|i263fj:,9223372036854775807,,,,,,,,"26/Feb/15 14:52;erans;The (attached) patch causes many tests to now fail:

{noformat}
Failed tests: 
  GaussianCurveFitterTest.testFit01:188 expected:<3496978.1837704973> but was:<3496976.3496594895>
  GaussianCurveFitterTest.testFit04:262 expected:<233003.2967252038> but was:<233002.8643516898>
  GaussianCurveFitterTest.testFit05:275 expected:<283863.81929180305> but was:<283863.7943168686>
  GaussianCurveFitterTest.testFit06:288 expected:<285250.66754309234> but was:<285250.6769121677>
  GaussianCurveFitterTest.testFit07:301 expected:<3514384.729342235> but was:<3514384.6669300473>
  GaussianCurveFitterTest.testMath519:347 expected:<53.1572792> but was:<26.0>
  GaussianCurveFitterTest.testMath798:375 expected:<420.8397296167364> but was:<420.8397358396097>
  GaussianCurveFitterTest.testWithMaxIterations1:204 expected:<3496978.1837704973> but was:<3496978.800406003>
  GaussianCurveFitterTest.testWithStartPoint:229 expected:<3496978.1837704973> but was:<3496978.800406003>
  HarmonicCurveFitterTest.testNoError:57 expected:<0.2> but was:<0.1999955548978442>
  PolynomialCurveFitterTest.testFit:56 best != coeff
 Elements at index 0 differ.  expected = 12.9 observed = -1.0E-20
 Elements at index 1 differ.  expected = -3.4 observed = 3.0E15
 Elements at index 2 differ.  expected = 2.1 observed = -5.0E25
  SimpleCurveFitterTest.testPolynomialFit:57 best != coeff
 Elements at index 0 differ.  expected = 12.9 observed = -1.0E20
 Elements at index 1 differ.  expected = -3.4 observed = 3.0E15
 Elements at index 2 differ.  expected = 2.1 observed = -5.0E25
  EvaluationTest.testComputeCost:175 Kirby2 expected:<3.9050739624> but was:<15.249602651743004>
  EvaluationTest.testComputeSigma:206 Kirby2, parameter #0 expected:<0.087989634338> but was:<0.17387860547376632>
  LevenbergMarquardtOptimizerTest.testControlParameters:116->checkEstimate:155 null
  MinpackTest.testMinpackBard:180->minpackTest:524 expected:<-1.58848033259565E8> but was:<-2.281145030247444E16>
  MinpackTest.testMinpackBox3Dimensional:327->minpackTest:523 expected:<0.0> but was:<0.0030343448521943407>
  MinpackTest.testMinpackBrownAlmostLinear:407->minpackTest:523 expected:<0.0> but was:<0.003621421870604769>
  MinpackTest.testMinpackBrownDennis:343->minpackTest:523 expected:<292.954288244866> but was:<292.9544389375816>
  MinpackTest.testMinpackChebyquad:374->minpackTest:523 expected:<1.88424820499347> but was:<1.18088726698391731E18>
  MinpackTest.testMinpackFreudensteinRoth:152->minpackTest:524 expected:<11.4124844654993> but was:<11.412121480926384>
  MinpackTest.testMinpackHelicalValley:132->minpackTest:523 expected:<0.0> but was:<6.274776288270002E-4>
  MinpackTest.testMinpackKowalikOsborne:196->minpackTest:523 expected:<0.017535837721129> but was:<0.03846867874661756>
  MinpackTest.testMinpackOsborne1:476->minpackTest:523 expected:<0.00739249260904843> but was:<0.007723842649443106>
  MinpackTest.testMinpackOsborne2:487->minpackTest:523 expected:<0.20034404483314> but was:<0.23226823584082015>
  MinpackTest.testMinpackPowellSingular:142->minpackTest:523 expected:<0.0> but was:<0.003097797251086307>
  MinpackTest.testMinpackRosenbrok:122->minpackTest:523 expected:<0.0> but was:<0.1732616669375091>
  MinpackTest.testMinpackWatson:266->minpackTest:523 expected:<0.0011831145921242> but was:<0.0033222964745821893>
{noformat}
","26/Feb/15 15:20;erans;That was to be expected, since the ""cost"" is used to control parts of the the Levenberg-Marquardt algorithm.
Now, we must determine _what_ quantity is used inside the algorithm. Was the implementation correct in using the square-root of the chi-square?
If so we can solve the issue by adding a new method ""getChiSquare()"" rather than change the meaning of ""getCost()"" (even though it can be confusing to use ""cost"" to mean the _square-root_ of the objective function).
","26/Feb/15 15:47;erans;This patch does not modify the semantics of the current code, it only adds a new ""getChiSquare()"" method that returns the value of the objective functon (defined as ""the sum of the squares of the residuals"").

OK to apply?
","26/Feb/15 18:00;erans;I also suggest that we add a
{code}
double getReducedChiSquare(int numFittedParameters);
{code}
method to the {{Evaluation}} interface.
",09/Apr/15 12:39;erans;Methods added in commit 0a499402d707bc8cf775d7f9b3840780a7401f7d,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractStorelessUnivariateStatistic should not extend AbstractUnivariateStatistic,MATH-1205,12776430,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tn,tn,20/Feb/15 12:44,13/Apr/15 20:17,07/Apr/19 20:38,13/Apr/15 20:17,3.4.1,,,,,,,4.0,,,0,,,,,,,,"For a storeless statistic it is wrong to extend AbstractUnivariateStatistic as various fields and methods are inherited that do not make sense in case of a storeless statistic.

This means a user can accidentially use a storeless statistic in a wrong way:

{code}
        Mean mean = new Mean();
        
        mean.increment(1);
        mean.increment(2);
        
        mean.setData(new double[] { 1, 2, 3});
        
        System.out.println(mean.getResult());
        System.out.println(mean.evaluate());
{code}

will output

{noformat}
1.5
2.0
{noformat}",,,,,,,,,,,,,,,,,,,,,12/Apr/15 19:17;tn;AbstractCachingUnivariateStatistic.java;https://issues.apache.org/jira/secure/attachment/12724840/AbstractCachingUnivariateStatistic.java,12/Apr/15 19:11;tn;MATH-1205_v2.patch;https://issues.apache.org/jira/secure/attachment/12724839/MATH-1205_v2.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2015-02-20 17:04:41.126,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 13 20:17:48 UTC 2015,,,,,,0|i25urz:,9223372036854775807,,,,,,,,20/Feb/15 17:04;psteitz;+1 to fix this for 4.x,"21/Feb/15 22:45;tn;I am working on  a patch, but there are at least two other odd things that I have seen wrt storeless statistics:

a) default implementation of equals / hashCode that compares solely on getResult() and getN(): I fail to see a use-case for this and it means that two different types of storeless statistics are considered to be equal if their current state is equal:

{code}
Mean m = new Mean();
Variance v = new Variance();        
System.out.println(m.equals(v)); // outputs true
{code}

I find this very strange, especially as a default.

b) evaluate(double[], ...): the default implementation in AbstractStorelessUnivariateStatistic does the following:

 * clear the internal result
 * call increment for all input values in the array
 * return the result

thus the method alters the internal state, which is different for non-storeless statistics.
Some storeless statistics override this method, e.g. Mean to do something else: calculate the result using an intermediate object and then return the result without altering state. This is quite confusing imho.","21/Feb/15 23:05;psteitz;+1 to drop the default equals override

On the second point, I think it would be best to modify the default evaluate to clear after it computes the result.  Alternatively, implementations could be required to addAll in overrides.  I think the first is a little better, as it conforms to what the current impls generally do and the contract is simpler. 

","12/Apr/15 18:07;tn;Attached a patch with the following changes:

 * added class AbstractCachingUnivariateStatistic which extends AbstractUnivariateStatistic and contains the storedData field and corresponding methods
 * Percentile extends now AbstractCachingUnivariateStatistic
 * fix AbstractStorelessUnivariateStatistic#evaluate(double[], int, int) method to be consistent with the other implementations: it does not alter the internal state anymore, but instead creates first a copy of the current statistic and the calls clear() and incrementAll() on the copy instead.
 * fix AbstractStorelessUnivariateStatistic#equals(Object): the equality check will only succeed for statistics of the same class. This might be useful, although I am not 100% sure, we might also just remove the default equals/hashCode.
 * fix Variance#evaluate(...) methods: they altered the internal state by calling clear(), although the javadoc stated the opposite.

If preferred, I can also make separate issues for the bullets, currently the changes.xml contains separate entries for all of them, but without a ticket number.

btw. the storedData fields and methods have been introduced in MATH-417 to improve the performance of the Percentile class, but have not been used for any other class.",12/Apr/15 19:11;tn;Updated patch as it was missing a file.,"12/Apr/15 19:13;psteitz;The storedData field was introduced in 2.2 because in Percentile we needed to sort and just working on the actual parameter was, well, dubious.  It may not be depended on by other impls; but the fact that the impl gets a copy instead of a reference to the actual parameter is a *good thing*, so +1 to keeping it and in fact just renaming (if necessary) AbstractUnivariateStatistic instead of extending it. There is no need for this excess hierarchy now that the storeless classes no longer extend AUS.  I would be +0 to just leaving things alone (leave AUS as is, but break the inheritance of ASUS).  

I am +0 on just dropping the equals/hc impls.

Otherwise, +1 for  the proposed changes.

I like the changelog style you have, but I would reference this ticket on each entry.",12/Apr/15 19:17;tn;Added missing file: newly added files are not part of a git diff command.,"12/Apr/15 19:21;tn;I wanted to keep AbstractUnivariateStatistic for the following reasons:

 * javadoc states that it is the base class for all UnivariateStatistic implementations, so I did not want to change that
 * there are various test(...) methods which are used by the storeless statistics, so this would have to be copied somewhere else or duplicated

That's the reason why I created the AbstractCachingUnivariateStatistic. btw the AbstractStorelessUnivariateStatistic still inherits from AUS for the reason mentioned above.","12/Apr/15 19:43;psteitz;The way I see it is that with this change, AUS is the base class for univariate stats that store data.  I don't see the need for the totem pole there.  The test methods could be moved to MathArrays (which is not a bad idea in itself - they are really just array validation methods).","12/Apr/15 20:03;tn;ok agreed, but there is still the special case of the SemiVariance which is not storeless, but also does not use the stored data in some way.","12/Apr/15 20:42;psteitz;It does use storedData, like all of the Storeless impls do, via its evaluate() method.",13/Apr/15 20:17;tn;Committed patch with discussed changes in commit 845e1d54231d3ff3fb04bdbf5dc5f6b631d9b01e.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bracket function gives up too early ,MATH-1204,12776265,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,evanward1,evanward1,19/Feb/15 20:14,18/Apr/15 09:28,07/Apr/19 20:38,19/Feb/15 20:22,3.4.1,,,,,,,3.5,4.0,,0,,,,,,,,"In UnivariateSolverUtils.bracket(...) the search ends prematurely if a = lowerBound, which ignores some roots in the interval. ",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-02-19 21:24:02.053,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 09:28:23 UTC 2015,,,,,,0|i25ts7:,9223372036854775807,,,,,,,,19/Feb/15 20:22;evanward1;Fixed in a56d499,"19/Feb/15 21:24;tn;Added missing changelog in commit f1b2fcd7f51995130d71f1703e5c292ec648e94c.
Backported change to the 3.5 branch in commit 9aa63826735104080d8eb612fd3a5c84264c9c17.",19/Feb/15 22:21;evanward1;Thanks Thomas!,18/Apr/15 09:28;luc;Closing resolved issue as 3.5 has been released.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getKernel fails for buckets with only multiple instances of the same value in random.EmpiricalDistribution,MATH-1203,12776213,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,kdkavanagh,kdkavanagh,19/Feb/15 17:46,19/Jun/15 08:21,07/Apr/19 20:38,14/Apr/15 12:18,3.4,3.4.1,,,,,,3.5,4.0,,0,,,,,,,,"After loading a set of values into an EmpericalDistribution, assume that there's a case where a single bin ONLY contains multiple instances of the same value.  In this case the standard deviation will equal zero.  This will fail when getKernel attempts to create a NormalDistribution.  The other case where stddev=0 is when there is only a single value in the bin, and this is handled by returning a ConstantRealDistribution rather than a NormalDistrbution.

See: https://issues.apache.org/jira/browse/MATH-984",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-03-08 21:28:14.734,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 09:28:24 UTC 2015,,,,,,0|i25th3:,9223372036854775807,,,,,,,,"08/Mar/15 21:28;tn;I have seen that Phil fixed this issue, but there is still something wrong in case a bin has a constant real distribution as kernel:

{code}
        final double[] data = {0, 0, 1, 1};
        EmpiricalDistribution dist = new EmpiricalDistribution(2);
        dist.load(data);

        System.out.println(dist.cumulativeProbability(0));
{code}

Will output NaN. I have seen this while taking a look, but did not yet have the time to further dig into it.","08/Mar/15 22:21;psteitz;Thanks, Thomas!  That looks like a different problem.  Looking into it...  Leaving this open for now.","09/Mar/15 00:06;psteitz;Fixed in 59912a0724200beef63df2d74497165c3d1081bf for 3.x
4aa4c6d31f98d8cd80ccbcf99ded77bfc6de25c5 for 4.0","10/Mar/15 08:23;tn;Sorry to bugger again, but there is something with the fix that should at least be documented.
The getKernel method is protected, thus users may override it to return a different distribution, but the cumulativeProbability method now explicitly checks if the bin kernel is of type ConstantRealDistribution. Either we document this explicitly in the javadoc as a requirements when overriding the method, or we do a check on the variance of the kernel to decide if we have to do interpolation or not.","10/Mar/15 13:36;psteitz;Thanks for reviewing, Thomas.  I did think about checking variance = 0 instead and probably should have done that.  In fact, since it is possible that a non-constant user-supplied kernel can have no support in a bin, the 0/0 computation that led to MATH-1208 could happen even without variance == 0.  So I guess a more robust test is first check if the distribution variance is 0, then check kB > 0.  Thanks again for reviewing.  ",18/Apr/15 09:28;luc;Closing resolved issue as 3.5 has been released.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect Kolmogorov–Smirnov Statistic for two samples ,MATH-1197,12768596,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,Danaja,Danaja,20/Jan/15 05:40,25/Jan/16 20:27,07/Apr/19 20:38,26/Apr/15 19:14,3.4.1,,,,,,,3.6,4.0,,0,,,,,,,,"kolmogorovSmirnovTest(double[],double[]) against the samples given below gives 5.699107852308316E-12 instead of 0.9793 (approx.) Traced the issue to kolmogorovSmirnovStatistic(double[],double[]) which gives 0.49507389162561577 instead of 0.064 (verified with ks.test in R and JDistlib)

  double[] x = {0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,2.202653,2.202653,2.202653
                ,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653
                ,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653
                ,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,3.181199,3.181199,3.181199,3.181199,3.181199,3.181199,3.723539
                ,3.723539,3.723539,3.723539,4.383482,4.383482,4.383482,4.383482,5.320671,5.320671,5.320671,5.717284,6.964001,7.352165
                ,8.710510,8.710510,8.710510,8.710510,8.710510,8.710510,9.539004,9.539004, 10.720619, 17.726077, 17.726077, 17.726077, 17.726077
                ,22.053875 ,23.799144 ,27.355308 ,30.584960 ,30.584960 ,30.584960, 30.584960, 30.751808};



         double[] y = {0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                 ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                 ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000
                 ,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,2.202653
                 ,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,2.202653,3.061758,3.723539,5.628420,5.628420,5.628420,5.628420
                 ,5.628420,6.916982,6.916982,6.916982, 10.178538, 10.178538, 10.178538, 10.178538, 10.178538 };",Ubuntu 14.04,,,,,,,,,,,,,,,,,,,,20/Jan/15 20:37;tn;MATH-1197.patch;https://issues.apache.org/jira/secure/attachment/12693378/MATH-1197.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-01-20 12:08:33.072,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 20:27:56 UTC 2016,,,,,,0|i24js7:,9223372036854775807,,,,,,,,"20/Jan/15 12:08;tn;One observation: the samples contain a lot of equal values.

The KS test statistic is implemented using Arrays.binarySearch, but this does not specify which index will be found when looking for a given value in a sorted array.
E.g. if you have samples [0, 0, 0, 0, 0, 1] and you search for 0, you might get an index in the range [0, 4]. As far as I understand the KS statistic, it is an empirical distribution function which calculates the cumulative density based on how many values are less or equal than the given observation, which is not equal to the result returned by Arrays.binarySearch.","20/Jan/15 16:49;psteitz;Yes, this is a bug.  Arrays.binarySearch should not have been used here.","20/Jan/15 20:37;tn;The attached patch fixes the two-sample KS test statistic calculation.
Actually, I did not fully understand the way it was calculated before, but I followed now the formula from the wiki page and it results in the correct result.

After fixing this issue, another problem popped up in test testTwoSampleProductSizeOverflow which relates to a TODO in the ksSum method.","20/Jan/15 22:07;tn;The exactP method also seems to have a problem when comparing it with the results from R.
Take this example:

{code}
        double[] x = new double[] { 0, 0, 0, 0, 1 };
        double[] y = new double[] { 0, 0, 1, 1, 2, 3 };
        
        final KolmogorovSmirnovTest test = new KolmogorovSmirnovTest();
        System.out.println(""p="" + test.kolmogorovSmirnovTest(x, y, true));
        System.out.println(""D="" + test.kolmogorovSmirnovStatistic(x, y));
        
        System.out.println(""approximateP="" + test.approximateP(test.kolmogorovSmirnovStatistic(x, y), x.length, y.length));
        System.out.println(""exactP="" + test.exactP(test.kolmogorovSmirnovStatistic(x, y), x.length, y.length, false));
{code}

returns:

{noformat}
p=0.35714285714285715
D=0.46666666666666673
approximateP=0.5925028311389975
exactP=0.4155844155844156
{noformat}

R computes the following:

{noformat}
data:  x and y
D = 0.4667, p-value = 0.5925
alternative hypothesis: two-sided
{noformat}

Edit: the reason seems to be that R can not compute exactP values in case of ties.","20/Jan/15 22:51;psteitz;Assuming whatever bugs in the D computation have been fixed, our exactP should actually be ""exact.""  I could not make sense of, or find documentation for, what R does for small samples.  Our code computes the exact distribution of the associated D statistic.  I suspect that R does some kind of approximation.  As you said, R I think also disallows ties.",20/Jan/15 22:58;psteitz;+1 on the patch,"26/Apr/15 19:14;tn;Committed patch in commit

 * a6abb8b00355db97486ab5023305c3f4e9e19ad3 for 4.0
 * 870e1d3d98318745d0552c7a6a6f5dada90d7c82 for 3.6",25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath.round (like Math.round) returns surprising results for some arguments,MATH-1196,12768233,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,17/Jan/15 17:51,05/Aug/16 21:04,07/Apr/19 20:38,05/Aug/16 21:04,3.4.1,,,,,,,4.0,,,0,,,,,,,,"The first assertion in the test case below succeeds, but the second fails

{code}
 /*
     * http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6430675
     */
    @Test
    public void testRoundDown() {
        final double x = 0x1.fffffffffffffp-2;
        Assert.assertTrue(x < 0.5d);
        Assert.assertEquals(0, FastMath.round(x));
    }
{code}

This is being tracked as a JDK (actually spec) bug here:
http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6430675",,,,,,,,,,,,,,,,,,,,,21/Apr/15 14:49;tn;MATH-1196.patch;https://issues.apache.org/jira/secure/attachment/12726892/MATH-1196.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-03-04 22:25:47.73,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sun May 29 23:34:39 UTC 2016,,,,,,0|i24hon:,9223372036854775807,,,,,,,,"04/Mar/15 22:25;tn;The implemented change in the jdk is very simple:

{code}
        if (a != 0x1.fffffffffffffp-2) // greatest double value less than 0.5
            return (long)floor(a + 0.5d);
        else
            return 0;
{code}

we could easily do the same.","06/Mar/15 10:29;ntysdd;Not so easy.
Java 7 still wrong.

{code:xml}
    public void test() {
        final double x = 4503599627370497.0; // x = Math.pow(2, 52) + 1;
        Assert.assertEquals(""4503599627370497"", new BigDecimal(x).toString());
        Assert.assertTrue(x == Math.rint(x));
        Assert.assertTrue(x == FastMath.round(x));
        Assert.assertTrue(x == Math.round(x));
    }
{code}",06/Mar/15 18:48;psteitz;Is the fix above in the code that fails this?,"07/Mar/15 07:37;ntysdd;Both Java 6 and Java 7 fail this test.
Java 8 has a different implementation.","21/Apr/15 14:49;tn;Attached a patch derived from jafama project (Apache licensed).

The change is derived from a posting to the openjdk mailinglist:

http://mail.openjdk.java.net/pipermail/core-libs-dev/2013-August/020247.html","29/Dec/15 21:31;psteitz;The patch makes sense to me.  Assuming current and tests above pass, +1 to commit, adding (at least) test case above. ",29/May/16 23:34;erans;This fix should have been considered for inclusion in 3.6 or 3.6.1.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Descriptive statistics"" has a non-final protected field",MATH-1192,12767527,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Duplicate,,erans,erans,14/Jan/15 19:23,16/Feb/15 22:51,07/Apr/19 20:38,16/Feb/15 22:51,3.4,,,,,,,4.0,,,0,APIBug,,,,,,,"Field ""windowSize"" should be private (there is setter method).

",,,,,,,,,,,,,MATH-760,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-02-16 22:51:39.028,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 22:51:39 UTC 2015,,,,,,0|i24dkf:,9223372036854775807,,,,,,,,16/Feb/15 22:51;tn;Closed as duplicate.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QRDecomposition decompose and performHouseholderReflection methods ignore matrix parameters,MATH-1191,12766653,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,10/Jan/15 16:09,18/Apr/15 09:28,07/Apr/19 20:38,10/Apr/15 19:46,3.3,,,,,,,3.5,4.0,,0,,,,,,,,"The protected decompose and performHouseholderReflection methods in QRDecomposition act on the qr matrix set by the constructor, not whatever matrix is supplied as actual parameter.  For 3.x, the javadoc should be updated to indicate that the actual parameter is ignored. For 4.0, the parameters should be removed.  Alternatively, implementations could be changed to act on the arguments.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-04-10 19:46:02.026,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 09:28:23 UTC 2015,,,,,,0|i24887:,9223372036854775807,,,,,,,,"10/Apr/15 19:46;luc;Fixed in git repository, for both 3.5 and 4.0.
Instead of removing the parameters, they are now properly used. It does not really change anything since the parameter passed was already the same matrix.",18/Apr/15 09:28;luc;Closing resolved issue as 3.5 has been released.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleRegression may not report fit statistics correctly for perfect linear models,MATH-1189,12766607,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,psteitz,psteitz,10/Jan/15 01:15,18/Apr/15 09:19,07/Apr/19 20:38,11/Jan/15 22:14,,,,,,,,3.5,,,0,,,,,,,,"The getSignificance(), getRSquare() and getR() methods can return infinite values when they should return 0, 1 and 1 respectively if the sum of squared errors is exactly 0.

From Jenkins test run:
{code}
java.lang.AssertionError: expected:<0.0> but was:<Infinity>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:494)
	at org.junit.Assert.assertEquals(Assert.java:592)
	at org.apache.commons.math3.stat.regression.SimpleRegressionTest.testPerfect(SimpleRegressionTest.java:546)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 11 22:14:36 UTC 2015,,,,,,0|i247y7:,9223372036854775807,,,,,,,,"11/Jan/15 22:14;psteitz;The test failure in the description is caused by incorrect cumulative probability computation in the t distribution on the host that generated the test failure.  If there is a bug somewhere, it is in the t distrbution, not this class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BesselJ java contains non-Java5 code.,MATH-1188,12765784,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,luc,luc,08/Jan/15 16:12,08/Jan/15 16:13,07/Apr/19 20:38,08/Jan/15 16:13,3.4,,,,,,,3.4.1,,,0,,,,,,,,"The class uses Arrays.copyOf, which is only available in Java 1.6+.

Apache Commons Math currently targets 1.5, so this method should not be used. MathArrays.copyOf should be used instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 08 16:13:54 UTC 2015,,,,,,0|i242zj:,9223372036854775807,,,,,,,,"08/Jan/15 16:13;luc;Issue has been identified and fixed by Sebb.

The fix is on commit 6a82f92.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing documentation for ParameterValidator,MATH-1184,12764211,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,bjohnson,bjohnson,30/Dec/14 15:07,26/Jan/15 21:17,07/Apr/19 20:38,26/Jan/15 21:17,3.4,,,,,,,,,,0,,,,,,,,"When I click on the documentation link for ParameterValidator in the fitting.leastsquares package  I get:

The requested URL /proper/commons-math/javadocs/api-3.4/org/apache/commons/math3/fitting/leastsquares/ParameterValidator.html was not found on this server.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-12-30 17:06:10.996,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 26 21:17:17 UTC 2015,,,,,,0|i23un3:,9223372036854775807,,,,,,,,"30/Dec/14 17:06;erans;The file exists in the RM's (Luc) RC3 copy of the site.
But the file is missing in https://svn.apache.org/repos/infra/websites/production/commons/content/proper/commons-math
I guess that the problem is that we forgot to commit ithe new files under svn.
Many others are missing: it looks like all the new classes/interfaces are missing.
We should add an ""svn status"" check in the release howto.
","30/Dec/14 17:30;erans;Added missing files.  I guess we'll have to wait a certain time before they appear on the site.
","30/Dec/14 18:34;erans;It's not fixed :(
The files are not in the doc for release 3.4.","26/Jan/15 21:17;tn;The files are now there for the latest 3.4.1 release, so closing this bug as resolved.

No fix version as there was no software change involved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUG - Use of a Broken or Risky Cryptographic Algorithm - RandomDataGenerator.java,MATH-1183,12763825,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Invalid,,david.espitia,david.espitia,26/Dec/14 14:14,26/Dec/14 16:58,07/Apr/19 20:38,26/Dec/14 16:58,3.3,,,,,,,,,,0,,,,,,,,"We are currently using Commons-math3-3.3 and in the analysis for veracode, found this bug in these class:

1. RandomDataGenerator (Line 285)

Description:

The use of a broken or risky cryptographic algorithm is an unnecessary risk that may result in the disclosure of
sensitive information
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-12-26 14:45:40.955,,,false,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 26 16:58:26 UTC 2014,,,,,,0|i23sbj:,9223372036854775807,,,,,,,,"26/Dec/14 14:45;erans;Please be more specific, starting with line numbers that refer to the development version: Commons Math v3.4 will be available shortly and we do not have the resources to handle older releases.

What is broken?
Which algorithm is risky?
","26/Dec/14 16:58;psteitz;The RandomDataGenerator has a pluggable PRNG.  For cryptographic use, users can supply whatever PRNG they wish.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUG - Insufficient Entropy in Commons-math3-3.3,MATH-1182,12763566,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,david.espitia,david.espitia,23/Dec/14 21:56,23/Dec/14 22:49,07/Apr/19 20:38,23/Dec/14 22:39,3.3,,,,,,,,,,0,,,,,,,,"We are currently using Commons-math3-3.3 and in the analysis for veracode, found this bug in these class:

1. FastMath.java (Line 813)
2. SynchronizedRandomGenerator.java (Line 78 and Line 85)
3. UniformIntegerDistribution.java (Line 164 and Line 172)
4. RandomAdaptor.java (Line 143  and 159)

Type : Insufficient Entropy

Description:

Standard random number generators do not provide a sufficient amount of entropy when used for security purposes.
Attackers can brute force the output of pseudorandom number generators such as rand().

Recommendations:

If this random number is used where security is a concern, such as generating a session key or session identifier, use
a trusted cryptographic random number generator instead. These can be found on the Windows platform in the
CryptoAPI or in an open source library such as OpenSSL.


Thanks.",,432000,432000,,0%,432000,432000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-12-23 22:11:10.284,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 23 22:49:24 UTC 2014,,,,,,0|i23qqv:,9223372036854775807,,,,,,,,"23/Dec/14 22:11;b.eckenfels;I think this is a false positive. If you need a scanner tool to analyse code, you also need to understand where the code is used for what. As the text suggests this is only a problem for security relevant code. Using FastMath for security relevant (i.e. crypto/nonces) code would be an error (but not one in FastMath but in your code).
","23/Dec/14 22:39;psteitz;None of the methods referenced are intended for cryptographic / secure token generating use.  Other PRNGs are available in the library and in most cases, user-supplied PRNGs may also be used.","23/Dec/14 22:49;erans;{quote}
1. FastMath.java (Line 813)
2. SynchronizedRandomGenerator.java (Line 78 and Line 85)
3. UniformIntegerDistribution.java (Line 164 and Line 172)
4. RandomAdaptor.java (Line 143 and 159)
{quote}

General note: to help solve issues, information (such as line numbers) should refer to the development version.

In all the classes above, the code uses a {{RandomGenerator}} (an interface); the actual implementation is chosen by the user!
In your testing, you may have chosen one that is indeed not recommended for secure applications. 
It would be interesting information to know which of the RNGs present in the Commons Math library are secure and which not.
Could you provide it?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KolmogorovSmirnov test algorithm choice prone to integer overflow,MATH-1181,12761742,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,15/Dec/14 13:09,26/Dec/14 19:50,07/Apr/19 20:38,15/Dec/14 13:50,3.3,,,,,,,3.4,,,0,,,,,,,,"The computation used to select the algorithm in the 2-sample KS test is prone to integer overflow, resulting in the ""exact"" method being chosen for very large samples, resulting in impractically long execution time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 15 13:50:36 UTC 2014,,,,,,0|i23fnr:,9223372036854775807,,,,,,,,15/Dec/14 13:50;psteitz;Fixed in 2fb2221d487d925fd5d716173a80c798986aadf0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kolmogorovSmirnovTest poor performance in monteCarloP method,MATH-1179,12761624,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,giladwa1,giladwa1,14/Dec/14 15:17,30/Jun/15 19:48,07/Apr/19 20:38,,,,,,,,,4.0,,,0,,,,,,,,"I'm using the kolmogovSmirnovTest method to calculate pvalues.

However, when i try running the test on two double[] of sizes 5 and 45 the results take over 10 seconds to calculate.
This seems very long, whereas in R it takes a few miliseconds for the same calculation.
I'd be very happy to hear any comment you may have on the subject.

   Gilad",,,,,,,,,,,,,,,,,,,,,14/Dec/14 18:43;giladwa1;KSTest-JavaAndR.txt;https://issues.apache.org/jira/secure/attachment/12687130/KSTest-JavaAndR.txt,14/Dec/14 15:39;giladwa1;KSTestSnippet.txt;https://issues.apache.org/jira/secure/attachment/12687122/KSTestSnippet.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-12-14 15:34:30.536,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 19:48:54 UTC 2015,,,,,,0|i23exr:,9223372036854775807,,,,,,,,"14/Dec/14 15:34;kinow;Hello Gilad!

Could you share some code too? Just some small snippets showing how you are using kolmogovSmirnovTest method, and maybe include your R code too, please? 

Thanks!
Bruno",14/Dec/14 15:39;giladwa1;This is some code for running a java example and an R example,"14/Dec/14 16:39;psteitz;As explained in the class javadoc, the default implementation uses the exact distribution when the product of the sample sizes is less than 200 and a monte carlo simulation when that product is less than 10000.  Both of these are relatively slow, but more accurate (exact, in the first case) for small samples than the asymptotic distribution, which R uses for at least the 45-size samples.   For faster results, you can force the asymptotic distribution to be used.  See the class javadoc and the references there for more information on choice of algorithms, including comments on poor accuracy and stability of the statistic for small samples and the R implementation.","14/Dec/14 18:42;giladwa1;Thank you for the quick reply.
I read the java doc thoroughly, and fully understand that performance on the monte carlo implementation is poorer than the asymptotic distribution.
However, it still seems extremely slow. For example, I added an attachment with two vectors which give the following results:
Java MonteCarlo p value = 0.207 (approximately 4 second calculation)
Java approximate p value = 0.286 (several miliseconds)

R p value = 0.217 (several miliseconds)

The R result seems much closer to the monte carlo result than the asymptotic distribution, however it is calculated extremely fast.
Therefore the current situation is that the Monte Carlo method is too slow, whereas the approximate method is too inaccurate.

Do you have any suggestions as to parameter adjustments or any other way to get results closer to what is provided by R?","14/Dec/14 19:52;psteitz;Read the articles referenced in the javadoc.  It is not obvious that the R answer is accurate.  If you have suggestions for alternative algorithms and can justify them numerically, patches are welcome.  If you can find bugs in the implementations (i.e. can show they do not match the algorithms or are incorrect mathematically) pls do report them.  ","14/Dec/14 20:04;giladwa1;I'm not suggesting the R implementation is in any way better.
However, if I assume the Monte Carlo method is relatively accurate, it is obvious that the R result is much closer than the asymptotic distribution.
I will try reading a bit more, and try adjusting the parameters in order to get better results.
Thanks again for the quick and in depth reply.

   Gilad","14/Dec/14 20:18;psteitz;Unfortunately, the Monte Carlo method can sometimes take a very long time to converge, so you should do multiple runs and use a large number of iterations (unfortunately making it even slower) to verify that the results are stable. 

I forgot to add above, if you can see ways to speed up either the Monte Carlo or exact implementations, patches are welcome. 
",14/Dec/14 20:45;giladwa1;Thank you very much,"15/Dec/14 07:06;giladwa1;By the way, i did find a small bug in your code in the kolmogorovSmirnovTest(double[], double[]) method.
All of your conditions are based on the product of the vector lengths in the following way: x.length * y.length.
However, if you have long vectors, such as 50,000 each (as in my case) the product is out of integer bounds, and therefore you get a negative number which applies to the exact calculation. This takes forever, and is obviously not intended. 
A possible patch could include casting to long data type.

   Gilad",15/Dec/14 13:03;psteitz;THANKS for reporting the length computation bug.  I will open a separate issue for that.,"15/Dec/14 13:05;erans;bq. All of your conditions are based on the product of the vector lengths in the following way: x.length * y.length. However, if you have long vectors, such as 50,000 each (as in my case) the product is out of integer bounds

That would be a bug. Thanks for spotting it.
Could you open a specific issue for it?
","15/Dec/14 13:06;psteitz;Bumping fix version to 4.0 (will change to subsequent 3.x if there is one), but leaving open as it would be great to get speed improvements or better alternatives for the small sample (especially mid-size) algorithms.","15/Dec/14 13:11;erans;Oops, parallel editing...
A possible improvement for the JIRA software: ensure that a user has seen the latest changes.
","01/May/15 10:23;tn;Regarding the inaccurate result for the approximateP method: this is due to the ksSum method whose series expansion does not work well for d values < 1. Using the pelzGood method in this case improves the result a lot.

Regarding the slowness: I think it would be better to adapt the logic to decide which method is used in the kolmogorovSmirnovTest. As explained above, for sizes < 10000 the monte carlo method is used, but it seems to be too slow for such large arrays. Either we reduce this number considerably, or even drop the monte carlo method completely from there. A user can still call it explicitly if needed/wanted. If we can improve the accuracy of the approximateP method, this should be good enough imho?","01/May/15 23:20;psteitz;I would be all for eliminating the Monte Carlo method for the mid-size samples if we can get something else that we know is accurate.  I wish I really understood what R is doing or could find some references explaining it or proposing something better.   Regarding improving approximateP, we should do that if we can in any case.","04/May/15 08:51;tn;Basically all implementation that I checked do the following:

{code}
    public double approximateP(double d, int n, int m) {
        final double dm = m;
        final double dn = n;

        final double en = FastMath.sqrt(dm * dn / (dm + dn));
        // this is added
        final double en2 = en + 0.12 + 0.11/en;

        return 1 - ksSum(d * en2, KS_SUM_CAUCHY_CRITERION, MAXIMUM_PARTIAL_SUM_COUNT);
    }
{code}

I could not find an explanation for this, but it is probably in one of the referenced papers (see link below).
In this Matlab file, there is also an estimation when the asymptotic P-value approximation is considered to be reasonably accurate:

{code}
   (n*m) / (n + m) > 4
{code}

Link: https://github.com/ICEACE/MATLAB/blob/master/kstest2.m

The same is also done in scipy and most likely also in R, but I did not check yet.

Using this correction, we get nearly the same result as in R: 0.2198891183722148","28/Jun/15 11:12;tn;The monte carlo method has been improved in the mean time, see the referenced issue. What remains is to improve the accuracy of the approximateP method and rethink the logic when to select the various methods.

Another approach would be to change the interface of the class to something like that:

{code}
    double kolmogorovSmirnovTest(double[] x, double[] y);
    double kolmogorovSmirnovTest(double[] x, double[] y, boolean strict);
    double kolmogorovSmirnovTest(double[] x, double[] y, boolean strict, Method method);
{code}

where Method is an enum with the values: EXACT, APPROXIMATE, MONTE_CARLO.

The default would call approximate, which is the most reasonable for general use imho (pending that the returned approximate values are improved).","28/Jun/15 14:44;psteitz;Nice work improving the Monte Carlo performance.  Unfortunately, it is slow to converge as you may have seen in testing.  I like the API improvement idea; but I don't like having APPROXIMATE as the default unless it is modified to be smarter than just KS sum based approximation for small samples.  See the discussion in the second reference (http://www.jstatsoft.org/v39/i11/paper) in the class javadoc for how bad that approximation is for small samples.","28/Jun/15 15:13;tn;{quote}
Nice work improving the Monte Carlo performance. Unfortunately, it is slow to converge as you may have seen in testing.
{quote}

Do you mean that the improved monte carlo method is slower for some inputs? I did not see such a behavior while testing, and in fact the method is the same, it is just more efficiently implemented.

Edit: You mean it requires more iterations now? I did some tests on this and could not see a significant difference. Can you provide some test for this?

{quote}
I like the API improvement idea; but I don't like having APPROXIMATE as the default unless it is modified to be smarter than just KS sum based approximation for small samples. See the discussion in the second reference (http://www.jstatsoft.org/v39/i11/paper) in the class javadoc for how bad that approximation is for small samples.
{quote}

Edit: Figure 2 in the paper should explain which method to use for which input data.","28/Jun/15 19:29;psteitz;Sorry, Thomas, I was not clear about the Monte Carlo improvement.  It is uniformly an improvement - a much more efficient way to do the simulation.  The problem is that the results are not consistent for a lot of problem instances without a huge number of iterations.  That is a limitation of the algorithm, not the implementation.  I will see if I can dig up some old examples or generate new ones.  You can see it by doing multiple runs with a moderate number of iterations and comparing results.  Not too stable for unfortunately exactly the class of problem instances it is being used for.

One question for you, Thomas.  I got stuck when implementing the 2-sample test because I could not convince myself that the analysis in the Simard-Ecuyer paper applied to the 2-sample case.  I would have proceeded as you are recommending if I could have convinced myself of that (and found a way to reduce n,m to n).  I tried to do it for the one-sample case, but could not convince myself that the 2-sample case could be viewed the same way.  I am probably missing something very simple here.  Can you explain how exactly the results apply?  Sorry I am being a little dense here.  I would be happy if I could convince myself that at least for the case n = m, the analysis applies directly or somehow we can use some function of m,n.

Regarding the failing test, it may not actually be a problem.  What the test is doing is seeing if a 2-sample KS statistic equal to a critical value shown in the reference table in [1] gives the expected p-value.  The table only provides 2 digits of accuracy in the critical values and the test may be failing falsely due to that.  How bad are the failures (how far off expected values)?

I am sorry I have not been able to really figure out what is going on in R or the other references you mention.  I have not been able to find a good reference covering the 2-sample case in detail.  I will keep looking.  References welcome!

[1] https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test
(section on 2-sample test)","29/Jun/15 19:30;tn;I am not sure myself, and after looking at so many other sources it gets more and more confusing. Actually, the wikipedia article is the best reference wrt the 2-sample test, but unfortunately is not that detailed and is lacking references.

Will keep looking when time allows and update references as I find them.","29/Jun/15 22:19;psteitz;I did a little more research.  The 2-sample case is different because  the test statistic has a discrete distribution.  This makes exact computation possible, which we do for small m,n.  We use a naively computed Smirnov approximation for large n*m and Monte Carlo for ""moderate"" n*m by default.  There are two things we can do to improve this:

#  I can't find a freely available reference, but I am chasing down dead trees for a much better exact computation algorithm that I have seen referenced [1].  What the exactP code does now is simple and correct but ridiculously slow.  I conjecture that the code that I have not been able to figure out in R may be using the algorithm in [1] for small n,m.  By implementing a better exact algorithm, we can use it up to n * m <= 10000, which is the cut R uses.  That basically eliminates the need for the Monte Carlo stuff.
 #  The sum computed in approximateP has known bad numerical properties and our code does not really do anything to correct for this.  The magic numbers in some of the code you reference above have to do with continuity corrections.  We need to research this a little more and apply the corrections to the computation of the sum.

[1] Kim P J and Jenrich R I (1973) Tables of exact sampling distribution of the two sample Kolmogorov–Smirnov  criterion D_mn(m<=n) Selected  Tables  in  Mathematical  Statistics 80–129  American Mathematical Society","30/Jun/15 19:48;psteitz;A note on exactP,  As pointed out in [1], if the combined dataset (two samples merged) contains ties, the permutation-based method used by exactP is not correct.  That's why R gives a warning when you ask for exact p-values in the presence of ties.  In [1] a more sophisticated Monte Carlo method is presented for dealing with the presence of ties.  The naive implementation we currently have (unless and until we find reference to and replace it with the Jenrich-Kim method) might actually be correct in the presence of ties if we insert the ties in the data from which the combinatorial enumeration is done or just use the actual data instead of n + m as an integer to compute the full set of possible D values.

A good general reference on the 2-sample statistic is [2], but that is unfortunately not freely available. 

[1] http://www.cirano.qc.ca/files/publications/2001s-56.pdf
[2] http://dx.doi.org/10.1080/01621459.1969.10501082",,,,,,,,,,,,,,,,,,,,,,,
Incorrect sample of usage of StatUtils at homepage,MATH-1178,12760293,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,zimy,zimy,08/Dec/14 22:58,26/Dec/14 19:50,07/Apr/19 20:38,08/Dec/14 23:17,,,,,,,,3.4,,,0,,,,,,,,"At http://commons.apache.org/proper/commons-math/userguide/stat.html you are providing three ways to compute mean, std, median with DescriptiveStatistics, SummaryStatistics, StatUtils.
But when you execute double std = StatUtils.variance(values), you are not actually get std in std, you get std*std, because DescriptiveStatistics actually returns FastMath.sqrt(getVariance()), but StatUtils returns variance directly.

The solution is to substitute ""double std = StatUtils.variance(values);"" with ""double std = FastMath.sqrt(StatUtils.variance(values));""
",Documentation at http://commons.apache.org/proper/commons-math/userguide/stat.html,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-12-08 23:17:07.19,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 08 23:17:07 UTC 2014,,,,,,0|i237af:,9223372036854775807,,,,,,,,"08/Dec/14 23:17;erans;Thanks for the report.
Fixed in commit 5d6e445484d75dad938cece6fd690427ea648203",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QRDecomposition does not detect the matrix singularity,MATH-1176,12759405,Bug,Reopened,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,,,albert_triv,albert_triv,04/Dec/14 11:20,17/Dec/14 13:15,07/Apr/19 20:38,,3.3,,,,,,,4.0,,,0,,,,,,,,"QRDecomposition fails this test. The default contructor sets the threshold=0, so we will never have abs(Rii) <= 0

public void testSimpleRankDeficient() throws Exception {
		double[][] A = new double[][] { 
				{ 1, 2, 3 }, 
				{ 4, 5, 6 },
				{ 7, 8, 9 }};
		//this matrix is singular			
		
		RealMatrix M2 = MatrixUtils.createRealMatrix(A);
		QRDecomposition qr2 = new QRDecomposition(M2);
		assertFalse(qr2.getSolver().isNonSingular());//this fails
}",,,,,,,,,,,,,,,,,,,,,04/Dec/14 13:23;erans;MATH-1176.failing.patch;https://issues.apache.org/jira/secure/attachment/12685110/MATH-1176.failing.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-12-04 12:48:12.85,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 15 11:03:38 UTC 2014,,,,,,0|i231wn:,9223372036854775807,,,,,,,,04/Dec/14 12:48;tn;This is related to MATH-1024. QRDecomposition uses by default a singularity threshold of 0.,"04/Dec/14 13:17;erans;""QRDecompositionTest"" contains a test with the *transposed* matrix of your example, and a check for singularity that _passes_.
Here are the values of ""rDiag""
* for the case reported:
rDiag: -8.12403840463596, 0.9045340337332888, 2.220446049250313E-16
* for the transpose:
rDiag: -3.7416573867739413, 1.9639610121239321, 0.0
",04/Dec/14 13:23;erans;I've attached a unit test which is currently failing.,"04/Dec/14 14:12;albert_triv;Is it not misleading that, for a given matrix A, we have QR.isSingular(A) != QR.isSingular(A.transpose()) ?","04/Dec/14 14:49;erans;Sure! :)
I posted the unit test to signal that there might be more to the problem than a choice of threshold.  The code that computes ""rDiag"" should probably be modified so as to avoid the inconsistency.
","04/Dec/14 17:02;tn;Yes, there seems to be a bug. I have compared this to octave and jama:

Octave:
{noformat}
A =

   1   2   3
   4   5   6
   7   8   9

octave:9> qr(A)
ans =

  -8.1240e+00  -9.6011e+00  -1.1078e+01
   4.9237e-01   9.0453e-01   1.8091e+00
   8.6164e-01   9.9547e-01  -8.1207e-16

octave:10> qr(transpose(A))
ans =

  -3.7417e+00  -8.5524e+00  -1.3363e+01
   5.3452e-01   1.9640e+00   3.9279e+00
   8.0178e-01   9.8869e-01  -5.3083e-16
{noformat}

Jama:
{noformat}
A.qr(): [-3.741657386773941, 1.963961012123933, -1.7763568394002505E-15]
A.transpose().qr(): [-8.12403840463596, 0.9045340337332888, 2.220446049250313E-16]
{noformat}","15/Dec/14 11:03;tn;I just want to add that our result is not wrong, just surprising that it results in a diagonal element to be exactly zero, while other implementations do return a very small number. The implementation of jama and ours are very similar, the main difference I have seen so far was the calculation of the squared norm.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Laplace's inverse cumulative probability of 0 is not 0, but -Inf",MATH-1175,12759131,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,kloesing,kloesing,03/Dec/14 10:05,26/Dec/14 19:50,07/Apr/19 20:38,03/Dec/14 11:34,,,,,,,,3.4,,,0,,,,,,,,"From reading the code, the following line should be changed from `return 0.0;` to `return Double.NEGATIVE_INFINITY`: https://git-wip-us.apache.org/repos/asf?p=commons-math.git;a=blob;f=src/main/java/org/apache/commons/math3/distribution/LaplaceDistribution.java;h=f8b9355867b6e580323de6862fd303744cb1b2e2;hb=HEAD#l110",,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-12-03 11:34:28.952,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 03 11:34:28 UTC 2014,,,,,,0|i2308f:,9223372036854775807,,,,,,,,"03/Dec/14 11:34;erans;Fixed in commit 17f52a2e559e68d5c6ccc98fcebb5523161fd0ed

Thanks for the report.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some thin rectangles are not handled properly as PolygonsSet,MATH-1174,12758986,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,luc,luc,02/Dec/14 21:04,26/Dec/14 19:50,07/Apr/19 20:38,02/Dec/14 21:22,3.3,,,,,,,3.4,,,0,,,,,,,,"If the width of a rectangle is smaller than the close point tolerances, some weird effects appear when vertices are extracted. Typically the size will be set to infinity and barycenter will be forced at origin.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 02 21:22:26 UTC 2014,,,,,,0|i22zan:,9223372036854775807,,,,,,,,"02/Dec/14 21:22;luc;Fixed in git repository (see commit e6aae3a).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""TricubicInterpolatingFunction"" to replace ""TricubicSplineInterpolatingFunction""",MATH-1173,12758868,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,erans,erans,02/Dec/14 11:18,25/Feb/15 22:05,07/Apr/19 20:38,25/Feb/15 22:05,,,,,,,,4.0,,,0,,,,,,,,"Similar to what has been done in MATH-1166, we could retain the functionality of the now deprecated ""TricubicSplineInterpolatingFunction"" class in a new ""TricubicInterpolatingFunction"" class.",,,,,,,,,,,,,,,,,,,,,02/Dec/14 11:24;erans;MATH-1173.patch;https://issues.apache.org/jira/secure/attachment/12684621/MATH-1173.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-02-25 22:05:34.968,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 25 22:05:34 UTC 2015,,,,,,0|i22ykv:,9223372036854775807,,,,,,,,02/Dec/14 11:24;erans;Proposed patch.,"03/Dec/14 11:56;erans;Code included in commit fd7a4362caf2cd69368a38f2f71ffe19706e341c
Please review.
Are the tolerances in the unit tests revealing some problem?
","14/Dec/14 17:32;erans;Class {{TricubicInterpolator}} to replace {{TricubicSplineInterpolator}} has been added in commit
753f278d10fc0c92e912965379401ac182644101

The tolerance for one unit test must be quite large for it to pass. Review needed.",25/Feb/15 22:05;tn;Removed deprecated classes in commit 0a5cd11327d50e5906fb4dc08bce5baea6b2d247.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clustering implementations have unnecessary overhead,MATH-1171,12757423,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,gWdfL59U,gWdfL59U,24/Nov/14 15:19,22/Apr/17 14:14,07/Apr/19 20:38,,,,,,,,,4.0,,,0,,,,,,,,"I want to apply clustering algorithms like KMeansPlusPlusClusterer to pictures. And creating a point instance for each pixel is not a good idea.

Therefore the interface should not be based on Collections, but on some interface that provides sort of ""get(index)"" accessors to data that is potentially stored in a pixel array etc.
",,,,,,,,,,,,,,,,,,,,,28/Dec/15 20:10;tn;image_clustering_example.png;https://issues.apache.org/jira/secure/attachment/12779692/image_clustering_example.png,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2015-06-29 08:05:11.161,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 28 20:10:27 UTC 2015,,,,,,0|i22pvb:,9223372036854775807,,,,,,,,"29/Jun/15 08:05;tn;Supporting this would require a complete re-design of the clustering interface.
I am wondering if it is worth doing the effort, as the clustering support in commons-math is not really sophisticated (its only useful for small and simple problems and intended to get quick results) and for more complex use-cases other libraries might be a better choice.","28/Dec/15 20:09;tn;In commit f0943a724, I have added an example for the userguide how to cluster images with the current API.

I did some first experiments to improve the API with a Dataset interface that provides access to all elements to cluster without the need to create explicit Clusterable instances.

In order to make the case of image clustering efficient, it would require some more refactoring to avoid unneeded allocations of double arrays (as usually an image is a large array or its pixels / samples). The distance API currently only works with arrays without offset / length arguments, thus for each pixel a separate array must be created, which is more or less the same as creating a Clusterable.

Changing the API to support distance calculations in arrays with offsets / length parameters would allow to create a Dataset that directly operates on the image data without creating intermediate objects. This might be beneficial for other use-cases as well.",28/Dec/15 20:10;tn;Attached a screenshot of the image clustering example.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PolygonSet instantiation throws NullPointerException,MATH-1168,12755175,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Cannot Reproduce,,gun,gun,13/Nov/14 22:13,02/Dec/14 21:25,07/Apr/19 20:38,02/Dec/14 21:25,3.3,,,,,,,,,,0,,,,,,,,"the following generates an exception

{noformat}
import org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet;
import org.apache.commons.math3.geometry.euclidean.twod.Vector2D;

class Test {
    public static void main(String[] args) {
		new PolygonsSet(1.0E-10,
					    new Vector2D(-76.8095991248, -15.087601999),
					    new Vector2D(-76.8095991245, -18.288001999), 
					    new Vector2D(-10.0583778834, -18.2880019979), 
					    new Vector2D(-10.0583778835, -15.0876019979),
					    new Vector2D(-54.8639991264, -15.0876019984));

    }
}
{noformat}

the exception is
{noformat}
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:301)
        at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:159)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.insertEdges(PolygonsSet.java:333)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.insertEdges(PolygonsSet.java:406)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.insertEdges(PolygonsSet.java:406)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.insertEdges(PolygonsSet.java:406)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.insertEdges(PolygonsSet.java:406)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.verticesToTree(PolygonsSet.java:309)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.<init>(PolygonsSet.java:156)
        at Test.main(test.java:6)
{noformat}
",windows 8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-11-23 20:56:06.426,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sun Nov 23 20:56:06 UTC 2014,,,,,,0|i22cnj:,9223372036854775807,,,,,,,,"23/Nov/14 20:56;luc;I was not able to reproduce the issue, neither with the current version after fix for MATH-1162, nor when reverting to a version prior to the fix for MATH-1162.

Could you check if it still occurs on your computer?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OLSMultipleLinearRegression STILL needs a way to specify non-zero singularity threshold when instantiating QRDecomposition,MATH-1167,12754736,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,neilireson,neilireson,12/Nov/14 11:06,26/Dec/14 19:50,07/Apr/19 20:38,12/Nov/14 11:22,3.3,,,,,,,3.4,,,0,,,,,,,,"A fix was made for this issue in MATH-1110 for the newSampleData method but not for the newXSampleData method.

It's a simple change to propagate the threshold to QRDecomposition:
237c237
<         qr = new QRDecomposition(getX());
---
>         qr = new QRDecomposition(getX(), threshold);","jdk1.7_71, OSX 10.10",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-11-12 11:22:27.25,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 12 11:22:27 UTC 2014,,,,,,0|i22a07:,9223372036854775807,,,,,,,,"12/Nov/14 11:22;erans;Fixed in commit 301ad592142079d36f4d33f5309c103c7f4f5dfb
Thanks for the report and patch.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BicubicInterpolation may have a bug,MATH-1166,12754541,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ajo.fod,ajo.fod,11/Nov/14 18:50,26/Dec/14 19:50,07/Apr/19 20:38,02/Dec/14 11:12,,,,,,,,3.4,,,0,,,,,,,,"I plotted a random image with bicubic interpolation hoping to see something like this:
http://en.wikipedia.org/wiki/Bicubic_interpolation
... but instead I got something weird.",,,,,,,,,,,,,,,,,,,,,11/Nov/14 18:52;ajo.fod;Draw.java;https://issues.apache.org/jira/secure/attachment/12680843/Draw.java,23/Nov/14 19:19;erans;MATH-1166.patch;https://issues.apache.org/jira/secure/attachment/12683234/MATH-1166.patch,11/Nov/14 18:56;ajo.fod;myimage.jpg;https://issues.apache.org/jira/secure/attachment/12680845/myimage.jpg,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2014-11-11 20:06:11.239,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 02 11:12:39 UTC 2014,,,,,,0|i228vb:,9223372036854775807,,,,,,,,11/Nov/14 18:56;ajo.fod;Notice the edges seem unmatched. Could it be that the interpolation output has its co-ordinates swapped at some point? ... or am I doing something wrong?,"11/Nov/14 20:06;erans;Are you using the latest code from the development version?
Or the 3.3 release?
","11/Nov/14 20:10;ajo.fod@gmail.com;The 3.3 release.

-Ajo


","11/Nov/14 20:12;erans;Actually, please try your code with the new (in the development version)
{{org.apache.commons.math3.analysis.interpolation.PiecewiseBicubicSplineInterpolator}}.","11/Nov/14 20:34;ajo.fod@gmail.com;Confirmed that the problem exists with the latest code.

-Ajo.


","11/Nov/14 20:35;ajo.fod@gmail.com;BTW, the code is all there ... just give it a run on your machine and point
the browser at your home directory.


-Ajo


",11/Nov/14 20:53;ajo.fod;Piecewise doesn't have the problem.,"23/Nov/14 19:19;erans;I finally tracked down the bug, thanks to web page that showed a code excerpt from the well-known ""reference-that-cannot-be-named"".

There were actually two bugs:
* one was indeed a transposition of the coefficient matrix,
* the other was that the implementation on Wikipedia was assuming a _one_-pixel ""delta"" between samples.

In the attached patch, I've created new classes:
* {{BicubicInterpolatingFunction}}
* {{BicubicInterpolator}}

The reason is that the fix entailed the failure of the tests in
{{BicubicInterpolatingFunctionTest}} related to the methods providing the partial derivatives functionality. A solution could have been to remove those but it cannot be done because of backward compatibility.

The {{TricubicSplineInterpolator}} must be deprecated since it is based on the buggy code, and the implementation requires the partial derivatives...

A problem remained in that the {{BicubicInterpolator}} needs to estimate the derivatives in order to use the {{BicubicInterpolatingFunction}}. This entails that, on the border of the sampling range, the interpolation is wrong.  As a workaround, the method ""isValidPoint"" is overridden, and for the ""valid"" points, the interpolation is within a reasonable tolerance.

This is a weak point of the {{BicubicInterpolator}}, but in some cases (as in the picture produced by Ajo's code), there could be less artefacts.

Is it OK to commit this patch?
Or do we proceed otherwise?
","23/Nov/14 21:04;luc;If I understand correctly, the former BicubiSplineInterpolatingFunction which was deprecated by Hank would still be deprecated and we would have two different implementation, this new one and the piecewise one?

I'm fine with this.","23/Nov/14 21:26;erans;bq. BicubiSplineInterpolatingFunction \[...\] would still be deprecated

Correct.

bq. two different implementation, this new one and the piecewise one?

Yes.
The new implementation is also ""piecewise"".  We should decide whether the names should be changed to something more specific. 

Also, there must be a way to build N-dimensional interpolation schemes, but the current {{Tricubic...}} is tied to a buggy implementation, so it's better to remove it, and perhaps think how a new one can be modular and use the different alternative implementations at runtime...
","02/Dec/14 11:12;erans;""BicubicSplineInterpolator"" class is deprecated, replaced by ""BicubicInterpolator"".

Commit: 136bf34297a9ac8049e451800df819ebf092cdf2
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer,MATH-1165,12753376,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,pasmod,pasmod,06/Nov/14 11:57,26/Dec/14 19:50,07/Apr/19 20:38,07/Nov/14 22:10,3.3,,,,,,,3.4,,,0,easyfix,,,,,,,"The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:

If the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.

So the if condition:
if (membershipMatrix[i][j] > maxMembership) {
                    maxMembership = membershipMatrix[i][j];
                    newCluster = j;
}
will never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line:
clusters.get(newCluster)
                    .addPoint(point);

Adding the following condition can solve the problem:
double d;
if (sum == 0)
d = 1;
else
d = 1.0/sum;",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-11-06 14:59:24.476,,,false,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 07 22:10:00 UTC 2014,,,,,,0|i221yf:,9223372036854775807,,,,,,,,"06/Nov/14 14:59;tn;This can only happen if the clusterer is initialized to form exactly 1 cluster and is then fed with only 1 data point.

It is quite unlikely that somebody will use the clusterer in that way, but we will fix this rare case.","06/Nov/14 21:57;tn;Fixed in commit 596ccd59a11ad5e9fda19ccd0f4fc714d8d3394d

Thanks for the report!",07/Nov/14 20:40;pasmod;I think the case that u mentioned is not the only one! What if one of the initial random centroids is exactly the same as the one of the data points?,"07/Nov/14 22:10;tn;Yes, I realized this while working on the fix.

The implemented fix takes care of these cases too now, so I assume this issue can be closed safely.

If you still encounter problems with the code in trunk, please let us know.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
contains(Region<S> region) method of AbstractRegion throws NullPointerException exception ,MATH-1162,12752726,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,gun,gun,04/Nov/14 14:34,26/Dec/14 19:50,07/Apr/19 20:38,23/Nov/14 20:53,3.3,,,,,,,3.4,,,0,,,,,,,,"the contains call on org.apache.commons.math3.geometry.partitioning.AbstractRegion throws an exception in the sample below:

{noformat}
import org.apache.commons.math3.geometry.partitioning.Region;
import org.apache.commons.math3.geometry.euclidean.twod.Vector2D;
import org.apache.commons.math3.geometry.euclidean.twod.Euclidean2D;
import org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet;

class Test {
    public static void main(String[] args) {
	Region<Euclidean2D> p = new PolygonsSet( 1.0e-10, new Vector2D(4.267199999996532, -11.928637756014894),
	                                                  new Vector2D(4.267200000026445, -14.12360595809307), 
	                                                  new Vector2D(9.144000000273694, -14.12360595809307), 
	                                                  new Vector2D(9.144000000233383, -11.928637756020067));

	Region<Euclidean2D> w = new PolygonsSet( 1.0e-10,  new Vector2D(2.56735636510452512E-9, -11.933116461089332),
	                                                   new Vector2D(2.56735636510452512E-9, -12.393225665247766), 
	                                                   new Vector2D(2.56735636510452512E-9, -27.785625665247778), 
	                                                   new Vector2D(4.267200000030211, -27.785625665247778), 
	                                                   new Vector2D(4.267200000030211, -11.933116461089332));

	p.contains(w);
    }
}
{noformat}

the exception thrown is:

{noformat}
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.commons.math3.geometry.partitioning.AbstractRegion.isEmpty(AbstractRegion.java:263)
        at org.apache.commons.math3.geometry.partitioning.AbstractRegion.isEmpty(AbstractRegion.java:267)
        at org.apache.commons.math3.geometry.partitioning.AbstractRegion.isEmpty(AbstractRegion.java:267)
        at org.apache.commons.math3.geometry.partitioning.AbstractRegion.isEmpty(AbstractRegion.java:267)
        at org.apache.commons.math3.geometry.partitioning.AbstractRegion.isEmpty(AbstractRegion.java:267)
        at org.apache.commons.math3.geometry.partitioning.AbstractRegion.isEmpty(AbstractRegion.java:267)
        at org.apache.commons.math3.geometry.partitioning.AbstractRegion.isEmpty(AbstractRegion.java:267)
        at org.apache.commons.math3.geometry.partitioning.AbstractRegion.isEmpty(AbstractRegion.java:251)
        at org.apache.commons.math3.geometry.partitioning.AbstractRegion.contains(AbstractRegion.java:295)
        at Test.main(test.java:19)
{noformat} 
",tested on windows 8 and heroku,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-11-05 19:51:29.484,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 09 20:54:37 UTC 2014,,,,,,0|i21y0f:,9223372036854775807,,,,,,,,"05/Nov/14 19:51;luc;I confirm the bug.
A first analysis shows that the problems is due to BSPTree.chopOffPlus(hyperplane), which may replace the cut hyperplane with null despite preserving two sub-trees. We end up with a node that should be a leaf node but has children. I guess the same problem could occur in the similar chopOffMinus(hyperplane) method too.

I'm looking at it.

Reducing hyperplane thickness allow the test to pass, but I would not really recommend it, as too small thickness may create numerical problems elsewhere.","05/Nov/14 22:01;gun;hi Luc,

thanks for looking into this. I'm going to mention another issue that I'm seeing that might be related since you mentioned BSPTree. here it is.... the following throws a different exception:

{noformat}
import org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet;
import org.apache.commons.math3.geometry.euclidean.twod.Vector2D;

class Test {
    public static void main(String[] args) {
		new PolygonsSet(1.0E-10,
					    new Vector2D(-76.8095991248, -15.087601999),
					    new Vector2D(-76.8095991245, -18.288001999), 
					    new Vector2D(-10.0583778834, -18.2880019979), 
					    new Vector2D(-10.0583778835, -15.0876019979),
					    new Vector2D(-54.8639991264, -15.0876019984));

    }
}
{noformat}

the exception I get is
{noformat}
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:301)
        at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:159)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.insertEdges(PolygonsSet.java:333)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.insertEdges(PolygonsSet.java:406)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.insertEdges(PolygonsSet.java:406)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.insertEdges(PolygonsSet.java:406)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.insertEdges(PolygonsSet.java:406)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.verticesToTree(PolygonsSet.java:309)
        at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.<init>(PolygonsSet.java:156)
        at Test.main(test.java:6)
{noformat}

","12/Nov/14 20:47;luc;I've made some progress in the analysis. The error is not in the chopOffPlus/chopOffMinus methods but it occurs earlier in the BSP tree merging phase. Debugging this is unfortunately quite hard. I hope I'll understand what happens in the nexxt few days.

I don't think the second problem is similar to the first one you reported. Could you open a separate JIRA issue for it?

",13/Nov/14 22:11;gun;I'll do that. thank you,"23/Nov/14 20:53;luc;The issue has been fixed in the git repository (see commit 046e3a2).

Thanks for the report.",09/Dec/14 20:54;gun;thanks again for fixing this Luc. we've verified the fix on our end and integrated the update. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
problem with the rotation,MATH-1157,12746377,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Not A Problem,,cesFrank,cesFrank,07/Oct/14 11:27,07/Oct/14 14:19,07/Apr/19 20:38,07/Oct/14 14:19,3.3,,,,,,,3.4,,,0,newbie,,,,,,,"Hello,

this is my problem:

myAngle = 0.3490658503988659

The angle is created with FastMath.toRadians(20).

myRotation = new Rotation(RotationOrder.XYZ, 0, 0, myAngle);

if I use this on my Vector3D myVector = new Vector3D(4,4,4) with myRotation.applyInverseTo(myVector) I'm get the result

x = 5.1268510564463075
y = 2.390689909840958
z = 3.999999999999999 (using the .getX() getY() and getZ() functions)

Im working with double values, but after the rotation just around the axis z, the z value of my vector shouldn't have changed, but it does. Maybe it is not a bug but a wrong result after a simple rotation, or did I use the rotation in a wrong way?

Best regards

Frank","Windows 7, Eclipse, Java SE 1.7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-10-07 12:48:33.13,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 07 14:19:19 UTC 2014,,,,,,0|i20vn3:,9223372036854775807,,,,,,,,"07/Oct/14 12:48;luc;I don't think it is a real problem.
What happens here is simply floating point accuracy which is not absolute.
Under the hood, the Rotation class does not use matrices but a more efficient way based on quaternions. This implies that the rotation you define has the following components: q0 = 0.984807753012208, q1 = 0, q2 = 0, q3 = -0.17364817766693033. The expression computing .applyInverseTo is a combination of q0, q1, q2, q3 and your vector x, y, z. The z component is:

(-2 q0 q2) x + (2 q0 q1) y + (2 q0 q0 - 1) z + 2 q3 s

where s = q1 x + q2 y + q3 z

In your case, q1 and q2 are 0, and floating points multiplications and additions involving 0 are exact, so in your specific case, (and only in this specific case)  the z component is (2 (q0^2 + q3^2)  - 1) z. As a rotation quaternion is normalized, this should be equal to z since q1 and q2 are 0. however, the floating point numbers q0 and q3 are not perfect, and the multiplicative factor for z is not exactly 1 but is rather 1-epsilon where epsilon is the gap between 1 and the primitive double number just below 1. This means that the values q0 and q3 were already computed to the full accuracy of the computer. I have checked them using our Dfp high accuracy numbers class, and in fact the error in q0 is about 3.905e-17 and the error in q3 is about -1.767e-17, so it is not possible to have better values for q0 and q3, we are already at the limit of the primitive double.

Do you agree with this explanation? Can we solve the issue as ""Not A Problem""?
","07/Oct/14 14:19;cesFrank;Thanks for your fast and detailed answer. I'll mark the issue as ""Not A Problem"".

Best regards and greetings",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mvn tests fail if JDK 8 is used,MATH-1156,12746354,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,luc,luc,07/Oct/14 09:20,26/Dec/14 19:50,07/Apr/19 20:38,07/Oct/14 11:56,3.3,,,,,,,3.4,,,0,,,,,,,,"Despite the fact the pom.xml specifies Java version to be 1.5, some tests for FastMath use reflection to verify that FastMath implements all methods found in StrictMath. As the class available in a Java 8 environment has newer methods, and these methods are not available in FastMath, the test fail and maven refuses to build the artifacts.

The missing methods should be added, just as the new methods were added when Java 6 was relesed.","Linux Debian Jessie, openJDK 8",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 07 11:56:48 UTC 2014,,,,,,0|i20vhz:,9223372036854775807,,,,,,,,07/Oct/14 11:56;luc;Solved in Git repository (commit a67f0a3),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Statistical tests in stat.inference package are very slow due to implicit RandomGenerator initialization,MATH-1154,12746013,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,Otmar Ertl,Otmar Ertl,05/Oct/14 19:34,26/Dec/14 19:50,07/Apr/19 20:38,15/Dec/14 20:22,3.3,,,,,,,3.4,,,0,,,,,,,,"Some statistical tests defined in the stat.inference package (e.g. BinomialTest or ChiSquareTest) are unnecessarily very slow (up to a factor 20 slower than necessary). The reason is the implicit slow initialization of a default (Well19937c) random generator instance each time a test is performed. The affected tests create some distribution instance in order to use some methods defined therein. However, they do not use any method for random generation. Nevertheless a random number generator instance is automatically created when creating a distribution instance, which is the reason for the serious slowdown. The problem is related to MATH-1124.

There are following solutions:
1) Fix the affected statistical tests by passing a light-weight RandomGenerator implementation (or even null) to the constructor of the distribution.
2) Or use for all distributions a RandomGenerator implementation that uses lazy initialization to generate the Well19937c instance as late as possible. This would also solve MATH-1124.

I will attach a patch proposal together with a performance test, that will demonstrate the speed up after a fix.",,,,,,,,,,,,,,,,,,,,,06/Oct/14 21:30;tn;MATH-1154.patch;https://issues.apache.org/jira/secure/attachment/12673201/MATH-1154.patch,05/Oct/14 19:40;Otmar Ertl;math3.patch;https://issues.apache.org/jira/secure/attachment/12673016/math3.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-10-05 20:06:14.769,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 15 20:22:08 UTC 2014,,,,,,0|i20tfb:,9223372036854775807,,,,,,,,"05/Oct/14 19:40;Otmar Ertl;This patch demonstrates a fix using lazy initialization of default random number generator instances. Furthermore a test is included which gave following results before
{noformat}
statistical tests performance test (calls per timed block: 100000, timed blocks: 100, time unit: ms)
           name      time/call      std error total time      ratio      difference
binomial test 1 1.38289492e-02 1.71975630e-04 1.3829e+05 1.0000e+00  0.00000000e+00
binomial test 2 1.38270752e-02 1.61613547e-04 1.3827e+05 9.9986e-01 -1.87395300e+01
chi square test 2.67553017e-02 2.29903602e-04 2.6755e+05 1.9347e+00  1.29263525e+05
{noformat}
and after
{noformat}
statistical tests performance test (calls per timed block: 100000, timed blocks: 100, time unit: ms)
           name      time/call      std error total time      ratio      difference
binomial test 1 7.26630369e-04 5.87472596e-05 7.2663e+03 1.0000e+00  0.00000000e+00
binomial test 2 7.27780967e-04 2.44728991e-05 7.2778e+03 1.0016e+00  1.15059780e+01
chi square test 5.21210430e-04 3.14044354e-05 5.2121e+03 7.1730e-01 -2.05419939e+03
{noformat}
the fix. A speedup up to a factor of 20 can be seen.","05/Oct/14 20:06;tn;The lazy initialization of the random generator makes sense imho.

I wonder if it would not also be a good idea to refactor the WellXXX random generators. Right now, every time we instantiate one of these a lot of computations are performed although most of them are always the same regardless of the chosen seed. I think it would be better to have a static data object for each WellXXX generator type containined the fields of the abstract base class, and this has to be initialized only once. This would also safe quite some memory. The only fields that need to be stored for each instance are the index and v fields.","06/Oct/14 21:30;tn;I have attached a proposed patch to address the issue in the following way:

 * the patch updates all inference tests to create distributions with a null rng, which avoid additional overhead as we will not sample from the created distributions

 * re-open MATH-1124 and discuss on the mailinglist if we go for the proposed change of a lazy initialization for the distributions or change the default rng from WellXXX to something else

 * create an additional ticket to address potential performance improvements for the WellXXX rngs, can not be done before 4.0 though.","07/Oct/14 11:07;erans;bq. the patch updates all inference tests to create distributions with a null rng, which avoid additional overhead [...]

Doesn't the OP's patch avoid the overhead?
It looks like the performance table above provides good ground to accept the change.

I'd just suggest to replace class ""DefaultRandomGenerator"" with a (static) method defined in ""RandomGeneratorFactory"".

","07/Oct/14 12:43;tn;The reason why I did not want to apply the OP's patch straight away was because of the comment in MATH-1124: lazy initialization was discouraged.

My patch should improve the situation for the inference tests while it does not require any changes to the distribution / random objects for now.","07/Oct/14 13:27;erans;bq. lazy initialization was discouraged

AFAICT, lazy initialization of the distribution's RNG was considered an unnecessary complication (of the distributions classes).

The patch seems to provide an elegant solution. It could be construed that the distribution's RNG is still _not_ lazily initialized, it's the underlying
implementation that is.
An ""average"" user who trusts the provided default will gain in all cases, and a ""power"" user (like you) can still force a ""null"" RNG for cases where he knows that no sampling will be requested.
","07/Oct/14 13:35;tn;I like the solution myself, thats why I am doing what I have outlined before:

 * use a null rng in places where I know that no sampling will be used (and which is hidden to users anyway), namley in the inference tests
 * re-open MATH-1124 to discuss the addition of lazy rng initialization in the distribution classes so that other users can also benefit from it (and may not be as advanced to know details about the use of a rng and when one can provide a null instance)","07/Oct/14 13:46;erans;bq. to discuss the addition of lazy rng initialization 

That was the most important point in my previous comment.
The solution provided here was not among the alternatives sketched in MATH-1224, and should be discussed on its own merit.
Strictly speaking there won't be a lazy initialization in the distribution class.
In fact, the proposal is actually to change the default RNG (from a concrete RNG to a delegating one). In practice, the issue is only a performance improvement, not a change in the distribution's code.
","07/Oct/14 14:07;tn;It is just hiding the aspect of the lazy initialization to another object, which is neat, but it is still lazy initialization, so not much difference to the original proposal imho.
But we do not need to discuss this here, please post a message to the ml to get opinions.","07/Oct/14 14:19;erans;Otmar,

Could you open another issue where you could propose part of the patch here as a new feature (the forwarding and lazily initialized RNG)?
That feature might not be used to fix the issue reported here, but could be included in CM even before asking the question about whether to use it as a default argument to the distributions.
","07/Oct/14 14:28;tn;why do you constantly ignore me, and oppose everything I do or say?

There is already an existing issue (re-opened already) which was even linked by the OP and that outlines the problem at hand.","07/Oct/14 15:02;erans;bq. why do you constantly ignore me, and oppose everything I do or say?

Aren't you reversing the situation?

Apart from your patch with ""null"" RNGs, the other considerations (about MATH-1224, about improving WellXXX RNGs) are related neither to the issue nor to the feature proposed to solve it.
It is (IMHO) clearer to separate concerns:
# Is the lazily initialized RNG a useful feature? \[Should be another issue (feature request).\]
# Is this issue (about tests) to be solved by initializing the RNG with ""null""? \[A possible fix for this issue.\]
# Is the default RNG (in a distribution instance) going to be a lazily-initialized RNG implementation? \[Another possible fix for this issue (if and once the feature exists in CM).\]
# Is the default concrete RNG instance to be changed (from WellXxx to ...)? \[Unrelated.\]
# Is the implementation of WellXxx to be improved? \[Not directly related.\]
# Is the implementation of other RNG (which use insecure constructs) to be improved? \[My addition of yet another similarly unrelated issue.\]

I'm proposing to tidy up things and process them in order, and you keep ignoring the suggestion.
If you and the OP want to resolve _this_ issue, then there is only one alternative at present: Your fix. Please apply it and let's move on to the other suggestions.

","07/Oct/14 15:19;tn;Well, I know it is pointless to discuss anything with you as you will never acknowledge anything else than your POV.

{quote}
I'm proposing to tidy up things and process them in order, and you keep ignoring the suggestion.
If you and the OP want to resolve this issue, then there is only one alternative at present: Your fix. Please apply it and let's move on to the other suggestions.
{quote}

but I can not leave this statement as is, how do I ignore anything or do not process things in a reasonable way?

I did the following:

 * attach a clean patch so solve the original issue at hand that does not require additional changes (open for discussion, I did not apply it)
 * create a separate issue to address the potential performance improvements for existing rngs
 * re-open the original issue that was referred to by the OP and suggested to discuss the potential solutions (as currently attached to this issue) on the ml

I mean, seriously, I am fed up with discussions like this.","07/Oct/14 15:52;psteitz;I agree that the root issue really is MATH-1124.  When we decided to move sampling into the distributions we created the need for distribution instances to have access to a PRNG.  When we decided we wanted everything to be final we forced ourselves into the MATH-1124 state, where the only way to avoid potentially expensive PRNG initialization when creating distribution instances that may never use sampling is the smelly workaround to null out the (final) PRNG at instance construction time.  Thomas' patch looks fine to me and unless and until we change one of the decisions above (relax final obsession or pull sampling back out), we should use the workaround in the unit tests (as the patch does) and try to reduce the initialization cost of the default or find a better workaround (reopening MATH-1124).   The OPs patch adds complexity, IMO, without really addressing the core problem, which I think we should address in MATH-1124.  So I am +1 to applying Thomas' patch, resolving this issue and moving back to MATH-1124.","08/Oct/14 11:03;Otmar Ertl;I agree with Phil that my proposed patch does not address the core problem, which is definitely MATH-1124. The patch was thought as a short-term fix, which improves the performance essentially with minimal invasive code changes. Once the root issue is solved, both patches proposed here (mine and Thomas') will become obsolete anyway. The question is when will MATH-1124 be resolved? In the case it is fixed for the next release, none of both patches need to be applied. If not, it makes sense to apply one of both patches for the short-term. The question remains, do you only want a speed up of the statistical tests (Thomas' patch) or also of the distribution classes (my patch) ? ","08/Oct/14 11:16;erans;\@Thomas:

bq. I mean, seriously, I am fed up with discussions like this.

Same here (because it takes at least two to discuss).
You did what you did, then ignored my suggestions, then rephrased your comments to look like a confrontation rather than a different proposal to handle _this_ issue (which you did not even consider).  Please learn how to read:  In my *first* comment, I gave my opinion on the part of Otmar's patch that provides a new feature (delegating RNG), and in my *second* comment, I indirectly acknowledged that your patch was fine to fix _this_ issue. I only advocated to not mix different (IMHO) things here (thus, implicitly agreeing, again, that other issues should be better discussed on the ML).
Rather than repeating what you did, you could have made a mental ""diff"" with what I actually wrote, and you'd have perhaps seen that there wasn't such a big difference.  At the very least, none that warranted this outburst of resentment.

\@Phil:

bq. the root issue really is MATH-1124

I don't think so, as I've explained above. Unless I'm mistaken, Otmar's proposal allows to keep all fileds ""final"" in the distribution classes:  From their perspective, the RNG is a black box, and whether an implementation uses lazy initialization (thus whether _its_ fields are ""final"" or not) is totally irrelevant.

bq. try to reduce the initialization cost of the default

That's what Otmar's proposal (i.e. the new ""LazyInitRNG"") does.

bq. The OPs patch adds complexity, IMO, without really addressing the core problem

Then I don't know what the ""core problem"" is.

bq. moving back to MATH-1124

That issue is named ""Instances of AbstractRealDistribution require a random generator"", and you yourself put an end to it by making clear that the statement is false.
As said by you (on MATH-1224), and by Thomas (and me) here, this issue is fixed by setting the RNG argument to ""null"". Why do you call that a ""smelly workaround"" whereas it's a quite clear and legitimate assessement on the caller's part?
Do we talk about ""aesthetics""? Then, I can certainly agree that (depending on the type of regular usage one is used to) it might look ugly to often call a constructor with a ""null"" argument.
I vaguely recall that we discarded a more flexible separation of concerns (through inheritance and/or additional interfaces) as unnecessarily complex. On the other hand, I recall clearly that everyone agreed that ""sampling"" was part of the concept of a distribution.

I do not deny that there is a link with Otmar's proposal, but only in the sense that his new feature would satisfy users with ""mixed"" needs (and do not want to use separate instances for when they need sampling and when they don't).
","08/Oct/14 11:25;erans;Otmar,

As you've seen, this forum is not for discussions or design decisions, and certainly not for propagating disparaging comments about contributors... ;)

If one thinks of removing the ""sample"" method from the distribution classes, this won't happen in a 3.x release.
I'm still at a loss with what the ""core problem"" and ""root issue"" are; thus please start a thread on the ""dev"" ML describing them.
","08/Oct/14 12:37;tn;@Gilles: well, I apologize for my public outburst, but it does not change my opinion on the discussion culture, and I will not further comment on it.",13/Oct/14 15:59;psteitz;Applied Thomas' patch in commit a3fdeb4da91d8aef50f40a3f9906494593ce2eca.  Still todo to resolve (for 3.x): update javadoc on distribution class constructors to recommend passing null RandomGenerator when sampling is not going to be used.,15/Dec/14 15:52;tn;I will take care of the javadoc changes.,15/Dec/14 20:22;tn;Added javadoc in commit 809f0f89cb53548a7d0a9f9f52c4b36f60c7b6c0 to all distributions.,,,,,,,,,,,,,,,,,,,,,,,,,
Suboptimal implementation of EnumeratedDistribution.sample(),MATH-1152,12740986,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sandris,sandris,12/Sep/14 07:46,26/Dec/14 19:50,07/Apr/19 20:38,30/Sep/14 19:17,3.3,,,,,,,3.4,,,0,,,,,,,,"org.apache.commons.math3.distribution.EnumeratedDistribution.sample() performs a *linear* search to find the appropriate element in the probability space (called singletons here) given a random double value. For large probability spaces, this is not effective. Instead, we should cache the cumulative probabilities and do a *binary* search.

Rough implementation:
{code:title=EnumeratedDistribution.java|borderStyle=solid}
void computeCumulative() {
  cumulative = new double[size]; 
  double sum = 0;
  for (int i = 1; i < weights.length - 1; i++) {
      cumulative[i] = cumulative[i-1] + weights[i-1];
   }
  cumulative[size - 1] = 1;
}
{code}

and then 
{code:title=EnumeratedDistribution.java|borderStyle=solid}
int sampleIndex() {
 double randomValue = random.nextDouble();
 int result = Arrays.binarySearch(cumulative, randomValue);
 if (result >= 0) return result;
 int insertionPoint = -result-1;
 return insertionPoint;
}

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-09-30 19:17:20.448,,,false,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 30 19:17:20 UTC 2014,,,,,,0|i1zynj:,9223372036854775807,,,,,,,,"30/Sep/14 19:17;tn;Fixed in commit 97accb47de63ee5063eda23641c6017e29ab81d7.

Thanks for the report and patch!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cast can fail in ""LazyUnweightedEvaluation""",MATH-1151,12739895,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,erans,erans,erans,08/Sep/14 15:48,26/Dec/14 19:50,07/Apr/19 20:38,18/Sep/14 13:17,,,,,,,,3.4,,,0,API,,,,,,,"In ""LeastSquaresFactory.java"" at line 487, a cast is performed that will fail when the ""model"" argument is not instantiated with the ""model"" method (defined at line 278).

A fix could consist in the methods defined in (currently internal, line 291) class ""LocalMultivariateJacobianFunction"" becoming part of the public API, in an extension of the ""MultivariateJacobianFunction"" interface.
The contructor of ""LocalLeastSquaresProblem"" (at line 371) should then throw an exception if lazy evaluation is requested but the ""model"" does not implement the appropriate interface.
",,,,,,,,,,,,,,,,,,,,,12/Sep/14 22:04;erans;MATH-1151.patch;https://issues.apache.org/jira/secure/attachment/12668480/MATH-1151.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 18 13:17:54 UTC 2014,,,,,,0|i1zsm7:,9223372036854775807,,,,,,,,12/Sep/14 22:04;erans;Proposed patch.,18/Sep/14 13:17;erans;Committed in revision 1625967.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix typo in changes.xml,MATH-1150,12738738,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,taueres,taueres,03/Sep/14 08:12,03/Sep/14 10:11,07/Apr/19 20:38,03/Sep/14 10:11,3.4,,,,,,,,,,0,,,,,,,,"There is an error in changes.xml. ""ation"" tag has been opened instead of ""action"".",,,,,,,,,,,,,,,,,,,,,03/Sep/14 08:16;taueres;patch;https://issues.apache.org/jira/secure/attachment/12666179/patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-03 10:11:31.051,,,false,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 03 10:11:31 UTC 2014,,,,,,0|i1zm93:,9223372036854775807,,,,,,,,03/Sep/14 08:16;taueres;Proposed patch,"03/Sep/14 10:11;erans;Fixed in r1622206.
Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unsafe initialization in DummyStepInterpolator,MATH-1149,12736040,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mjkim0324,mjkim0324,22/Aug/14 11:35,26/Dec/14 19:50,07/Apr/19 20:38,13/Sep/14 15:37,3.3,,,,,,,3.4,,,0,,,,,,,,"{code:title=DummyStepInterpolator.java|borderStyle=solid}
  public DummyStepInterpolator(final DummyStepInterpolator interpolator) {
    super(interpolator);
    currentDerivative = interpolator.currentDerivative.clone();
  }
   @Override
  protected StepInterpolator doCopy() {
    return new DummyStepInterpolator(this);
  }
{code}

A constructor in DummyStepInterpolator dereferences a field of the parameter, but a NullPointerException can occur during a call to doCopy().

{code:title=Test.java|borderStyle=solid}
public void test1() throws Throwable {
  DummyStepInterpolator var0 = new DummyStepInterpolator();
  var0.copy();
}
{code}

Here in Test.java, a NPE occurs because copy() calls doCopy() which calls  DummyStepInterpolator(final DummyStepInterpolator) that passes var0 as an argument.

I think this constructor should have a null check for  interpolator.currentDerivative like NordsieckStepInterpolator does.

{code:title=NordsieckStepInterpolator.java|borderStyle=solid}
    public NordsieckStepInterpolator(final NordsieckStepInterpolator interpolator) {
        super(interpolator);
        scalingH      = interpolator.scalingH;
        referenceTime = interpolator.referenceTime;
        if (interpolator.scaled != null) {
            scaled = interpolator.scaled.clone();
        }
        if (interpolator.nordsieck != null) {
            nordsieck = new Array2DRowRealMatrix(interpolator.nordsieck.getDataRef(), true);
        }
        if (interpolator.stateVariation != null) {
            stateVariation = interpolator.stateVariation.clone();
        }
    }

    @Override
    protected StepInterpolator doCopy() {
        return new NordsieckStepInterpolator(this);
    }
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-09-13 15:37:03.005,,,false,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 13 15:37:03 UTC 2014,,,,,,0|i1z86f:,9223372036854775807,,,,,,,,"13/Sep/14 15:37;tn;Fixed in r1624752.

Thanks for the report!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MonotoneChain handling of collinear points drops low points in a near-column,MATH-1148,12735440,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,gmarceau,gmarceau,20/Aug/14 16:17,26/Dec/14 19:50,07/Apr/19 20:38,29/Sep/14 16:28,3.3,,,,,,,3.4,,,0,,,,,,,,"This code
{code}
val points = List(
  new Vector2D(
    16.078200000000184,
    -36.52519999989808
  ),
  new Vector2D(
    19.164300000000186,
    -36.52519999989808
  ),
  new Vector2D(
    19.1643,
    -25.28136477910407
  ),
  new Vector2D(
    19.1643,
    -17.678400000004157
  )
)
new hull.MonotoneChain().generate(points.asJava)
{code}

results in the exception:
{code}
org.apache.commons.math3.exception.ConvergenceException: illegal state: convergence failed
	at org.apache.commons.math3.geometry.euclidean.twod.hull.AbstractConvexHullGenerator2D.generate(AbstractConvexHullGenerator2D.java:106)
	at org.apache.commons.math3.geometry.euclidean.twod.hull.MonotoneChain.generate(MonotoneChain.java:50)
	at .<init>(<console>:13)
	at .<clinit>(<console>)
	at .<init>(<console>:11)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:704)
	at scala.tools.nsc.interpreter.IMain$Request$$anonfun$14.apply(IMain.scala:920)
	at scala.tools.nsc.interpreter.Line$$anonfun$1.apply$mcV$sp(Line.scala:43)
	at scala.tools.nsc.io.package$$anon$2.run(package.scala:25)
	at java.lang.Thread.run(Thread.java:662)
{code}

This will be tricky to fix. Not only is the point (19.164300000000186, -36.52519999989808) is being dropped incorrectly, but any point dropped in one hull risks creating a kink when combined with the other hull.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-08-20 20:46:32.75,,,false,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 29 16:28:42 UTC 2014,,,,,,0|i1z4x3:,9223372036854775807,,,,,,,,"20/Aug/14 16:55;gmarceau;Inversely, points in a column that should be dropped will sometimes be kept instead if `includeCollinearPoints = true` and `tolerance=0`.

This input: 
{code}
[{0; -29.959696875}, {0; -31.621809375}, {0; -28.435696875}, {0; -33.145809375}, {3.048; -33.145809375}, {3.048; -31.621809375}, {3.048; -29.959696875}, {4.572; -33.145809375}, {4.572; -28.435696875}]
{code}
computes this upper hull:
{code}
[{4.572; -28.435696875}, {0; -28.435696875}, {0; -31.621809375}, {0; -29.959696875}]
{code}
which retrogrades at the end.
","20/Aug/14 20:46;tn;We do not yet check if a collinear point leads to a concave section of the lower/upper hull.

So far the assumption was that a collinear sequence of points will be like this: A - B - C, thus never creating a concave section.
In your example it is like this (A and B currently part of the hull): C - A - B, thus the next point is collinear in the other direction.

This case is indeed tricky to handle. The best way would be to check if the point C is in the same direction as B, if not, we need to backtrack (i.e. remove points) on the hull until it is convex again.","29/Sep/14 16:28;tn;Fixed in commit 4080feff61e8dc1cd4af2361990c33c9f1014147.

The problem could be solved by taking the tolerance factor for collinear points into account when sorting the input which is the first necessary step for the Monotone chain algorithm.

Then the construction of the upper/lower hull works correctly.

I also improved the isConvex method in ConvexHull2D which actually did not work reliable in the presence of collinear points.

Thanks for the report, please let us know if you encounter any other problems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
class Mean returns incorrect result after processing an Infinity value,MATH-1146,12734473,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,cogen,cogen,15/Aug/14 18:03,26/Dec/14 19:50,07/Apr/19 20:38,16/Dec/14 23:53,3.3,,,,,,,3.4,,,0,,,,,,,,"1. Create a Mean object.
2. call increment() with Double.POSITIVE_INFINITY.
3. Call getResult(). Result is INFINITY as expected.
4. call increment() with 0.
5. Call getResult(). Result is NaN; not INFINITY as expected.

This is apparently due to the ""optimization"" for calculating mean described in the javadoc. Rather than accumulating a sum, it maintains a running mean value using the formula ""m = m + (new value - m) / (number of observations)"", which unlike the ""definition way"", fails after an infinity.

I was using Mean within a SummaryStatistics. Other statistics also seem to be affected; for example, the standard deviation also incorrectly gives NaN rather than Infinity. I don't know if that's due to the error in Mean or if the other stats classes have similar bugs.",,,,,,,,,,,,,,,,,,,,,16/Aug/14 01:40;erans;MATH-1146.patch;https://issues.apache.org/jira/secure/attachment/12662218/MATH-1146.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-08-16 01:40:37.38,,,false,,,,,,,,,,,,,,412413,,,Tue Dec 16 23:53:56 UTC 2014,,,,,,0|i1yy4n:,412400,,,,,,,,"16/Aug/14 01:40;erans;Here is a tentative patch which I thought would solve the problem.
However, it makes another unit test fail.

{noformat}
Tests run: 5, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec <<< FAILURE! - in org.apache.commons.math3.stat.descriptive.AggregateSummaryStatisticsTest                                                                                                                                                      
testAggregateSpecialValues(org.apache.commons.math3.stat.descriptive.AggregateSummaryStatisticsTest)  Time elapsed: 0.001 sec  <<< FAILURE!                  
java.lang.AssertionError: expected:<Infinity> but was:<NaN>                                                                                                  
        at org.junit.Assert.fail(Assert.java:88)                                                                                                             
        at org.junit.Assert.failNotEquals(Assert.java:743)                                                                                                   
        at org.junit.Assert.assertEquals(Assert.java:494)                                                                                                    
        at org.apache.commons.math3.TestUtils.assertEquals(TestUtils.java:55)                                                                                
        at org.apache.commons.math3.stat.descriptive.AggregateSummaryStatisticsTest.assertEquals(AggregateSummaryStatisticsTest.java:236)                    
        at org.apache.commons.math3.stat.descriptive.AggregateSummaryStatisticsTest.testAggregateSpecialValues(AggregateSummaryStatisticsTest.java:222)      
{noformat}","16/Aug/14 01:53;psteitz;I am leaning toward improving the javadoc to be clear about what happens with infinities (if possible to do consistently).  The updating formulas and setup are designed for fast, accurate computations and I want to make sure we do not add a performance hit for users with standard (non NaN, non-Inf) data so we can return meaningful results in the presence of INFs. ","16/Aug/14 02:35;erans;Is there a consistent policy throughout CM for handling NaN and infinities?

Unit tests for some statistics classes refer to ""special values"", so it looks like they were meant to be handled. If so, shouldn't we assume that they must be handled correctly, in all cases?
Alternately, it would be fine to just signal that if special values are passed to those classes, results are undefined (filtering is the caller's responsibility).
Anything in between is confusing, IMHO.

Consistency should come first. Once the policy is defined, we can then look for an efficient way to implement it.
","16/Aug/14 13:41;psteitz;I think the policy should be we document fully our algorithms and NaNs / INFs impact computations according to Java's extended arithmetic rules.   This is the de facto policy we have in most places now, when you get down to it.  Otherwise, we end up having to pre-process data for users and make (sometimes arbitrary) rules about what computations should actually mean in the presence of these values.  I do think its best to specify behavior though, so I favor improving javadoc where we can.  In the present case, the updating formula is provided in the javadoc, so one could argue there is nothing to do, but I would not be averse to adding a statement describing behavior when a NaN or INF is added to the data.  I am -0 on trying to extend the definition of the mean to datasets including infinities.","16/Aug/14 15:02;erans;By ""policy"" here, I don't mean best coding practices (which includes good documentation).
But rather, can we state a general policy on handling special values instead of repeating all over the place (and forgetting to do so) that results could be garbage if NaN or infinities are involved?
There are several places in CM where the implementations do not behave as one would expect from the plain math definition, and it has led to bug reports (recall e.g. ""Complex""). I don't dispute that there may be good reasons, and that the doc may state it clearly enough (although I don't think it that the doc should contain what could be deemed an implementation detail, such as the actual computation used to maintain the mean); I find it preferrable to state once and for all that if users want to manipulate NaN and infinities, they should not expect that CM will deliver correct results. The rationale being:
bq. \[CM is\] designed for fast, accurate computations \[... and\] we do not add a performance hit for users with standard (non NaN, non-Inf) data \[... in order to\] return meaningful results in the presence of INFs.
which is a quite well stated and perfectly acceptable policy IMO.

","16/Aug/14 15:39;psteitz;I am fine with stating the general policy that NaNs and infinities are not specially handled by commons math.  I think documenting algorithms is important, though, and I will continue to do it where I can.   Most of what we do amounts to providing numerical approximations of mathematically defined quantities.  The numerical algorithms used in the approximation are part of the API contract, IMO, as it is their results that we are returning.   I see no harm in pointing out where possible what happens when infinities or NaNs are included in user-supplied data.  I think there are a few cases where we filter / specially handle these values today.  I am OK agreeing to change this behavior, but we should talk about it on the dev list and target a major release to implement these API changes.

Back to this issue - I think the right resolution is to edit javadoc to make clear how NaNs and infinities impact the mean computation.  Same for other stats mentioned in this ticket.","18/Aug/14 12:24;cogen;Phil Steitz says ""The updating formulas and setup are designed for fast, accurate computations and I want to make sure we do not add a performance hit for users with standard (non NaN, non-Inf) data so we can return meaningful results in the presence of INFs. ""

I fail to see how the current implementation of Mean could be faster than the definition way: maintaining a sum and dividing by the number of items.

The current implementation maintains a current average, rather than computing the average from the sum when requested. So the number of divide operations is proportional to N rather than being equal to 1 for the definition way. (Assuming the normal use case of processing a bunch of values then getting the mean at the end. Whereas the current implementation is only an optimization if somebody wants to know the mean often - like after processing each input.)","18/Aug/14 17:05;erans;bq. \[...\] fast, accurate \[...\]

There is a trade-off; this implementation of ""Mean"" is going to be accurate for longer sequences than with the other formula.
But it may be interesting to provide the alternative implementation which, as you indicate, is more efficient (and handles correctly the use-case reported here).
Could you please request further opinions on the ""dev"" ML?
",05/Oct/14 18:39;psteitz;Will edit javadoc for 3.4 release.,16/Dec/14 23:53;psteitz;Javadoc fixes committed in 26e61145839e4da47eb83edfba406ceefc0b67bf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integer overflows MannWhitneyUTest#mannWhitneyU,MATH-1145,12733315,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,aconbere,aconbere,11/Aug/14 18:33,26/Dec/14 19:50,07/Apr/19 20:38,13/Sep/14 15:47,3.3,3.4,,,,,,3.4,,,0,,,,,,,,"In the calculation of MannWhitneyUTest#mannWhitneyU there are two instances where the lengths of the input arrays are multiplied together. Because Array#length is an integer this means that the effective maximum size of your dataset until reaching overflow is Math.sqrt(Integer.MAX_VALUE).

The following is a link to a diff, with a test the exposes the issue, and a fix (casting lengths up into doubles before multiplying).

https://gist.github.com/aconbere/4fef56e5182e510aceb3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-09-13 15:47:12.764,,,false,,,,,,,,,,,,,,411343,,,Sat Sep 13 15:47:12 UTC 2014,,,,,,0|i1yro7:,411335,,,,,,,,"13/Sep/14 15:47;tn;Fixed in r1624756.

Changed the calculation to long as this should be sufficient and is already done like this in other places.

Thanks for the report and patch!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LevenbergMarquardtOptimizer does not allow to change current point during optimization,MATH-1144,12733235,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Implemented,,omovchan,omovchan,11/Aug/14 13:35,26/Dec/14 19:50,07/Apr/19 20:38,03/Nov/14 11:19,3.3,,,,,,,3.4,,,0,fitting,,,,,,,"It's a regression to commons-math v2.0.

Our software uses LevenbergMarquardtOptimizer for surface fitting by sampled points. Our parameterization of the surface we are fitting may be unconstrained, for example it is enough to have only 4 variables to represent cylinder axis and origin (using euler angles and origin distance), but to simplify derivative computation we instead use 6 parameter representation (vector + point). To make sure that the we constrain our search to valid vectors and origins, we need to renormalize and update surface parameters on every step of optimization.

Please see this article for details of 3d surface fitting and parameter normalization:
http://nvlpubs.nist.gov/nistpubs/jres/103/6/j36sha.pdf

Attached surface_fitting_tests.zip package with 2 unit tests that reproduce this problem.
Contents of package:
1) simple - simple single file test that tests only in/out side effect of patched library
2) full - complex test that fits cylinder using sampled points (uses cylinder, fit, utils sources)
3) lib - contains commons-math3 jar libraries: v3.3 and v3.3A1 (patched). There are also library sources.
4) patch - contains SVN patch file

To reproduce:
Run SurfaceFitterFullTest.java and SurfaceFitterSimpleTest.java tests with commons-math3-3.3.jar OR commons-math3-3.3A1.jar libraries.
",,,,,,,,,,,,,,,,,,,,,11/Aug/14 13:40;omovchan;LevenbergMarquardtOptimizer.java.patch;https://issues.apache.org/jira/secure/attachment/12660986/LevenbergMarquardtOptimizer.java.patch,15/Oct/14 20:41;erans;MATH-1144.patch;https://issues.apache.org/jira/secure/attachment/12675101/MATH-1144.patch,16/Oct/14 12:56;omovchan;SurfaceFitterSimpleTest.java;https://issues.apache.org/jira/secure/attachment/12675274/SurfaceFitterSimpleTest.java,20/Aug/14 09:57;omovchan;surface_fitting_tests.zip.001;https://issues.apache.org/jira/secure/attachment/12663095/surface_fitting_tests.zip.001,20/Aug/14 09:57;omovchan;surface_fitting_tests.zip.002;https://issues.apache.org/jira/secure/attachment/12663096/surface_fitting_tests.zip.002,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2014-08-14 11:04:36.468,,,false,,,,,,,,,,,,,,411263,,,Mon Nov 03 11:19:25 UTC 2014,,,,,,0|i1yr6f:,411255,,,,,,,,"11/Aug/14 13:40;omovchan;Added patch that fixes this regression in behavior.

We pass current point by reference to evaluator. So that we allow IN/OUT side effect for current point value.","14/Aug/14 11:04;erans;It seems that you were using an unspecified and undocumented side-effect of the implementation.
The new statements (which your patch would revert) specifically intends to forbid IN/OUT arguments (i.e. it's not a bug).

Do you implement a custom ""Evaluation"" class?

IMO, the feature which you need should be implemented in a proper way, and requires a discussion on the ""dev"" ML.
","14/Aug/14 11:36;omovchan;No, we use LocalLeastSquaresProblem class with default implementation for evaluation. We use default implementation for Evaluation interface too. 

In any case, it would be nice to have an enhancement that allows to renormalize input points. It's very hard to implement the surface fitting algorithm without normalization of parameters on every step (or maybe impossible).","14/Aug/14 12:43;erans;I don't understand what is going on.
At line 403 in ""LeastSquaresFactory.java"", a defensive copy is performed; so, whether a reallocation is performed or not within ""LevenbergMarquardtOptimizer"" should not matter (and should not have a side-effect).

It would be useful if you could provide a minimal example that shows the problem and how your patch works around it.

Also, please start a discussion on the ""dev"" ML, perhaps with a pseudo-code showing what should be accessible to the user for the purpose of renormalizing the parameters.
","14/Aug/14 13:09;omovchan;I can try to clarify the problem using pieces of code and my comments.

What is ""dev"" ML and where can I find it?","14/Aug/14 14:45;erans;bq. I can try to clarify the problem using pieces of code and my comments.

What would be more readily useful is a unit test.

bq.  What is ""dev"" ML and where can I find it?

http://commons.apache.org/proper/commons-math/mail-lists.html

Please subscribe to the ""developers"" ML of the Commons project.
Then you should post with a prefix of ""[Math]"" in the subject line because the ML is shared with many other ""Commons"" components.
",15/Aug/14 11:56;omovchan;I am working on the unit test now. I will try to factor out the fitting code from our software.,"20/Aug/14 09:57;omovchan;Added surface_fitting_tests.zip package that is split into 2 parts.
It contains SurfaceFitterFullTest.java and SurfaceFitterSimpleTest.java unit tests that reproduce the problem.","13/Oct/14 17:59;erans;Here is a patch based on the conclusion of the discussion held on the ""dev"" ML: a user-defined {{ParameterValidator}} can be passed to the {{LeastSquaresProblem}}.
Could you please test it with your problem and let us know if it solves your issue?
","15/Oct/14 17:01;omovchan;Hi Gilles,
I have applied and tested your patch. My unit tests failed. Unfortunately the patch is incomplete.

There are 2 conditions in normalization approach:
- point should be normalized before evaluation
- the updated point should be returned to optimizer

Your changes made the normalization explicit now and normalized point is returned inside Evaluation object. But ""currentPoint"" vector is not updated in LevenbergMarquardtOptimizer.optimize():

        Evaluation current = problem.evaluate(new ArrayRealVector(currentPoint)); // <= currentPoint is not updated
        double[] currentResiduals = current.getResiduals().toArray();
        double currentCost = current.getCost();",15/Oct/14 17:06;omovchan;Probably we can validate currentPoint before passing it to problem.evaluate().,"15/Oct/14 20:41;erans;Thanks for the feedback.

bq. Probably we can validate currentPoint before passing it to problem.evaluate().

It seems that this would go against the latest design (and require to pass the {{ParameterValidator}} to each optimizer).

In the new version of the patch, I've added statements that update the point to be used by the optimizer (retrieving it from the current {{Evaluation}} instance).
Let me know how it goes with this one.

In the unit test I've created, it does not change anything (wrt the previous patch), unfortunately; probably it is too trivial.   Could you provide a simple unit test that we can put in the CM test suite?
","16/Oct/14 12:59;omovchan;I verified the latest patch and it works fine now.

I attached SurfaceFitterSimpleTest.java unit test that checks one step of optimization. This test depends only on commons-math3 classes.","16/Oct/14 14:26;erans;bq I verified the latest patch and it works fine now.

Great.

bq. I attached SurfaceFitterSimpleTest.java unit test that checks one step of optimization.

Actually, I had meant something that would demonstrate the usefulness (in terms of preformance and/or accuracy) of the ""validator"" feature.
\[My trivial test already showed that the point can be changed, but it is contrived since it replaces it with the optimum found without validator.\]
",16/Oct/14 14:43;omovchan;surface_fitting_tests.zip package that is attached to this jira contains SurfaceFitterFullTest.java test. This unit test verifies the fitting of 3d cylinder and validator can be used there. But that's not very simple unit test.,"03/Nov/14 11:19;erans;Implemented in:
http://git-wip-us.apache.org/repos/asf/commons-math/commit/321fd029
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UniformIntegerDistribution should make constructer a exclusive bound or made parameter check more relax,MATH-1141,12730981,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,milesandnick,milesandnick,31/Jul/14 02:25,26/Dec/14 19:50,07/Apr/19 20:38,04/Aug/14 21:02,,,,,,,,3.4,,,0,,,,,,,,"UniformIntegerDistribution constructer  public UniformIntegerDistribution(RandomGenerator rng,
                                      int lower,
                                      int upper) 
the lower and the upper all inclusive. but the parameter check made a   if (lower >= upper) {
            throw new NumberIsTooLargeException(
                            LocalizedFormats.LOWER_BOUND_NOT_BELOW_UPPER_BOUND,
                            lower, upper, false);
check, i think it is too strict
to construct UniformIntegerDistribution (0,0) 
this should make it possible",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-08-04 11:21:31.358,,,false,,,,,,,,,,,,,,409053,,,Mon Aug 04 21:02:26 UTC 2014,,,,,,0|i1ydr3:,409049,,,,,,,,"04/Aug/14 11:21;erans;bq. construct UniformIntegerDistribution (0,0)

Why should this be allowed?
","04/Aug/14 15:40;psteitz;I agree with the OP that this should be fixed.  Setting upper = lower here amounts to an integer-valued constant distribution, which is a legitimate degenerate case.  I guess in theory you could say the same thing about the real (continuous) case, but it makes more sense to me in the discrete case.",04/Aug/14 21:02;erans;Fixed in revision 1615790.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect result from MannWhitneyUTest#mannWhitneyUTest with large datasets,MATH-1140,12730092,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Not A Problem,,aconbere,aconbere,27/Jul/14 21:26,11/Aug/14 18:27,07/Apr/19 20:38,08/Aug/14 17:38,3.3,,,,,,,,,,0,,,,,,,,"On large datasets MannWhitneyUTest#mannWhitneyUTest returns the double value 0.0 instead of the correct p-value. I suspect this is an overflow but haven't been able to trace it down yet.

I'm afraid I'm not very good at java, but I'm including a link to a public repository where you can reproduce the issue, unfortunately my implementation is written in clojure.

https://github.com/aconbere/apache-commons-mann-whitney-bug

The summary is that by calling MannWhitneyUTest#mannWhitneyUTest with two randomly generated arrays (50k elements with a max value of 300) I can reliably reproduce the result 0.0. By reducing that to something more modest  like 2k I get correct p-value calculations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-08-08 17:37:17.948,,,false,,,,,,,,,,,,,,408165,,,Mon Aug 11 18:27:20 UTC 2014,,,,,,0|i1y8ev:,408170,,,,,,,,"08/Aug/14 15:59;aconbere;Ouch, somewhat embarrassed to say that our experimental data was just often large enough that we often hit 0 :-/",08/Aug/14 17:37;erans;Thanks for the follow-up.,11/Aug/14 18:27;aconbere;I found my actual source of the issue I'm experiencing which has to do with an integer overflow when calculating U1 in mannWhitneyU and multiplying array lengths together. Since array lengths are ints this imposes a pretty tiny maximum size to the length of your array inputs Math.sqrt(Integer.MAX_VALUE). I would recommend casting those into longs or doubles to improve usability or asserting the maximum length of the arrays early on.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BicubicSplineInterpolator is returning incorrect interpolated values,MATH-1138,12726859,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,abedrossian,abedrossian,11/Jul/14 18:51,26/Dec/14 19:50,07/Apr/19 20:38,17/Oct/14 08:54,3.3,,,,,,,3.4,,,2,,,,,,,,"I have encountered a use case with the BicubicSplineInterpolator where the interpolated values that are being returned seem incorrect.  Furthermore, the values do not match those generated by MatLab using the interp2 'cubic' method.

Here is a snippet of code that uses the interpolator:

        double[] xValues = new double[] {36, 36.001, 36.002};
        double[] yValues = new double[] {-108.00, -107.999, -107.998};

        double[][] fValues = new double[][] {{1915, 1906, 1931},
                                        {1877, 1889, 1894},
                                        {1878, 1873, 1888}};

        BicubicSplineInterpolator interpolator = new BicubicSplineInterpolator();
        BicubicSplineInterpolatingFunction interpolatorFunction = interpolator.interpolate(xValues, yValues, fValues);

        double[][] results = new double[9][9];
        double x = 36;
        int arrayIndexX = 0, arrayIndexY = 0;

        while(x <= 36.002) {
            double y = -108;
            arrayIndexY = 0;
            while (y <= -107.998) {
                results[arrayIndexX][arrayIndexY] = interpolatorFunction.value(x,  y);
                System.out.println(results[arrayIndexX][arrayIndexY]);
                y = y + 0.00025;
                arrayIndexY++;
            }

            x = x + 0.00025;
            arrayIndexX++;
        }

Attached is a grid showing x and y values and the corresponding interpolated value from both commons math and MatLab.

The values produced by commons math are far off from those created by MatLab.",,,,,,,,,,,,,,,,,MATH-1166,,,,11/Jul/14 18:52;abedrossian;Interpolated Values from CM and MatLab.docx;https://issues.apache.org/jira/secure/attachment/12655278/Interpolated+Values+from+CM+and+MatLab.docx,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-09-13 13:22:39.158,,,false,,,,,,,,,,,,,,404966,,,Tue Oct 21 02:27:06 UTC 2014,,,,,,0|i1xp0n:,405003,,,,,,,,11/Jul/14 18:52;abedrossian;Contains x and y values and the corresponding interpolated values from both CM and MatLab.,"13/Sep/14 13:22;HankG;I have been investigating this much further.  The good news is that from what I can tell the implementation of this function is in line with the Wikipedia article it is based on.  The bad news is that this is just not working.  The unit tests associated with this aren't even passing a basic 2D linear function test. It should be getting identical results to the expected values. These aren't even close.

I re-implemented the WIkipedia implementation in Octave and I get the same results the Apache Math function does.  I rederived the equations from the Wikipedia article and confirmed that the coefficients being used are the correct.  It's just not working though.

I looked into how Octave performs the same function.  They do not do it this way.  They do it by building up from a series of cubic splines.  I have a mechanism for implementing it in that way and could take a crack at it if you would like.  I know I just joined the Apache project here so let me know what the proper path forward would be to get this assigned to me and contribute it back. 

If I do get it, I intend to reform the unit tests so that they have more appropriate tolerances for the functions listed.  Even the ones that are passing are doing so because their tolerances are jacked up way too high, which isn't really testing the function numerical accuracy at all.","16/Sep/14 19:02;erans;bq. I re-implemented \[...\] rederived \[...\] confirmed

Thanks a lot for this thorough review.

bq. It's just not working though. \[...\] I looked into how Octave performs the same function. They do not do it this way.

It would be interesting to go to the reference mentioned in the Wikipedia article.  Maybe it will reveal where the bugs are.
A web search came up with this site:
{noformat}
  http://www.paulinternet.nl/?page=bicubic
{noformat}
At first sight, there would seem to be a discrepancy with the contents of the matrix...

bq. I have a mechanism for implementing it in that way and could take a crack at it if you would like.

That would be nice but be careful to not use a reference that would forbid inclusion in Commons Math.

bq. let me know what the proper path forward would be to get this assigned to me and contribute it back.

One way would be to generate the expected values from another package (e.g. Octave) and create a unit test that will miserably fail with the current code when ""reasonable"" tolerances are used.
Then, you are most welcome to fix the implementation, trying to not change the public API.
","17/Sep/14 12:55;HankG;Actually, the source they used for that derivation was not that link, but this one (also in their references section):

http://www.geovista.psu.edu/sites/geocomp99/Gc99/082/gc_082.htm

You can see in Section 3.8 how they are sampling just the corner points (0,0), (1,0), (0,1) and (1,1) in determining coefficients, just like their derivation.  The reference you cited is sampling a 4x4 grid, which makes more sense to me.  The methodology I was talking about existing in the Octave code is the same as the methodology in his site as well. Specifically, to do a 2D interpolation you call a series of 1D interpolations.  The coefficient term version at the bottom of the reference you cited is useful for when the region being sampled between iterations often stays the same.    

I am aware that Octave uses GPL, which is not compatible with the Apache license, so I was not going to be using  their code (it is written in Matlab scripting language and FORTRAN too).  I was simply looking at which math algorithm they chose to use.  I did not want to use the code off of the site you listed as a baseline either because there is no mention of licensing rights at all. It simply says, ""Anything at this page may be copied and modified."" but it doesn't list what the requirements are for when you do that.  I thought about e-mailing the author asking them if they could put an explicit license, Creative Commons or something, to address the ambiguity.

My implementation strategy was going to be as follows: I was therefore going to be implementing the algorithm using existing Apache Math 1D interpolation functions after I created my own validating implementation in Octave source code and a test set of data in Octave as well.  I would then create versions of the test that does random samplings of the interpolation region in Octave to figure out what the statistical distributions of errors are.  I would then use that for the tolerances of the same test implementation in Apache.  

I believe the public API for this should be fine, but I will not change it without consulting team members here.  I would also like to implement lower order BivariateGridInterpolators.  This can be overkill for a lot of applications.  That would be done in a seperate story/issue though.

",05/Oct/14 19:05;psteitz;Patch?,"06/Oct/14 01:12;HankG;I'm still working on the new coding up.  I'm trying to investigate why I'm not getting the same numerical accuracies out of my Apache Math implementation as I am out of my Octave implementation.  My linear case is only matching to within 1e-13 while it should be matching to within 1e-14, according to the results I get from Octave's built in function and my own hand coded function.  My parabaloid test should be within 1e-14 as well, but I'm no where near that.  I'm currently investigating the Spline interpolator.  The sensitivities on that object's tests aren't cranked down anywhere near 1e-14/1e-15.  I checked Math.NET unit tests and theirs are cranked down to that level, as are the levels I'm seeing in Octave.  I was therefore going to add some additional tests modeled after the Math.NET tests (the are using an MIT license, so that's compatible with this project).","06/Oct/14 01:32;HankG;Nevermind, I've modified the tolerances on the test to match those of Octave's built in functions for the same values.  So that is now properly cranked down to the tolerances for these algorithms for those curve fits.  No changes to the spline interpolator was necessary to match those values.  ","06/Oct/14 02:08;HankG;That said, the spline interpolator is not returning correct values for the Bicubic test data set.  For example, given:
xi = [    3.0000   4.0000   5.0000   6.5000 ];
zi = [ 25.000    47.000    73.000   119.500];

The correct value of the interpolation of z for x = 4.5 is 59.5.  The spline interpolator in Apache Math is returning 59.388... . I'll continue to investigate this this week.  The accuracy of my implementation will not be correct until I figure out why spline is not working right.  

Another note, TricubicSplineInterpolator was dependent on the partial derivatives in the original BicubicSplineInterpolatingFunction.  There is no such thing in this implementation therefore TricubicSpline will need to be implemented as a piece-wise spline based on the 2D spline, in the same way that the 2D spline is based on the piece-wise implementation based on the 1D spline.
","06/Oct/14 13:55;HankG;Reviewing Octave and Math.NET I have identified the source of the discrepancy.  It appears that Octave uses B-splines for their spline interpolation.  The implementation in Apache Math is the Natural Spline.  When I use the Math.NET natural spline interpolator I get the same results as I get with the Apache Math method.  However if I use the Akima Cubic Spline algorithm from Math.NET I get valid numbers.  The only disadvantage of the Akima Spline that I see that we don't have with the standard cubic spline is that it needs a minimum of five elements, not four.  

Because the current spline works I was going to code up an Akima spline based on the Math.NET implementation (referencing that source (they are a compatible MIT license), plus a book source.  I will then use that as the basis for the splines in the BicubicSpline Interpolation.","06/Oct/14 22:34;HankG;I've finished coding up the Akima spline interpolator, and while it works better than the natural spline it is still nowhere near as close as the b-splines in Octave.  It nails it for linear and quadratic curves, but even a cubic function throws it (and the natural spline) for a loop.  I've confirmed the uncertainties of both spline methods by comparing to Math.NET output.  I therefore want to implement a B-Spline.  I found this BSD-licensed B-spline library.  I intend to use for that implementation.

http://www.eol.ucar.edu/homes/granger/bspline/doc/index.html

","08/Oct/14 03:04;HankG;I have completed work on this to a point where it is generating substantially better values.  I am using the Akima Spline algorithm.  For the planar test the error off the truth function went from 6 to 6e-14.  On the parabaloid function test it went from 224 to 6e-14.  The corresponding errors on the Akima Spline test for linear, parabolic and cubic functions are 1e-15, 6e-14 and 3.8, respectively.  While that is an improvement over the Natural Spline, that could have errors over 15 on the cubic test, the B-spline would collapse errors on the higher order functions to something comparable to the linear and parabolic tests, and thus further enhance the accuracy of the interpolation of the higher dimension interpolators too.  Patch is attached to this incident.","08/Oct/14 03:05;HankG;Pull request for this is at:

https://github.com/apache/commons-math/pull/2

","17/Oct/14 08:54;luc;Patch applied as of b5e155e.

Thanks for the patch!","18/Oct/14 08:34;tn;I do not know if we can already close this issue, as the original problem got now even worse: the minimum number of required data points for the new Akima spline method is 5, thus the above example fails with an InsufficientDataException.","18/Oct/14 11:14;HankG;Adam was working on a project for me when he uncovered the accuracies issues with the interpolators.  He posted this to the discussion board and then ultimately here to help resolve the matter.  The field we were running our interpolation on was much greater than the number of elements given.  We were just trying to come up with a concise example to send here.  This was before we started tearing into the test harness for these functions, which is when I took over the investigation of the problem.  We ultimately had to write our own bilinear interpolation rather than use the Apache Math libraries due to time constraints, but I do intend to switch that out for this method in a future sprint once this is part of a shipping Apache Math release.","18/Oct/14 11:38;tn;I understand and appreciate the effort that has been put into this issue, but I just wanted to point out that we might create a potential regression here for people that have been using the interpolator with less than 3 data points.

To be honest, I do not know exactly how to proceed, as the current code clearly improves the results, maybe we should explicitely note this change in the release notes.","18/Oct/14 11:57;HankG;We can go back to only requiring four elements if we use the regular spline instead of the Akima spline.  The tolerances on the interpolation tests will have to be significantly loosened however since the regular spline algorithm produces larger deviations from the truth function by the nature of the algorithm.  I was opting for the increased accuracy.  In a future revision we were considering giving people the option to select their interpolation method between regular, Akima and B-Spline.  I just haven't gotten around to coding that.  Perhaps now would be a good time.  We could default to the regular spline and give people the Akima spline option, noting that it is of higher accuracy.  I do not know what the minimum number of points for the B-spline will be as I haven't started investigating an implementation yet.  It may be that too is four and thus once all is said and done the default spline algorithm would always be four.",18/Oct/14 12:10;tn;Giving the user control over the type of spline algorithm used in the interpolator / function would certainly be good imho.,"18/Oct/14 13:00;HankG;Agreed, we just need to discuss it in the message board.  I can create a new JIRA issue and start that process.","21/Oct/14 02:27;HankG;I have restored the original files but added deprecation and accuracy warnings.  The new interpolators are now in their own ""Piecewise"" surnamed classes.  All tests in the entire JUnit suite passed. 

You can find the details of the pull request here:

https://github.com/apache/commons-math/pull/3

You can browse my version of the repository, or pull your own copy down, here:

https://github.com/HankG/commons-math

This pull request seems to also have all of the changes from apache:master that I fetched and merged with my local repository before I started these edits.  I'm not sure how to get GitHub to ignore those came from the root, nor have I figured out how to select a limited range of changes to get around that.  I also don't want to let my fork go stale.  Between the two I figured it was better to keep my fork up to date before making changes than it was for the person processing a pull request to try to shoe horn it into whatever the change is. 

Along with these changes I did do a scan of my code to make sure I didn't miss any defensive programming practices, used magic numbers, et cetera.  Nothing stood out to me, but proofreading your own writing is never a good thing.  If whatever ""minor changes"" I need to make to get this code ""on par with what we had usually committed as new contributions"" can be pointed out here or in JIRA I'd appreciate it.",,,,,,,,,,,,,,,,,,,,,,,,,,
BOBYQA incorrect indexing,MATH-1137,12726791,Bug,Open,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,,,essence,essence,11/Jul/14 11:47,18/Apr/17 15:22,07/Apr/19 20:38,,3.3,,,,,,,4.0,,,0,,,,,,,,,,,,,,,,,,,,,,,,,MATH-1282,MATH-1375,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-07-11 12:19:31.929,,,false,,,,,,,,,,,,,,404898,,,Sun Oct 05 19:08:25 UTC 2014,,,,,,0|i1xolr:,404936,,,,,,,,"11/Jul/14 11:52;essence;I don't know my way around here...

I commented in Math-621 about what I think is incorrect indexing, and it seems to still be incorrect.

Line 530 of BOBYQA has
{code}
  double curv = modelSecondDerivativesValues.getEntry((j + j * j) / 2);
{code}
but I think it should be
{code}
  double curv = modelSecondDerivativesValues.getEntry((j + 1 +  (j+1)*(j+1)) / 2-1);
{code}
The original Fortran is
{code}
              CURV=HQ((J+J*J)/2)
{code}
So a change from 1 to 0 indexing should translate into the above.","11/Jul/14 12:19;erans;Could you provide a unit test for that section of the code?
","11/Jul/14 13:40;goodwin@essence-property.com;I have never done a unit test in my life! I wouldn't know how to do it.

To be honest, the only way I would check (in addition to reading) would 
be to run the original Fortran and the Java and compare the two results.

I read the code very carefully, and walk through it, and make some notes 
on pencil and paper, and walk through it again.

I have been using the corrected version for some years without any 
problems. I am only going back to this issue because I am doing an 
overall upgrade to 3.3. I have been using a corrected pre release 
version of BOBYQA previously.

I wrote a lot more on the original MATH-621 (or was it 612?).

It may interest you to know I did a conversion of L-BFGS-B from Fortran 
to Java, but it is a mess (I think you will understand why it is a 
mess....).

","11/Jul/14 14:41;erans;bq. I have never done a unit test in my life! I wouldn't know how to do it.

A unit test is nothing more than a ""use-case"", IOW a mini-application demonstrating that the code behaves as expected. The unit test should fail with the current code, and pass with the proposed correction.
Currently, no tests in CM exercise the block where the line is located... :(
Thus, we need a set of initial conditions such that the algorithm will
# enter this block
# behave differently with each of the alternatives
","11/Jul/14 14:51;essence;I know, I was being provocative, my bible of testing is Myers Art of Software Testing. The purpose of testing is to find errors, not to pass tests.

The problem is I don't know what the expected behaviour is. I also struggle to see where this bit of code fits into the algorithm and where it is described in Powell's paper.

If I knew this was to do with matrix diagonal elements, and if I knew that all diagonal elements are expected to be positive, and if I knew how to get the code to go through this path, then maybe there is a chance of a test.

Do you have any idea where this code is described in the BOBYQA paper? Or is it part of NEWUOA, or is it some line search function?

I can verify I have run BOBYQA hundreds of times on lots of problems, but have no idea whether this bit of code has ever been exercised.","11/Jul/14 17:02;erans;bq. The purpose of testing is to find errors, not to pass tests.

If there is no test to leave a trace of something that has been changed, then that thing can be changed again later without anyone noticing.

bq. Do you have any idea where this code is described in the BOBYQA paper?

No.
One goal of CM is that the _implementation_ should be documented (or self-documenting).
IMO, we'd be better off if the algorithm were reimplemented from scratch (as was already discussed on the MATH-621 page).

bq. I can verify I have run BOBYQA hundreds of times on lots of problems, but have no idea whether this bit of code has ever been exercised.

That would be quite easy, if you can easily rerun those codes. You'd just have to uncomment line 538, and the code will throw an exception if it ever enters the block.
","11/Jul/14 17:36;essence;Easy enough to say, but it is now in commercial production code....

I have spent a few hours trying to understand the Powell paper and see where the code is described, but I haven't made much progress. I think it is to do with adjusting RHO, equations 6.8 - 6.11.

Implement from scratch from the paper? You must be joking!!! Have you tried to read the paper? [I think you must have because you did the original conversion?]

To be honest, we can talk about unit tests and test cases for ever, but can't you just read the code and confirm by manual inspection it is incorrect? That is one of the messages of Myers - most bugs are found by reading.

I think I found this bug by visual inspection rather than any particular problem/symptom.

If the original code had (J + J*J)/2 and looped around J from 1 to N, then with a 0 index we have

(J+1 + (J+1)*(J+1))/2 -1 = (J+J*J)/2 + J

with a loop around J from 0 to (N-1). It is dimensioned to 

new ArrayRealVector(dimension * (dimension + 1) / 2);

so when J = N-1 we are looking at the element

(N + N*N)/2 -1 

which seems pretty sensible to me, it is the last element of the array, which is of course the last diagonal of the triangular system.

It's obvious, surely? It was just something missed in the original conversion, you don't have a test case for every single conversion. Myers also said testing is a matter of economics - in this case the economics and common sense says just fix it, it is obviously wrong.

Another mental check you can do is to check there are no array bound exceptions. You don't need to write any code, just test by hand and visually. Do a walk through.

Is there an apache rule 'for every bug correction there shalt be an individual specific test case'? I think we can all agree BOBYQA is one of the most complex classes in apache, and there was some debate whether it should be released or not. So some 'rules' need to be treated with some common sense.

But the reality is it is very useful to me, at least, and I use it in production commercial code, and never had a problem.","11/Jul/14 17:59;essence;ps. Gilles, way back in Aug 2011 you said the lines

{code}
                   if (newPoint.getEntry(j) == lowerDifference.getEntry(j)) {
                        bdtest = work1.getEntry(j);
                    }
                    if (newPoint.getEntry(j) == upperDifference.getEntry(j)) {
                        bdtest = -work1.getEntry(j);
                    }
{code}

are never entered, and if that is still true, then bdtest = bdtol and so the line in question is also never executed!

Note also the C++ version has (J + J*J)/2, so IMHO that is also incorrect.

In my experience with my L-BFGS-B conversion, the trickiest part was dealing with parameter v. value argument passing and what happened when an argument was changed inside a subroutine. Java and Fortran are very different. It took me days with detailed comparisons of outputs to trace and correct bugs. A good check would be to look at all places where methods change values of arguments.....this is all part of Java 101 which I haven't yet taken =}. bdtest is a primitive, so I think it's OK to change it and then check against bdtol. If bdtest was a Double, I think it would be different.","11/Jul/14 18:04;essence;pps. it seems that a script was used at least initially to do the 1 to 0 indexing changes, and that would explain why the relevant line wasn't picked up, the script probably didn't implement that kind of change. It just shows even if you use an automatic script, you still have to go through line by line to check it. Indeed, a script can be dangerous as it may give a false sense of security....","11/Jul/14 18:09;essence;my last comment (today)!

The C++ version was created by running f2c, so presumably that converter also had difficulties with that line. Not surprising, really, I'm not sure how an automatic converter could do that kind of thing, it needs to understand what is happening both syntactically and semantically. Semantics is for humans.","12/Jul/14 13:02;erans;bq. Easy enough to say, but it is now in commercial production code....

The issue is whether you'd have some saved copy of your development codes which you could rerun with the version of the code that can ""crash"". We don't ask for the complete source code of your applications.

This is a way to help us improve the quality of Commons Math.

Another approach would be to get in touch with an expert who could provide rationale for the obscure bits of Powell's code.
As you seem quite knowledgeable in using such algorithms (and a fan of this one in particular), you are the best candidate for this job. ;-)

A third approach to help users would be to describe a typical use-case where BOBYQA outperforms all the other algorithms implemented in Commons Math. This could provide incentives for improving the situation to people interested in performance.
I must stress again that the CM's implementation is currently in a state with _a lot of room for improvement_, by which I specifically mean here: performance-wise (see discussion in MATH-621).

bq. Implement from scratch from the paper? You must be joking!!! Have you tried to read the paper?

I tried. I was hoping to find direct relationships between the mathematical structures described in the paper and the data structures in the Fortran implementation. Unfortunately, the reference code is overwhelmed by loops, low-level fiddling with indices, global variables, (disguised) gotos, and what not that so much obscures the essence of the computation that I did not bother to go further in that direction. See also the discussion with Dietmar on MATH-621.

bq. It's obvious, surely? It was just something missed in the original conversion, you don't have a test case for every single conversion.

Because the code came with a long list of unit tests, we had the impression that it was relatively safe to include it, despite the code itself broke all the expectations required to be part of Commons Math (a.o. documentation and maintainability).

Personally, I do not want to have to read that code (again). Much less try and guess what this or that block's purpose was supposed to be.

bq. Is there an apache rule 'for every bug correction there shalt be an individual specific test case'?

I think there is. It's not always applied. But following the ""commit"" log, you can see that it's often the case, fortunately; thanks to dedicated developers!

Concerning this specific issue, I propose that you post to the ""dev"" ML, asking for the change to be performed based on the arguments you've submitted here. If there is no objection there, I'll commit the correction.
","12/Jul/14 15:54;essence;Nothing ever crashed, and I don't even know whether that bit of code is ever exercised, but I will take up the challenge and see if I can make some tests with difficult functions (e.g. Rosenbrook).

I met Prof Powell many decades ago (I am aged 60, which is probably 3x the age of the average contributor to Apache) and he never claimed to be a good programmer.

http://iridia.ulb.ac.be/IridiaTrSeries/link/IridiaTr2010-010.pdf

I have looked at the NLopt C version and have little confidence in it....particularly the rescue code, which seems to not be there in the apache java version (probably a good thing).

One other thing I noticed in Apache is line 1710 and 1744 where there is
{code}
  final int ih = nfx * (nfx + 1) / 2 - 1;
{code}

so 1 is subtracted.

All I am doing in my suggested correction of
{code}
  double curv = modelSecondDerivativesValues.getEntry ((j + 1 + (j +1) * (j+ 1)) / 2 - 1);
{code}

is to do the same subtraction of 1 and adjust because j starts from 0.

How do I post to the ""dev"" ML?

","12/Jul/14 17:29;essence;I made a test program by applying bobyqa to a 100 dimensional rosenbrock function. The bit of code under discussion was never executed. indeed, the IF statement a few lines above was never true

{code}
          dnorm = FastMath.min(deltaOne, deltaTwo);
            if (dnorm < HALF * rho) {
                ntrits = -1;
                // Computing 2nd power
 {code}

so I am scratching my head trying to work out how to force this bit of code to execute!

I tried with various trust region sizes.","15/Jul/14 11:07;erans;bq. Nothing ever crashed, and I don't even know whether that bit of code is ever exercised, but I will take up the challenge and see if I can make some tests with difficult functions (e.g. Rosenbrook).

Just to make sure that we mean the same thing: By ""crash"", I mean that you should use a _modified_ version of CM, where the ""throw"" statement is not commented out in that block. If a code code raises this exception, we can use it to further examine the behaviour of the block (by commenting out the ""throw"" and fiddling with the parameters).

bq. the rescue code, which seems to not be there in the apache java version (probably a good thing).

The ""rescue"" method was removed in revision 1158448. There was a discussion about this on the MATH-621 page.


bq. How do I post to the ""dev"" ML?

See this page:
http://commons.apache.org/proper/commons-math/mail-lists.html

You should subscribe to the ""dev"" ML.
Then when you post, please add a ""\[Math\]"" prefix to the subject line (because the ML is shared for all ""Apache Commons"" projects).
","15/Jul/14 12:02;essence;How do I find out what tests have already been done? I'm a bit worried this bit of code is never executed in my tests. Has a detailed comparison been done between the Fortran and Java on a range of test problems? I know machine precision is a problem, but has there been any attempt to compare the paths the tests go through? I spent many hours comparing the results at all stages between Fortran and java versions of L-BFGS-D, so I know how painful the process is.

I am less concerned about the line we have been discussing here, than I am about other bugs which we are not discussing and have not yet been found!

You can often have optimisation algorithms which have bugs but still find the minimum, albeit inefficiently.","15/Jul/14 12:25;essence;OK, I think I have found the test code (in the test directory!).

But my question is - with these tests (which include Rosenbrock) was there a comparison between Java and the Fortran to see if the code reproduced the same results at each stage, not just the final result? Was it checked to see if they both went through the same path?

If not, this is some testing work which I can contribute, it is important to me the code works optimally.

Much more useful than unit tests, particularly as the code does not exactly use nice units!","15/Jul/14 15:15;erans;bq. I am less concerned about the line we have been discussing here, than I am about other bugs which we are not discussing and have not yet been found!
bq. You can often have optimisation algorithms which have bugs but still find the minimum, albeit inefficiently.

A general point worth raising on the ""dev"" ML!

bq. \[...\] was there a comparison between Java and the Fortran to see if the code reproduced the same results at each stage, not just the final result?

Dietmar could probably answer. He might still read the ""dev"" ML, so better to ask there.
IIRC, the tolerances in the unit/validation tests are so stringent that even seemingly innocuous changes made some tests fail; hence it is unlikely that different paths would not trigger a failure. Although this might reassure you, it is a liability in the way to the refactoring (see MATH-621).
","15/Jul/14 15:28;essence;As far as I can see, the tests only test the final value. They don't test how many functions evaluations there have been, or what paths have been followed.

It is very easy to have bugs which do not alter the final solution, but which change the behaviour and efficiency.

If nobody else has, I will have to allocate some time and do side to side comparisons of Java and Fortran versions.

The first step may be to see if the Fortran version goes through the steps which the Java does not....","15/Jul/14 15:51;essence;Good news!

The test example provided by Prof Powell DOES go through this bit of code. Now I have to use his same example test, and see if the Java also goes through it. If it does not, again, good news, I've found a bug!

[to those of you who think finding bugs is bad news, read Myers. The bug is there whether you find it or not, so it's best to find it. Finding bugs is a success, the aim of testing is to find bugs.]","15/Jul/14 16:45;erans;bq. They don't test \[...\] what paths have been followed.

For coverage (i.e. code paths explore by the unit tests) we have the following report:
http://commons.apache.org/proper/commons-math/jacoco/org.apache.commons.math3.optimization.direct/BOBYQAOptimizer.html

bq. They don't test how many functions evaluations there have been

Unless I'm mistaken, it would be easy to change the ""maxEvalutations"" parameter in ""doTest"".
You are welcome to try it if you have the used by the original code.","15/Jul/14 17:36;essence;I'm making some progress.

With the test problems provided by Powell in his Fortran, the Java does go through the relevant lines - but the behaviour between Java and Fortran is different, so I think it will be good to trace in both cases the paths.

Just checking the fact that branches are covered in tests is not enough - they have to be covered in the same way in the same order for Fortran and Java.

Not sure what nationality you are, ever seen the sketch by Morecombe and Wise and Andre Previn?","16/Jul/14 11:05;erans;bq. the behaviour between Java and Fortran is different

An example?

bq. \[ branches\] have to be covered in the same way in the same order for Fortran and Java.

I recall that some statements seemed to rely on the finite precision of floating-point arithmetic.
If that's the case, I don't think that the goal should be to mimic this (fragile) behaviour. And if we don't, then the code paths might start to differ.
Finding the same optimum, within some reasonable tolerance, and with about the same number of evaluations seems (IMHO) sufficient.

Again, you should raise this point on the ""dev"" ML to gather more opinions.

bq. ever seen the sketch by Morecombe and Wise and Andre Previn?

:)
Thanks for the reference. I knew the relevant excerpt but did not know its origin.
","16/Jul/14 23:50;essence;My suspicion is being directed towards 

trustRegionCenterInterpolationPointIndex

In some cases this seems to be zero based, in others 1 based.

I was looking at the values of k in the loop and the values of the test at about line 790. The skipped k values were not consistent between Fortran and Java, even when knew was set the same at line 790....

Also KOPT v KNEW looks suspicious - the Fortran has a test against KOPT in ALTMOV, whereas the java has a test against knew...

This is what I mean about the long hard slog comparing Java and Fortran. Unit tests are for lazy people!","17/Jul/14 12:49;essence;This is now drawing to a close...

The good news is that I managed to reproduce the Fortran behaviour until close to convergence, when rounding errors start to become more important. I compare the values of all the function evaluations until close to convergence, and the paths.

I altered some comparison statements between doubles, as sometimes two numbers almost identical were being compared and the rounding errors caused changes in logic (as you know).

I did a few changes of initialisation of indices, Fortran set them to zero, I set them to minus one, this may have an effect.

So, in short:

- I am now pretty confident my Java version works as it should

- I still couldn't get the Fortran to exercise the lines which started all this (although the Java does), so I'm not much further on whether my fix is confirmed correct or not.

Some time soon I will try to put this to bed. It is very time consuming. But also, I have provided a further visual check of quite a lot of the code, so that can't be a bad thing.","17/Jul/14 15:17;erans;bq. I did a few changes of initialisation of indices, Fortran set them to zero, I set them to minus one, this may have an effect.

The effect of raising an exception (if the index is not set to a proper value before use), you mean?

bq. I am now pretty confident my Java version works as it should

Anything that should be updated in our version?

bq. I still couldn't get the Fortran to exercise the lines which started all this (although the Java does)

I don't understand; our current version does not exercise that section of the code.

bq. I have provided a further visual check of quite a lot of the code, so that can't be a bad thing.

No, but how shall we keep a record of what is correct (and whose logic should be maintained), what is probably not correct and what is undecided yet? If we leave it at that, CM users and developers won't benefit from your review...
","17/Jul/14 16:27;essence;I have changed very, very little.

The current tests may not exercise that code, but the tests provided by Powell in his Fortran, when implemented in Java, do exercise it (but not when implemented in Fortran, so far...). At the very least, we should add the Powell tests to the set of tests (see below).

Changes:

Line 403 initialise trustRegionCenterInterpolationPointIndex to -1

Line 420 initialise knew to -1

Line 894 set knew to -1

Line 1137 change to if (knew > -1)   (of course, this has no effect, but is more consistent and readable because -1 means 'not set')

Of course, -1 is just a Fortran workaround to mean 'null' or NA or whatever. I would suggest using Integer.NaN, but there isn't such a thing, which is strange....So -1 will have to do. But we ought to make a static final integer to take the place of '-1', so it is more readable.

I don't know whether the above changes had any effect, but they are certainly a better and more consistent  translation of the Fortran. Line 1117 already set knew to -1, but in other places it was set to 0 (as per the Fortran), which is incorrect because knew is a zero based index. This was dangerous practice.

I also did changes to make double comparisons more consistent with the Fortran. This was so I could compare as closely as possible to the Fortran, and these changes were necessary (but not sufficient) to do this. I can see, longer term, value in making it as close as possible, and ideally the same changes would be done to the Fortran. In this way, we can do a head to head comparison between Java and Fortran. They start deviating as they converge to the solution, and the higher the dimensions and the number of interpolation points used, the more quickly they deviate. But I still like the idea of trying to make them as close as possible.

The things I changed were:

Line 787  if ((temp * den - scaden) > scaden * 1.0e-10)
Line 915 if ((temp * den - scaden) > scaden * 1.0e-10)
Line 1403 if ((predsq - presav)> presav * 1.0e-10)

No doubt there are others, but this was enough for the moment.

Now, there is no reason to believe that in general these will behave the same as the Fortran, so really the Fortran should have the same changes, and then we have a closer correspondence between java and Fortran which we can use for future testing.

Does that make sense?

When I tested, I ran the Fortran and Java side by side, output a host of diagnostics, used the debugger extensively, and gradually convinced myself that, with the above changes, they were doing the same thing. I now need to go to specsavers. I still don't know what country you are based in.

The Powell test example looks like a good test, and it almost certainly covers branches which the existing tests did not. I have, of course, the java code for my test environment.

Now, I have never participated in open source, I don't know what the procedure is, I don't know your role, bla bla bla. But it would be nice to get my changes into Apache. What is the best way to do this? Of course, I want somebody else (you?) to examine my changes and review them. Also, all the existing tests need to be re run and results compared.

I run my own self employed business, I have spent maybe $6000 USD on this per my day rates. I don't have an employer (or taxpayer) who can absorb these costs.

Also, of course, not to forget my original bug correction!

What is the best way forward?

","18/Jul/14 10:49;erans;bq. really the Fortran should have the same changes, and then we have a closer correspondence between java and Fortran which we can use for future testing.
bq. Does that make sense?

Yes and no.

From a testing perspective, it certainly does; undoubtedly, it helps to try and reproduce the output of a trusted source.
But from a design perspective, this option is completely at odds with the goal of a ""state-of-the-art"" Java (implying OO) code: At some point the codebases should diverge, and the unit tests suite will provide the (relative) confidence that the CM code does what is expected.

bq. I still don't know what country you are based in.

Belgium. I'm a European. ;)

bq. The Powell test example looks like a good test, and it almost certainly covers branches which the existing tests did not.

I don't know the ""Powell test example"". Dietmar Wolz did the port to Java and brought along most of the tests which we currently have for this code. Unless I'm mistaken, they were part of the Fortran test suite.
If you happen to have more tests, they would certainly be a useful addition, especially if they cover still uncovered areas.

bq. Now, I have never participated in open source, I don't know what the procedure is, I don't know your role, bla bla bla. But it would be nice to get my changes into Apache. What is the best way to do this?

Unfortunately, I still don't know what is _the_ best way.
In fact the best way will surely vary from one developer to another!

To start somewhere:

* http://www.apache.org/foundation/getinvolved.html
* http://commons.apache.org/patches.html

Then, as I suggested several times, you _should_ subscribe to the Commons projects' ""dev"" ML, and summarize there your proposal for proceding with the issue (and provide the link to this page - it's not necessary to copy everything that was said here).

bq. I have spent maybe $6000 USD on this per my day rates.

It doesn't tell whether you spent an awful lot of of time or if you are very well paid. :)

From the project's point-of-view, we are all individual volunteers, hopefully driven by the common ideal of providing reasonably well-designed codes that implement standard mathematical algorithms. Unfortunately, not many people get paid for that altruistic goal. Of course, some developers naturally contribute more to codes which they also use, and contributors usually get back more than they put in, either by their contribution being maintained by a larger group or by simply using CM (i.e. the contributions made by other people).
","05/Oct/14 18:34;psteitz;Is there a patch in progress here?  At least unit test showing something failing?  Otherwise, we should close this.","05/Oct/14 18:49;goodwin@essence-property.com;I am 99% sure that my suggested corrections are correct, but this issue is not amenable to unit tests. I have done various tests, and none of them so far go through the relevant lines of code, so I can't check whether my changes work or not, nor their effect.

That is the nature of complex code like this.

I can't even find where in the original paper the relevant code relates to.

So this is a case of testing by reading the code, not testing by unit tests.

I have retained my personal copy of the code which contains the corrections, and cannot afford any more time on this.

All I can ask is for others to look through my suggested changes and check them by reading them.","05/Oct/14 18:50;goodwin@essence-property.com;ps. another way of looking at it is that if none of the tests ever goes through the relevant code, there isn't much harm or risk in implementing the changes!",05/Oct/14 19:08;psteitz;Can you attach a patch with your suggested changes?,,,,,,,,,,,,,,,
BinomialDistribution deals with degenerate cases incorrectly,MATH-1136,12726786,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,dievsky,dievsky,11/Jul/14 11:30,26/Dec/14 19:50,07/Apr/19 20:38,11/Jul/14 19:23,3.3,,,,,,,3.4,,,0,bug,patch,,,,,,"The following calculation returns false results:

{{new BinomialDistribution(0, 0.01).logProbability(0)}}

It evaluates to Double.NaN when it should be 0 (cf., for example, ""dbinom(0, 0, 0.01, log=T)"" in R).

I attach a patch dealing with the problem. The patch also adds a test for this bug.",,,,,,,,,,,,,,,,,,,,,11/Jul/14 11:31;dievsky;BINOMIAL_DEGENERATE.patch;https://issues.apache.org/jira/secure/attachment/12655198/BINOMIAL_DEGENERATE.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-11 19:23:31.066,,,false,,,,,,,,,,,,,,404893,,,Fri Jul 11 19:23:31 UTC 2014,,,,,,0|i1xokn:,404931,,,,,,,,11/Jul/14 19:23;psteitz;Patch applied in r1609775.  Thanks for reporting this and thanks for the patch!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in MonotoneChain: a collinear point landing on the existing boundary should be dropped (patch),MATH-1135,12726642,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,gmarceau,gmarceau,10/Jul/14 18:00,26/Dec/14 19:50,07/Apr/19 20:38,10/Jul/14 21:40,3.3,,,,,,,3.4,,,0,patch,,,,,,,"The is a bug on the code in MonotoneChain.java that attempts to handle the case of a point on the line formed by the previous last points and the last point of the chain being constructed. When `includeCollinearPoints` is false, the point should be dropped entirely. In common-math 3,3, the point is added, which in some cases can cause a `ConvergenceException` to be thrown.

In the patch below, the data points are from a case that showed up in testing before we went to production.

{code:java}
Index: src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java
===================================================================
--- src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java	(revision 1609491)
+++ src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java	(working copy)
@@ -160,8 +160,8 @@
                 } else {
                     if (distanceToCurrent > distanceToLast) {
                         hull.remove(size - 1);
+                        hull.add(point);
                     }
-                    hull.add(point);
                 }
                 return;
             } else if (offset > 0) {
Index: src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java
===================================================================
--- src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java	(revision 1609491)
+++ src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java	(working copy)
@@ -204,6 +204,24 @@
     }
 
     @Test
+    public void testCollinnearPointOnExistingBoundary() {
+        final Collection<Vector2D> points = new ArrayList<Vector2D>();
+        points.add(new Vector2D(7.3152, 34.7472));
+        points.add(new Vector2D(6.400799999999997, 34.747199999999985));
+        points.add(new Vector2D(5.486399999999997, 34.7472));
+        points.add(new Vector2D(4.876799999999999, 34.7472));
+        points.add(new Vector2D(4.876799999999999, 34.1376));
+        points.add(new Vector2D(4.876799999999999, 30.48));
+        points.add(new Vector2D(6.0959999999999965, 30.48));
+        points.add(new Vector2D(6.0959999999999965, 34.1376));
+        points.add(new Vector2D(7.315199999999996, 34.1376));
+        points.add(new Vector2D(7.3152, 30.48));
+
+        final ConvexHull2D hull = generator.generate(points);
+        checkConvexHull(points, hull);
+    }
+
+    @Test
     public void testIssue1123() {
 
         List<Vector2D> points = new ArrayList<Vector2D>();
{code}",,,,,,,,,,,,,,,,,,,,,10/Jul/14 18:01;gmarceau;MonotoneChain_testCollinnearPointOnExistingBoundary.patch;https://issues.apache.org/jira/secure/attachment/12655042/MonotoneChain_testCollinnearPointOnExistingBoundary.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-07-10 21:22:44.9,,,false,,,,,,,,,,,,,,404749,,,Thu Jul 10 21:40:09 UTC 2014,,,,,,0|i1xnov:,404787,,,,,,,,"10/Jul/14 21:22;tn;Ah, this is quite an embarrassing bug, but thanks a lot for the report and especially for a reproducible test case.",10/Jul/14 21:40;tn;Fixed in r1609577.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unsafe initialization in BicubicSplineInterpolatingFunction,MATH-1134,12724526,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,dscherger,dscherger,30/Jun/14 19:35,27/Feb/15 14:13,07/Apr/19 20:38,27/Feb/15 14:13,3.3,,,,,,,3.4,,,0,,,,,,,,"The lazy initialization of the internal array of partialDerivatives in BicubicSplineInterpolatingFunction is not thread safe. If multiple threads call any of the partialDerivative functions concurrently one thread may start the initialization and others will see the array is non-null and assume it is fully initialized. If the internal array of partial derivatives was initialized in the constructor this would not be a problem.

i.e. the following check in partialDerivative(which, x, y)
        if (partialDerivatives == null) {
            computePartialDerivatives();
        }
will start the initialization. However in computePartialDerivatives()
        partialDerivatives = new BivariateFunction[5][lastI][lastJ];

makes it appear to other threads as the the initialization has completed when it may not have.
",,,,,,,,,,,,,,,,,,,,,02/Jul/14 22:43;psteitz;MATH-1134.patch;https://issues.apache.org/jira/secure/attachment/12653709/MATH-1134.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-06-30 22:28:24.215,,,false,,,,,,,,,,,,,,402709,,,Wed Feb 25 22:19:27 UTC 2015,,,,,,0|i1xbcn:,402776,,,,,,,,"30/Jun/14 22:28;erans;Fields made ""final"" in revision 1606940.
Could you please check the new code?
We'd also be interested to get more unit tests for this interpolator. You are welcome to provide feedback.
","30/Jun/14 22:37;dscherger;Wow, that was fast. At a glance the new code looks good, it will certainly fix the race I found earlier today.
The only downside to initializing everything in the constructor is that you pay that cost up front, and if you weren't going to use the partial derivatives that's wasted. Not sure if that's a big deal or not.
","30/Jun/14 23:39;psteitz;An alternative would be to just put the null check / initialization code above in a sync block.  The performance hit caused by the change is probably not large, but users who just want to interpolate may be annoyed by it.  ",01/Jul/14 09:31;erans;Another possibility is to add a constructor with a flag indicating that the derivatives must be computed.,01/Jul/14 11:59;sebb@apache.org;Yet another might be to use the IODH pattern (I've not checked if that could work here),"01/Jul/14 16:16;erans;bq. IODH

I don't think so: the fields are not static.","01/Jul/14 17:03;psteitz;Yes, we are dealing with cached member data here.  There are four logical options, that I will list in my personal order of preference

0) Revert the change and just document that the class is not threadsafe.  This sounds wimpy, but basically the choice is between burdening all users of the class with the overhead associated with one of the other options or just burdening those who actually want to use instances as singletons with protecting them themselves.  That is the strategy we have used elsewhere and it has always made sense to me - users who use math objects that have internal state know better than us what the concurrency context of their applications are and can eat all and only the overhead they need to ensure correctness of their code.  We should make it clear which objects maintain internal state and mark them as not threadsafe, but we should not force ourselves to precompute everything or forego caching / mutability to maintain threadsafety everywhere.
1) Protect the cached data by synchronizing access to it (adding sync blocks / explicit locks where needed)
2) Get rid of the caching - actually compute the partials each time they are requested
3) State of trunk as of r1606940 (force cache preload at construction)

Gilles suggestion to make 3) itself configurable sort of combines 3) and 0) IIUC what he is suggesting.","01/Jul/14 17:54;erans;0) is a bit sad, because it can be thread-safe in an obvious way (all fields ""final"").

Affirming thread-safety with 1) might not be as obvious. If only the ""private"" method that computed the derivatives were ""synchronized"", there might still be race conditions (leading to multiple computations of the same fields). While having all accessors synchronized will bring a penalty to applications that really make concurrent accesses.

2) is not efficient, always (if derivatives are used).

3) is not efficient, at instantiation.

bq. Gilles suggestion to make 3) itself configurable sort of combines 3) and 0)

Yes, IIUC what you means by ""combines"".
Users who need the derivatives would have to specifically request that derivatives are computed (at instantiation), while the default would leave them at ""null"" (and if called later a NPE will be raised, by the JVM).
\[This is my preferred option. And you didn't rank it ;)\]

Another option might be to deprecate the derivatives code altogether. And perhaps move it to a subclass. And reimplement it, using the ""o.a.c.m.analysis.differentiation.DerivativeStructure"".
","02/Jul/14 03:20;dscherger;I'm all for callers having the option to not initialize for partial derivatives on construction, as long as there is some explicit and obvious way to ensure that everything has been initialized before unleashing a bunch of threads on the interpolator.

At the moment, it's not obvious that there is any internal state that requires initialization before using the class, the only way to find out is to experience the problem of incomplete initialization, and then to look at the source to see what is going on and discover that there is lazy initialization being done in both the partialDerivative[XY...] and value methods. The solution I've landed on for now is to make an initial partialDerivativeX(x,y) call as the objects are constructed, but some sort of explicit init() or alternative constructor with eager/lazy init options would be better.

I'm not particularly fond of the idea of declaring methods synchronized unless that's really what is required to make them safe. The overhead there may be small, but that's relative and when running on many threads doing lots of computation it adds up, and sometimes becomes a significant problem, so if it can be avoided all the better. Similarly, recalculating the partials on every call is not very good if they can be computed once and re-used for every subsequent call, which seems like it could be a major performance win.

","02/Jul/14 18:14;erans;New constructor added in r1607434: a flag will indicate whether initialization of the internal data needed to call the partial derivatives methods is to be performed. If set to false (default), calling a method anyway will trigger a NPE.

Not sure that ""false"" should be the default...
",02/Jul/14 18:19;psteitz;I don't like the NPE part.  Can we at least make it so that the flag basically says precompute and cache derivatives and false means there is no caching (i.e. they are computed each time).,"02/Jul/14 21:04;erans;bq. I don't like the NPE part.

This is similar to how other parts of the CM code would behave if preconditions are not satisfied. Here, the user requests an interpolating function whose base interface is a ""BivariateFunction""; the derivative part is a ""bonus"" for those who comply with the precondition (which is that the flag must be set to true).

In fact, I chose ""false"" as the default because you advocated that a user who just wants to interpolate should not pay the price needed to use the derivative functionality.
But the converse is safer: let then the default be ""true""; a user that explicitly requests no initialization can only blame himself if he calls one of the derivative methods afterwards.

Using on-demand caching complicates the code and prevents making the field final.
Also, the derivatives were initially intended for internal purposes (to be used in ""TricubicInterpolator"").

The initial code (initialization at access time) was really based on (untested) efficiency considerations, and I would consider it premature optimization. As it is now the code is both safe (if the user abides by the simple precondition) and efficient.
","02/Jul/14 22:43;psteitz;Attached is a patch against the pre-1606940 code that should fix the race.  Since we require JDK 1.5+, the double-checked locking should be OK.  This fix avoids having to add constructor arguments, etc. and will only impose sync overhead (one time) for those wanting the partials.","03/Jul/14 09:22;erans;It's an elegant fix if the goal is to have as little change as possible.
I still think that the goal of having all fields final has higher priority (as per our numerous discussions on avoiding non-final fields).

I won't oppose your applying this patch if other people think that it's better than the current version.
","03/Jul/14 21:46;erans;Phil,

You seem always reluctant to let the code throw a NPE. Although I think that it is perfectly fine behaviour to signal a programming error, would you like it better if we change the exception type to ""MathIllegalStateException""?
","04/Jul/14 19:44;psteitz;I agree that ISE would be better, but I think the API is a little awkward.  The r1607434 code does clearly document preconditions so the RTE would not be ""unexpected"" but I think the current code forces users to think about the constructor flag when we can just fix the code to be threadsafe.  I think we should strive to make our APIs as simple as possible, avoiding situations where you have to provide special constructor arguments for instance methods to work when we can.

I think we should either get rid of the partials caching, have the flag control that (meaning false means do not cache partials), or just make the cache initialization threadsafe (the patch I provided is one way to do this).",25/Feb/15 22:07;tn;BicubicSplineInterpolatingFunction has been already been deprecated for 3.4 and now removed in 4.0 so I would suggest to close this issue or is there still anything planned for 3.5?,"25/Feb/15 22:19;erans;Closing is fine with me.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Kolmogorov-Smirnov Tests takes 'forever' on 10,000 item dataset",MATH-1131,12723580,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ysb33r,ysb33r,25/Jun/14 08:12,26/Dec/14 19:50,07/Apr/19 20:38,19/Sep/14 16:23,3.3,,,,,,,3.4,,,2,,,,,,,,"I have code simplified to the following:

    KolmogorovSmirnovTest kst = new KolmogorovSmirnovTest();
    NormalDistribution nd = new NormalDistribution(mean,stddev);
    kst.kolmogorovSmirnovTest(nd,dataset)

I find that for my dataset of 10,000 items, the call to kolmogorovSmirnovTest takes 'forever'. It has not returned after nearly 15minutes and in one my my tests has gone over 150MB in  memory usage. ",Java 8,,,,,,,,,,,,,,,,,,,,25/Jun/14 08:14;ysb33r;1.txt;https://issues.apache.org/jira/secure/attachment/12652357/1.txt,27/Jun/14 07:48;tn;MATH-1131.patch;https://issues.apache.org/jira/secure/attachment/12652776/MATH-1131.patch,25/Jun/14 08:15;ysb33r;ReproduceKsIssue.groovy;https://issues.apache.org/jira/secure/attachment/12652359/ReproduceKsIssue.groovy,25/Jun/14 08:14;ysb33r;ReproduceKsIssue.java;https://issues.apache.org/jira/secure/attachment/12652358/ReproduceKsIssue.java,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-06-25 16:37:43.222,,,false,,,,,,,,,,,,,,401765,,,Sat Sep 13 15:59:58 UTC 2014,,,,,,0|i1x5jz:,401834,,,,,,,,25/Jun/14 08:14;ysb33r;Dataset that was used.,25/Jun/14 08:14;ysb33r;Example Java code to reproduce issue,25/Jun/14 08:15;ysb33r;Example Groovy code that also reproduces the issue,25/Jun/14 09:38;ysb33r;See the code examples for the specific _mean_ & _stddev_ that was used.,25/Jun/14 16:37;psteitz;Thanks for reporting this and providing the code and data.  I suspect the problem is in the matrix exponentiation done in the roundedK method.  Anyone interested in patching this should start by looking at the reference in the class javadoc (and other sources) to identify optimizations that can be done for large samples.,"25/Jun/14 18:14;tn;I did briefly debug the example and indeed the calculation hangs when calling roundedK, or more precisely in createH.

There powers of BigFraction objects are created with really big numerators and denominators. Some of the calculations later on take then forever because of this, e.g. when internally calculating the gcd.

Looking at the implementation from the referenced paper, there the H values are computed with double precision. Was there a specific reason to use BigFraction in our implementation? Is there a specific need for that level of accuracy for the Kolmogorov-Smirnov Test? The other inference tests do not seem to be so stringent.

It looks like there is no easy way to limit the maxDenominator when calling multiply() as it is possible when creating a BigFraction object.
","25/Jun/14 18:43;ysb33r;This section of code in createH might be part of the problem. A quick test on my macbook shows that the most of 36 minutes are spent inside there for d=0.029357223978016822, n=9999. (I specifically tried 9,999 as it was one less than 10,000).

{code:java}
for (int i = 0; i < m; ++i) {
  for (int j = 0; j < i + 1; ++j) {
    if (i - j + 1 > 0) {
 	for (int g = 2; g <= i - j + 1; ++g) {
   	  Hdata[i][j] = Hdata[i][j].divide(g);
 	}
    }
  }
}

{code}
","25/Jun/14 20:43;ysb33r;[~phil@steitz.com] said on the ML:

bq. Sorry for responding to the list but I have only mobile atm .  IIRC the roundedK method should not be creating matrices of BigFractions, but rather using doubles.

I did a quick hack on the test code I used for createH earlier to use double instead and the speed improvement as expected is immense - down from 36min to 9min. I cannot comment on whether the change in precision is significant, but not was not the point of the test.

","26/Jun/14 13:15;tn;My previous comment wrt performance of matrix.power\(n\) was wrong.
This is not the limiting factor when using a BlockRealMatrix as the number of actual matrix multiplications is only log\(n\).

The problem when using so large samples is that the matrix elements quickly grow and lead to NaN computations. The reference code does a special trick when computing power\(n\):

 * after every multiplication check if the center element is > 1e140 and if so divide the whole matrix by this factor.
 * update the factor each time it is applied to the matrix
 * after computing power\(n\), the factor is applied in a reverse manner on the element to be returned.","27/Jun/14 07:48;tn;Attached a patch that uses doubles to evaluate H for the rounded case.

This allows to evaluate the test up a N of ~700, for larger datasets overflow happens when calculating H.power\(n\).

We need to decide if we want to implement the same trick as in the reference implementation from the paper.","28/Jun/14 21:38;psteitz;I think the patch definitely improves things, so +1 to commit that for now.  I am not sure that the Marsaglia-Tsang method is best for large n, though.  It might be best to either a) just use the Kolmogorov approximation or b) use what Simard-L'Ecuyer ([2] in the class javadoc) refer to as the Pelz-Good method for large n (or more precisely large n*d).  I think R does a).  The two-sample tests do a).","28/Jun/14 22:55;psteitz;A few of comments not directly related to the performance issue, but likely relevant to the OP and anyone using KolmogorovSmirnovTest to evaluate the null hypothesis that a sample comes from a normal (Gaussian) distribution:

1. The KS test using parameters estimated from the data is in general not the best test to use to test normality.  We do not currently implement the Lillifors or other tests.  Patches welcome :)  (Discuss first on the mailing list, then open separate tickets for these if interested.)
2.  *No* classical frequentist test really works for large samples.  KS, Liilifors, Shapiro-Wilks et al are uniformly too powerful to be meaningful for samples even as small as 5000 observations.  See, e.g. [1].
3.  An interesting alternative for large samples is [2].   Here again, patches welcome.  A similar approach implementable using Commons Math version 3.x would be to bin the data in standard deviation units and then apply a G-test with expected counts computed using quantiles of the normal distribution.

[1] http://www.statisticalmisses.nl/index.php/frequently-asked-questions/77-what-is-wrong-with-tests-of-normality
[2] https://ideals.illinois.edu/bitstream/handle/2142/29878/largesamplenorma93171bera.pdf","04/Jul/14 14:19;tn;The implementation of R is a 1:1 copy of the code from the Marsaglia-Tsang paper, including the 1e140 trick.","04/Jul/14 14:28;tn;Committed patch in r1607864.

Need to decide what to do with large samples. I would be more confident with the Pelz method though.","04/Jul/14 20:28;psteitz;Thomas
bq. The implementation of R is a 1:1 copy of the code from the Marsaglia-Tsang paper, including the 1e140 trick.
Yes, but from the R code (what calls the C code) and online docs it looks to me like R only does this for n < 100.  Beyond that, it looks like the ks sum is used.  I agree though that based on Lecuyer-Simard's analysis, the Pelz method would be better, with ""exact"" computations as we have now for <n, d> up to the bounds they suggest.  I will implement this if there are no objections.
","04/Jul/14 21:04;tn;Ok great and thanks for the info, as I did only look at the c source code in the R project.

I am happy to review your code and have no objections.",26/Jul/14 20:56;psteitz;Pelz-Good implemented in r1613723.  ,31/Jul/14 13:43;psteitz;OK to resolve this?,"01/Aug/14 15:45;ysb33r;Before closing, it would be good to have a note in the javadoc to indicate over which value of N samples, performance will become an issue. It will prevent users like me from falling into a pit :)","01/Aug/14 18:41;psteitz;@Schaik:  have you tested with the latest code in trunk?  I would not expect the current code to degrade for large n, unless ""exact"" is specified and that already has a warning label.",01/Aug/14 19:19;ysb33r;[~phil@steitz.com] Not yet. But I can try on Monday.,"13/Sep/14 15:59;tn;Code looks good.

The original problem is also solved, thus I think we can close the issue.",,,,,,,,,,,,,,,,,,,,,,,,
Percentile Computation errs,MATH-1129,12721640,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,carlwitt,carlwitt,17/Jun/14 09:22,26/Dec/14 19:50,07/Apr/19 20:38,05/Oct/14 18:44,3.2,,,,,,,3.4,,,0,,,,,,,,"In the following test, the 75th percentile is _smaller_ than the 25th percentile, leaving me with a negative interquartile range.

{code:title=Bar.java|borderStyle=solid}
@Test public void negativePercentiles(){

        double[] data = new double[]{
                -0.012086732064244697, 
                -0.24975668704012527, 
                0.5706168483164684, 
                -0.322111769955327, 
                0.24166759508327315, 
                Double.NaN, 
                0.16698443218942854, 
                -0.10427763937565114, 
                -0.15595963093172435, 
                -0.028075857595882995, 
                -0.24137994506058857, 
                0.47543170476574426, 
                -0.07495595384947631, 
                0.37445697625436497, 
                -0.09944199541668033
        };
        DescriptiveStatistics descriptiveStatistics = new DescriptiveStatistics(data);

        double threeQuarters = descriptiveStatistics.getPercentile(75);
        double oneQuarter = descriptiveStatistics.getPercentile(25);

        double IQR = threeQuarters - oneQuarter;
        
        System.out.println(String.format(""25th percentile %s 75th percentile %s"", oneQuarter, threeQuarters ));
        
        assert IQR >= 0;
        
    }
{code}",Java 1.8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-06-17 16:13:25.783,,,false,,,,,,,,,,,,,,399836,,,Wed Jun 18 12:21:38 UTC 2014,,,,,,0|i1wtxb:,399944,,,,,,,,"17/Jun/14 09:25;carlwitt;It's the NaN value, but there's no note in the percentile documentation that this is not allowed.
Adding a NaNStrategy like in rank conversions might be a solution. 
This also creates doubts that the other methods handle NaN values correctly.","17/Jun/14 16:13;erans;The [Javadoc|http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/stat/descriptive/rank/Percentile.html] for {{Percentile}} does provide some warning about NaN within data:
{noformat}
To compute percentiles, the data must be at least partially ordered. Input arrays are copied and recursively partitioned using an ordering definition. The ordering used by Arrays.sort(double[]) is the one determined by Double.compareTo(Double). This ordering makes Double.NaN larger than any other value (including Double.POSITIVE_INFINITY). Therefore, for example, the median (50th percentile) of {0, 1, 2, 3, 4, Double.NaN} evaluates to 2.5.

Since percentile estimation usually involves interpolation between array elements, arrays containing NaN or infinite values will often result in NaN or infinite values returned.
{noformat}
but the caveat does not appear in {{DescriptiveStatistics}}.

Even when no NaN is returned, the result varies with the position of the NaN value in the data array. :(
It looks like the sorting is wrong in the presence of NaN. See below.

bq. This also creates doubts that the other methods handle NaN values correctly.

I don't know whether the intention was that the result should always be considered undefined in the presence of NaN.

Local sort
Without NaN: 25th percentile -0.1773147094639404 75th percentile 0.2748649403760461
With NaN: 25th percentile 0.24166759508327315 75th percentile -0.028075857595882995
With +inf: 25th percentile -0.15595963093172435 75th percentile 0.37445697625436497

java.util.Arrays.sort (sorting the whole data array)
Without NaN: 25th percentile -0.1773147094639404 75th percentile 0.2748649403760461
With NaN: 25th percentile -0.15595963093172435 75th percentile 0.37445697625436497
With +inf: 25th percentile -0.15595963093172435 75th percentile 0.37445697625436497

I've attempted to fix the local sort:
Without NaN: 25th percentile -0.1773147094639404 75th percentile 0.2748649403760461
With NaN: 25th percentile -0.15595963093172435 75th percentile 0.37445697625436497
With +inf: 25th percentile -0.15595963093172435 75th percentile 0.37445697625436497

If nobody objects, I'll commit this modification, and further tests can be devised to ensure that it works correctly for other inputs.
","17/Jun/14 16:20;erans;Actually, the standard {{java.util.Arrays}} class has the needed functionality (a method for sorting part of an array in place).
The Javadoc indicates that it is a tuned quicksort, while the function in CM is called ""insertionSort"".
I'd rather use the JDK one, and remove the ""local"" sort. Any objection?
","17/Jun/14 16:32;erans;Fixed ""insertionSort"" method in revision 1603217.

Waiting for comment about removing it in favour of JDK's implementation.
","17/Jun/14 16:43;erans;bq. Adding a NaNStrategy like in rank conversions might be a solution. 

This suggestion is also to be discussed for MATH-1120.
Could you please raise the issue on the ""dev"" ML?
","17/Jun/14 16:57;carlwitt;Me? This was my first post here, so I'm not sure what to do.","17/Jun/14 17:21;erans;Whenever you submit a report here, a discussion might ensue about how to best fix the problem, or to properly implement a feature request, so it is quite useful to be subscribed to the project's development mailing list (""dev@commons.apache.org""):
http://commons.apache.org/proper/commons-math/mail-lists.html

Then when you post there, you should prefix the ""subject"" line with ""\[Math\]"" (because the list is shared with many other projects under the ""Commons"" common ;) umbrella).

","17/Jun/14 17:30;carlwitt;Gilles, would you mind doing that for me? I'm not what you would call a power user of the commons math library, so I'd rather like to stay out of development issues.

Thank you!","18/Jun/14 09:46;erans;bq. [...] so I'd rather like to stay out of development issues.

Development is also driven by user needs.

If you are satisfied with the current status of this issue, I'm fine to leave it at that. Thanks for the report.
",18/Jun/14 12:21;carlwitt;I think I expressed my needs. You're welcome!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2.0 equal to -2.0,MATH-1127,12721089,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,luc,luc,13/Jun/14 11:38,26/Dec/14 19:50,07/Apr/19 20:38,13/Jun/14 14:21,3.3,,,,,,,3.4,,,0,,,,,,,,"The following test fails:

{code}
    @Test
    public void testMath1127() {
        Assert.assertFalse(Precision.equals(2.0, -2.0, 1));
    }
{code}
","Linux, Java 5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,399287,,,Fri Jun 13 14:21:15 UTC 2014,,,,,,0|i1wqkf:,399397,,,,,,,,"13/Jun/14 14:21;luc;Fixed in subversion repository as of r1602438.

This was a fun one!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""LevenbergMarquardtOptimizer"": Divergent behavior of new code",MATH-1126,12720874,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Cannot Reproduce,,erans,erans,12/Jun/14 13:36,13/Jun/14 15:37,07/Apr/19 20:38,13/Jun/14 15:37,3.3,,,,,,,,,,0,regression,,,,,,,"The new implementation of ""LevenbergMarquardtOptimizer"" (package ""o.a.c.m.fitting.leastsquares"") behaves differently from the previous one (package ""o.a.c.m.optim.nonlinear.vector.jacobian"").

This shows up not so much in the solutions respectively found by one and the other implementation; there are fairly similar, but in my use-case, the number of function evaluations is quite different. And this could explain an observed 35% performance degradation.
",,,,,,,,,,,,,,,,,,,,,12/Jun/14 14:07;erans;LM_cost_NEW;https://issues.apache.org/jira/secure/attachment/12650055/LM_cost_NEW,12/Jun/14 14:07;erans;LM_cost_OLD;https://issues.apache.org/jira/secure/attachment/12650054/LM_cost_OLD,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,399073,,,Fri Jun 13 15:37:17 UTC 2014,,,,,,0|i1wpav:,399190,,,,,,,,"12/Jun/14 14:07;erans;I could trace one possible cause of the problem up to the usage of variable ""currentCost"". As shown by the attachments, the value differ between the two implementations, although the core of the algorithm uses them in the same way to control the exploration of the search space.
This could explain why the behaviours differ.
","12/Jun/14 14:33;erans;It seems that in the old implementation, ""currentCost"" is computed using _unweighted_ residuals, while the new implementation uses _weighted_ residuals (cf. override of ""getResiduals"" in class ""DenseWeightedEvaluation"").","12/Jun/14 14:50;erans;bq. [...] in the old implementation, ""currentCost"" is computed using unweighted residuals [...]

No, that's not it :(
Cf. line 80 in ""AbstractLeastSquaresOptimizer"" (in ""o.a.c.m.optim.nonlinear.vector.jacobian"").","13/Jun/14 15:37;erans;It seems that after several cycles of recompiling, I cannot reproduce different outputs from the two implementations!  Maybe I was using ""stale"" files somewhere...
The results are almost exactly the same.

The performance degradation remains, although down to about 20% (vs 35% as initially observed).  I still get a different number of evaluations but it's not caused by the core of the optimization algorithm...
Thus closing this report. Sorry for the noise.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TDistribution density function can be sped up.,MATH-1125,12716877,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ajo.fod,ajo.fod,27/May/14 19:08,26/Dec/14 19:50,07/Apr/19 20:38,29/May/14 17:47,,,,,,,,3.4,,,0,,,,,,,,Some precomputable calculations are being done in the TDistirbution.density function.,,,,,,,,,,,,,,,,,,,,,27/May/14 19:12;ajo.fod;TDist.java;https://issues.apache.org/jira/secure/attachment/12646964/TDist.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-29 17:47:31.663,,,false,,,,,,,,,,,,,,395085,,,Thu May 29 17:47:31 UTC 2014,,,,,,0|i1w0vj:,395220,,,,,,,,"27/May/14 19:12;ajo.fod;Here is a modified version of TDistibution that compares the original class with the new vesion.

The new version is about 5x faster.","29/May/14 17:47;tn;Fixed in r1598342.

thanks for the patch and report!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Instances of AbstractRealDistribution require a random generator.,MATH-1124,12715582,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ajo.fod,ajo.fod,20/May/14 19:50,22/Apr/16 22:31,07/Apr/19 20:38,22/Apr/16 22:31,,,,,,,,4.0,,,0,,,,,,,,"A couple of observations:
... The default random generator takes a while to instantiate. 
... Many functions of these distributions don't require a random generator. Generally speaking only sampling requires it.
So, why force the default constructor to initialize with a new random generator ... why not use a global generic or simple generator? 

Or do away with random generator except for sampling?

This issue was observed with the TDistribution class , but it is probably applicable to many classes as well. 

",,,,,,,,,,,,,,,,,MATH-1154,MATH-1158,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-05-20 21:28:43.096,,,false,,,,,,,,,,,,,,393867,,,Fri Apr 22 22:31:21 UTC 2016,,,,,,0|i1vthj:,394014,,,,,,,,"20/May/14 21:28;erans;Quoting Phil's answer on the ML:

{quote}
The subclasses will still need to either provide RandomGenerator
instances to constructors or use the default.  Passing null should
work to avoid the overhead you are referring to but will of course
cause NPE if you try to sample.  Lazy initialization or static
default instances are not likely to be supported.
{quote}

Unless you provide a use-case demonstrating an unbearable slowliness, this issue will be closed.
",06/Oct/14 21:36;tn;To be discussed on the mailinglist again if we chose another default rng or use lazy initialization.,"07/Oct/14 17:44;ajo.fod;How about having a new class called Sampler that is initialized with random generator and AbstractRealDistribution?
That way you can keep objects final. 

Looking through my code here, I don't use sampling nearly as often as the other functions of a distribution (less than 1 in 20 times). Perhaps others can chime in what fraction of times people use sampling on abstract real distributions.",07/Oct/14 17:53;psteitz;Ajo - that's how we used to have things set up (sampling implementations used to be in the .random package).  Consensus was to move sampling to the distribution classes.  Lets discuss options on how to improve the implementation on the mailing list.,"07/Oct/14 18:16;Otmar Ertl;What about the C++ STL design choice? The corresponding Java design would be to pass a random generator instance to the sample method. For instance, have a look on the code example given on http://www.cplusplus.com/reference/random/uniform_real_distribution/

EDIT:
This would also allow the distribution classes to be entirely immutable.","07/Oct/14 19:13;ole;How about: 

sample(double uniformRandomNumber)","07/Oct/14 19:28;Otmar Ertl;@Ole Ersoy: For some distributions it is quite common to use multiple uniform distributed random numbers to generate a single random number following the given distributions (e.g normal distribution). Therefore, passing the random number generator is a much better design choice.","07/Oct/14 20:13;ole;Ah - OK.  Another thought would be to have a wrapper for each distribution.  The wrapper would know how to generate samples and would also enable a fluid API (Listenting for sample events, filtering, etc.) similar to what Reactive Extensions have:

https://github.com/Reactive-Extensions/RxJS

Each Sampler could create it's own RNG or receive one:

//Constructor
Sampler.create();
Sampler.create(RNGInstance);

//A Single Sample
NormalDistributionSampler.create().sample();

//Fluid API
NormalDistributionSampler.create().take(3).takeLast(1);

//Listeners
NormalDistributionSampler.create().subscribe(listener);

Just throwing out some ideas at this point.  First had this idea while while watching this video:
http://www.infoq.com/presentations/game-functional-reactive-programming

In this case the actual sample() method on the distribution interface could be kept simple (Actually this may be true in all cases):

sample(double[] uniformRNArray)

If the distribution needs more than one RN then the rest are on the array as well.  I think light weight parameters are better for testing.","08/Oct/14 05:57;Otmar Ertl;It is fairly easy to write a RandomGenerator mock using AbstractRandomGenerator that returns a predefined sequence of random numbers, if testing is your major concern. In my opinion, your proposed interface 
{quote}
sample(double[] uniformRNArray)
{quote}
is much more complex than
{quote}
sample(RandomGenerator rng)
{quote}
More important, there are distributions for which rejection sampling is used to generate random numbers. In this case, the number of required uniform distributed random numbers to generate a single random number obeying the given distribution is random too (see http://en.wikipedia.org/wiki/Rejection_sampling).
","08/Oct/14 13:55;ole;I agree that writing a RandomGenerator mock is simple, but I do believe that in order to maximize the simplicity, flexibility, and focus of the design it is best to inject both the RNG and the distribution into a class/classes that is/are designed for sampling and things related to sampling.

For instance there could be a minimal SamplingObserver interface:

subscribe(SampleListener)
next() //triggers 1 iteration of observing the sampling
next(int n) //triggers n iterations of observing sampling

I'm not an expert here, but I suspect that the strategy design pattern could be used to design one instance that samples from all the distributions and that could be easily extended.","30/Mar/16 14:52;erans;Issue is fixed in branch ""feature-MATH-1158"".
","22/Apr/16 22:31;erans;Code merged into ""develop"" branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in BSPTree#fitToCell(),MATH-1123,12714765,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,arcadien,arcadien,16/May/14 13:33,26/Dec/14 19:50,07/Apr/19 20:38,19/May/14 15:06,3.3,,,,,,,3.4,,,0,,,,,,,,"Hello, 
I faced a NPE using  BSPTree#fitToCell() from the SVN trunk. I fixed the problem using a small patch I will attach to the ticket.
",Win32_64,,,,,,,,,,,,,,,,,,,,19/May/14 13:18;arcadien;ConvexHullTest.java;https://issues.apache.org/jira/secure/attachment/12645550/ConvexHullTest.java,19/May/14 15:41;tn;convex.png;https://issues.apache.org/jira/secure/attachment/12645569/convex.png,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-05-19 10:36:37.733,,,false,,,,,,,,,,,,,,393078,,,Tue May 20 07:24:58 UTC 2014,,,,,,0|i1vosn:,393245,,,,,,,,"16/May/14 13:37;arcadien;Patch :
### Eclipse Workspace Patch 1.0
#P com.pollentech.maths.stats
Index: apacheMaths/src/main/java/org/apache/commons/math3/geometry/partitioning/BSPTree.java
===================================================================
--- apacheMaths/src/main/java/org/apache/commons/math3/geometry/partitioning/BSPTree.java	(revision 1595194)
+++ apacheMaths/src/main/java/org/apache/commons/math3/geometry/partitioning/BSPTree.java	(working copy)
@@ -300,6 +300,9 @@
             } else {
                 s = s.split(tree.parent.cut.getHyperplane()).getMinus();
             }
+            if (s == null){
+                return s;
+            }
         }
         return s;
     }
","19/May/14 09:57;arcadien;Eclipse Workspace Patch 1.0
#P com.pollentech.maths.stats
Index: apacheMaths/src/main/java/org/apache/commons/math3/geometry/partitioning/BSPTree.java
===================================================================
apacheMaths/src/main/java/org/apache/commons/math3/geometry/partitioning/BSPTree.java (revision 1595194)
+++ apacheMaths/src/main/java/org/apache/commons/math3/geometry/partitioning/BSPTree.java (working copy)
@@ -300,6 +300,9 @@
} else { s = s.split(tree.parent.cut.getHyperplane()).getMinus(); }
+ if (s == null)
{ + return s; + }
}
return s;
}","19/May/14 10:36;luc;Could you explain in which situation you encounter the exception?

The fitToCell method is a private one and should be called only in controlled case where the error should not occur. So rather if you encounter this, it is an indication something wrong already occurs before the call, and I guess it should be fixed too.

The best way would be to have a small reproducible test case that we could add as a non-regression unit test once the problem is fixed.",19/May/14 13:18;arcadien;Test case for ConvexHull2D which triggers a NPE in BSPTree#fitToCell().,"19/May/14 13:20;arcadien;Hello Luc,
Thanks for your reply. I just added a test case with the original data i used to trigger the NPE.

regards,

Aurelien Labrosse","19/May/14 15:06;luc;Fixed in subversion repository as of r1595924.

The problem was triggered as several points from the convex hull were aligned when the convec region was built. As such convex regions can also be built directly by users from regular public API, the place you fixed it was appropriate. So I simply moved the test as a stop condition in the loop, but it was quite good as is.

Thanks for the report and for the patch.","19/May/14 15:21;tn;Is the convex hull in this example correctly constructed, or is this something we have to further investigate?

From the code I see that collinear points on the hull shall be included.","19/May/14 15:41;tn;Ok, I quickly analyzed it using the example application, and the resulting convex hull looks correct.

The red line is the convex hull, while the blue line is the enclosing ball (see the attached screenshot convex.png)","19/May/14 16:43;luc;Yes, the hull is correct and I agree colinear point can be included. It was the BSPTree built from these points that triggered a problem.
As users could have chosen to build a convex region directly from these points, it was really at BSP level that such points should be handled properly.

We had already encountered this problem previously, but I failed to fix it. The previous cases were due to numerical noise, as the original point were not exactly aligned. However, exactly aligned point do occur and should be accepted. This new use case is really interesting since it does trigger the problem with exact integer coordinates. I have added it as a junit test in the general abstract test for all convex hull algorithms as it seemed interesting even for hull themselves, even if they were not really involved this time.","19/May/14 21:18;tn;Ok thanks for the update. I slightly changed the unit test to use the proper generator.

Regarding the collinear points: they are correctly added as specified by the flag in the constructor. If only a minimal hull shall be constructed, the collinear points are not included. I thought this might be a useful distinction.

Unfortunately, these collinear points were quite tricky to handle for the other algorithms that I implemented and could not get it to work reliably. Otoh, other libraries also seem to have chosen MonotoneChain as primary algorithm for convex hulls as it seems to be the most robust in this regard.","20/May/14 07:24;arcadien;Thomas, Luc,

Thanks for your quick replies and actions. I'll continue using your excellent library, and try to help as much as i can.

regards,

Aurelien Labrosse",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brent optimizer doesn't use the Base optimizer iteration counter,MATH-1121,12713731,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ajo.fod,ajo.fod,12/May/14 20:05,13/May/14 09:40,07/Apr/19 20:38,13/May/14 09:40,,,,,,,,,,,0,,,,,,,,"BrentOptimizer uses ""iter"" defined in ""doOptimize""  to count iterations.
It should ideally use the iteration counter defined for the BaseOptimizer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-05-13 09:40:43.464,,,false,,,,,,,,,,,,,,392047,,,Tue May 13 09:40:43 UTC 2014,,,,,,0|i1vimn:,392242,,,,,,,,"13/May/14 09:40;erans;Thanks for the report.
Fixed in revision 1594174.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Complex: semantics of equals != Double equals, mismatch with hashCode",MATH-1118,12708189,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,Telcontar,Telcontar,14/Apr/14 06:04,25/Jan/16 20:28,07/Apr/19 20:38,07/May/15 14:47,3.2,,,,,,,3.6,4.0,,0,,,,,,,,"Two complex numbers with real/imaginary parts 0.0d but different signs compare as equal numbers. This is according to their mathematical value; the comparison is done via 

                return (real == c.real) && (imaginary == c.imaginary);

Unfortunately, two Double values do NOT compare as equal in that case, so real.equals(c.real) would return false if the signs differ.

This becomes a problem because for the hashCode, MathUtils.hash is used on the real and imaginary parts, which in turn uses Double.hash.

This violates the contract on equals/hashCode, so Complex numbers cannot be used in a hashtable or similar data structure:

    Complex c1 = new Complex(0.0, 0.0);
    Complex c2 = new Complex(0.0, -0.0);
    // Checks the contract:  equals-hashcode on c1 and c2
    assertTrue(""Contract failed: equals-hashcode on c1 and c2"", c1.equals(c2) ? c1.hashCode() == c2.hashCode() : true);","Mac OS 10.9, Java 6, 7",,,,,,,,,,,,,,,,,,,,17/Apr/14 11:13;erans;MATH-1118.patch;https://issues.apache.org/jira/secure/attachment/12640615/MATH-1118.patch,14/Apr/14 13:12;erans;MATH-1118.patch;https://issues.apache.org/jira/secure/attachment/12640066/MATH-1118.patch,14/Apr/14 06:04;Telcontar;Report5.java;https://issues.apache.org/jira/secure/attachment/12640028/Report5.java,14/Apr/14 07:42;Telcontar;Report5a.java;https://issues.apache.org/jira/secure/attachment/12640036/Report5a.java,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-04-14 12:53:23.365,,,false,,,,,,,,,,,,,,386512,,,Mon Jan 25 20:28:01 UTC 2016,,,,,,0|i1ul1b:,386776,,,,,,,,14/Apr/14 07:42;Telcontar;Dfp instances are affected in the same way (-0.0 == 0.0 but their hash code differs).,"14/Apr/14 12:53;erans;When introducing the ""correct"" (according to JDK for use with ""Map"") semantics, other unit test fails, especially one that was introduced to ensure the _other_ semantics: MATH-221

Could you please raise this issue on the ""dev"" ML? Thanks.
","14/Apr/14 13:12;erans;Patch: proposed change (with one failing test).
","14/Apr/14 22:55;Telcontar;It is indeed not clear which way this bug should be fixed.
Another option (assuming the problem only occurs due to the sign with value 0.0) is to override hashCode such that hashCode(-0.0d) returns hashCode(0.0d), and works as is in the other cases.","14/Apr/14 23:05;psteitz;Given the decision in MATH-221, I am inclined to agree that the better approach is to just fix the hashcode impl.","14/Apr/14 23:08;Telcontar;I've posted a message on the mailing list. While writing that message, it occurred to me that unnormalized floating point values may break the idea of overriding hashCode for special cases, as there are too many possible cases of unnormalized floats with equal values but different internal representations.
Therefore, I think Gilles' suggestion is the safest, even though people expecting a mathematical comparison in equals() may be surprised. The update should make it clear in the Javadoc that for a mathematical comparison, ""=="" must be used.","14/Apr/14 23:18;Telcontar;It seems my previous message overlapped with Phil's. A fix in hashCode would be the most intuitive behavior, but what about unnormalized floats? Does anyone know how that works in Java?
For example, can we get different representations of two Double numbers with the same value? For example, 1 * 2^1 or 2 * 2^0, which are both 2? In that case, fixing hashCode is hopeless.","15/Apr/14 12:23;erans;In revision 1587548, I've added the utility method ""equals(double, double)"" in ""MathUtils"".
","17/Apr/14 11:13;erans;Uploaded patch for making ""equals"" compatible with ""hashCode"" and adding new ""equals"" methods for testing floating-point equality (using implementations in ""o.a.c.m.util.Precision"").

OK to commit?","18/Apr/14 16:00;erans;Committed in revision 1588500.
Thanks for the report.
",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,"07/May/15 06:35;Telcontar;The bug has been fixed for class Complex, but not for Dfp. The code in ""Report5a.java"" still fails. The nature of the problem is exactly the same as in ""Report5.java"", so I have reported the bug in the same place.

If you would prefer a new issue on this instead, I can file ""Report5a.java"" as a new bug.","07/May/15 14:47;luc;Fixed in git repository on both the master branch and the 3.X branch.

Thanks for the reminding.",25/Jan/16 20:28;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
twod.PolygonsSet.getSize produces NullPointerException if BSPTree has no nodes,MATH-1117,12708180,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,Telcontar,Telcontar,14/Apr/14 03:02,19/May/14 15:13,07/Apr/19 20:38,26/Apr/14 16:57,3.2,,,,,,,3.3,,,0,,,,,,,,"org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.getSize() uses a tree internally:

final BSPTree<Euclidean2D> tree = getTree(false);

However, if that tree contains no data, it seems that the reference returned is null, which causes a subsequent NullPointerException.

Probably an exception with a message (""tree has no data"") would clarify that this is an API usage error.","Mac OS 10.9, Java 6, 7",,,,,,,,,,,,,,,,,,,,14/Apr/14 03:04;Telcontar;Report3.java;https://issues.apache.org/jira/secure/attachment/12640014/Report3.java,14/Apr/14 06:12;Telcontar;Report3_1.java;https://issues.apache.org/jira/secure/attachment/12640029/Report3_1.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-04-26 16:57:06.591,,,false,,,,,,,,,,,,,,386503,,,Mon May 19 15:13:19 UTC 2014,,,,,,0|i1ukzb:,386767,,,,,,,,14/Apr/14 03:04;Telcontar;JUnit test to reproduce problem.,"14/Apr/14 06:12;Telcontar;Just calling the constructor with the right numbers produces the same effect, for example this one-liner:

new org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet(0.0d, 0.0d, 0.0d, 10.3206397147574d);","26/Apr/14 16:57;luc;The two problems you report are completely different from each other.

The first report is really a wrong API use. The call did not fulfill the constraints that were already given in
the javadoc, i.e. that the provided tree MUST have proper Boolean attributes at leaf nodes. There is no
verification here for performance reasons (it would imply walking the full trees all the time). I really don't
think it would be a good idea to do such verifications. So I only improved the javadoc of the constructor
to make it more clear this constructor is for expert use only (building a tree is difficult) and that there is
no verifications, adding in the documentation that failing to provide appropriate arguments is the
responsibility of users.

In fact, general users should never use this specific constructor, but should rely on the other ones which
are there precisely to avoid this kind of errors : the other constructors ensure the tree is correct before
calling this constructor.

The second one is a real problem, I have fixed it (see r1590251).

Please reopen the issue if you do not agree with the fix for the first problem, as it is documentation only.","28/Apr/14 00:12;Telcontar;Thanks for clarifying this one. I guess making these constructors package-private may break existing code so the documentation is the best way to warn the user.
By the way, the new documentation has a typo in ""task"" (currently ""taks""). The same typo is also in PolyhedronsSet.java. Of course that's easy to fix :-)","28/Apr/14 06:49;luc;Yes, these constructors are used outside of their package. Typically, the constructor for polygons (in 2D) is called from the split method in SubPlane and also from an internal class in OutlineExtractor, both in the 3D package. As the methods have been public for a while, they may also be used from user code. The call from the split method is a very important one: it is used a very large number of times as part of building 3D BSP trees, and here the tree has been built using a complex algorithm to ensure it is correct, so we don't want to visit all its leafs to check it.

Thanks for the hint about the typos, I have fixed them now.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException not advertized in Javadoc,MATH-1116,12708179,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,Telcontar,Telcontar,14/Apr/14 02:51,25/Jan/16 20:27,07/Apr/19 20:38,19/May/15 11:47,3.2,,,,,,,3.6,4.0,,0,javadoc,,,,,,,"The following statement produces a NullPointerException:

new org.apache.commons.math3.optim.nonlinear.vector.jacobian.LevenbergMarquardtOptimizer().getWeight();

The documentation does not seem to indicate that other data must be set before getWeight is used (at least I could not find that information). In this case, weightMatrix is still null because it has not been initialized.

This call should probably throw an IllegalStateException, which makes it clear that this API usage is incorrect.

This test uses LevenbergMarquardtOptimizer but any instantiable subclass of MultivariateVectorOptimizer probably works the same way.
","Mac OS 10.9, Java 6, 7",,,,,,,,,,,,,,,,,,,,16/Apr/14 06:47;Telcontar;IllegalStateB.java;https://issues.apache.org/jira/secure/attachment/12640400/IllegalStateB.java,14/Apr/14 02:51;Telcontar;Report2.java;https://issues.apache.org/jira/secure/attachment/12640012/Report2.java,07/May/15 06:43;Telcontar;Report6.java;https://issues.apache.org/jira/secure/attachment/12731096/Report6.java,07/May/15 06:43;Telcontar;Report7.java;https://issues.apache.org/jira/secure/attachment/12731097/Report7.java,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2014-04-14 13:03:29.073,,,false,,,,,,,,,,,,,,386502,,,Mon Jan 25 20:27:57 UTC 2016,,,,,,0|i1ukz3:,386766,,,,,,,,14/Apr/14 02:51;Telcontar;JUnit test to reproduce this issue.,"14/Apr/14 06:28;Telcontar;getTarget() instead of getWeight() also produces a NPE:

new org.apache.commons.math3.optim.nonlinear.vector.jacobian.LevenbergMarquardtOptimizer().getTarget();","14/Apr/14 13:03;erans;This is wrong usage; you cannot get something before your set it. ;-)
In this class, those data must be passed as argument to the ""optimize"" method; the requirements are documented in parent classes, where ""optimize"" is defined.
Anyway, this class, and the whole package, is going to be deprecated in release 3.3. The replacement is available in the development version.
","14/Apr/14 23:33;Telcontar;Thank you for the quick feedback.
There are a number of other classes that are not going to be deprecated in 3.3, which have the same issue. For example, there are five cases where I have found a one-liner that shows the problem:

    new org.apache.commons.math3.random.ValueServer().resetReplayFile();
    new org.apache.commons.math3.stat.regression.GLSMultipleLinearRegression().estimateErrorVariance();
    new org.apache.commons.math3.random.EmpiricalDistribution().getGeneratorUpperBounds();
    new org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression().calculateHat();
    new org.apache.commons.math3.stat.correlation.PearsonsCorrelation().getCorrelationStandardErrors();

I can post these in separate bug reports if there is interest, along with a few cases that are multiple lines.

I think it would be good to do something about these cases:

(1) Deprecate the constructors that build incomplete instances, or
(2) throw an IllegalStateException or a NullPointerException with an error message (""xy needs to be set""), or
(3) make the constructors non-public, or
(4) add a comment such as in NordSieckStepInterpolator:
http://commons.apache.org/proper/commons-math/javadocs/api-3.2/org/apache/commons/math3/ode/sampling/NordsieckStepInterpolator.html#NordsieckStepInterpolator%28%29

That comment clearly puts the blame on the user if a NullPointerException happens. However, it would be nice to have this also in the executable (as an exception message, perhaps).","15/Apr/14 09:40;erans;I don't think that it'necessary to open a new report if adding ""@throws"" tag would fix the issue.
","15/Apr/14 10:13;erans;For those:

{quote}
new org.apache.commons.math3.random.ValueServer().resetReplayFile();
new org.apache.commons.math3.random.EmpiricalDistribution().getGeneratorUpperBounds();
new org.apache.commons.math3.stat.regression.OLSMultipleLinearRegression().calculateHat();
{quote}

NullPointerException is advertized in revision 1587494.

bq. new org.apache.commons.math3.stat.correlation.PearsonsCorrelation().getCorrelationStandardErrors();

The Javadoc contained a warning for this one.

bq. new org.apache.commons.math3.stat.regression.GLSMultipleLinearRegression().estimateErrorVariance();

This method is defined in the parent class.

bq. I think it would be good to do something about these cases [...]

I agree. In fact I don't see the an advantage in constructing an incomplete instance and having to call afterwards what looks like a constructor (i.e. ""newSampleData"") that reallocates and reinitializes everything.
Perhaps, you could raise this design issue on the ""dev"" ML.
","16/Apr/14 06:47;Telcontar;Two more tests that show a similar issue; one is a one-liner:

    new org.apache.commons.math3.optim.nonlinear.scalar.noderiv.NelderMeadSimplex(new double[] {1.0d, 1.0d}).getPoint(1);","16/Apr/14 06:48;Telcontar;Thanks. I've added another attachment that shows two more similar cases, after removing deprecated classes and trying to simplify the tests. The attachment IllegalStateB.java appears above, along with the one-liner for the shorter of the two tests.","16/Apr/14 16:29;erans;Actually the whole contents of the ""optim"" package will be replaced with the approach that has been implemented in package ""o.a.c.m.fitting.leastsquares"".
","07/May/15 06:42;Telcontar;In general, the need to initialize newly constructed objects with more data is now documented, but we have found two cases where a NullPointerException is thrown because of missing data.

The documentation should be updated to reflect this. This is similar to issues report in MATH-1116 but concerns classes that are not going to be deprecated (as far as we can tell).

    org.apache.commons.math3.ode.nonstiff.HighamHall54Integrator var1 = new org.apache.commons.math3.ode.nonstiff.HighamHall54Integrator(0.0d, 0.0d, 0.0d, 0.0d);
    double[] var2 = new double[] { 0.0d };
    var1.computeDerivatives(0.0d, var2, var2); // NPE

    new org.apache.commons.math3.stat.correlation.SpearmansCorrelation().getCorrelationMatrix(); // NPE",07/May/15 06:43;Telcontar;Self-contained unit test to reproduce NullPointerException on HighamHall54Integrator (the need to initialize it further is not documented yet).,07/May/15 06:43;Telcontar;Self-contained unit test to reproduce NullPointerException on SpearmansCorrelation (the need to initialize it further is not documented yet).,"14/May/15 02:14;Telcontar;This particular issue (SpearmansCorrelation) has been fixed upstream, see bug 1224: https://issues.apache.org/jira/browse/MATH-1224",19/May/15 11:47;luc;Fixed in git repository for the remaining part (ODE package).,25/Jan/16 20:27;luc;Closing all resolved issues that were included in 3.6 release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Constructor of PolyhedronsSet throws NullPointerException,MATH-1115,12708178,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,Telcontar,Telcontar,14/Apr/14 02:38,19/May/14 15:13,07/Apr/19 20:38,26/Apr/14 17:38,3.2,,,,,,,3.3,,,0,,,,,,,,"The following statement throws a NullPointerException:
new org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);

I found that other numbers also produce that effect. The stack trace:
java.lang.NullPointerException
        at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)
        at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)
        at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)
        at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)
        at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)","Mac OS 10.9 with Java 6, 7",,,,,,,,,,,,,,,,,,,,14/Apr/14 02:40;Telcontar;Report1.java;https://issues.apache.org/jira/secure/attachment/12640009/Report1.java,14/Apr/14 08:01;Telcontar;Report1a.java;https://issues.apache.org/jira/secure/attachment/12640037/Report1a.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2014-04-26 17:38:27.688,,,false,,,,,,,,,,,,,,386501,,,Mon May 19 15:13:32 UTC 2014,,,,,,0|i1ukyv:,386765,,,,,,,,14/Apr/14 02:40;Telcontar;JUnit test to reproduce problem.,14/Apr/14 08:01;Telcontar;PolygonsSet (the 2D case) shows the same issue.,"26/Apr/14 17:38;luc;Fixed in subversion repository as of r1590254.

We have also included the same javadoc improvements as in MATH-1117.

Thanks for the report.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spelling mistake in org.apache.commons.math3.fitting ,MATH-1111,12702431,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,aurellem,aurellem,19/Mar/14 17:38,19/Mar/14 18:42,07/Apr/19 20:38,19/Mar/14 18:34,3.2,,,,,,,3.3,,,0,,,,,,,,"in the paragraph containing : 
""should pass through sample points, and were the objective function is the"" 

at http://commons.apache.org/proper/commons-math/javadocs/api-3.2/org/apache/commons/math3/fitting/package-summary.html

""were"" should be ""where""",,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-03-19 18:34:36.13,,,false,,,,,,,,,,,,,,380770,,,Wed Mar 19 18:42:48 UTC 2014,,,,,,0|i1tlsf:,381049,,,,,,,,"19/Mar/14 18:34;erans;Revision 1579343.
",19/Mar/14 18:42;aurellem;:),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OLSMultipleLinearRegression needs a way to specify non-zero singularity threshold when instantiating QRDecomposition,MATH-1110,12701620,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ejs,ejs,14/Mar/14 22:27,19/May/14 15:13,07/Apr/19 20:38,01/May/14 11:54,3.2,,,,,,,3.3,,,0,,,,,,,,"OLSMultipleLinearRegression uses QRDecomposition to perform a least-squares solution. QRDecomposition has the capability to use a non-zero threshold for detecting when the design matrix is singular (see https://issues.apache.org/jira/browse/MATH-665, https://issues.apache.org/jira/browse/MATH-1024, https://issues.apache.org/jira/browse/MATH-1100, https://issues.apache.org/jira/browse/MATH-1101) but OLSMultipleLinearRegression does not use this capability and therefore always uses the default singularity test threshold of 0. This can lead to bad solutions (see in particular https://issues.apache.org/jira/browse/MATH-1101?focusedCommentId=13909750&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13909750) when a SingularMatrixException should instead be thrown. 

When I encountered this situation, I noticed it because the solution values were extremely large (in the range 1e09 - 1e12). Normal values in the domain I am working with are on the order of 1e-3. To find out why the values are so large, I traced through the source and found that an rDiag value was on the order of 1e-15, and that this passed the threshold test. I then noticed that two columns of the design matrix are linearly dependent (one column is all 1's because I want an intercept value in the solution, and another is also all 1's because that's how the data worked out). Thus the matrix is definitely singular. 

If I could specify a non-zero threshold, this situation would result in  a SingularMatrixException, but without that, the bad solution values would be blindly propagated. That is a problem because this solution is intended for controlling a physical system, and damage could result from a bad solution. 

Unfortunately, I see no way to change the threshold value from outside -- I would have to in effect re-implement OLSMultipleLinearRegression to do this as a user of the package. ","Windows 7, jdk1.6.0_45",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-03-15 19:12:43.214,,,false,,,,,,,,,,,,,,379966,,,Mon May 19 15:13:31 UTC 2014,,,,,,0|i1tgw7:,380250,,,,,,,,15/Mar/14 19:12;psteitz;Agreed this should be fixed.  The default should not be 0 and it should be configurable.  Patches welcome.,"01/May/14 11:54;tn;In r1591624, added a new constructor to be able to specify a custom singularity threshold.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brent optimizer returns sub-optimal results at times,MATH-1109,12701032,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Incomplete,,ajo.fod,ajo.fod,12/Mar/14 17:39,05/Oct/14 18:50,07/Apr/19 20:38,05/Oct/14 18:50,,,,,,,,,,,0,,,,,,,,"There are cases where the BrentOptimizer returns an ""optimum"" that is worse than the edges. Is this avoidable?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-03-19 18:53:36.542,,,false,,,,,,,,,,,,,,379378,,,Sun Oct 05 18:50:19 UTC 2014,,,,,,0|i1tdb3:,379669,,,,,,,,"19/Mar/14 18:53;erans;Could you provide a (Junit) test case that shows the failure?
",05/Oct/14 18:50;psteitz;Please reopen if / when a test case or more details on failure are available.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CMAESOptimizer fails sometimes when bounds are violated,MATH-1107,12698249,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,bjohnson,bjohnson,02/Mar/14 20:53,19/May/14 15:13,07/Apr/19 20:38,03/Mar/14 10:00,,,,,,,,3.3,,,0,,,,,,,,"The CMAESOptimizer repairs points that are out of bounds by moving them into bounds, and adding a penalty based on how far they were moved.

The penalty added is scaled by the range of values in the current population.

The calculation of the valueRange, however, includes the penalty so at each iteration the amount of penalty can grow multiplicatively.  One solution, is to keep the value and penalties separate before calculating the scale factor for the penalties.  A patch that does this will be attached.
",,,,,,,,,,,,,,,,,,,,,02/Mar/14 20:58;bjohnson;cmaes_penalty.patch;https://issues.apache.org/jira/secure/attachment/12632075/cmaes_penalty.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-03 10:00:00.939,,,false,,,,,,,,,,,,,,376678,,,Mon May 19 15:13:31 UTC 2014,,,,,,0|i1swpr:,376973,,,,,,,,"03/Mar/14 10:00;luc;Patch applied (with minor changes) as of r1573506.

Thanks for the report and for the patch!",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LevenburgMaquardt switched evaluation and iterations,MATH-1106,12698030,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,evanward1,evanward1,28/Feb/14 21:40,19/May/14 15:13,07/Apr/19 20:38,02/Mar/14 14:04,,,,,,,,3.3,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Feb/14 21:40;evanward1;0001-Fix-switched-iterations-and-evaluations.patch;https://issues.apache.org/jira/secure/attachment/12631847/0001-Fix-switched-iterations-and-evaluations.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-03-02 14:04:48.281,,,false,,,,,,,,,,,,,,376504,,,Mon May 19 15:13:27 UTC 2014,,,,,,0|i1svnb:,376800,,,,,,,,"02/Mar/14 14:04;luc;Patch applied as of r1573308.

Thanks for the report and for the patch!",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Convergence Checker Fixes,MATH-1103,12697239,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,evanward1,evanward1,25/Feb/14 22:47,19/May/14 15:13,07/Apr/19 20:38,02/Mar/14 14:04,,,,,,,,3.3,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,28/Feb/14 21:57;evanward1;0001-Fix-checker-seeing-not-old-point.patch;https://issues.apache.org/jira/secure/attachment/12631852/0001-Fix-checker-seeing-not-old-point.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-25 23:30:57.038,,,false,,,,,,,,,,,,,,375713,,,Mon May 19 15:13:34 UTC 2014,,,,,,0|i1sqrz:,376009,,,,,,,,"25/Feb/14 23:30;erans;Please create different tickets for different problems.
The second patch seems to introduce a dependency.

The first patch shows how brittle code can become without defensive copying.

{noformat}
-        Evaluation current = problem.evaluate(new ArrayRealVector(currentPoint, false));
+        Evaluation current = problem.evaluate(new ArrayRealVector(currentPoint));
{noformat}

Although the second may be correct, it is no less dangerous because nothing at this level warns the developer that a new instance is required for correctness. The ""evaluate"" method should be creating a copy of an object that will be used outside the class (i.e. without control).
It is fine to be concerned about performance but IMO robustness must come before premature optimization.
","27/Feb/14 15:00;luc;I don't think the call without the boolean parameter is as dangerous as the first one. In the first case, the developer explicitly states he want to reuse the array, and it was an error. In the second case, no assumption is made and the safe option (copying) is used.

Concerning the second patch, there is no added dependency, org.hamcrest is embedded into Junit.

So I think we could apply the patches, is this OK for you?","27/Feb/14 15:16;erans;bq. [...] is this OK for you?

I don't think so.
The concern I state is that the implementation of the optimization algorithm should not have to figure out whether a copy must be made or not.
The default implementation of the ""LeastSquareProblem"" (i.e. ""LocalLeastSquaresProblem"" in ""LeastSquaresFactory"") should use a ""LeastSquareProblem.Evaluation"" that makes a defensive copy.
","27/Feb/14 15:23;evanward1;Gilles solution is o.k. by me. Let's add some documentation to the interface since I think users will be implementing both Optimizer and Problem.

I put the patches in the same issue because the tests in #1 still fail for LM due to the bug fixed in #2.

Fixing #1 here is making me rethink the Jacobian copying issues. Maybe it is best to have a copy of the Jacobian for each Evaluation so that there aren't stale data issues. (Slower and safer) I think we could still apply diagonal weights without copying the Jacobian matrix.","27/Feb/14 15:41;luc;OK, I'll wait for the next patch.",28/Feb/14 21:41;evanward1;created MATH-1106 for the second patch,28/Feb/14 21:57;evanward1;Now we copy on both sides of the LSP.evaluate() call.,"02/Mar/14 14:04;luc;Patch applied as of r1573307.

Thanks for the report and for the patch!",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QR and Rank-revealing QR fail to find a least-squares solution,MATH-1101,12696499,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,roman.werpachowski,roman.werpachowski,21/Feb/14 11:39,19/May/14 15:13,07/Apr/19 20:38,23/Feb/14 11:15,3.2,,,,,,,3.3,,,0,solver,,,,,,,"QR and RRQR (rank-revealing) algorithms fail to find a least-squares solution in some cases.

The following code:

final RealMatrix A = new BlockRealMatrix(3, 3);
        A.setEntry(0, 0, 1);
        A.setEntry(0, 1, 6);
        A.setEntry(0, 2, 4);
        A.setEntry(1, 0, 2);
        A.setEntry(1, 1, 4);
        A.setEntry(1, 2, -1);
        A.setEntry(2, 0, -1);
        A.setEntry(2, 1, 2);
        A.setEntry(2, 2, 5);
        final RealVector b = new ArrayRealVector(new double[]{5, 6, 1});
        final QRDecomposition qrDecomposition = new QRDecomposition(A);
        final RRQRDecomposition rrqrDecomposition = new RRQRDecomposition(A);
        final SingularValueDecomposition svd = new SingularValueDecomposition(A);
        final RealVector xQR = qrDecomposition.getSolver().solve(b);
        System.out.printf(""QR solution: %s\n"", xQR.toString());
        final RealVector xRRQR = rrqrDecomposition.getSolver().solve(b);
        System.out.printf(""RRSQ solution: %s\n"", xRRQR.toString());
        final RealVector xSVD = svd.getSolver().solve(b);
        System.out.printf(""SVD solution: %s\n"", xSVD.toString());

produces

QR solution: {-3,575,212,378,628,897; 1,462,586,882,166,368; -1,300,077,228,592,326.5}
RRSQ solution: {5,200,308,914,369,308; -2,127,399,101,332,898; 1,891,021,423,407,021}
SVD solution: {0.5050344462; 1.0206677266; -0.2405935347}

Showing that QR and RRQR algorithms fail to find the least-squares solution. This can also be verified by calculating the dot product between columns of A and A*x - b:

// x = xQR, xRRQR or xSVD
final RealVector r = A.operate(x).subtract(b);
        for (int i = 0; i < x.getDimension(); ++i) {
            final RealVector columnVector = A.getColumnVector(i);
            assertEquals(name, 0.0, r.dotProduct(columnVector), tolerance);
        }

Only SVD method passes this test with decent tolerance (1E-14 or so).",,,,,,,,,,,,,,,,,,,,,21/Feb/14 11:41;roman.werpachowski;math-1101-bug.java;https://issues.apache.org/jira/secure/attachment/12630290/math-1101-bug.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-21 13:50:56.408,,,false,,,,,,,,,,,,,,374975,,,Mon May 19 15:13:24 UTC 2014,,,,,,0|i1sm8v:,375274,,,,,,,,21/Feb/14 11:41;roman.werpachowski;Attached test code for convenience.,"21/Feb/14 13:50;luc;I am not sure to understand. The matrix is exactly singular here, which is correctly identified if you pass a threshold of about 2e-16 to the QRDecomposition constructor (without passing it, the default threshold is an exact 0). With the default 0 threshold, the last diagonal element is really small (8.88e-16) and using it implies computing big values.

As all dimensions are 3, I don't understand were you intend to have a least squares solution. What is attempted here seems to be computing a full linear solution of a singular problem.","21/Feb/14 23:19;roman.werpachowski;In this case the description of QRDecomposition.getSolver() is misleading, since it says:

""Get a solver for finding the A × X = B solution in least square sense."" (same for RRQRDecomposition).

Also the documentation in http://commons.apache.org/proper/commons-math/userguide/linear.html says that the QR decomposition method can handle any matrix and return a least squares solution.

If the QRDecomposition works better with non-zero threshold, then it may be worth considering changing the default threshold value from zero, since the current default behaviour is not what the user might expect based on the documentation.

Finally, it is always possible to have a least squares solution of || A * x - b ||^2, because the norm is bounded from below by zero and diverges to infinity as the norm of x goes to infinity. The solution may not be unique though. Also note that the SVD solver returns a much more sensible solution, which minimizes || A * x - b ||^2.","23/Feb/14 11:15;luc;Documentation has been improved as of r1570994, both in the javadoc and in the user guide.

Concerning the default threshold to 0, the decision to keep this value was taken when discussing issue MATH-664.

Our implementation of QR decomposition does not try to circumvent singular matrices by itself. I am not sure about this feature as it may fool the user into thinking everything is OK when in fact the matrix was singular and an approximate solution was found when the user expected an exact one.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QR factorization fails in revealing rank-deficient matrix,MATH-1100,12696286,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,albert_triv,albert_triv,20/Feb/14 16:06,19/May/14 15:13,07/Apr/19 20:38,21/Feb/14 10:03,3.2,,,,,,,3.3,,,0,,,,,,,,"given a matrix that has not full rank, the method getSolver().isNonSingular() of the class QRDecomposition returns true.","Java HotSpot(TM) Client VM (build 19.1-b02, mixed mode, sharing) an windows 7 professional",,,,,,,,,,,,,,,,,,,,20/Feb/14 16:08;albert_triv;QRtest.zip;https://issues.apache.org/jira/secure/attachment/12630083/QRtest.zip,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-20 16:51:58.953,,,false,,,,,,,,,,,,,,374762,,,Mon May 19 15:13:20 UTC 2014,,,,,,0|i1skxr:,375062,,,,,,,,20/Feb/14 16:08;albert_triv;the matrix and the related JUnit test,"20/Feb/14 16:51;luc;In your code, you don't set the singularity threshold as you call new QRDecomposition(M2) with only the matrix as an argument.
This ends up with using an exact 0 as the threshold.

If you use new QRDecomposition(M2, 2.2e-14), the matrix is corrctly identified as singular. There seem to be two very small values after decomposition on the diagonal of the triangula matrix. One is about 2.05e-14 the other is about 2.2e-14, so depending on the threshold, you should get a rank of 379, 380, or 381.

Do you agree with this analysis?","20/Feb/14 17:07;albert_triv;Thank Luc for your fast reply.
Given the dimension of the matrix, there is a high probability of a loss 
of precision in java double calculations, and so that small values 
appear in the diagonal of the triangular factor.
I agree with your analysis and I will try to use the threshold.
Thank you again,
best regards


-- 
Alberto Trivellato

","21/Feb/14 10:03;luc;As per comments above, specifying the threshold allows to correctly identify the rank.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
implementation of smallest enclosing ball algorithm sometime fails,MATH-1096,12692129,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Blocker,Fixed,luc,luc,luc,29/Jan/14 20:28,02/Feb/14 20:55,07/Apr/19 20:38,02/Feb/14 20:54,3.3,,,,,,,,,,0,,,,,,,,"The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.

In some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.

Some very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are

* WelzlEncloser2DTest.testReducingBall
* WelzlEncloser2DTest.testLargeSamples
* WelzlEncloser3DTest.testInfiniteLoop
* WelzlEncloser3DTest.testLargeSamples",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,370720,,,Sun Feb 02 20:54:58 UTC 2014,,,,,,0|i1rw67:,371031,,,,,,,,02/Feb/14 20:54;luc;Fixed in subversion repository as of r1563712.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect linear system solution,MATH-1094,12691235,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,bvk256,bvk256,25/Jan/14 23:00,26/Jan/14 19:42,07/Apr/19 20:38,26/Jan/14 19:42,3.2,,,,,,,,,,0,patch,,,,,,,"Firstly I would like to point out that I'm not very proficient in linear algebra, but in my opinion the following behavior should not occur. When I solve the following linear system by hand it has no solution, but in commons math using QRDecomposition it outputs  x1= 7.5 x2 = 5.5","Oracle JDK7
Linux x86-64",,,,,,,,,,,,,,,,,,,,25/Jan/14 23:03;bvk256;Main.java;https://issues.apache.org/jira/secure/attachment/12625226/Main.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-26 11:35:44.623,,,false,,,,,,,,,,,,,,369980,,,Sun Jan 26 11:35:44 UTC 2014,,,,,,0|i1rrmn:,370282,,,,,,,,"26/Jan/14 11:35;luc;I think the behaviour is correct.

The getSolver method for QR decomposition provides a solver for A X = B in the least squares sense.

This means the X comuted by the solver is minimizes || A X - B ||, it does not find 0, only a minumum norm.
In your case, the system is rectangular, so there is no way a true solution can be achieved.

I have verified (using simple loops and gnuplot countour plots) that indeed the x = 7.5, y = 5.5 does minimizes the norm, which is 39.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
arcs set split covers full circle instead of being empty,MATH-1093,12691036,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,luc,luc,24/Jan/14 14:11,19/May/14 15:13,07/Apr/19 20:38,24/Jan/14 16:04,3.3,,,,,,,3.3,,,0,,,,,,,,"When splitting an arcs set using an arc very close to one of the boundaries (but not at the boundary), the algorithm confuses cases for which end - start = 2pi from cases for which end - start = epsilon.

The following test case shows such a failure:
{code}
    @Test
    public void testSplitWithinEpsilon() {
        double epsilon = 1.0e-10;
        double a = 6.25;
        double b = a - 0.5 * epsilon;
        ArcsSet set = new ArcsSet(a - 1, a, epsilon);
        Arc arc = new Arc(b, b + FastMath.PI, epsilon);
        ArcsSet.Split split = set.split(arc);
        Assert.assertEquals(set.getSize(), split.getPlus().getSize(),  epsilon);
        Assert.assertNull(split.getMinus());
    }
{code}

The last assertion (split.getMinus() being null) fails, as with current code split.getMinus() covers the full circle from 0 to 2pi.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,369779,,,Mon May 19 15:13:31 UTC 2014,,,,,,0|i1rqe7:,370081,,,,,,,,24/Jan/14 16:04;luc;Fixed in subversion repository as of r1561047.,19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NonLinearConjugateGradientOptimizer's Line search is a gradient search returns obviously suboptimal point.,MATH-1092,12690284,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ajo.fod,ajo.fod,22/Jan/14 14:50,19/May/14 15:13,07/Apr/19 20:38,30/Apr/14 22:18,,,,,,,,3.3,,,0,,,,,,,,"In package : org.apache.commons.math3.optim.nonlinear.scalar.gradient

In a minimization problem, a line search should not return a point where the value is greater than the values at the edges of the interval. The line search violates this obvious requirement by focusing solely on solving for gradient=0 and ignoring the value.

Moreover LineSearchFunction is something that can be used in other contexts, so perhaps this should be a standalone class.



",,,,,,,,,,,,,,,,,,,,,27/Feb/14 16:08;erans;MATH-1092.patch;https://issues.apache.org/jira/secure/attachment/12631553/MATH-1092.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-02-27 15:37:56.816,,,false,,,,,,,,,,,,,,369237,,,Mon May 19 15:13:26 UTC 2014,,,,,,0|i1rn33:,369542,,,,,,,,"27/Feb/14 15:37;erans;Here is a proposal to factor out the ""LineSearch"" defined in ""PowellOptimizer""
(which uses ""BrentOptimizer"").
When replacing what needed be in ""NonlinearConjugateGradientOptimizer"", a series of unit test started to fail.
I've performed some checks that the convergence criterion is indeed met even though the result found is now farther from the expected optimum.

The change also uncovered that by using the root solver, the optimizer could not correctly count the evaluations (cf. ""MultiStartMultivariateOptimizerTest"").
",27/Feb/14 15:49;luc;LineSearch seems to be missing in the patch.,27/Feb/14 16:08;erans;Oops...,"27/Feb/14 16:29;luc;This seems good to me.

The missing function evaluations count is probably due to the GradientMultivariateOptimizer.computeObjectiveGradient method,
which lacks a call to incrementEvaluationCount() which is used for example in MultivariateOptimizer.computeObjectiveValue.

I don't remember if this was intentional or not. Both computeObjectiveValue and computeObjectiveGradient are called in NonLinearConjugateGradientOptimizer.doOptimize, and computeObjectiveGradient was also called (and was the only function called)
for the initial gradient-based line search. So we should probably also call incrementEvaluationCount for computeObjectiveGradient, and
this will probably increase even more the total number of calls.","27/Feb/14 16:49;erans;bq. The missing function evaluations count is probably due to the GradientMultivariateOptimizer.computeObjectiveGradient method, [...]

Oh, yes, you are right; I overlooked the ""Gradient"" part of the name...

bq.  I don't remember if this was intentional or not.

I recall that we indeed decided not to count the calls to that method.

Anyway the ""computeObjectiveGradient"" is not called anymore in the new line search.
Is that OK?  I mean: Can the class still be considered a ""standard"" implementation of the algorithm?
","27/Feb/14 17:28;bjohnson;There is a good discussion in Numerical Recipes on using gradients in a line minimization (the conclusion being basically to use them very judiciously, for example to figure out what side of the interval to search in).  Since we have gradients available there might be some advantage in optionally using them in the line minimization).  For now, I'm quite happy that with your fix the line minimizer should never return a point with a higher value than the starting point (assuming we're doing minimization rather than maximization).

","28/Feb/14 16:45;erans;Code updated in revision 1572988.
Could please check that it fixes your issue?",28/Feb/14 16:58;bjohnson;I've downloaded 3.3 snapshot with svn and switched my code to use it.  Right now there seems to be a change elsewhere in 3.3 that is causing a problem before I get to my use of NonLinearConjugateGradientOptimizer.  Trying to figure out now what the problem is.,"01/Mar/14 22:08;bjohnson;The new line search works, but I think it needs one change.  The old version used a BracketingStep parameter to set the initial search size.  That is not used in the new LineSearch code, which has a hard-coded an upper limit of 1:
        bracket.search(f, goal, 0, 1);

For my optimization problem I find (with both the old and new code) I need a much smaller upper limit (1.0e-5 for example).  Otherwise the minimum found isn't as good.  So I think it would be important to pass the BracketingStep parameter into the new new LineSearch code.","02/Mar/14 19:39;erans;Parameter added (revision 1573316).

However, I'm worried that some unit tests for ""PowellOptimizer"" fail when this parameter is set to a small value (1e-8). Thus, for ""PowellOptimizer"", I've hard-coded the value to 1 so that the previous behaviour is retained.
Maybe there is an interpendence between this value and the other parameters of ""BracketFinder"".

In your problem, are there oscillations in the vicinity of the optimum?
It is difficult for me to figure out to the appropriate improvements without a unit test.
","02/Mar/14 21:11;bjohnson;Couldn't we have another constructor that takes a fourth argument (the initial upper limit for the bracket search).  The current constructor could default to 1.0 for that value so existing tests would pass.

The problem I'm working on is the energy minimization of a complex molecule.  I'm sure there are many local minima and if the bracket is too large it finds a minimum that is not as good as one near the origin of the search.  So it seems essential to be able to specify the size of the search region. (As an aside, I use the CMAESOptimizer to initially search in this complex landscape, but (when I can use gradients) the ConjugateGradientOptimizer is useful to quickly refine the solution. ",02/Mar/14 21:14;bjohnson;I'm referring to the constructor for the new LineSearch class.,"02/Mar/14 22:42;erans;bq. Couldn't we have another constructor [...] 

Why another constructor?
""PowellOptimizer"" does use 1 for the parameter.

We should have units tests for ""LineSearch"". It could help to understand what is going on as the initial step gets smaller.
","03/Mar/14 00:13;bjohnson;Another constructor so one could do:

        line = new LineSearch(this, relativeTolerance,  absoluteTolerance,initialStep);

where initialStep is the value set with BracketingStep in NonLinearConjugateGradientOptimizer

I presume the BracketingStep parameter was added for a reason to NonLinearConjugateGradientOptimizer, but we've lost this capability in the new code.  Setting a smaller initial step (than 1.0) is clearly necessary for actual use of my application, but I'm not sure I can easily represent this need in a unit test case.
","03/Mar/14 02:45;erans;bq. line = new LineSearch(this, relativeTolerance, absoluteTolerance,initialStep);

But this is indeed what I did in revision 1573316 (as advertized a couple comments above)...
","03/Mar/14 02:54;bjohnson;Aaaah, I missed that you actually added this.  Thanks!.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Precision.round() returns different results when provided negative zero as double or float,MATH-1089,12689364,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tn,tn,16/Jan/14 21:44,19/May/14 15:13,07/Apr/19 20:38,16/Jan/14 22:11,,,,,,,,3.3,,,0,,,,,,,,"Precision.round(-0.0d, x) = 0.0
Precision.round(-0.0f, x) = -0.0

After discussion on the mailinglist, the result should always be -0.0.",,,,,,,,,,,,,,,,,,,,,17/Jan/14 10:37;erans;MATH-1089.patch;https://issues.apache.org/jira/secure/attachment/12623624/MATH-1089.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-01-17 10:37:51.904,,,false,,,,,,,,,,,,,,368331,,,Mon May 19 15:13:32 UTC 2014,,,,,,0|i1rhi7:,368636,,,,,,,,"16/Jan/14 21:53;tn;This not only affects negative zero, but all negative values that would be rounded to 0.
In these cases the result should be negative zero to be consistent with the result of Precision.round(float, x).",16/Jan/14 22:11;tn;Fixed in r1558933.,"17/Jan/14 10:37;erans;I propose the attached patch; overall it is probably faster, though the absolute speed difference will be marginal in most cases.
","17/Jan/14 10:45;tn;Yes, this solution is cleaner, feel free to already commit the patch.","17/Jan/14 10:53;erans;Committed in revision 1559067.
",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MultidimensionalCounter does not throw ""NoSuchElementException""",MATH-1088,12689265,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,erans,erans,erans,16/Jan/14 15:21,19/May/14 15:13,07/Apr/19 20:38,16/Jan/14 15:27,3.2,,,,,,,3.3,,,0,,,,,,,,"The iterator should throw when ""next()"" is called even though ""hasNext()"" would return false.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-05-19 15:13:27.172,,,false,,,,,,,,,,,,,,368232,,,Mon May 19 15:13:27 UTC 2014,,,,,,0|i1rgw7:,368537,,,,,,,,16/Jan/14 15:27;erans;Revision 1558833.,19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect results from BinomialConfidenceInterval#getWilsonScoreInterval,MATH-1086,12688265,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,,khamenka,khamenka,10/Jan/14 15:48,19/May/14 15:13,07/Apr/19 20:38,11/Jan/14 07:59,3.3,,,,,,,3.3,,,0,,,,,,,,"It looks like BinomialConfidenceInterval#getWilsonScoreInterval produces not accurate results for interval lower bound.
E.g. for input (10,9,0.95) it returns 0.818 instead of 0.596; 
       for input (10,10,0.95) it returns 0.856 instead of 0.722.

Used also http://epitools.ausvet.com.au/content.php?page=CIProportion&SampleSize=10&Positive=9&Conf=0.95&Digits=3 and http://www.measuringusability.com/wald.htm to verify it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-01-10 20:18:04.445,,,false,,,,,,,,,,,,,,367284,,,Mon May 19 15:13:32 UTC 2014,,,,,,0|i1rb4v:,367593,,,,,,,,"10/Jan/14 20:18;tn;Running the following code on the latest trunk

{noformat}
        BinomialConfidenceInterval interval = new BinomialConfidenceInterval();
        
        ConfidenceInterval i1 = interval.getWilsonScoreInterval(10,9,0.95);
        System.out.println(i1.getLowerBound());
        
        ConfidenceInterval i2 = interval.getWilsonScoreInterval(10,10,0.95);
        System.out.println(i2.getLowerBound());
{noformat}

produces:

{noformat}
0.5958499732047616
0.7224672001371106
{noformat}

which looks like the correct result. Can you verify this?

btw. I wonder why the getXXXXInterval methods in BinomialConfidenceInterval are not static.","10/Jan/14 20:57;psteitz;Thomas - your comment about static methods applies to many methods in the stat package.  This class is just following the pattern of the others in stat.inference.  We decided some years back to favor non-static implementation methods when we had more strategy patterns and focus on extensibility.  We have moved more in the direction of single implementations and less concern for extensibility since, so it may make sense to revisit those decisions.  Personally, I still favor non-static implementation methods though (by ""implementation methods"" I mean methods that actually implement mathematical algorithms, as opposed to convenience methods like the ones in StatUtils), but am open to change if others feel strongly about it.","10/Jan/14 21:09;tn;In general I am not against this practice, but in the case of the BinomialConfidenceInterval I was really wondering if it is not better to make them static:

 * the class itself has no internal state
 * it does not implement any interface
 * is not supposed to be sub-classed
 * javadoc mentions factory methods (which actually got me started)

For the various tests there is a TestUtils class, which means a user does not have to create an instance of the respective test, but can directly use the static methods there. For the BinomialConfidenceInterval one would have to instantiate an object first, which is at least inconvenient imho.","10/Jan/14 22:00;psteitz;All good points, Thomas; though one could quibble with the first and third bullets - i.e., you never know when something might be extended and that was in fact the reason for the old ""policy"" to keep implementation methods non-static.  It is a weak argument in this case, though; as the impls are really trivial and supposed to be definitive.  Sorry I missed the ""factory methods"" reference in the javadoc.  That should be removed.  I am fine changing the methods to static in this case.","10/Jan/14 22:29;tn;Actually, the BinomialConfidenceInterval class would also be suited for sub-classing, as all the getXXXInterval methods have the same signature. The interface would be like this

{noformat}
public interface BinomialConfidenceInterval {
   ConfidenceInterval createInterval(int numberOfTrials, int numberOfSuccesses, double confidenceLevel);
}
{noformat}

And there would be an implementation for each method:

 * Wilson
 * NormalApproximation
 * ...

Maybe an overkill, but at least very flexible ;-)","11/Jan/14 07:59;khamenka;Sorry, I compared values calculated with confidence interval 0.5 and 0.95. So obviously they were different. ","11/Jan/14 19:47;psteitz;Thomas - agree that is a nice design.  I would be +1 for this change, but it would be nice to then expose static methods somewhere to just get the intervals.  Not sure what the best place for these would be.  Maybe best to take this to commons-dev.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Existing cutOff mechanism in SimplexSolver can lead to wrong solutions,MATH-1082,12685878,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tn,tn,20/Dec/13 19:26,19/May/14 15:13,07/Apr/19 20:38,20/Dec/13 19:33,,,,,,,,3.3,,,0,,,,,,,,"The cutOff mechanism introduced in MATH-828 does not work in call cases correctly.

Tests with the example from netlib have shown that sometimes an invalid solution is returned because of this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-05-19 15:13:25.771,,,false,,,,,,,,,,,,,,366242,,,Mon May 19 15:13:25 UTC 2014,,,,,,0|i1r4pr:,366553,,,,,,,,"20/Dec/13 19:33;tn;Fixed in r1552792.

Plan to include several unit tests when mps loader is finished.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The LinearConstraintSet shall return its constraints in a deterministic way,MATH-1080,12684973,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,tn,tn,16/Dec/13 15:08,19/May/14 15:13,07/Apr/19 20:38,16/Dec/13 21:05,,,,,,,,3.3,,,0,,,,,,,,"As previously discussed on the mailinglist, the LinearConstraintSet should return its internally stored LinearConstraints in the same iteration order as they have been provided via its constructor.

This ensures that the execution of the same linear problem results in the same results each time it is executed. This is especially important when linear problems are loaded from a file, e.g. mps format, and makes it simpler to debug problems and compare with other solvers which do the same thing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-05-19 15:13:21.129,,,false,,,,,,,,,,,,,,364050,,,Mon May 19 15:13:21 UTC 2014,,,,,,0|i1qr13:,364350,,,,,,,,16/Dec/13 21:05;tn;Changed in r1551355.,19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect rounding of float,MATH-1070,12682369,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,lvovlink,lvovlink,03/Dec/13 12:31,19/May/14 15:13,07/Apr/19 20:38,09/Jan/14 13:18,3.2,,,,,,,3.3,,,0,,,,,,,,"package org.apache.commons.math3.util 
example of usage of round functions of Precision class:

Precision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01
Precision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01
Precision.round((float) 0.0, 2) = 0.0
Precision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0

Seems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.

I think, same problem will be found at usage of other round modes.
","Windows 7, IntelliJ IDEA 10.5.4.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-12-03 23:51:10.299,,,false,,,,,,,,,,,,,,361626,,,Mon May 19 15:13:34 UTC 2014,,,,,,0|i1qc5j:,361924,,,,,,,,"03/Dec/13 23:51;tn;Fixed in r1547649.

According to BigDecimal.ROUND_UP, the value shall only be rounded up if the discarded fraction is non-zero.","09/Jan/14 11:53;lvovlink;Still left problem with round of zero value for float usage:
Precision.round(-0.0d, 0) = 0.0
Precision.round(-0.0f, 0) = -0.0
","09/Jan/14 13:18;tn;Could you please create a different issue for this problem.

Calling Precision.round(x, y) will use the rounding mode ROUND_HALF_UP thus is it different to the problem reported before.","09/Jan/14 13:31;tn;The problem you outline is quite interesting. In the case of Precision.round(double, scale), the double value gets internally converted to a BigDecimal, but BigDecimal loses the sign if the value is 0. This is even documented in the javadoc: http://docs.oracle.com/javase/7/docs/api/java/math/BigDecimal.html#BigDecimal%28java.lang.String%29","16/Jan/14 22:46;tn;Created issue MATH-1089 for the problem with negative zero, which has already been fixed.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KendallsCorrelation suffers from integer overflow for large arrays.,MATH-1068,12681992,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,Terran-Ghost,Terran-Ghost,01/Dec/13 11:48,19/May/14 15:13,07/Apr/19 20:38,07/Dec/13 13:08,3.3,,,,,,,3.3,,,0,newbie,,,,,,,"For large array size (say, over 5,000), numPairs > 10 million.
in line 258, (numPairs - tiedXPairs) * (numPairs - tiedYPairs) possibly > 100 billion, which will cause an integer overflow, resulting in a negative number, which will result in the end result in a NaN since the square-root of that number is calculated.
This can easily be solved by changing line 163 to
final long numPairs = ((long)n) * (n - 1) / 2; // to avoid overflow",,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-12-01 19:17:36.014,,,false,,,,,,,,,,,,,,361251,,,Mon May 19 15:13:23 UTC 2014,,,,,,0|i1q9u7:,361550,,,,,,,,"01/Dec/13 19:17;tn;Fixed in r1546840.

Thanks for the report!","05/Dec/13 20:37;Terran-Ghost;I've noticed a few more overflow issues for very large arrays (100k's), e.g., correlation < -1 or > 1.
Changing all the tiedX/Y/XYPairs, consecutiveX/Y/XYTies and swaps to long fixed the correlation out of bounds error at least.","05/Dec/13 21:33;tn;Can you please attach a test case for this?

Thanks",07/Dec/13 13:08;tn;Changed in r1548907.,19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stack overflow in Beta.regularizedBeta,MATH-1067,12681737,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,florian.erhard,florian.erhard,28/Nov/13 10:47,19/May/14 15:13,07/Apr/19 20:38,28/Nov/13 11:42,3.2,,,,,,,3.3,,,0,easyfix,performance,,,,,,"In org.apache.commons.math3.special.Beta.regularizedBeta(double,double,double,double,int), the case

 } else if (x > (a + 1.0) / (a + b + 2.0)) {
      ret = 1.0 - regularizedBeta(1.0 - x, b, a, epsilon, maxIterations);
} 

is prone to infinite recursion: If x is approximately the tested value, then 1-x is approximately the tested value in the recursion. Thus, due to loss of precision after the subtraction, this condition can be true for the recursive call as well.

Example:
double x= Double.longBitsToDouble(4597303555101269224L);
double a= Double.longBitsToDouble(4634227472812299606L);
double b = Double.longBitsToDouble(4642050131540049920L);
System.out.println(x > (a + 1.0) / (a + b + 2.0));
System.out.println(1-x>(b + 1.0) / (b + a + 2.0));
System.out.println(1-(1-x)>(a + 1.0) / (a + b + 2.0));

Possible solution: change the condition to
x > (a + 1.0) / (a + b + 2.0) && 1-x<=(b + 1.0) / (b + a + 2.0)",Java build 1.7.0_45-b18,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-11-28 11:42:52.441,,,false,,,,,,,,,,,,,,361001,,,Mon May 19 15:13:19 UTC 2014,,,,,,0|i1q8av:,361300,,,,,,,,"28/Nov/13 11:42;erans;Thanks for the report, and the fix!
Committed in revision 1546350.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EnumeratedRealDistribution.inverseCumulativeProbability returns values not in the samples set,MATH-1065,12680726,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,matteodg@infinito.it,matteodg@infinito.it,22/Nov/13 11:21,19/May/14 15:13,07/Apr/19 20:38,09/Feb/14 11:21,3.2,,,,,,,3.3,,,0,,,,,,,,"The method EnumeratedRealDistribution.inverseCumulativeProbability() sometimes returns values that are not in the initial samples domain...
I will attach a test to exploit this bug.
",,,,,,,,,,,,,,,,,,,,,22/Nov/13 11:31;matteodg@infinito.it;EnumeratedRealDistributionTest.java;https://issues.apache.org/jira/secure/attachment/12615317/EnumeratedRealDistributionTest.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-24 14:29:51.395,,,false,,,,,,,,,,,,,,359991,,,Mon May 19 15:13:30 UTC 2014,,,,,,0|i1q23b:,360290,,,,,,,,"22/Nov/13 11:31;matteodg@infinito.it;Failing tests for numerical issues are:
- testFailing1
- testFailing2
- testFailing3
- testFailing4

Failing tests with very wrong results are:
- testFailing5
- testFailing6
","24/Nov/13 14:29;tn;Hi Matteo,

regarding the numerical issues:

the distribution works internally with an absolute accuracy of 1e-6. The results should be compared with an epsilon value like this.

There is currently no easy way to set another accuracy, but you can instantiate an EnumeratedRealDistribution like this, and override the getSolverAbsoluteAccuracy method:

{noformat}
        DISTRIBUTION = new EnumeratedRealDistribution(new double[]{
            14.0, 18.0, 21.0, 28.0, 31.0, 33.0
        }, new double[]{
            4.0 / 16.0, 5.0 / 16.0, 0.0 / 16.0, 3.0 / 16.0, 1.0 / 16.0, 3.0 / 16.0
        }) {

            @Override
            protected double getSolverAbsoluteAccuracy() {
                return 1e-9;
            }
            
        };
{noformat}","24/Nov/13 15:08;tn;For the other failing tests: I am not sure what the enumerated distribution tries to achieve:

 * a discrete probability distribution
 * a distribution with continuous and discrete parts

in case of a discrete probability distribution the inverseCumulativeProbability method is clearly wrong, and would have to fixed like this:

{noformat}
    public double inverseCumulativeProbability(final double p) throws OutOfRangeException {
        double probability = 0;
        double x = getSupportLowerBound();
        for (final Pair<Double, Double> sample : innerDistribution.getPmf()) {
            if (sample.getValue() == 0.0) {
                continue;
            }
            probability += sample.getValue();
            if (probability <= p) {
                x = sample.getKey();
            }

            if (probability >= p) {
                break;
            }
        }

        return x;
    }
{noformat}

But then your other test cases are wrong imho:

{noformat}
  inverseCumulativeProbability(0.5) = 14 instead of 18
  inverseCumulativeProbability(0.5624) = 14 instead of 18
  inverseCumulativeProbability(0.5626) = 18 instead of 28
  inverseCumulativeProbability(0.7600) = 28 instead of 31
{noformat}","24/Nov/13 17:12;psteitz;I think the intent of this class is to represent a discrete, but real-valued distribution.  EnumeratedDistributions are discrete distributions with a finite, ""enumerated"" set of values.  The problem here (as Thomas notes) is that EnumeratedRealDistribution extends AbstractRealDistribution and does not override the default inverse cum method that basically assumes a continuous distribution.  What it should implement is a discrete algorithm like what Thomas has suggested.","01/Dec/13 19:20;tn;Thanks for the better explanation of the problem.
I am still searching for a mathematical definition of such a distribution as I mainly looked at the referenced wikipedia page and derived the above algorithm from the cdf graph that is available there, although I am of course unsure if it is correct.","08/Feb/14 18:10;psteitz;By contract in RealDistribution (same actually in IntegerDistribution), what the inverse cum needs to return is
{code} 
inf{x in R | P(X<=x) >= p} for 0 < p <= 1}
inf{x in R | P(X<=x) > 0} for p = 0.
{code}

Looks to me like your algorithm returns {code} sup{x in R | P(X <= x) < p} {code} which is not the same.  The key is to be consistent with the way we have defined the inverse cum for both discrete (""Integer"") and continuous (""Real"") distributions.
 ","08/Feb/14 19:31;tn;Hi Phil,

thanks for looking into this. I updated the code to this:

{noformat}
    public double inverseCumulativeProbability(final double p) throws OutOfRangeException {

        if (p < 0.0 || p > 1.0) {
            throw new OutOfRangeException(p, 0, 1);
        }

        double probability = 0;
        double x = getSupportLowerBound();
        for (final Pair<Double, Double> sample : innerDistribution.getPmf()) {
            if (sample.getValue() == 0.0) {
                continue;
            }
            probability += sample.getValue();
            x = sample.getKey();

            if (probability >= p) {
                break;
            }
        }

        return x;
    }
{noformat}

Which returns the same results as the EnumeratedIntegerDistribution for the same input values and also succeeds in running all the test cases from the attached unit test.",08/Feb/14 21:31;psteitz;Code and tests look correct to me.,"09/Feb/14 11:21;tn;Applied changes in r1566274.

Thanks for the report and the testcase!",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kalman filter uses CholeskyDecomposition with too stringent symmetry checks,MATH-1062,12677764,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,tn,tn,tn,06/Nov/13 13:16,19/May/14 15:13,07/Apr/19 20:38,07/Nov/13 15:15,,,,,,,,3.3,,,0,,,,,,,,"In real-world examples the matrix inversion with the CholeskyDecomposition with default threshold settings may fail as the given matrices will not be perfectly symmetric.

Better use QRDecomposition or the new MatrixUtils.inverse method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-05-19 15:13:19.442,,,false,,,,,,,,,,,,,,357139,,,Mon May 19 15:13:19 UTC 2014,,,,,,0|i1pkfz:,357429,,,,,,,,07/Nov/13 15:15;tn;Fixed in r1539676.,19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MathInternalError when calculating barycenter of PolyhedronsSet,MATH-1060,12677708,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Cannot Reproduce,,iperera,iperera,06/Nov/13 01:55,19/May/14 15:13,07/Apr/19 20:38,23/Feb/14 15:36,3.2,,,,,,,3.3,,,0,,,,,,,,"I'm not sure if I could replicate this well enough to be useful - I am trying to construct a PolyhedronsSet from a collection of Planes using RegionFactory.buildConvex(). The input planes are created from running QHull - I don't have the actual normals but I can add them to this report if I run into this again.

Exception in thread ""KQML-Dispatcher-1"" org.apache.commons.math3.exception.MathInternalError: illegal state: internal error, please fill a bug report at https://issues.apache.org/jira/browse/MATH
	at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.followLoop(PolygonsSet.java:736)
	at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.getVertices(PolygonsSet.java:613)
	at org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.computeGeometricalProperties(PolygonsSet.java:523)
	at org.apache.commons.math3.geometry.partitioning.AbstractRegion.getSize(AbstractRegion.java:413)
	at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet$FacetsContributionVisitor.addContribution(PolyhedronsSet.java:188)
	at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet$FacetsContributionVisitor.visitInternalNode(PolyhedronsSet.java:170)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:263)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:262)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:262)
	at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.computeGeometricalProperties(PolyhedronsSet.java:135)
	at org.apache.commons.math3.geometry.partitioning.AbstractRegion.getBarycenter(AbstractRegion.java:428)
","Ubuntu, Java 1.7.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-11-08 12:51:18.381,,,false,,,,,,,,,,,,,,357083,,,Mon May 19 15:13:29 UTC 2014,,,,,,0|i1pk3j:,357373,,,,,,,,"06/Nov/13 22:49;iperera;I realized that this issue came up when trying to create a Convex polyhedron with very thin triangles. Since this is an edge case that should probably be avoided, I changed this to Minor.","08/Nov/13 12:51;luc;Such cases may occur when the lines that form the polygon boundaries are almost parallel and intersection location cannot be computed reliably. This is a numerical issue due to limited accuracy of double precision numbers.

We cannot do anything about this without a way to reproduce the error. If you could provide us a test case reproducing it (a small one if possible), we could look at it.","23/Feb/14 15:36;luc;Without further information from the original poster, this is issue is closed as unable to reproduce.",14/Apr/14 02:39;Telcontar;JUnit test to reproduce NullPointerException.,19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use FastMath instead of Math within CM,MATH-1059,12677579,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tn,tn,05/Nov/13 14:30,19/May/14 15:13,07/Apr/19 20:38,03/Dec/13 23:03,3.2,,,,,,,3.3,,,0,,,,,,,,"Some code in CM still uses Math.xxx instead of the counterparts in FastMath. This could lead to subtle differences with different jvms as could be seen in MATH-1057.

All calls to Math shall be replaced by calls to FastMath.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-05-19 15:13:29.779,,,false,,,,,,,,,,,,,,356954,,,Mon May 19 15:13:29 UTC 2014,,,,,,0|i1pjbb:,357244,,,,,,,,"03/Dec/13 23:03;tn;Finished changes in r1547633.

Remaining uses of Math:

 * PI
 * ulp
 * for FastMath validation and performance tests",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Beta, LogNormalDistribution, WeibullDistribution give slightly wrong answer for extremely small args due to log/exp inaccuracy",MATH-1058,12677292,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,srowen,srowen,03/Nov/13 21:47,19/May/14 15:13,07/Apr/19 20:38,05/Nov/13 13:52,3.2,,,,,,,3.3,,,0,exp,expm1,log,log1p,,,,"Background for those who aren't familiar: math libs like Math and FastMath have two mysterious methods, log1p and expm1. log1p(x) = log(1+x) and expm1(x) = exp(x)-1 mathetmatically, but can return a correct answer even when x was small, where floating-point error due to the addition/subtraction introduces a relatively large error.

There are three instances in the code that can employ these specialized methods and gain a measurable improvement in accuracy. See patch and tests for an example -- try the tests without the code change to see the error.",,,,,,,,,,,,,,,,,,,,,03/Nov/13 21:49;srowen;MATH-1058.patch;https://issues.apache.org/jira/secure/attachment/12611838/MATH-1058.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-05 13:52:10.308,,,false,,,,,,,,,,,,,,356667,,,Mon May 19 15:13:28 UTC 2014,,,,,,0|i1phjr:,356957,,,,,,,,"05/Nov/13 13:52;erans;Thanks.
Committed in revision 1538998.
",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BOBYQAOptimizerTest has two failing tests,MATH-1057,12677282,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,srowen,srowen,03/Nov/13 18:33,19/May/14 15:13,07/Apr/19 20:38,08/Nov/13 15:20,3.2,,,,,,,3.3,,,0,,,,,,,,"I see two test failures, in both the copies of BOBYQAOptimizerTest:

{code}
Failed tests: 
  BOBYQAOptimizerTest.testAckley:209->doTest:282->doTest:338 expected:<0.0> but was:<1.047765607609108E-8>
  BOBYQAOptimizerTest.testAckley:208->doTest:281->doTest:336 expected:<0.0> but was:<1.047765607609108E-8>

Tests in error: 
  BOBYQAOptimizerTest.testDiffPow:187->doTest:282->doTest:322 » TooManyEvaluations
  BOBYQAOptimizerTest.testDiffPow:186->doTest:281->doTest:326 » TooManyEvaluations
{code}

(This predated the patches I've worked on so I don't think it's me!)

I tried on Mac OS X and Linux and see the same, so don't think it is an environment issue. I'll see if a little digging can uncover the issue from a recent commit.",Mac OS X 10.9 and also Linux 3.4 kernel; Java 7; Maven 3.1.1,,,,,,,,,,,,,,,,,,,,03/Nov/13 20:45;srowen;MATH-1057.patch;https://issues.apache.org/jira/secure/attachment/12611836/MATH-1057.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-03 19:41:40.706,,,false,,,,,,,,,,,,,,356657,,,Mon May 19 15:13:35 UTC 2014,,,,,,0|i1phhj:,356947,,,,,,,,"03/Nov/13 19:07;srowen;Hmm, I show that this fails even from the first time the test was added in r1420684. Anyone else seeing the same? if not, what platform I wonder?","03/Nov/13 19:41;tn;Tests fail when run with jdk 1.5 in my environment.

Running them with java 1.6 or 1.7 is successful.","03/Nov/13 20:22;tn;For the testDiffPow testcase, I further debugged with the two different jdks, and after iteration 2219 the objective function output changes.

The difference/error then accumulates and results that the algorithm converges much slower. I could track down the problem to the Math implementation, most likely the sqrt or exp. Changing all the Math calls to our own FastMath solves the problem and the test executes correctly for all jdks.

I wonder why we use Math in the first place, as normally we eat our own dog-food and use FastMath.

btw. the problem also is existent in the 3.2 release so not at all related to your previous patch.",03/Nov/13 20:45;srowen;Here's a patch implementing your change. Yes it fixes the failure for testDiffPow for me too and sounds like a good change. Not sure about the other. Maybe the tolerance needs to be loosened?,"03/Nov/13 20:57;tn;With all my different jdks I used to test the problems, I only had failures with the testDiffPow and testsDiffPow methods, never with the testAckley.

What is your environment?","03/Nov/13 21:13;srowen;Oh I see -- see above, Java 7 on OS X 10.9 and on Linux.","03/Nov/13 21:21;tn;Yes, but what exact jvm?

I use the following:

java version ""1.7.0_25""
OpenJDK Runtime Environment (IcedTea 2.3.10) (7u25-2.3.10-1ubuntu0.12.04.2)
OpenJDK Server VM (build 23.7-b01, mixed mode)
","03/Nov/13 21:29;tn;Ah ok, now I get the same error, strange that I did not see it before.
In this case I think it would be fine to change the epsilon to 1e-7 but I would like to understand why we get different results for jdk 1.5 and 1.7 here.","03/Nov/13 21:37;tn;Ok I understand now.
The difference was happening before, when we used Math.xxx calls, which returned different results for different jdk versions.

Now, when using FastMath, the result is of course consistent with all jdks, as FastMath is a pure java implementation. The result for this test has change slightly and a epsilon of 1e-7 should be used to compensate the for change imho.

@Gilles: in case you read here, as you have done most of the work on this optimizer, do you think that the switch from Math to FastMath is ok, or was there a specific reason why Math was used in this case?","03/Nov/13 21:42;srowen;For the record, my java versions:

OS X:
{code}
java version ""1.7.0_45""
Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)
{code}

Linux:
{code}
java version ""1.7.0_45""
OpenJDK Runtime Environment (amzn-2.4.3.2.32.amzn1-x86_64 u45-b15)
OpenJDK 64-Bit Server VM (build 24.45-b08, mixed mode)
{code}
","03/Nov/13 22:32;erans;The extreme sensitivity of some tests was noticed quite some time ago. The main problem is that we introduced this algorithm into CM although the code was nowhere near to something a Java programmer can understand. And this was already after I performed extensive work to modify the code that had been auto-generated from the original FORTRAN implementation.
Further code readability improvements were stalled due to 
tests failing after seemingly innocuous changes; hence the need for expert advice in order to know what is actually to be expected from the tests and by how much the tolerance can be lowered (while still retaining the ability to catch erroneous changes during the code rewrite).","08/Nov/13 15:20;tn;Fixed in r1540075.

Thanks for your patch!
For now I did only update the test classes as this seems to be sufficient.
I created another issue to replace calls to Math.xxx with calls to FastMath.xxx throughout CM.","20/Jan/14 08:26;Gwendal;Hello,

Sorry to post a message on this Resolved bug, but I appear to have the exact same problem with commons-math3-3.2. Here are the interesting parts of my Maven output:
------------------------------------------------------------------------------------------
testDiffPow(org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizerTest)  Time elapsed: 2.662 sec  <<< ERROR!
org.apache.commons.math3.exception.TooManyEvaluationsException: illegal state: maximal count (12,000) exceeded: evaluations
	at org.apache.commons.math3.optim.BaseOptimizer$MaxEvalCallback.trigger(BaseOptimizer.java:213)
	at org.apache.commons.math3.util.Incrementor.incrementCount(Incrementor.java:156)
	at org.apache.commons.math3.optim.BaseOptimizer.incrementEvaluationCount(BaseOptimizer.java:162)
	at org.apache.commons.math3.optim.nonlinear.scalar.MultivariateOptimizer.computeObjectiveValue(MultivariateOptimizer.java:115)
	at org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizer.bobyqb(BOBYQAOptimizer.java:823)
	at org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizer.bobyqa(BOBYQAOptimizer.java:329)
	at org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizer.doOptimize(BOBYQAOptimizer.java:241)
	at org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizer.doOptimize(BOBYQAOptimizer.java:49)
	at org.apache.commons.math3.optim.BaseOptimizer.optimize(BaseOptimizer.java:143)
	at org.apache.commons.math3.optim.BaseMultivariateOptimizer.optimize(BaseMultivariateOptimizer.java:66)
	at org.apache.commons.math3.optim.nonlinear.scalar.MultivariateOptimizer.optimize(MultivariateOptimizer.java:64)
	at org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizerTest.doTest(BOBYQAOptimizerTest.java:322)
	at org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizerTest.doTest(BOBYQAOptimizerTest.java:282)
	at org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizerTest.testDiffPow(BOBYQAOptimizerTest.java:187)

testDiffPow(org.apache.commons.math3.optimization.direct.BOBYQAOptimizerTest)  Time elapsed: 2.907 sec  <<< ERROR!
org.apache.commons.math3.exception.TooManyEvaluationsException: illegal state: maximal count (12,000) exceeded: evaluations
	at org.apache.commons.math3.optimization.direct.BaseAbstractMultivariateOptimizer.computeObjectiveValue(BaseAbstractMultivariateOptimizer.java:108)
	at org.apache.commons.math3.optimization.direct.BOBYQAOptimizer.bobyqb(BOBYQAOptimizer.java:828)
	at org.apache.commons.math3.optimization.direct.BOBYQAOptimizer.bobyqa(BOBYQAOptimizer.java:334)
	at org.apache.commons.math3.optimization.direct.BOBYQAOptimizer.doOptimize(BOBYQAOptimizer.java:246)
	at org.apache.commons.math3.optimization.direct.BaseAbstractMultivariateOptimizer.optimizeInternal(BaseAbstractMultivariateOptimizer.java:206)
	at org.apache.commons.math3.optimization.direct.BaseAbstractMultivariateOptimizer.optimize(BaseAbstractMultivariateOptimizer.java:145)
	at org.apache.commons.math3.optimization.direct.BOBYQAOptimizerTest.doTest(BOBYQAOptimizerTest.java:326)
	at org.apache.commons.math3.optimization.direct.BOBYQAOptimizerTest.doTest(BOBYQAOptimizerTest.java:281)
	at org.apache.commons.math3.optimization.direct.BOBYQAOptimizerTest.testDiffPow(BOBYQAOptimizerTest.java:186)

testAckley(org.apache.commons.math3.optimization.direct.BOBYQAOptimizerTest)  Time elapsed: 0.025 sec  <<< FAILURE!
java.lang.AssertionError: expected:<0.0> but was:<1.047765607609108E-8>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:443)
	at org.apache.commons.math3.optimization.direct.BOBYQAOptimizerTest.doTest(BOBYQAOptimizerTest.java:336)
	at org.apache.commons.math3.optimization.direct.BOBYQAOptimizerTest.doTest(BOBYQAOptimizerTest.java:281)
	at org.apache.commons.math3.optimization.direct.BOBYQAOptimizerTest.testAckley(BOBYQAOptimizerTest.java:208)



testAckley(org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizerTest)  Time elapsed: 0.025 sec  <<< FAILURE!
java.lang.AssertionError: expected:<0.0> but was:<1.047765607609108E-8>
	at org.junit.Assert.fail(Assert.java:93)
	at org.junit.Assert.failNotEquals(Assert.java:647)
	at org.junit.Assert.assertEquals(Assert.java:443)
	at org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizerTest.doTest(BOBYQAOptimizerTest.java:338)
	at org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizerTest.doTest(BOBYQAOptimizerTest.java:282)
	at org.apache.commons.math3.optim.nonlinear.scalar.noderiv.BOBYQAOptimizerTest.testAckley(BOBYQAOptimizerTest.java:209)


Failed tests: 
  BOBYQAOptimizerTest.testAckley:208->doTest:281->doTest:336 expected:<0.0> but was:<1.047765607609108E-8>
  BOBYQAOptimizerTest.testAckley:209->doTest:282->doTest:338 expected:<0.0> but was:<1.047765607609108E-8>

Tests in error: 
  BOBYQAOptimizerTest.testDiffPow:186->doTest:281->doTest:326 » TooManyEvaluations
  BOBYQAOptimizerTest.testDiffPow:187->doTest:282->doTest:322 » TooManyEvaluations
------------------------------------------------------------------------------------------

I am using Fedora 20 x86_64 with kernel 3.12.7 and with java-1.7.0-openjdk-1.7.0.60-2.4.4.1.fc20.x86_64.",20/Jan/14 08:54;srowen;The error is indeed reported against 3.2 and fixed for 3.3. Are you saying you see this in HEAD?,"20/Jan/14 09:03;Gwendal;I've simply downloaded this source archive : http://apache.mirrors.multidist.eu//commons/math/binaries/commons-math3-3.2-bin.tar.gz

Then I tried to compile it with Maven, and I obtained these errors.

I thought the problem was (theoretically) fixed in 3.3. Maybe I misunderstood?","20/Jan/14 09:11;srowen;Yes, but you show you are working with 3.2. Look at what you downloaded. ","20/Jan/14 09:20;Gwendal;Oh, right! I misread 3.3 instead of 3-3... My apologies! I will try to be more careful in the future.

Thanks a lot for answering this, and sorry again!","20/Jan/14 11:13;ebourg;It might be a good idea to rename the source archive for the next releases to commons-math-3.x-bin.tar.gz, that's indeed confusing.",20/Jan/14 13:20;srowen;Yeah I see the point but the artifact is now called 'math3' as it was not backwards compatible with 2.x and was sometimes necessary to deploy together. So it really is math3 3.2.,19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,
Small error in PoissonDistribution.nextPoisson() algorithm,MATH-1056,12677275,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,srowen,srowen,03/Nov/13 14:19,19/May/14 15:13,07/Apr/19 20:38,08/Nov/13 23:31,3.2,,,,,,,3.3,,,0,,,,,,,,"Here's a tiny bug I noticed via static inspection, since it flagged the integer division. PoissonDistribution.java:325 says:

{code:java}
final double a1 = FastMath.sqrt(FastMath.PI * twolpd) * FastMath.exp(1 / 8 * lambda);
{code}

The ""1 / 8 * lambda"" is evidently incorrect, since this will always evaluate to 0. I rechecked the original algorithm (http://luc.devroye.org/devroye-poisson.pdf) and it should instead be:

{code:java}
final double a1 = FastMath.sqrt(FastMath.PI * twolpd) * FastMath.exp(1 / (8 * lambda));
{code}

(lambda is a double so there is no int division issue.) This matches a later expression.

I'm not sure how to evaluate the effect of the bug. Better to be correct of course; it may never have made much practical difference.",,,,,,,,,,,,,,,,,,,,,03/Nov/13 14:20;srowen;MATH-1056.patch;https://issues.apache.org/jira/secure/attachment/12611822/MATH-1056.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-11-03 21:37:53.65,,,false,,,,,,,,,,,,,,356650,,,Mon May 19 15:13:23 UTC 2014,,,,,,0|i1phfz:,356940,,,,,,,,"03/Nov/13 21:37;psteitz;Good catch, Sean!   I played a little with this today to see if fixing it allowed me to increase sensitivity in the nextPossionConsistency test in RandomDataGeneratorTest.  I did not succeed, which is unfortunate as this definitely looks like a bug.  I am +1 for committing the patch, but would really like to get a test that fails before and succeeds after.

The nextPossionConsistency test is a ChiSquare-based test that looks at the binned distribution of values generated by nextPoission (which is now implemented in the distribution class itself).  When the test was developed, we had not yet implemented the G test, which may work better for homogeneity testing in this kind of setting.  It might be interesting to experiment with G tests to get a pre/post fail/fix here.  Another thing to dig into is exactly what generated values for what means will be materially impacted by the bug.  That might allow us to set up a specific G or Chi-square test that will consistently fail.","08/Nov/13 16:13;tn;I did take a look at this myself, and the introduced error is so small that it is very difficult to make a reasonable test that outlines this.

Due to the fact that the wrong code is only executed when a PoissonDistribution with mean >= 40 is used, the error is <= 3e-3. Now when looking at the iteration that constructs the sample, it looks at random numbers [0, 1) and does different things dependent on whether the random number is smaller or larger than some of these pre-computed values. I did not do further analysis of the expected failure from such a small difference, but it must be *very* small compared to an ideal poisson distribution.

btw. we have there several variables that are calculated each time nextPoisson() is called, although they are fixed with the given mean (which is immutable for the distribution), thus could be cached to improve performance.","08/Nov/13 23:31;psteitz;Patch applied in r1540217.  I have still not been able to produce a test case showing real impact; but the pre-patch code clearly departs from the intent of the algorithm, so best to fix.","08/Nov/13 23:49;psteitz;Forgot to respond to Thomas' observation about repeat computations.  The problem is that the actual parameter to nextPoisson can't be eliminated, i.e., it is not always the mean. nextPoisson is (conditionally) called recursively by itself when computing y2 with actual parameter lambdaFractional.  That means all of the variables set at the beginning need to be recomputed in that case.  I guess we could test the actual parameter value against the mean and used cached values in that case, but that would make the code a little harder to follow.","09/Nov/13 07:35;tn;But lambdaFractional is always in the range [0, 1) thus your recursive call will just once the other part of the conditional.

nextPoisson(mean) is only called from the sample method, and we could make a nextPoisson() method that decides to call either nextPoissonSmallMean or LargeMean depending on the actual mean value. nextPoissonLargeMean itself calls SmallMean for the fractional part.","09/Nov/13 21:37;psteitz;I see.  Would slightly complicate the code for I am not sure how much benefit, but might be worth doing.  Probably best to open a separate ticket for this.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EigenDecomposition may not converge for certain matrices,MATH-1051,12676947,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tn,tn,31/Oct/13 20:01,19/May/14 15:13,07/Apr/19 20:38,31/Oct/13 20:06,3.2,,,,,,,3.3,,,0,,,,,,,,"Jama-1.0.3 contains a bugfix for certain matrices where the original code goes into an infinite loop.

The commons-math translations would throw a MaxCountExceededException, so fails to compute the eigen decomposition.

Port the fix from jama to CM.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-05-19 15:13:19.905,,,false,,,,,,,,,,,,,,356323,,,Mon May 19 15:13:19 UTC 2014,,,,,,0|i1pfev:,356611,,,,,,,,31/Oct/13 20:06;tn;Fixed in r1537611.,19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EigenDecomposition.Solver should consider tiny values 0 for purposes of determining singularity,MATH-1045,12675022,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,srowen,srowen,22/Oct/13 13:10,19/May/14 15:13,07/Apr/19 20:38,29/Oct/13 15:55,3.2,,,,,,,3.3,,,0,eigenvalue,singular,,,,,,"EigenDecomposition.Solver tests for singularity by comparing eigenvalues to 0 for exact equality. Elsewhere in the class and in the code, of course, very small values are considered 0. This causes the solver to consider some singular matrices as non-singular.

The patch here includes a test as well showing the behavior -- the matrix is clearly singular but isn't considered as such since one eigenvalue are ~1e-14 rather than exactly 0.

(What I am not sure of is whether we should really be evaluating the *norm* of the imaginary eigenvalues rather than real/imag components separately. But the javadoc says the solver only supports real eigenvalues anyhow, so it's kind of moot since imag=0 for all eigenvalues.)",,,,,,,,,,,,,,,,,MATH-1049,,,,23/Oct/13 10:09;srowen;MATH-1045.patch;https://issues.apache.org/jira/secure/attachment/12609836/MATH-1045.patch,22/Oct/13 14:23;srowen;MATH-1045.patch;https://issues.apache.org/jira/secure/attachment/12609658/MATH-1045.patch,30/Oct/13 13:54;srowen;MATH-1045_2.patch;https://issues.apache.org/jira/secure/attachment/12611064/MATH-1045_2.patch,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-10-22 14:38:02.314,,,false,,,,,,,,,,,,,,354644,,,Mon May 19 15:13:29 UTC 2014,,,,,,0|i1p52n:,354933,,,,,,,,"22/Oct/13 14:38;erans;Isn't the code in this class (and others similarly) supposed to work for a matrix with very small entries too? I mean that, if all eigenvalues are of the order of, say, EPSILON / 10, should the matrix be considered singular right away?","22/Oct/13 15:23;srowen;That's a good point. If you make the example matrix non-singular, but then divide elements by 1e12, it will report it as singular. This seems wrong. On the other hand it seems a bit undesirable to return an 'inverse' in this case -- it's dominated by the inverse of that tiny eigenvalue, which is huge, and the result is pretty unreliable. 

I'm a bit out of my depth here but I wonder if it's more reasonable to examine the eigenvalues in sorted order and examine ratio of one to the next. When that ratio is below epsilon it makes more sense to declare it ""0"".

I could also see this being a case of ""caller beware"". That's the more conservative thing here.","22/Oct/13 17:59;erans;bq. On the other hand it seems a bit undesirable to return an 'inverse' in this case – it's dominated by the inverse of that tiny eigenvalue, which is huge, and the result is pretty unreliable. 

This could be case-dependent and the code should perhaps be able to detect and accept input that can return a reliable result. In r1534709, I've committed an example that seems to work, even as the eigenvalues are quite small indeed.

bq. it's more reasonable to examine the eigenvalues in sorted order and examine ratio

That's an interesting idea.
Could you try and see whether it would let the new test pass, while intercepting the singular matrix of your test?
","23/Oct/13 10:09;srowen;On a little more research, it seems the thing to do is look at the ratio with the largest eigenvalue. Attached is a new patch where your new test and my (2) new tests all pass.

Comments welcome from linear algebra experts but I think this is at least as principled as the existing code.

We could also let the user specify the zero threshold as in the QRDecomposition class.","23/Oct/13 10:54;erans;bq. We could also let the user specify the zero threshold as in the QRDecomposition class.

That would be best, I also think.
However, there is a practical problem in that there is currently a (deprecated) constructor with the required signature. :(
Could you raise this issue on the ""dev"" ML, and ask confirmation on how to proceed?  I seem to recall that such a (functionally non-compatible) change would now be acceptable, even in a minor release.
","23/Oct/13 21:02;tn;I just realized that in the case of a non-symmetric matrix, the eigenvalues are not sorted by value.
The reason is that for the two cases, symmetric and non-symmetric there are completely different ways to do the decomposition.

So you should not rely on that a particular order of the eigenvalues before we change that first.","24/Oct/13 09:32;srowen;These are good points, as well as the comments from the thread on commons-dev -- Ted in particular notes that you can use the threshold in the decomposition itself to simply stop computing eigenvalues when they get small.

Now, these more advanced changes are near the limit of my ability and I am not sure I feel confident making them. I propose these additional changes be considered in another issue: (maybe) moving a threshold parameter, maybe modifying the decomposition.

The patch here does I think represent a small, distinct positive change, in that it employs a reasonable test for singularity after the fact.","24/Oct/13 11:25;tn;Sure np.

I think we are just collecting all relevant information in order to decide how to proceed. The necessary changes can then be done, e.g. by myself or another maintainer.","25/Oct/13 16:04;erans;bq. I propose these additional changes be considered in another issue

I agree.
","29/Oct/13 15:55;erans;Applied in revision 1536766.

I created MATH-1049 for discussing further improvements.
","29/Oct/13 21:08;tn;We need to change the behavior for non-symmetric matrices as the eigenvalues are not sorted in this case.
This patch relies on a descending sort order to determine if the decomposed matrix is singular, so this may fail in such a case.

I will create a separate issue for this as it makes sense to always sort the eigenvalues imho.","30/Oct/13 10:34;erans;bq. [...] this may fail [...]

For this issue, I could add a loop in order to find the one with largest absolute value. WDYT?
But I have no idea how to construct a matrix for a unit test that would exhibit the problem.
","30/Oct/13 11:56;srowen;Yes, this is a good point. It's safest to find the largest eigenvalue (by absolute value) with a loop I think.

The final matrix in testUnsymmetric(), which is unsymmetric, shows this.

The symmetric matrix in testSquareRootNonPositiveDefinite() also shows this -- the last eigenvalue is the most negative, but is the largest in absolute value.","30/Oct/13 12:50;erans;Would you mind creating a patch?
","30/Oct/13 13:54;srowen;Ah right, I should have said that these show out-of-order eigenvalues but that's not what we need to check. We need one that puts a very small eigenvalue first. That's easy to generate as something like

[ d 0 ]
[ 1 1 ]

for tiny d. Patch attached.","30/Oct/13 14:17;erans;Thank you. Committed in revision 1537099.
","31/Oct/13 14:07;srowen;This is a separate issue, but so minor not sure if it merits another JIRA. While looking at this code I noticed this loop at EigenDecomposition:945 that does nothing:

        // Vectors of isolated roots
        for (int i = 0; i < n; i++) {
            if (i < 0 | i > n - 1) {
                for (int j = i; j < n; j++) {
                    matrixP[i][j] = matrixT[i][j];
                }
            }
        }

The 'if' can never be true. (Not to mention non-short-circuit boolean op there.)","31/Oct/13 14:27;erans;bq. issue [...] so minor not sure if it merits another JIRA

That's certainly worth a report!
Thanks.
","31/Oct/13 19:48;tn;This is an artifact from the original Jama source code.

There was a similar code construct, which has been removed when the code has been translated, but this one remained. Imho it is safe to remove this part also.

While checking this I have seen there was a new release of Jama last November with a bugfix for possible infinite loops. Need to check if our code is also affected, but most likely.","31/Oct/13 20:08;tn;Removed the spurious code fragment in r1537616.

Created and fixed also MATH-1051 to port a bugfix from Jama-1.0.3 to CM.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,
Distribution tests are mostly meaningless due to high tolerance,MATH-1037,12670489,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,dievsky,dievsky,25/Sep/13 14:09,19/May/14 15:13,07/Apr/19 20:38,11/Oct/13 20:47,3.2,,,,,,,3.3,,,0,,,,,,,,"The tolerance used for value comparison in {{IntegerDistributionAbstractTest}} is {{1E-4}}. However, most values being compared are much smaller, so they are considered equal even if they otherwise differ by orders of magnitude. For example, a typo in {{GeometricDistributionTest}} puts 29 in the test points instead of 19, while the test probability value is correctly given for 19. The test passes, disregarding the fact that {{2.437439e-05}} (test value for 19) and {{1.473826e-07}} (actual value for 29) differ almost hundredfold.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-26 16:23:22.364,,,false,,,,,,,,,,,,,,350318,,,Mon May 19 15:13:28 UTC 2014,,,,,,0|i1oein:,350611,,,,,,,,26/Sep/13 16:23;psteitz;Thanks for reporting this.  This is bad and the example calls it out.  The Geometric distribution test needs to be fixed. I am inclined to get rid of the default and make getTolerance abstract so tests have to set it explicitly.,"11/Oct/13 20:38;tn;Actually, the tolerance value has been overridden in the GeometricDistributionTest, but unfortunately, the base class seems to ignore it: instead of accessing the tolerance value by calling getTolerance(), the class accesses the field directly.

I will fix this together with the typo in the GeometricDistributionTest.","11/Oct/13 20:47;tn;Fixed in r1531413.

Thanks for the report!",14/Oct/13 10:08;dievsky;Glad to be of help!,19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GradStructure: A DerivativeStructure that works for large number of variables.,MATH-1036,12669464,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ajo.fod,ajo.fod,19/Sep/13 14:18,19/May/14 15:13,07/Apr/19 20:38,27/Oct/13 10:10,,,,,,,,3.3,,,0,,,,,,,,"Ran into problems with DerivativeStructure today. It requires a lot of
memory with 6000 or so independent variables. The problem starts with the number of DSCompiler objects instantiated.

Here is a faster/leaner(less memory) GradStructure that only computes up to the first derivative for the case where there are a large number of independent variables.",,,,,,,,,,,,,,,,,,,,,19/Sep/13 14:21;ajo.fod;GradStructure.java;https://issues.apache.org/jira/secure/attachment/12604040/GradStructure.java,20/Sep/13 00:56;ajo.fod;grad-patch.txt;https://issues.apache.org/jira/secure/attachment/12604173/grad-patch.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-10-27 10:10:34.495,,,false,,,,,,,,,,,,,,349396,,,Mon May 19 15:13:26 UTC 2014,,,,,,0|i1o8uv:,349694,,,,,,,,19/Sep/13 14:21;ajo.fod;Changed the name to GradStructure to reflect the relationship with the more generic DerivativeStructure and the idea that it is limited to computing gradients.,19/Sep/13 14:24;ajo.fod;IMHO: This can be extended to higher orders using more complex hashmap key structure.,20/Sep/13 00:56;ajo.fod;A patch with a few tests are  included here.,"27/Oct/13 10:10;luc;Fixed in subversion as of r1536073.

The name of the class has been changed to SparseGradient and the factory methods for constant and variables changed from get to create.

The RealFieldElement<SparseGradient> interface is implemented, meaning instances can be used in the same kind of computation as all other field elements, including with functions like trigonometric, hyperbolic or power functions. The high accuracy linear combinations are also available for these instances.

Thanks for the report and the patch.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kalman filter does not work if covarance matrix is not of dimension 1,MATH-1033,12667131,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,abyssqu,abyssqu,05/Sep/13 16:15,19/May/14 15:13,07/Apr/19 20:38,11/Oct/13 21:39,3.2,,,,,,,3.3,,,0,,,,,,,,"In org.apache.commons.math3.filter.KalmanFilter,

The check below doesn't look right, it reques measNoise's column dimension to be 1 at all time.
        
// row dimension of R must be equal to row dimension of H
        if (measNoise.getRowDimension() != measurementMatrix.getRowDimension() ||
            measNoise.getColumnDimension() != 1) {
            throw new MatrixDimensionMismatchException(measNoise.getRowDimension(),
                                                       measNoise.getColumnDimension(),
                                                       measurementMatrix.getRowDimension(), 1);
        }",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-09-06 07:44:21.275,,,false,,,,,,,,,,,,,,347068,,,Mon May 19 15:13:27 UTC 2014,,,,,,0|i1nuiv:,347367,,,,,,,,"06/Sep/13 07:44;tn;Hmm, this looks indeed wrong as the measurement noise is supposed to be a matrix. I can not remember again why this check has been put there but will investigate.","11/Oct/13 21:39;tn;Fixed in r1531430.

Thanks for the report.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating BigFraction objects is not consistent for negative/positive values,MATH-1029,12666470,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,tn,tn,tn,31/Aug/13 19:48,19/May/14 15:13,07/Apr/19 20:38,17/Oct/13 21:13,,,,,,,,3.3,,,0,,,,,,,,"Creating a BigFraction object for large integer values will return different results whether the argument is positive or negative:

{noformat}
  BigFraction f1 = new BigFraction(-1e10, 1000); -> will return a fraction with a numerator of -1e10 and denominator of 1
  BigFraction f2 = new BigFraction(1e10, 1000); -> will throw a FractionConversionException
{noformat}

the problem is in the check for overflow, it is not done on the absolute value of the argument, also it should be done only after the check if the argument is actually an integer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2014-05-19 15:13:33.662,,,false,,,,,,,,,,,,,,346409,,,Mon May 19 15:13:33 UTC 2014,,,,,,0|i1nqh3:,346710,,,,,,,,17/Oct/13 21:13;tn;Fixed in r1533260 by checking the absolute value of the parameter for overflow.,19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Confused by the API docs for org.apache.commons.math3.analysis.function,MATH-1022,12663388,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,dtonhofer,dtonhofer,13/Aug/13 09:57,19/May/14 15:13,07/Apr/19 20:38,13/Aug/13 16:48,3.2,,,,,,,3.3,,,0,,,,,,,,"Something is wrong or unclear...

We read:

http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/function/Logistic.Parametric.html

   ""Parametric function where the input array contains the parameters
    of the logit function, ordered as follows: ""

 --> But the ""logit"" function is not the ""logistic"" function.

http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/function/Sigmoid.Parametric.html

   ""Parametric function where the input array contains the parameters of
   the logit function, ordered as follows: ""

 --> But the ""logit"" function is not the ""sigmoid"" function, and what is
     the difference between the Logistic Function snd the Sigmoid function?

http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/function/Logit.Parametric.html

   ""Parametric function where the input array contains the parameters of
    the logit function, ordered as follows: ""

 --> That sounds correct.


References:

http://en.wikipedia.org/wiki/Logistic_function
http://en.wikipedia.org/wiki/Logit
http://en.wikipedia.org/wiki/Sigmoid_function

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-13 10:52:17.974,,,false,,,,,,,,,,,,,,343389,,,Mon May 19 15:13:24 UTC 2014,,,,,,0|i1n7wf:,343693,,,,,,,,"13/Aug/13 10:15;dtonhofer;Must be a copy-paste issue, at least in large parts.

See also the description of Logistic.Parametric in 

http://commons.apache.org/proper/commons-math/apidocs/org/apache/commons/math3/analysis/function/Logistic.html

which doesn't match the constructor of ""Logistic"" at all.

","13/Aug/13 10:52;erans;Thanks for the report. The Javadoc is fixed in revision 1513430.

bq. Must be a copy-paste issue, at least in large parts.

A pure copy/paste bug, indeed.

bq. [...] and what is the difference between the Logistic Function snd the Sigmoid function?

Extracted from [Wikipedia|http://en.wikipedia.org/wiki/Sigmoid_function]:
""Often, sigmoid function refers to the special case of the logistic function [...]""
","13/Aug/13 15:59;dtonhofer;In furtherance of which: 

References to articles are alternately to Wikipedia or Wolfram's Mathworld. Might there be a need to standardize?","13/Aug/13 16:15;erans;bq. Might there be a need to standardize?

If you wish so.
You could start a discussion about this on the ""dev"" ML.

If you agree, I'll set this issue to ""Resolved"".
","13/Aug/13 16:17;dtonhofer;Go for Resolved, then.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HypergeometricDistribution.sample suffers from integer overflow,MATH-1021,12663001,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brianbloniarz,brianbloniarz,10/Aug/13 00:00,19/May/14 15:13,07/Apr/19 20:38,29/Aug/13 21:35,,,,,,,,3.3,,,0,,,,,,,,"Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values -- the example code below should return a sample between 0 and 50, but usually returns -50.

{code}
import org.apache.commons.math3.distribution.HypergeometricDistribution;

public class Foo {
  public static void main(String[] args) {
    HypergeometricDistribution a = new HypergeometricDistribution(
        43130568, 42976365, 50);
    System.out.printf(""%d %d%n"", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints ""0 50""
    System.out.printf(""%d%n"",a.sample());                                             // Prints ""-50""
  }
}
{code}

In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean() -- instead of doing
{code}
return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();
{code}
it could do:
{code}
return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());
{code}
This seemed to fix it, based on a quick test.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-10 00:59:36.629,,,false,,,,,,,,,,,,,,343003,,,Mon May 19 15:13:25 UTC 2014,,,,,,0|i1n5in:,343307,,,,,,,,"10/Aug/13 00:59;erans;Thanks for the report and suggested fix.
Committed in revision 1512546.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""nextPermutation"" method broken (in class ""o.a.c.m.random.RandomDataGenerator"")",MATH-1020,12662855,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,erans,erans,09/Aug/13 13:25,19/May/14 15:13,07/Apr/19 20:38,10/Aug/13 00:27,,,,,,,,3.3,,,0,,,,,,,,"Correct behaviour of this method relied on the [broken behaviour of ""shuffle""|MATH-1019].
With a fixed ""shuffle"", the ""nextSample()"" unit test caught the bug in this method.",,,,,,,,,,,,,,,,,,,,,09/Aug/13 13:35;erans;MATH-1020.patch;https://issues.apache.org/jira/secure/attachment/12597080/MATH-1020.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2014-05-19 15:13:28.242,,,false,,,,,,,,,,,,,,342857,,,Mon May 19 15:13:28 UTC 2014,,,,,,0|i1n4m7:,343161,,,,,,,,"09/Aug/13 13:35;erans;IIUC, the whole array must be randomized (which is what the broken ""shuffle"" was doing), then from the ""n"" entries, ""k"" are picked (see attached patch).
",09/Aug/13 13:56;erans;Proposed fix committed in revision 1512306.,"09/Aug/13 21:59;erans;""Affects Version"" entry set to empty to indicate that the bug does not affect applications.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""shuffle"" method broken (in class ""o.a.c.m.random.RandomDataGenerator"")",MATH-1019,12662726,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,erans,erans,08/Aug/13 22:43,19/May/14 15:13,07/Apr/19 20:38,10/Aug/13 00:26,,,,,,,,3.3,,,0,,,,,,,,"The method does not abide by its contract: elements before the ""end"" index are included in the shuffle.

{code}
/**
 * Uses a 2-cycle permutation shuffle to randomly re-order the last elements 
 * of list.
 *
 * @param list list to be shuffled
 * @param end element past which shuffling begins
 */
private void shuffle(int[] list, int end) {
    int target = 0;
    for (int i = list.length - 1; i >= end; i--) {
        if (i == 0) { // XXX ""0"" should be ""end""
            target = 0; // XXX ""0"" should be ""end""
        } else {
            // NumberIsTooLargeException cannot occur
            target = nextInt(0, i); // XXX ""0"" should be ""end""
        }
        int temp = list[target];
        list[target] = list[i];
        list[i] = temp;
    }
}
{code}

I'm going to introduce the above corrections in the new implementation to be located in ""MathArrays"" (cf. issue MATH-1010).",,,,,,,,,,,,,,,,,MATH-1010,MATH-1020,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-09 15:18:20.532,,,false,,,,,,,,,,,,,,342728,,,Mon May 19 15:13:32 UTC 2014,,,,,,0|i1n3tj:,343032,,,,,,,,"08/Aug/13 23:35;erans;In ""RandomDataGenerator"", I tried to replace the use of ""shuffle"" that appears in ""nextPermutation(int n, int k)"" (at line 642) by a call to the new ""MathArrays.shuffle"" (cf. MATH-1010) but this makes the ""testNextSample"" unit test fail!
","09/Aug/13 13:09;erans;With the replacement, the unit test ""testNextPermutation()"" passes but it uses a fully shuffled array where there is no difference in behaviour between the old (in ""RandomDataGenerator"") and the fixed (in ""MathArrays"") code.
","09/Aug/13 15:18;psteitz;It might be better not to log this and MATH-1020 as separate issues - i.e., this is just work related to making the formerly private internal method public.  As you note, the bugged API spec / impl does not impact its use in RandomDataGenerator.  Tracking this and MATH-1020 as issues against 3.2 and reporting their resolution in 3.3 may be misleading to users reading the release notes.  The 3.2 implementation of nextPermutation in RandomDataGenerator performs according to spec and seeing the bug report and resolution in the 3.3 release notes may give users the impression that there is an actual usage-impacting bug in the 3.2 impl, which is not true.  May be best to just note the problem in the other issue (MATH-1010) and drop this and MATH-1020 as separate issues.","09/Aug/13 21:23;erans;IMHO, the svn log and the bug tracking system are not solely directed to library users.
Developers too might like to be able to follow why and how code changes.

I understand your concern, but ""historically"" MATH-1010 was not meant to fix something, just to move code from one place to another; I fortuitously uncovered the bug because the method was intended to do more than is actually needed in ""RandomDataGenerator"" (""user"" behaviour was indeed fine but ""developer"" code was misleading).

In order to not frighten users, I can add a comment in the ""changes.xml"" file to that effect; i.e. for MATH-1020, something like: This bug does not affect applications using a previous version of Commons Math. For this issue, it should be obvious since the method ""shuffle"" was private.
","09/Aug/13 21:52;psteitz;I like to keep what is in JIRA understandable to users, but I get your point and am OK with keeping these as separate issues as long as you make it clear in changes.xml.  Also, what would you think about changing the affects version to 3.3?  Unfortunately, ""nightly builds"" does not seem to be an option.","09/Aug/13 21:58;erans;""Affects Version"" entry set to _empty_ to indicate that the bug does not affect applications.","10/Aug/13 21:38;erans;The Javadoc for ""shuffle"" must be updated: a request to the
reference link
  http://www.maths.abdn.ac.uk/~igc/tch/mx4002/notes/node83.html
replies ""Forbidden"".
","11/Aug/13 23:51;psteitz;This looks like a published version of the course notes in the now dead reference:
http://citeseerx.ist.psu.edu/viewdoc/download;?doi=10.1.1.173.1898&rep=rep1&type=pdf

The algorithm is on page 68.  Not sure how long this will be available online, so it may be best just to reference the dead trees version.","12/Aug/13 13:35;erans;bq. [...] so it may be best just to reference the dead trees version.

I'm not sure I understand: Do you suggest to leave an unreachable link in the Javadoc?

Wouldn't [this link|http://en.wikipedia.org/wiki/Fisher–Yates_shuffle] be an adequate reference?
","12/Aug/13 13:46;psteitz;Sorry, but the ""dead trees version"" I meant the published book.  The link above is to a pdf of the book.  I think it is better to stick with this, which is the original reference that describes what we have implemented exactly.  The wikipedia article covers several different algorithms, one of which is similar to what we have.","12/Aug/13 14:07;erans;bq. [...] ""dead trees version"" [...]

Oh, I got the expression now. ;)

But I still don't see the reference; the one in the Javadoc is not a link to a book, and since the link is dead, it couldn't be more useless.
Then the one which you mention is a PDF, and IIUC, you said that it might not be reliable to reference that link.

bq. The wikipedia article covers several different algorithms, one of which is similar to what we have.

Thus we can put a link to the appropriate [section|http://en.wikipedia.org/wiki/Fisher–Yates_shuffle#The_modern_algorithm].
Also it's more user-friendly IMO to be redirected to that section than to have to download the whole textbook and refer to a page number.
","12/Aug/13 14:28;psteitz;OK, assuming the section link works and what is described there matches what we have, I agree it is easier.  It would be nice to reference the e-book in any case, as that was the original source. The lecture notes linked in the javadoc were eventually published as the ebook and removed from the university web site.  At least the relevant section matches my recollection of the nice description and justification for the shuffle algorithm in the original link.  I see now that there does not actually appear to be a ""dead trees version"" after all.  apologies for that :)  I assumed the pdf was an image of a physical book published by the University of Aberdeen.  Looks like it is only distributed as an ebook, generally behind a paywall.  It can be referenced as  _Algorithms_, by Ian Craw, John Pulham, University of Aberdeen 1999.","13/Aug/13 14:21;erans;I hope that changes brought in revision 1513501 are fine.
",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractUnivariateStatistic and AbstractStorelessUnivariateStatistic,MATH-1017,12661476,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ajo.fod,ajo.fod,02/Aug/13 20:44,02/Aug/13 21:07,07/Apr/19 20:38,02/Aug/13 21:07,,,,,,,,,,,0,,,,,,,,"The class design of AbstractStorelessStats (Storeless) suggests that it is storing data in its parent AbstractUnivariate (Parent)

private double[] storedData;

... perhaps Percentile etc should inherit from another subclass of the Parent?


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-08-02 21:07:02.751,,,false,,,,,,,,,,,,,,341665,,,Fri Aug 02 21:07:02 UTC 2013,,,,,,0|i1mxan:,341972,,,,,,,,"02/Aug/13 21:07;psteitz;This should be discussed on the dev list.  Once there is consensus that a design change is warranted, we can open tickets to implement the change.  JIRA should not be used for general design discussion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add support for embedding Tex in javadoc and site docs via MathJax,MATH-1006,12657664,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,psteitz,psteitz,15/Jul/13 03:40,09/May/17 09:01,07/Apr/19 20:38,19/Jul/13 19:37,3.2,,,,,,,3.3,,,0,,,,,,,,"It would be convenient to be able to embed Tex expressions in javadoc, xdocs and apt.  This can be accomplished via the [MathJax|http://www.mathjax.org/] javascript display engine.  MathJax can be integrated into javadoc by passing a -header option to the doclet that points to the MathJax javascript sources.  Both maven and ant support this via configuration.  Once pom.xml (maven) and build.xml (ant) are modified to make the MathJax functions available, javadoc can embed Tex expressions by using standard Tex escapes: &#92;\( ... &#92;\) for inline, &#92;\[ ... &#92;\] for formulas.",,,,,,,,,,,,,,,,,,,,,15/Jul/13 04:57;psteitz;mathjax.patch;https://issues.apache.org/jira/secure/attachment/12592276/mathjax.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-07-15 14:00:42.095,,,false,,,,,,,,,,,,,,337884,,,Fri Jul 19 19:37:23 UTC 2013,,,,,,0|i1ma2v:,338206,,,,,,,,"15/Jul/13 04:57;psteitz;First attempt at a patch to activate this.  Works when javadoc is explicitly invoked in maven, but not via the site goal.","15/Jul/13 14:00;sebb@apache.org;mvn javadoc:javadoc uses the BUILD section; mvn site uses the REPORTING section.

Generally if one wants to generate a report as part of site generation and stand-alone, the configuration has to appear it two places.

See for example the apache-rat-plugin configuration in CP32.","15/Jul/13 14:45;psteitz;Thanks, Sebb!

So I guess what I need to do is put in twice. I will do that and commit if there are no objections.",17/Jul/13 22:43;psteitz;Fixed in r1504314.,19/Jul/13 19:31;psteitz;Reopening to add site docs to scope.  This can be enabled by adding a <head> element to site.xml.,19/Jul/13 19:37;psteitz;head element added to site.xml in r1504975,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException in MathArrays.linearCombination,MATH-1005,12657428,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,roman.werpachowski,roman.werpachowski,12/Jul/13 09:39,19/May/14 15:13,07/Apr/19 20:38,12/Jul/13 11:32,3.2,,,,,,,3.3,,,0,,,,,,,,"When MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line:

double prodHighNext = prodHigh[1];

linearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-07-12 11:32:18.969,,,false,,,,,,,,,,,,,,337650,,,Mon May 19 15:13:33 UTC 2014,,,,,,0|i1m8n3:,337973,,,,,,,,12/Jul/13 11:32;erans;Proposed fix committed in revision 1502516. Thanks for the report.,19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fraction specified with maxDenominator and a value very close to a simple fraction should not throw an overflow exception,MATH-996,12654547,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,tallison@apache.org,tallison@apache.org,24/Jun/13 17:21,19/May/14 15:13,07/Apr/19 20:38,31/Aug/13 19:43,3.1.1,,,,,,,3.3,,,0,,,,,,,,"An overflow exception is thrown when a Fraction is initialized with a maxDenominator from a double that is very close to a simple
fraction.  For example:

double d = 0.5000000001;
Fraction f = new Fraction(d, 10);

Patch with unit test on way.",,,,,,,,,,,,,,,,,,,,,24/Jun/13 17:50;tallison@apache.org;COMMONSMATH-996.patch;https://issues.apache.org/jira/secure/attachment/12589450/COMMONSMATH-996.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-08-31 19:43:25.491,,,false,,,,,,,,,,,,,,334824,,,Mon May 19 15:13:31 UTC 2014,,,,,,0|i1lr93:,335150,,,,,,,,24/Jun/13 17:50;tallison@apache.org;Simple patch attached.,24/Jun/13 17:51;tallison@apache.org;Found same behavior in BigFraction.  Same fix applied with simple unit test added.,"31/Aug/13 19:43;tn;Applied the patch with some modifications in r1519204.

Thanks for the report and patch!",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adaptive division of segments in Quadrature  Legendre-Gauss,MATH-995,12653966,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ajo.fod,ajo.fod,20/Jun/13 16:40,19/May/14 15:13,07/Apr/19 20:38,12/Jul/13 11:06,,,,,,,,3.3,,,0,,,,,,,,I think the existing Legendre-Gauss object fails for certain integrals. An example of failure and a solution that divides segments based on error is provided. Please let me know if I'm not using the Legendre-Gauss object correctly.,,,,,,,,,,,MATH-994,,,,,,,,,,04/Jul/13 13:54;erans;gaussian__sigma_1.png;https://issues.apache.org/jira/secure/attachment/12590891/gaussian__sigma_1.png,04/Jul/13 13:54;erans;gaussian__sigma_1000.png;https://issues.apache.org/jira/secure/attachment/12590892/gaussian__sigma_1000.png,04/Jul/13 13:54;erans;gaussian__sigma_1000_zoom.png;https://issues.apache.org/jira/secure/attachment/12590893/gaussian__sigma_1000_zoom.png,02/Jul/13 02:48;ajo.fod;patch-code;https://issues.apache.org/jira/secure/attachment/12590359/patch-code,02/Jul/13 00:32;ajo.fod;patch-code;https://issues.apache.org/jira/secure/attachment/12590343/patch-code,28/Jun/13 17:58;ajo.fod;patch-code;https://issues.apache.org/jira/secure/attachment/12590064/patch-code,28/Jun/13 17:58;ajo.fod;patch-test;https://issues.apache.org/jira/secure/attachment/12590065/patch-test,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,2013-06-27 21:29:41.312,,,false,,,,,,,,,,,,,,334243,,,Mon May 19 15:13:21 UTC 2014,,,,,,0|i1lnp3:,334569,,,,,,,,"20/Jun/13 22:51;ajo.fod;The attached files show how to use AdaptiveQuadrature and how the existing method fails. I've wrapped the IterativeLegendreGaussIntegrator in an InfiniteIntegral object to show a specific instance of a failure of the class. The AdaptiveQuadrature object is more efficient at solving problems (in function evaluation counts) because it selectively increases resolution where the error is high. 

This problem is not limited to infinite integrals because the underlying IterativeLegendreGaussIntegrator is integrating in the region [-1,1]. 

The attached solution uses 1st and 2nd order polynomials, but it can be generalized to a higher order polynomial solutions.","27/Jun/13 21:29;erans;Your ""InfiniteIntegral"" class instantiates an ""IterativeLegendreGaussIntegrator"" with only 3 sample points.
Could you test whether the problem persists when increasing the number of points?
",28/Jun/13 17:58;ajo.fod;These patches show the failure of the LGQ and the success of the Adaptive method. The adaptive method uses a fixed resolution quadrature (3) ... the concept itself is more general and work with higher orders. I've left system printline code to show failure/success ... feel free to remove/edit the code if you chose to accept it.,"28/Jun/13 20:22;erans;In revision 1497904, I've added a unit test.
One step at a time: can we first figure out _what_ exactly is going wrong?
",02/Jul/13 00:32;ajo.fod;Look at the digits in accuracy per evaluation for LGQ vs AQ,02/Jul/13 02:48;ajo.fod;An improvement in accuracy as suggested by Konstantin. This avoids the redundant computations in of the middle function values.,"04/Jul/13 13:54;erans;I attached plots that hint to why the ""IterativeGaussLegendreIntegrator"" is not adapted to this particular change of variable.
It appears to be a limitation of the algorithm but not a bug in the implementation.","04/Jul/13 14:30;erans;The ML contains a [lengthy discussion|http://markmail.org/message/vshi3gka7a7tdvbb] about this issue, that is drifting away from the original report about a bug in CM.

A warning has been added (revision 1499765) in the class's Javadoc to avoid the kind of usage that hits the limitation of this algorithm.

Adaptive strategies are nice features to have in CM; but such a request belongs to another report, and how to best implement them in CM should be discussed in a specific thread on the ""dev"" ML.

","11/Jul/13 18:21;ajo.fod;I like the plots ... does explain what is happening. 

However, if the IterativeGaussLegendreIntegrator is retained in the codebase, I'd suggest at the very least reporting on the non-convergence of the integration process or throwing an exception when the integration does'nt converge. Otherwise, any user of the code is likely to have to deal with expensive debugging session ... not good for confidence.","11/Jul/13 21:07;erans;bq. [...] if the IterativeGaussLegendreIntegrator is retained in the codebase [...]

It is not the only instance where CM contains an algorithm that has shortcomings or drawbacks. Not every condition that leads to (numerical) problems can be easily characterized, short of seeing that the result is incorrect.
[I recall a discussion about the ""Regula falsi"" root solver where the algorithm was stuck in an infinite loop; in that case it was possible to cheaply test for the condition and throw an exception.]

The Javadoc now draws attention that the algorithm is not 100% fool-proof.
","12/Jul/13 04:18;ajo.fod;Well, I suppose if there is nothing one can do, you might as well mark this issue closed.","12/Jul/13 11:04;erans;When some issue has been dealt with, it is first marked as ""resolved""; it is ""closed"" post release.","12/Jul/13 11:06;erans;Warning added (in r1499765) in the code documentation (cf. comment above).
",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GaussNewtonOptimizer convergence on singularity,MATH-993,12653421,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,choeger,choeger,18/Jun/13 11:40,19/May/14 15:13,07/Apr/19 20:38,28/Jun/13 10:23,3.2,,,,,,,3.3,,,0,,,,,,,,"I am (ab-)using the GaussNewtonOptimizer as a MultivariateFunctionSolver (as I could not find one in commons.math). Recently I stumbled upon an interesting behavior in one of my test cases: If a function is defined in a way that yields a minimum (a root in my case) at a singular point, the solver crashes. This is because of the following lines in doOptimize():

catch (SingularMatrixException e) {
                throw new ConvergenceException(LocalizedFormats.UNABLE_TO_SOLVE_SINGULAR_PROBLEM);
            }

I would propose to add a convergence check into the catch-phrase, so the solver returns the solution in that special case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-06-18 11:58:47.146,,,false,,,,,,,,,,,,,,333699,,,Mon May 19 15:13:23 UTC 2014,,,,,,0|i1lkcn:,334027,,,,,,,,"18/Jun/13 11:58;erans;Could you please provide a unit test showing the desired behaviour? Thanks.

bq. I would propose to add a convergence check into the catch-phrase, [...]

At first sight, it seems that would suffice to move the convergence check so that it happens _before_ the code block that can potentially raise the exception.
","28/Jun/13 10:23;erans;Proposed change committed in revision 1497713.
All unit tests still pass, but obviously your use-case is not tested.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow code to avoid OutOfRangeException,MATH-989,12650935,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ajo.fod,ajo.fod,04/Jun/13 17:11,19/May/14 15:13,07/Apr/19 20:38,10/Aug/13 01:13,,,,,,,,3.3,,,0,,,,,,,,"Exceptions are generally costlier than a test. In the BicubicSplineInterpolatingFunction, there is no way to determine if a given x is in domain of the interpolator. So, I suggest a function to allow this check.",,,,,,,,,,,,,,,,,,,,,04/Jun/13 17:21;ajo.fod;patch;https://issues.apache.org/jira/secure/attachment/12586130/patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-04 23:24:02.344,,,false,,,,,,,,,,,,,,331262,,,Mon May 19 15:13:30 UTC 2014,,,,,,0|i1l5db:,331595,,,,,,,,04/Jun/13 17:21;ajo.fod;This patch adds a rangeSign() function to allow a check to see if x is within range of the Interpolator. There probably are other objects/cases where such a check can save time.,"04/Jun/13 23:24;erans;bq. [...] there is no way to determine if a given x is in domain of the interpolator.

The domain is an argument to the constructor.
","05/Jun/13 13:36;ajo.fod;Sure Gilles, but wouldn't it require redundant storage of limits, when the class stores it as well?","05/Jun/13 14:16;erans;bq. but wouldn't it require redundant storage of limits, when the class stores it as well?

IMHO, it is beyond the role of this class to provide the proposed method.
From the POV of the interpolator, it is an error to pass values outside the range (hence raising an exception is the appropriate behaviour).
Since the caller knows the valid range, it can, and should, intercept what would be invalid calls to the library code (at the quite modest cost of storing 4 {{double}} variables).
","06/Jun/13 19:13;ajo.fod;I currently have a wrapper class around this class to intercept calls that are outside the interpolator's domain. But, why make the life of the API user's life any harder that it should be? Why not extend the role of the class to include flagging inappropriate calls when asked to? Is there a more ""efficient"" way to flag inappropriate calls? Perhaps there is some deeper phiolosophy there that I don't see. If so, please give me a link.","06/Jun/13 22:53;erans;What I don't understand is the intended usage. IOW, what kind of (non-buggy) application would try to perform ""inappropriate"" calls?
","07/Jun/13 10:25;erans;Also, please note that this function is two-dimensional; a single check is not enough to ensure a valid call.

We could add a query method like
{noformat}
  public boolean isValidPoint(double x, double y) {
    // ...
  }
{noformat}

Would that be fine for your use-case?
","07/Jun/13 16:30;ajo.fod;Yes, that would work well. Thanks!","10/Jun/13 21:24;erans;Method ""isValidPoint"" committed in revision 1491606.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when calling SubLine.intersection() with non-intersecting lines,MATH-988,12650507,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,andreashuber,andreashuber,02/Jun/13 13:06,19/May/14 15:13,07/Apr/19 20:38,03/Jun/13 07:17,3.0,3.1,3.1.1,3.2,,,,3.3,,,0,,,,,,,,"When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations.

The attached patch fixes both implementations and adds the required test cases.

",,,,,,,,,,,,,,,,,,,,,02/Jun/13 13:07;andreashuber;SubLineIntersection.patch;https://issues.apache.org/jira/secure/attachment/12585757/SubLineIntersection.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-06-03 07:17:01.331,,,false,,,,,,,,,,,,,,330834,,,Mon May 19 15:13:20 UTC 2014,,,,,,0|i1l2qf:,331167,,,,,,,,02/Jun/13 13:07;andreashuber;This patch fixes both implementations and adds test cases.,"03/Jun/13 07:17;luc;Fixed in subversion repository as of r1488866.
Patch applied with minor whitespace changes.

Thanks for the report and for the patch.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I don't think SimpleRegression works with n>1 dimensions of X.,MATH-986,12650266,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,,ajo.fod,ajo.fod,31/May/13 02:41,31/May/13 02:46,07/Apr/19 20:38,31/May/13 02:46,,,,,,,,,,,0,,,,,,,,"SimpleRegression only works with one dimension of X. With more dimensions it just uses the first dimension, so why have the method : 


public void addObservation(final double[] x,final double y) ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,330593,,,Fri May 31 02:46:33 UTC 2013,,,,,,0|i1l193:,330927,,,,,,,,31/May/13 02:46;ajo.fod;This is probably because SimpleRegression needs to ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BicubicSpline interpolation returns unexpected values (BicubicSplineInterpolator/BicubicSplineInterpolationFunction),MATH-985,12649848,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,jkool,jkool,29/May/13 06:01,16/Sep/14 18:30,07/Apr/19 20:38,26/Jan/14 15:58,3.1.1,,,,,,,3.3,,,0,,,,,,,,"I've been testing out the Tricubic spline functions, and have been getting some strange values.  I dug down a bit, and it seems like they start at the Bicubic level.  SplineInterpolator/PolynomialSplineFunction seem to be returning correct values.  I set up a block of data that increases linearly, so you'd expect the interpolated values to follow the same trend, except the values tend to overshoot to half the distance between knot points, and then undershoot for the remaining half.  Probably the easiest thing would be to show some tests.  First - 1D which works fine:

{code:title=SplineTest.java|borderStyle=solid}
import org.apache.commons.math3.analysis.interpolation.SplineInterpolator;
import org.apache.commons.math3.analysis.polynomials.PolynomialSplineFunction;

public class SplineTest {

	double[] x = new double[]{.52,.54,.56,.58,.6};
	double[] y = new double[]{76,77,78,79,80};
	
	public static void main(String[] args){
		SplineTest st = new SplineTest();
		st.go();
	}
	
	public void go(){
		SplineInterpolator si = new SplineInterpolator();
		PolynomialSplineFunction sf = si.interpolate(x, y);
		
		System.out.println(sf.value(0.52));
		System.out.println(sf.value(0.5225));
		System.out.println(sf.value(0.525));
		System.out.println(sf.value(0.5275));
		System.out.println(sf.value(0.53));
		System.out.println(sf.value(0.5325));
		System.out.println(sf.value(0.535));
		System.out.println(sf.value(0.5375));
		System.out.println(sf.value(0.54));
		System.out.println(sf.value(0.5425));
		System.out.println(sf.value(0.545));
		System.out.println(sf.value(0.5475));
		System.out.println(sf.value(0.55));
		System.out.println(sf.value(0.5525));
		System.out.println(sf.value(0.555));
		System.out.println(sf.value(0.5575));
		System.out.println(sf.value(0.56));
	}
}
{code}

and next, 2D which doesn't:

{code:title=BicubicSplineTest.java|borderStyle=solid}
import org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolatingFunction;
import org.apache.commons.math3.analysis.interpolation.BicubicSplineInterpolator;

public class BicubicSplineTest {

	double[] x = new double[]{0,1,2};
	double[] y = new double[]{.52,.54,.56,.58,.6};
	double[] v1 = new double[]{76,77,78,79,80};
	double[][] v2 = new double[][]{v1,v1,v1};
	
	public static void main(String[] args){
		BicubicSplineTest bt = new BicubicSplineTest();
		bt.go();
	}
	
	public void go(){
		BicubicSplineInterpolator bi = new BicubicSplineInterpolator();
		BicubicSplineInterpolatingFunction bf = bi.interpolate(x, y, v2);
		
		System.out.println(bf.value(1, 0.52));
		System.out.println(bf.value(1, 0.5225));
		System.out.println(bf.value(1, 0.525));
		System.out.println(bf.value(1, 0.5275));
		System.out.println(bf.value(1, 0.53));
		System.out.println(bf.value(1, 0.5325));
		System.out.println(bf.value(1, 0.535));
		System.out.println(bf.value(1, 0.5375));
		System.out.println(bf.value(1, 0.54));
		System.out.println(bf.value(1, 0.5425));
		System.out.println(bf.value(1, 0.545));
		System.out.println(bf.value(1, 0.5475));
		System.out.println(bf.value(1, 0.55));
		System.out.println(bf.value(1, 0.5525));
		System.out.println(bf.value(1, 0.555));
		System.out.println(bf.value(1, 0.5575));
		System.out.println(bf.value(1, 0.56));
	}
}
{code}

The data points increase from 76 to 80 in a linear way.  Incrementing by 1/8 the distance to the next point, the 1D spline returns:

76.0
76.125
76.25
76.375
76.5
76.625
76.75
76.875
77.0
77.125
77.25
77.375
77.5
77.625
77.75
77.875
78.0

The 2D spline returns:

76.0
80.14453124999996
80.84375000000003
79.24609375000007
76.50000000000003
73.75390625000007
72.15625000000001
72.85546874999996
76.99999999999997
81.14453124999993
81.84374999999997
80.24609375000006
77.50000000000003
74.75390625000009
73.15625000000004
73.85546874999997
78.0

Even though it's still effectively a 1D problem.  I'm not sure exactly what's causing it - maybe something when multiplying the coefficients, but thought I should flag it.","Windows 7 64 bit, 64 bit JVM",,,,,,,,,,,,,,,,MATH-1138,,,,31/May/13 10:57;erans;math985.png;https://issues.apache.org/jira/secure/attachment/12585581/math985.png,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-05-31 10:45:48.007,,,false,,,,,,,,,,,,,,330175,,,Mon May 19 15:13:33 UTC 2014,,,,,,0|i1kyof:,330509,,,,,,,,"31/May/13 10:45;erans;bq. [...] thought I should flag it.

Thanks a lot!

There was an indexing bug (I'm sorry for that) which is now fixed (revision 1488145).
","31/May/13 10:57;erans;I've attached a plot of your example, comparing the old and new code.

There are still discrepancies with the original data, but this happens ""only"" on the border of the interpolation range. I don't know whether this is expected or not.

I'm going to replace the unit tests that now fail; that they succeeded with a buggy code proves their uselessness :(. You are warmly welcome to suggest some tests that properly demonstrate expectations from this algorithm.
","31/May/13 11:36;erans;Unit test classes that currently contain ""@Ignore""d tests (which should be replaced):
* BicubicSplineInterpolatingFunctionTest
* TricubicSplineInterpolatorTest
* BicubicSplineInterpolatorTest
","31/May/13 14:45;erans;New unit tests in ""BicubicSplineInterpolatingFunctionTest"" (r1488256).

TBD is whether the tolerances for the tests to pass (i.e. the error on the interpolated values) are correct for this kind of interpolation.
","31/May/13 22:20;erans;New unit tests in ""BicubicSplineInterpolatorTest"" (r1488417).",22/Jan/14 22:46;tn;Can this issue be closed?,"22/Jan/14 22:50;jkool;I believe so - it's been a while since I checked it.  I'd say go ahead and close it.  If anything else crops up, I'll generate a new ticket.

Thanks!

Dr. Johnathan Kool 
Marine Ecologist  |  Coastal, Marine and Climate Change
Environmental Geoscience Division  |  GEOSCIENCE AUSTRALIA
","26/Jan/14 15:58;tn;Added changelog entry in r1561510.

For the remaining tests which are ignored atm I would suggest to create a separate ticket.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect (bugged) generating function getNextValue() in .random.EmpiricalDistribution,MATH-984,12649629,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,rtsvet,rtsvet,28/May/13 07:12,26/Dec/14 19:50,07/Apr/19 20:38,22/Jun/14 18:55,3.1.1,3.2,,,,,,3.4,,,0,,,,,,,,"The generating function getNextValue() in org.apache.commons.math3.random.EmpiricalDistribution
will generate wrong values for all Distributions that are single tailed or limited. For example Data which are resembling Exponential or Lognormal distributions.

The problem could be easily seen in code and tested.

In last version code
...
490               return getKernel(stats).sample();
...
it samples from Gaussian distribution to ""smooth"" in_the_bin. Obviously Gaussian Distribution is not limited and sometimes it does generates numbers outside the bin. In the case when it is the last bin it will generate wrong numbers. 

For example for empirical non-negative data it will generate negative rubbish.

  Additionally the proposed algorithm boldly returns only the mean value of the bin in case of one value! This last makes the generating function unusable for heavy tailed distributions with small number of values. (for example computer network traffic)

On the last place usage of Gaussian soothing in the bin will change greatly some empirical distribution properties.

The proposed method should be reworked to be applicable for real data which have often limited ranges. (either non-negative or both sides limited)


",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-05-28 17:29:11.935,,,false,,,,,,,,,,,,,,329956,,,Sun Jun 22 18:55:22 UTC 2014,,,,,,0|i1kxbz:,330291,,,,,,,,"28/May/13 17:29;psteitz;Ack on the first issue (deviates should be constrained to come from the target bin).  Thanks for raising this issue.  A workaround available since version 3.2 is to provide a custom kernel by subclassing and overriding EmpiricalDistribution's getKernel method.  Patches are welcome to fix the issue with the current code.  The private method, kB, can be used to get the mass of a bin under its kernel.  In 4.0, we may want to consider a more direct way to plug in an alternative kernel.

Sorry if I am being dense, but I don't understand the second issue.  If there is no variation among values in a bin, returning their common value makes sense to me.","29/May/13 07:22;rtsvet;For the second issue:
Consider long tailed distribution as shown on http://en.wikipedia.org/wiki/Long_tail  (In case of network traffic. The biggest 90% data volume comes comes from less then 10% of connections.) 
In this case we have extremely wide spread __important__ values with only ___single_ occurrences.
If we want to generate similar variable (for ex. larger sample) we'll get fixed values for all this bins in the long tail.
It signifies 10% of generated values will be __fixed_ values - their respective bin means!


Last Issue: I would question usage of Gaussian Kernel at all. Without having a mathematical prove, I nevertheless suppose it could disturb parameters of generation if we have non Gaussian empirical data. (for ex. Pareto, Tweedie, ..)

Why we don't stick with triangular or uniform distribution as default for Kernel within the bean?","29/May/13 19:11;psteitz;Thanks, I get the second problem now.  To really address that issue, I think we would need to depart from the current simple equal-sized bins model.  I have thought before about either introducing a new class or a config option for EmpiricalDistribution that supported alternative binning structures, such as:

1. equiprobable bins (so bin size is not constant)
2. variable bin sizes (break the total range into subranges, allow a number of fixed-size bins to be specified for each range, so grid could become very fine in densely packed subranges).

Regarding the default kernel choice, in the absence of any information about the within-bin distributions, I would expect the (correctly truncated) Gaussian smoother to perform better than uniform or triangular. I don't have a proof of this statement either; but I will see if I can hunt down some references, starting with [1], referenced in the class javadoc as what the current implementation is based on.  

As [1] states, heavy tails create problems for this approach and in some cases an alternative to the Gaussian kernel may work better.  This could actually be tested on a case basis by comparing probabilities computed using the Distribution methods now implemented in the class with ""true"" empirical probabilities from the raw data.  Some experiments doing this with different kernels and data would be interesting to look at.

[1] http://ned.ipac.caltech.edu/level5/March02/Silverman/Silver2_4.html
","31/May/13 11:59;rtsvet;Simplest way would be to use  inverse CDF (quantile funct) with some uniformly distributed input. But unfortunately seems it's also having Kernel calculations problems. 
I'm getting NotStrictlyPositiveException from EmpiricalDistribution.getKernel l(EmpiricalDistribution.java:846) )

 
I thought of looking in what algorithms R is using. 
http://en.wikibooks.org/wiki/R_Programming/Random_Number_Generation","31/May/13 19:43;psteitz;That should work.  Looks like you have hit a new bug, which should be opened as a separate issue if you don't mind doing that.  What I suspect is going on is that your data has singleton bins, which results in zero variance within bin.  The getKernel method tries to create a NormalDistribution instance using the bin stats.  This throws NotStrictlyPositiveException if the standard deviation parameter is not strictly positive.  This is part of the reason that the singleton check is there in getNextValue.  I forgot to account for this case in inverseCumulativeProbability (added in 3.2).  A unit test demonstrating the bug would be most appreciated.

I think it would probably be a little more efficient though to keep the direct implementation of getNextValue as it is now, but just fix the bug.","31/May/13 19:49;psteitz;One more comment on getNextValue implementation.  If we want to just do straight inversion-based sampling, that is available for free from the superclass, AbstractRealDistribution.  To use that, we would just need to reverse the roles of sample() and getNextValue - i.e., have getNextValue call sample() and drop the override of sample() in EmpiricalDistribution.","03/Jun/13 06:54;rtsvet;To generate some Exception on attempt to calculate St.Dev. from single observation is correct. It’s just that the _Exception Text_ generated by getKernel() is not clearly pointing the cause. An average user should dig and search to get to to the ""real"" cause. But as getKernel() should deliver the ""kernel"" and not proliferate his chosen internal method restrictions outside, better would be _for single observation_ to deliver uniformly distributed value. (and perhaps warning).

generate() within EmpiricalDistribution could be ""fixed"" easily if it uses truncated Gaussian on the end bins. 

*So would propose 2 code changes:*

1. Add generate(double min, double max) function. Where *min* and *max* should indicate the hard limits of generation. They should be checked if the conform to input data. It means *min* should not be greater than the minimal empirically observed value, and *max* should be no less then the biggest empirical value. Then the new generate function should use truncated Gaussian on the edges. (open if one implements it through additional getKernel(limits.. ) function )

2. Change getKernel() to deliver uniformly distributed value on single observation. (and perhaps warning)
 

The first change does not touch old code. So, it's a practically no risk and we retain 100% code compatibility.
The second change is also low risk and is compatible with old code.

As a result we have smooth and correct generation and positively fixed  inverse CDF.","04/Jun/13 14:14;psteitz;Thanks, Radoslav.  I agree with your point 2 and after looking at the code some more, I am going to retract my comment above about direct implementation of getNextValue being more efficient.  I think the simplest and best fix for both of these problems is to fix getKernel to return a uniform distribution on one value for singleton bins, drop the implementation of sample() and have getNextValue delegate to the parent's (inversion-based) sample() implementation.  The probability methods basically truncate the bin kernels now.  The problem is in the direct implementation of sampling.",20/Feb/14 09:20;luc;Are there any progress on this issue?,"20/Feb/14 14:44;psteitz;Yes, I am working on it.  Thanks for the nudge.",22/Jun/14 18:55;psteitz;Fixed in r1604639.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LinearInterpolator Misconception,MATH-972,12646213,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Not A Problem,,oskar_hertwig,oskar_hertwig,06/May/13 13:25,10/Aug/13 22:21,07/Apr/19 20:38,10/Aug/13 22:21,3.2,,,,,,,,,,0,,,,,,,,"The method interpolate() of the class LinearInterpolator return a polynomialSplineFunction althought a piecewiseContinuousFunction have to be returned.

This cause a bug. Indeed when the interpolated serie is localy constant the linear interpolation should lead to constant value between this points. This is not possible when the returned object is a PolynomialSplineFunction
",All,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,07/May/13 14:42;oskar_hertwig;PiecewiseLinearFunction.java;https://issues.apache.org/jira/secure/attachment/12582100/PiecewiseLinearFunction.java,07/May/13 14:42;oskar_hertwig;Test.java;https://issues.apache.org/jira/secure/attachment/12582099/Test.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-05-06 14:28:45.558,,,false,,,,,,,,,,,,,,326571,,,Tue May 07 15:29:31 UTC 2013,,,,,,0|i1kc8v:,326916,,,,,,,,"06/May/13 14:28;erans;Could you please upload a unit test showing the problem?
","07/May/13 14:40;oskar_hertwig;Hi Gilles,

First of all tanks for your quick response.

I went a little bit too fast in my bug report and it appears I screw up and your code is ok. (The code is in test.java)

Nevertheless I am stilling interrogating myself about the object returned by Linear Interpolator. Indeed, the function returned by a linear interpolator should not be derivable on the entire domain (on each knots points) although a Polynomial Spline Function is always derivable inside the definition domain.

Consequently your solution allows using the Derivative Method with no restriction although exceptions should be raised in some points. (After test your code returns the derivative number at right)

So I am asking to you if it wouldn’t be better that linear Interpolator returns PiecewiseLinearFunction (I add the code is in PiecewiseLinearFunction.java) which implements UnivariateFunction but not DifferentiableUnivariateFunction. In another hand I am aware that my solution doesn’t allow the use of DifferentiableUnivariateSolver.

If you find my suggestion is too purist. I have no problem for closing the ticket.

Kinds regard.









","07/May/13 15:14;erans;bq. [...] about the object returned by LinearInterpolator [...] should not be derivable on the entire domain [...]

Strictly speaking it is not, since the ""interpolate"" method in ""UnivariateInterpolator"" is defined to return a ""UnivariateFunction"".
The actual object used is an ""implementation detail"" which users should not rely on.

At first sight, your ""PiecewiseLinearFunction"" is just another name for a special case of our ""PolynomialSplineFunction"" (where all its components have degree 1). IMHO, it doesn't warrant the code duplication.

It seemed that the name ""PolynomialSplineFunction"" is at the origin of the confusion.
","07/May/13 15:29;oskar_hertwig;Ok, That's clear. Thank you for your time. The issue is closed for me.



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath.atanh wrong?,MATH-971,12645862,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,nezda,nezda,02/May/13 19:40,03/May/13 13:40,07/Apr/19 20:38,03/May/13 13:40,3.1.1,,,,,,,,,,0,,,,,,,,"When I compare the values computed using FastMath.atanh with those produced by similar C++ code (using standard math.h), the results are different.  The C++ code agrees with WolframAlpha (e.g., [http://www.wolframalpha.com/input/?i=atanh(1)]), so I think it is probably right.

||Input||Wolfram-Alpha||C++ / math.h||FastMath.atanh||
|0.1 |0.10033534773107558|0.100335|0.09966865249116204|
|0.2 |0.2027325540540822| 0.202733|0.19739555984988078|
|0.3 |0.30951960420311175|0.30952 |0.2914567944778671|
|0.4 |0.4236489301936|    0.423649|0.3805063771123649|
|0.5 |0.5493061443340548| 0.549306|0.4636476090008061|
|0.6 |0.6931471805599453| 0.693147|0.5404195002705842|
|0.7 |0.8673005276940531| 0.867301|0.6107259643892086|
|0.8 |1.0986122886681098| 1.09861 |0.6747409422235527|
|0.9 |1.4722194895832204| 1.47222 |0.7328151017865066|
|1	 |Infinity|           inf     |0.7853981633974483|
|1.1 |1.522 and 1.571i|   -nan    |0.8329812666744317|
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-05-03 11:22:39.989,,,false,,,,,,,,,,,,,,326221,,,Fri May 03 13:40:40 UTC 2013,,,,,,0|i1ka33:,326566,,,,,,,,"03/May/13 11:22;erans;Here is what I obtain:

----+--------------------
 x  |  FastMath.atanh(x)
----+--------------------
0.0 | 0.0
0.1 | 0.10033534773107558
0.2 | 0.2027325540540821
0.3 | 0.3095196042031118
0.4 | 0.42364893019360184
0.5 | 0.5493061443340549
0.6 | 0.6931471805599454
0.7 | 0.8673005276940534
0.8 | 1.0986122886681098
0.9 | 1.4722194895832204
1.0 | Infinity
----+---------------------

Please post your code.
",03/May/13 13:40;nezda;What a spectacular public fail - I was calling {{FastMath.atan\(x\)}} instead of {{FastMath.atanh\(x\)}} - you'd think I would've double-checked that!?  I apologize for wasting your time Gilles.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistency between the init method and the other ones in step and event handlers,MATH-965,12641364,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,08/Apr/13 12:52,19/May/14 15:13,07/Apr/19 20:38,08/Apr/13 14:42,3.2,,,,,,,3.3,,,0,,,,,,,,"The EventHandler and StepHandler interfaces allow user code to be called by ODE integrators at some points. Both interfaces provide an ""init"" method which get the initial state as a double array ""y0"".
The ""g"" and ""eventOccurred"" methods also get a double array ""y"" corresponding to current state. The size of the array in these two methods correspond only to the primary state whereas in the ""init"" methods it holds both the primary state and the secondary states.

It would be better to always provide the complete state (primary and secondary) in all methods, so users can also trigger events based on secondary states.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,321780,,,Mon May 19 15:13:24 UTC 2014,,,,,,0|i1jipb:,322125,,,,,,,,08/Apr/13 14:42;luc;Fixed in subversion repository as of r1465654.,19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Interface ""RandomGenerator"" is missing ""nextLong(long)"" method",MATH-963,12640779,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Won't Fix,,erans,erans,04/Apr/13 14:34,05/Apr/13 21:40,07/Apr/19 20:38,05/Apr/13 21:40,3.2,,,,,,,,,,0,api-change,,,,,,,"There is a method ""nextInt(int)"" but not ""nextLong(long)"".

The ""BitsStreamGenerator"" class already implements it.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-04-04 17:14:02.575,,,false,,,,,,,,,,,,,,321238,,,Fri Apr 05 10:57:55 UTC 2013,,,,,,0|i1jfdb:,321583,,,,,,,,"04/Apr/13 17:14;psteitz;The reason the method is missing from the interface is that it is not present in j.u.Random and RandomGenerator is supposed to be a straight replacement for j.u.Random.  It might be better to just add it to the abstract class, with a decent impl like what exists in the abstract bitstream generator.  We should probably also replace the default impl of nextInt(int) with something better if we can figure this out.  If we can find a good way to provide a good default impl just based on nextBytes, then I guess I am +0 on adding this.  If not, I don't think it is a good idea to force all RandomGenerators to implement something or inherit a not so great default.","04/Apr/13 22:21;erans;bq. [...] RandomGenerator is supposed to be a straight replacement for j.u.Random.

In what sense?
""j.u.Random"" not being an interface (CM's ""RandomGenerator"" is one, and thus cannot extend the former), a user code cannot pass an instance of ""RandomGenerator"" in place of ""j.u.Random"".

If the goal is for ""RandomGenerator"" to provide identically named methods, this is fine but nothing forbids CM to be more complete and add other methods.

If the goal (for whatever reason) is to exactly mimic the contents of ""j.u.Random"", I would change this issue to ask for the removal of the additional ""setSeed"" methods that exist in ""RandomGenerator"" but not in ""j.u.Random"".

bq. better to just add it to the abstract class

Then, what about removing the interface (i.e. merging) and keep only the abstract class?
","05/Apr/13 02:04;psteitz;While you can't use a RandomGenerator instance directly to replace a Random, you can use a RandomAdaptor.  That is really what led to tearing off the interface from Random.  I use this so would not like to drop the interface.

I am OK with adding the method (in 4.0) if someone can come up with a good default impl to be added to AbstractRandomGenerator.  ""Good"" means similar to what AbstractBitstreamGenerator does.","05/Apr/13 10:19;erans;bq. [...] you can use a RandomAdaptor. That is really what led to tearing off the interface from Random.

If, as it looks like, ""RandomAdaptor"" is a bridge from ""RandomGenerator"" to ""j.u.Random"", it does not need to implement the ""RandomGenerator"" API.
This is what a bridge is for: change from one API to another. Hence, it is also not necessary to mimic the target API.

If ""RandomGenerator"" were an abstract class, you'd be able to use it in the same way with ""RandomAdaptor"".

It seems that the purpose of ""RandomAdaptor"" being also a ""RandomGenerator"" is for the _same_ object reference to be passed both to code that expects a ""j.u.Random"" and to code that expects a ""RandomGenerator"".
I find it better to not mix the two types in this way.
","05/Apr/13 10:57;erans;bq. [...] good default impl to be added to AbstractRandomGenerator. ""Good"" means similar to what [...]BitstreamGenerator does.

What about using the same? :)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vector3DFormat.parse does not ignore whitespace,MATH-962,12640536,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,leonfrench,leonfrench,03/Apr/13 15:44,19/May/14 15:13,07/Apr/19 20:38,14/Apr/13 16:19,3.1.1,,,,,,,3.3,,,0,,,,,,,,"Vector3DFormat notes it ingores whitespace in the javadoc but in the below example it does not:
	Vector3DFormat vf = new Vector3DFormat(""("", "")"", "","");
	System.out.println(vf.parse(""(1, 2, 3)"")); //prints {1; 2; 3}
	System.out.println(vf.parse(""(1,2,3)""));   //prints null
","Macosxm, Java 1.6.0",86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-04-07 20:33:06.776,,,false,,,,,,,,,,,,,,320996,,,Mon May 19 15:13:25 UTC 2014,,,,,,0|i1jdun:,321337,,,,,,,,"07/Apr/13 20:33;tn;This is because internally the number format for the default locale is used in your test-case (without specifying another number format). As in this case grouping is activated (with ',' being the grouping character, e.g. for US locale), the string ""1, 2, 3"" is interpreted as 123. As there are no other values but 3 are expected, the return value is null.

To use the Vector3DFormat with ',' as separator, you should explicitly disable grouping like this:

{noformat}
        NumberFormat nf = NumberFormat.getInstance(Locale.getDefault());
        nf.setGroupingUsed(false);
        Vector3DFormat vf = new Vector3DFormat(""("", "")"", "","", nf);
        System.out.println(vf.parse(""(1, 2, 3)"")); // prints {1; 2; 3}
        System.out.println(vf.parse(""(1,2,3)"")); // prints null
{noformat}

Afaik, this is one of the reasons why we use ';' as default separator for these vector formats, as this one does not interfere with the usual grouping characters for various locale settings.

I am not sure this is really a bug, but the error handling could be improved.","08/Apr/13 12:44;leonfrench;Oh, I see. Thanks for clearing that up.

I'm not sure how to make that easier for the next person. A note to the API documentation may help. ","14/Apr/13 16:19;tn;Added clarification to the javadoc in r1467801.

Thanks for the report.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Numerous array dimensions problems with secondary equations in ODE,MATH-961,12640314,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,luc,luc,luc,02/Apr/13 14:35,07/Apr/13 09:22,07/Apr/19 20:38,02/Apr/13 19:06,3.1.1,,,,,,,3.2,,,0,,,,,,,,"When secondary equations are used in differential equations, there are several places where the array dimensions are not computed properly, due to a difference between the primary equation dimension and the sum of this primary dimension plus the remaining secondary equations.

A typical case is with multi-step integrators, where the starter integrator is not configured properly even when the routine integrator is configured properly (some information is lost between the two integrators).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,320777,,,Sun Apr 07 09:22:00 UTC 2013,,,,,,0|i1jchz:,321118,,,,,,,,02/Apr/13 19:06;luc;Fixed in subversion repository as of r1463684.,07/Apr/13 09:22;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ODE integrator do not handle some additional equations properly,MATH-960,12640304,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,02/Apr/13 13:55,07/Apr/13 09:21,07/Apr/19 20:38,02/Apr/13 19:05,3.1.1,,,,,,,3.2,,,0,,,,,,,,"Differential equations may be split as a primary equation and several secondary equations. In some cases, the secondary equations may have an effect on he primary derivatives. This can be handled using the method computeDerivatives(double t, double[] primary, double[] primaryDot, double[] secondary, double[] secondaryDot) from the SecondaryEquations interface.

However, when the secondary equations updateds the primaryDot array using this method, the update is ignored.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,320767,,,Sun Apr 07 09:21:53 UTC 2013,,,,,,0|i1jcfr:,321108,,,,,,,,02/Apr/13 19:05;luc;Fixed in subversion repository as of r1463680.,07/Apr/13 09:21;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use analytical function for UniformRealDistribution.inverseCumulativeProbability,MATH-957,12639427,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,evanward1,evanward1,27/Mar/13 19:18,07/Apr/13 09:21,07/Apr/19 20:38,28/Mar/13 10:25,3.2,,,,,,,3.2,,,0,,,,,,,,"The inverse CDF is currently solved by a root finding function. It would be much simpler (and faster) to use the analytical expression. This would save the user from having to set the inverseCumAccuracy correctly.

I've attached a patch that implements this.",,,,,,,,,,,,,,,,,,,,,27/Mar/13 19:20;evanward1;uniform.patch;https://issues.apache.org/jira/secure/attachment/12575749/uniform.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-28 07:02:43.363,,,false,,,,,,,,,,,,,,319897,,,Sun Apr 07 09:21:45 UTC 2013,,,,,,0|i1j72f:,320238,,,,,,,,27/Mar/13 19:20;evanward1;Implementation and test case attached.,"28/Mar/13 07:02;dhendriks;bq. Implementation and test case attached.

At line 63: ""new  Well19937c()"": there are two spaces after 'new'... Line 79 as well.
","28/Mar/13 10:25;luc;Fixed in subversion repository as of r1462018.

Thanks for the report and for the patch.",07/Apr/13 09:21;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EmpiricalDistributionTest fails if path contains spaces,MATH-955,12639082,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,evanward1,evanward1,26/Mar/13 12:26,07/Apr/13 09:21,07/Apr/19 20:38,26/Mar/13 15:12,3.2,,,,,,,3.2,,,0,,,,,,,,"testLoad fails at loading the file because URL replaces the spaces with ""%20""

I've attached a patch that converts the URL to a URI before passing it to File. ( see http://stackoverflow.com/questions/8928661/how-to-avoid-getting-url-encoded-paths-from-url-getfile )

Thanks in advance,
Evan",,,,,,,,,,,,,,,,,,,,,26/Mar/13 12:26;evanward1;empiricalTest.patch;https://issues.apache.org/jira/secure/attachment/12575510/empiricalTest.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-26 12:59:02.456,,,false,,,,,,,,,,,,,,319552,,,Sun Apr 07 09:21:30 UTC 2013,,,,,,0|i1j4xr:,319893,,,,,,,,"26/Mar/13 12:59;erans;I think that the ""load(URL)"" method should be deprecated.

The user could easily use ""load(File)"":
{noformat}
load(new File(url.toURI());
{noformat}
","26/Mar/13 15:12;luc;Fixed in subversion repository as of r1461172.

Thanks for the report and for the patch.",07/Apr/13 09:21;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
event state not updated if an unrelated event triggers a RESET_STATE during ODE integration,MATH-950,12637756,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,luc,luc,luc,19/Mar/13 13:13,07/Apr/13 09:12,07/Apr/19 20:38,19/Mar/13 14:11,3.1.1,,,,,,,3.2,,,0,,,,,,,,"When an ODE solver manages several different event types, there are some unwanted side effects.

If one event handler asks for a RESET_STATE (for integration state) when its eventOccurred method is called, the other event handlers that did not trigger an event in the same step are not updated correctly, due to an early return.

As a result, when the next step is processed with a reset integration state, the forgotten event still refer to the start date of the previous state. This implies that when these event handlers will be checked for In some cases, the function defining an event g(double t, double[] y) is called with state parameters y that are completely wrong. In one case when the y array should have contained values between -1 and +1, one function call got values up to 1.0e20.

The attached file reproduces the problem.
",,,,,,,,,,,,,,,,,,,,,19/Mar/13 13:13;luc;wrong-event-scheduling.patch;https://issues.apache.org/jira/secure/attachment/12574346/wrong-event-scheduling.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,318236,,,Sun Apr 07 09:12:56 UTC 2013,,,,,,0|i1iwtb:,318577,,,,,,,,19/Mar/13 13:13;luc;Test case reproducing the bug.,19/Mar/13 14:11;luc;Fixed in subversion repository as of r1458294.,07/Apr/13 09:12;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LevenbergMarquardtOptimizer reports 0 iterations,MATH-949,12637239,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,evanward1,evanward1,15/Mar/13 18:11,07/Apr/13 09:20,07/Apr/19 20:38,20/Mar/13 11:38,3.2,,,,,,,3.2,,,0,,,,,,,,"The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount()

I've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.

{noformat}
    @Test
    public void testGetIterations() {
        // setup
        LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();

        // action
        otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),
                new Weight(new double[] { 1 }), new InitialGuess(
                        new double[] { 3 }), new ModelFunction(
                        new MultivariateVectorFunction() {
                            @Override
                            public double[] value(double[] point)
                                    throws IllegalArgumentException {
                                return new double[] { FastMath.pow(point[0], 4) };
                            }
                        }), new ModelFunctionJacobian(
                        new MultivariateMatrixFunction() {
                            @Override
                            public double[][] value(double[] point)
                                    throws IllegalArgumentException {
                                return new double[][] { { 0.25 * FastMath.pow(
                                        point[0], 3) } };
                            }
                        }));

        // verify
        assertThat(otim.getEvaluations(), greaterThan(1));
        assertThat(otim.getIterations(), greaterThan(1));
    }

{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-16 14:47:46.652,,,false,,,,,,,,,,,,,,317731,,,Sun Apr 07 09:20:57 UTC 2013,,,,,,0|i1itp3:,318072,,,,,,,,"16/Mar/13 14:47;erans;Actually, the number of iterations is used in the ""optim.linear"" package while the number of evaluations is used in the ""optim.nonlinear"" package. The counters were both moved to a unique base class in order to reduce the amount of code duplication.

It seems indeed that now we should indeed update the iterations counter in every optimizer implementation.

We could also take both limits (evaluations and iterations) into account but I'm not sure that would be useful. I think that it would be confusing; some time ago, we agreed that the number of evaluations is a better measure of an optimization algorithm (at least when the evaluation is more costly than the optimization's bookkeeping).
","19/Mar/13 14:53;erans;Iteration counter is now incremented (revision 1458323). Could you please check that the behaviour is what you'd expect?


","19/Mar/13 17:21;evanward1;I can confirm that I get reasonable values now. For the above test case I get 24 iterations and 42 evaluations. I'm not an expert, so I don't know if these are the right numbers...

Thanks for the fast fix!",07/Apr/13 09:20;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SingularValueDecomposition constructor blocks with NaN in the matrix,MATH-947,12637012,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,mafa,mafa,14/Mar/13 13:00,07/Apr/13 09:20,07/Apr/19 20:38,15/Mar/13 12:37,3.0,,,,,,,3.2,,,0,,,,,,,,"//Create coefficient (A) Matrix
        RealMatrix coefficients =
            new Array2DRowRealMatrix(coeffs);

               SingularValueDecomposition svd = new SingularValueDecomposition(coefficients);

//When coeffs is a 2x2 matrix with all elements Nan, the constructor blocks indefinitely /w 100% CPU usage","jdk1.7.0_13 32bit, Netbeans 7.2.1, Win7 x64",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-14 13:45:21.01,,,false,,,,,,,,,,,,,,317504,,,Sun Apr 07 09:20:41 UTC 2013,,,,,,0|i1isan:,317845,,,,,,,,"14/Mar/13 13:45;erans;Obviously, you can't hope to get a useful result from such an input.

There is probably a loop with an exit condition that's never satisfied...
One way to avoid it would be to check the validity of the input. This could be done either in Commons Math or in the user's code.","15/Mar/13 12:37;luc;Fixed in subversion repository as of r1456931.

Thanks for the report.",07/Apr/13 09:20;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type,MATH-942,12636190,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,wydrych,wydrych,09/Mar/13 15:05,07/Apr/13 09:20,07/Apr/19 20:38,10/Mar/13 12:02,,,,,,,,3.2,,,0,,,,,,,,"Creating an array with {{Array.newInstance(singletons.get(0).getClass(), sampleSize)}} in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:
* {{singleons.get(0)}} is of type T1, an sub-class of T, and
* {{DiscreteDistribution.sample()}} returns an object which is of type T, but not of type T1.

To reproduce:
{code}
List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>();
list.add(new Pair<Object, Double>(new Object() {}, new Double(0)));
list.add(new Pair<Object, Double>(new Object() {}, new Double(1)));
new DiscreteDistribution<Object>(list).sample(1);
{code}

Attaching a patch.",,,,,,,,,,,,,,,,,,,,,09/Mar/13 22:17;wydrych;DiscreteDistribution.java.patch;https://issues.apache.org/jira/secure/attachment/12572933/DiscreteDistribution.java.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-09 15:41:46.689,,,false,,,,,,,,,,,,,,316682,,,Sun Apr 07 09:20:25 UTC 2013,,,,,,0|i1in8f:,317024,,,,,,,,"09/Mar/13 15:41;luc;We have encountered the same kind of issue with Field implementations. For this reason, we have added the method:

{code}
Class<? extends FieldElement<T>> getRuntimeClass();
{code}

in the interface, so appropriate objects can be created (this is what is used in the buildArray method in MathArrays). However, we cannot do the same here because we want to allow any class in the distribution, not only implementations of one of our interfaces.

Passing the runtime class as an argument to the distribution would work, but seems also cumbersome.

I am a bit reluctant to use Object[] as your patch, though. Are you sure it would always work? I know type erasure occurs, but with your patch, we mainly apply it ourselves. Is it safe?","09/Mar/13 22:17;wydrych;bq. I am a bit reluctant to use Object[] as your patch, though. Are you sure it would always work? I know type erasure occurs, but with your patch, we mainly apply it ourselves. Is it safe?

I did some reading, scanning other generics classes code, and testing. It seems that there are two proper ways to do it:
# return {{Object[]}}, or
# return {{List<T>}}.

I suggest returning {{Object[]}}, like {{ArrayList.toArray()}}. I've attached an updated patch.

Then, the following code will work:

{code}
public class X {

    public static void main(String[] args) {
        List<Pair<X, Double>> list = new ArrayList<Pair<X, Double>>();
        list.add(new Pair<X, Double>(new X(), new Double(1)));
        X[] xarr;
        Object[] oarr;
        try {
            xarr = (X[]) new DiscreteDistribution<X>(list).sample(1);
            throw new RuntimeException(""Expected ClassCastException"");
        } catch (ClassCastException e) {
        }
        oarr = new DiscreteDistribution<X>(list).sample(1);
    }
}
{code}","10/Mar/13 12:02;luc;Fixed in subversion repository as of r1454840.

Thanks for the report and for the patch.","10/Mar/13 12:41;tn;What do you think about the following extension:

{noformat}
    /**
     * Generate a random sample from the distribution.
     * <p>
     * If the requested samples fit in the specified array, it is returned
     * therein. Otherwise, a new array is allocated with the runtime type of
     * the specified array and the size of this collection.
     *
     * @param sampleSize the number of random values to generate.
     * @param array the array to populate.
     * @return an array representing the random sample.
     * @throws NotStrictlyPositiveException if {@code sampleSize} is not
     * positive.
     * @throws IllegalArgumentException if {@code array} is null
     */
    public T[] sample(int sampleSize, final T[] array) throws NotStrictlyPositiveException {
        if (sampleSize <= 0) {
            throw new NotStrictlyPositiveException(LocalizedFormats.NUMBER_OF_SAMPLES,
                    sampleSize);
        }

        if (array == null) {
            throw new IllegalArgumentException(""array may not be null"");
        }

        T[] out;
        if (array.length < sampleSize) {
            @SuppressWarnings(""unchecked"") // safe as both are of type T
            final T[] unchecked = (T[]) Array.newInstance(array.getClass().getComponentType(), sampleSize);
            out = unchecked;
        } else {
            out = array;
        }

        for (int i = 0; i < sampleSize; i++) {
            out[i] = sample();
        }

        return out;

    }
{noformat}

Similar to the toArray(T[]) methods of Collections, a user could already provide an array which is used if the sample size fits into it, otherwise it is created in a type-safe way, and the return-type is of T[] so that a user does not have to do a cast.",10/Mar/13 12:43;tn;The other sample(int sampleSize) method could stay there too. So a user has the choice.,10/Mar/13 12:50;luc;This seems a good idea to me.,07/Apr/13 09:20;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractExtendedFieldElementTest should be renamed to ExtendedFieldElementAbstractTest,MATH-940,12635538,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Blocker,Fixed,,wydrych,wydrych,06/Mar/13 10:10,07/Apr/13 09:12,07/Apr/19 20:38,06/Mar/13 11:00,,,,,,,,3.2,,,0,,,,,,,,"According to build.xml, abstract tests should be placed in files **/*AbstractTest.java. Currently (rev 1453206), building jar crashes on AbstractExtendedFieldElementTest. It should be renamed to ExtendedFieldElementAbstractTest.",,,,,,,,,,,,,,,,,,,,,06/Mar/13 10:11;wydrych;AbstractExtendedFieldElementTest.patch;https://issues.apache.org/jira/secure/attachment/12572300/AbstractExtendedFieldElementTest.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-06 11:00:32.069,,,false,,,,,,,,,,,,,,316031,,,Sun Apr 07 09:12:29 UTC 2013,,,,,,0|i1ij7z:,316374,,,,,,,,"06/Mar/13 10:11;wydrych;Apply the patch. Then, rename the file src/test/java/org/apache/commons/math3/AbstractExtendedFieldElementTest.java to src/test/java/org/apache/commons/math3/ExtendedFieldElementAbstractTest.java","06/Mar/13 11:00;luc;Fixed in subversion repository as of r1453287.

Thanks for the report and for the patch.",07/Apr/13 09:12;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stat.correlation.Covariance should allow one-column matrices,MATH-939,12635523,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,wydrych,wydrych,06/Mar/13 09:02,07/Apr/13 09:20,07/Apr/19 20:38,06/Mar/13 10:31,,,,,,,,3.2,,,0,,,,,,,,"Currently (rev 1453206), passing 1-by-M matrix to the Covariance constructor throws IllegalArgumentException. For consistency, the Covariance class should work for a single-column matrix (i.e., for a N-dimensional random variable with N=1) and it should return 1-by-1 covariance matrix with the variable's variance in its only element.",,,,,,,,,,,,,,,,,,,,,06/Mar/13 09:05;wydrych;covariance.patch;https://issues.apache.org/jira/secure/attachment/12572290/covariance.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-06 10:31:41.412,,,false,,,,,,,,,,,,,,316016,,,Sun Apr 07 09:20:07 UTC 2013,,,,,,0|i1ij4n:,316359,,,,,,,,"06/Mar/13 10:31;luc;Fixed in subversion repository as of r1453271.

The patch has been applied with minor changes (a test case with a nominal value has been added).

Thanks for reporting the issue and providing a patch.",07/Apr/13 09:20;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Line.revert() is imprecise,MATH-938,12635361,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,evanward1,evanward1,05/Mar/13 16:10,07/Apr/13 09:19,07/Apr/19 20:38,06/Mar/13 09:00,3.2,,,,,,,3.2,,,0,,,,,,,,"Line.revert() only maintains ~10 digits for the direction. This becomes an issue when the line's position is evaluated far from the origin. A simple fix would be to use Vector3D.negate() for the direction.

Also, is there a reason why Line is not immutable? It is just comprised of two vectors.",,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-06 09:00:43.616,,,false,,,,,,,,,,,,,,315854,,,Sun Apr 07 09:19:58 UTC 2013,,,,,,0|i1ii4n:,316197,,,,,,,,"05/Mar/13 16:16;evanward1;Test Case:

{noformat} 
    @Test
    public void testRevert() {
        // setup
        Line line = new Line(new Vector3D(1653345.6696423641,
                6170370.041579291, 90000), new Vector3D(1650757.5050732433,
                6160710.879908984, 0.9));
        Vector3D expected = line.getDirection().negate();

        // action
        Line reverted = line.revert();

        // verify
        assertArrayEquals(expected.toArray(),
                reverted.getDirection().toArray(), 0);
    }
{noformat}","06/Mar/13 09:00;luc;Fixed in subversion repository as of r1453218.

Line should be immutable, and in fact we want to make all of the Hyperplane/SubHyperplane/Embedding/BSPTree instances immutable, but this is a large incompatible change. So it will occur only at a major release (hopefully 4.0).

Thanks for reporting the issue.",07/Apr/13 09:19;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoBracketingException after event was found,MATH-937,12634597,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,choeger,choeger,28/Feb/13 11:48,07/Apr/13 09:19,07/Apr/19 20:38,13/Mar/13 21:54,3.1.1,,,,,,,3.2,,,0,,,,,,,,"The BracketingNthOrderBrentSolver used by the EmbeddedRungeKuttaIntegrator fails, if an event is detected twice with a NoBracketingException.

The problem lies in line EventState.java line 262 (version 3.1.1). Here the event detection function f is applied to an arbitrary choosen value of time. If the event detector crosses zero before this time, the solver throws the mentioned exception.",,,,,,,,,,,,,,,,,,,,,28/Feb/13 11:49;choeger;ApacheCommonsBouncingBall.java;https://issues.apache.org/jira/secure/attachment/12571404/ApacheCommonsBouncingBall.java,28/Feb/13 11:49;choeger;ApacheCommonsBouncingBallTest.java;https://issues.apache.org/jira/secure/attachment/12571405/ApacheCommonsBouncingBallTest.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-01 15:11:09.186,,,false,,,,,,,,,,,,,,315090,,,Sun Apr 07 09:19:50 UTC 2013,,,,,,0|i1idfb:,315434,,,,,,,,28/Feb/13 11:49;choeger;A model and a Junit test case that demonstrate the problem.,28/Feb/13 12:05;choeger;Note that this does not happen with an euler integrator.,"01/Mar/13 12:31;choeger;I had a look at TestProblem4. Here (in class Bounce) a sign variable is introduced. This quirk seems to fix my problem too. This might be a problem with the event detection funtion g. 

The documentation states that: 

> The switching function must be continuous in its roots neighborhood

So it needs to be continuous _after_ an event as well? The documentation should clearly state that if it is the case IMO, since events are usually used to implement non-continuous behavior in a system.","01/Mar/13 15:11;luc;I have analyzed the problem.

As you identified yourself, the function is not continuous near the zero. In fact, it is both discontinuous as you wrote in the comment above, and it reverts its slope. I'm not sure we can identify this in all cases. This is exactly the reason why we have written in the javadoc that continuity is required in the *neighborhood* of the root, i.e. both just before and just after the event. I will improve the documentation in this case.

What happens under the hood is that we use a solver to locate the instant at which the function crosses 0. For this, we need continuity before the event is triggered. Then, if the event did not trigger a stop, the integrator continues after the event. In order to detect the next event properly, it needs to have a state corresponding to what happens just after the event. If computations were perfect, the value g0 = g(t0) should be exactly 0, but due to convergence criteria, it may be slightly non-zero. So we store t0, g0 and a boolean g0Positive that indicate that indicates the sign just after t0. This helps keeping everything consistent.

In this issue, the event occurs as the g function decreases from positive to negative values. We find a very good estimate of the root at t0 = 1.4278431229270647, which corresponds to g0 = g(t0) = -2.7755575615628914E-16. It is very close to zero and the solver properly selected a value after the real root, i.e. a negative one. As additional protection the g0Positive was properly set to false, i.e. we are really sure after the event the function should (and in fact is) negative.

However, the user code explicitly changes the slope at this time and the function starts to increase again. This is of course what the user wants because we want the function to represent something bouncing on the floor, i.e. when the altitude decreases down to zero, it should increase again. So the function does not really *crosses* the zero line, it reaches it and then we change it. As numerical inaccuracies arise, we introduce a discontinuity near the zero and in fact create another which really occurs before the one we have already detected. Then the algorithm gets lost as it did not expect this discontinuity.

We have already identified this problem when we developed TestProblem4 and we introduced the sign attribute in the Bounce event class exactly for this reason. This sign attribute helps preserving consistency of the g function on both sides of the event.

So I would say we cannot improve the code here, but we should probably explain better the problem and show the sign trick to users. We could write something like:

{panel:title=improved javadoc}
The discrete events are generated when the sign of this
switching function changes. The integrator will take care to change
the stepsize in such a way these events occur exactly at step boundaries.

The switching function must be continuous in its roots neighborhood
(but not necessarily smooth), as the integrator will need to find its
roots to locate precisely the events.

Also note that the integrator expect that once an event has occurred, the
sign of the switching function at the start of the next step (i.e. just
after the event) is the opposite of the sign just before the event. This
consistency between the steps *must* be preserved, otherwise exceptions
related to root not being bracketed will occur.

This need for consistency is sometimes tricky to achieve. A typical example
is using an event to model a ball bouncing on the floor. The first idea to
represent this would be to have g(t) = h(t) where h is the height above the floor
at time t. When g(t) reaches 0, the ball is on the floor, so it should bounce
and the typical way to do this is to reverse its vertical velocity. However,
this would mean that before the event g(t) was decreasing from positive values
to 0, and after the event g(t) would be increasing from 0 to positive values
again. Consistency is broken here! The solution here is to have g(t) = sign * h(t),
where sign is a variable that starts at +1. Then, each time eventOccurred is called,
sign should be reset to -sign. This allows the g(t) function to remain continuous
(and even smooth) even across events, despite h(t) is not. 
{panel}

Would this be OK for you?
","01/Mar/13 15:28;choeger;Hey, 

thanks for the response and the in-depth explanation. I think the changed javadoc is fine, except I would mention @{link: NoBracketingException} explicitly, as people will probably google first before reading the docs (sad but true). 

I tried to workaround the issue by adding an explicit check in the solver code. Unfortunately it failed a test, but what speaks against testing the bracketing or even catching the exception and just go on without detecting an event? I would be willing to implement it if someone would be willing to review my implementation ;).","01/Mar/13 17:16;luc;OK, I'll do the javadoc change with the link you suggest.

I'm not sure about ignoring the exception, so if you want to give it a try, I'll review your proposal.
If we go along this way, we should take care to ignore *only* this specific exception, as there can be
many other reasons for which we fail to find the root of arbitrary functions provided by user, and such
failures should be reported.

The call is already set up to force bracketing, and as you noticed, when it fails to do so there is a
reason for failure. In this case, it was because we found the same event again because the function was
""almost"" continuous (the discontinuity was at computing precision level). Here, ignoring the error and
continuing would have been the way to go. I'm not sure this would always be the case, and I'm not sure
either that the algorithm can recover a sane state after this occurs. So we have to see what you propose
befre deciding to implement it or not. ",01/Mar/13 17:37;luc;Javadoc updated as of r1451658.,"13/Mar/13 21:54;luc;As there are no further comments, we set the issue as fixed.",07/Apr/13 09:19;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomDataGenerator#nextLong violates bounds,MATH-936,12632812,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,rw7,rw7,18/Feb/13 10:49,07/Apr/13 09:19,07/Apr/19 20:38,10/Mar/13 19:29,3.1,,,,,,,3.2,,,0,random,,,,,,,"I attached a test.

If the underlying RandomGenerator returns 0.0, then nextLong returns Long.MIN_VALUE, although the lower bound is Long.MIN_VALUE+1.

The javadoc of RandomGenerator#nextDouble does not clearly define, whether the result includes the lower border of 0.0 or not.

In java.util.Random it clearly defined as included: ""uniformly from the range 0.0d (inclusive) to 1.0d (exclusive)"". And the existence of JDKRandomGenerator suggests, that RandomGenerator should have the same contract.

I tested with version 3.1.1 from mvnrepository

",,,,,,,,,,,,,,,,,,,,,09/Mar/13 15:35;tn;MATH-936.patch;https://issues.apache.org/jira/secure/attachment/12572917/MATH-936.patch,18/Feb/13 10:50;rw7;RandomGeneratorLongTest.java;https://issues.apache.org/jira/secure/attachment/12569794/RandomGeneratorLongTest.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-03-09 15:35:11.963,,,false,,,,,,,,,,,,,,313308,,,Sun Apr 07 09:19:40 UTC 2013,,,,,,0|i1i2fj:,313653,,,,,,,,"09/Mar/13 15:35;tn;We can not ensure that the resulting value of nextLong will really be uniformly distributed within the given bounds due to limited precision of the calculation (we would need to do the scaling calculation with a Dfp field for example, but this would be very slow).

The attached patch at least ensures that the resulting value will be strictly inside the given bounds.","09/Mar/13 15:53;luc;Shouldn't we completely avoid using nexDouble() here?
The underlying RandomGenerator does provide nextBytes(). Looking at BitsStreamGenerator, we see how the stream of bytes is used to implement nextInt(int) with a rejection method. The same could be done for a long.

As a side note, it would even be worth (for 4.0) to add nextLong(long) in the RandomGenerator interface too.","09/Mar/13 16:07;tn;If this can be adapted to uniformly provide numbers in the range of [lower, upper], also taking into account that lower can be negative, than this would be a better solution.","10/Mar/13 19:29;luc;Fixed in subversion repository as of r1454897.

We now use discrete raw elements to build the int/long rather than relying on floating point arithmetic.

Thanks for the report.",07/Apr/13 09:19;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DerivativeStructure.atan2(y,x) does not handle special cases properly",MATH-935,12632538,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,luc,luc,15/Feb/13 08:05,07/Apr/13 09:19,07/Apr/19 20:38,15/Feb/13 08:22,3.1.1,,,,,,,3.2,,,0,,,,,,,,"The four special cases +/-0 for both x and y should give the same values as Math.atan2 and FastMath.atan2. However, they give NaN for the value in all cases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,313034,,,Sun Apr 07 09:19:31 UTC 2013,,,,,,0|i1i0qv:,313380,,,,,,,,15/Feb/13 08:22;luc;Fixed in subversion repository as of r1446473.,07/Apr/13 09:19;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Complex.ZERO.reciprocal() returns NaN but should return INF.,MATH-934,12631642,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,strauss,strauss,10/Feb/13 12:34,07/Apr/13 09:19,07/Apr/19 20:38,22/Mar/13 18:56,2.1,,,,,,,3.2,,,0,,,,,,,,"Complex.ZERO.reciprocal() returns NaN but should return INF.

Class: org.apache.commons.math3.complex.Complex;
Method: reciprocal()
@version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $
",any,120,120,,0%,120,120,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-22 18:56:40.422,,,false,,,,,,,,,,,,,,312138,,,Sun Apr 07 09:19:19 UTC 2013,,,,,,0|i1hv7j:,312484,,,,,,,,"22/Mar/13 18:56;luc;Fixed in subversion repository as of r1459927.

Thanks for the report.",07/Apr/13 09:19;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver finds suboptimal solution or throws NoFeasibleSolutionException,MATH-930,12628011,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,konstantin.fro,konstantin.fro,17/Jan/13 09:17,07/Apr/13 09:18,07/Apr/19 20:38,20/Jan/13 10:13,3.1.1,,,,,,,3.2,,,0,,,,,,,,"When I run this code sometimes I get NoFeasibleSolutionException, and sometimes the result is 0.37522987682323883. 

Octave gives result 0.70679 and a point = {1.59032, 1.00000, 0.70679, 0.40399, 1.04004, 0.67396, 0.37868, 0.22823, 0.98909, 0.68793, 0.17021,
0.09192, 0.67501, 0.44573, 0.07829, 0.00000, 0.81316, 0.63520, 0.55634
0.40399, 0.48504, 0.45944, 0.22823, 0.22823, 0.34873, 0.32313, 0.09192,
0.09192, 0.25681, 0.23122, 0.00000, 0.00000, 1.59032

double[][] coefficients = new double[97][];
        double[] value = new double[97];
        Relationship[] relationship = new Relationship[97];
        int i = 0;
        double[] m0  = {1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 0};
        coefficients[i] = m0;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m1  = {1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1};
        coefficients[i] = m1;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m2  = {1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1};
        coefficients[i] = m2;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m3  = {0, 1, 0, -1, 0, -1, 0, 1, 0, -1, 0, 1, 0, 1, 0, -1, 0, -1, 0, 1, 0, 1, 0, -1, 0, 1, 0, -1, 0, -1, 0, 1, 0};
        coefficients[i] = m3;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m4  = {0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.628803};
        coefficients[i] = m4;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m5  = {0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.676993};
        coefficients[i] = m5;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m6  = {0, 0, 1, -1, 0, 0, -1, 1, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 1, -1, 0, 0, -1, 1, 0};
        coefficients[i] = m6;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m7  = {0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.136677};
        coefficients[i] = m7;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m8  = {0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.444434};
        coefficients[i] = m8;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m9  = {0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, -1, 0};
        coefficients[i] = m9;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m10  = {0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.254028};
        coefficients[i] = m10;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m11  = {0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.302218};
        coefficients[i] = m11;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m12  = {0, 0, 0, 0, 1, -1, -1, 1, 0, 0, 0, 0, -1, 1, 1, -1, 0, 0, 0, 0, -1, 1, 1, -1, 0, 0, 0, 0, 1, -1, -1, 1, 0};
        coefficients[i] = m12;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m13  = {0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.653981};
        coefficients[i] = m13;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m14  = {0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.690437};
        coefficients[i] = m14;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m15  = {0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 1, 0, -1, 0};
        coefficients[i] = m15;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m16  = {0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.423786};
        coefficients[i] = m16;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m17  = {0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.486717};
        coefficients[i] = m17;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m18  = {0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 1, -1, 0};
        coefficients[i] = m18;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m19  = {0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.049232};
        coefficients[i] = m19;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m20  = {0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.304747};
        coefficients[i] = m20;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m21  = {0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 1, 0};
        coefficients[i] = m21;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m22  = {0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.129826};
        coefficients[i] = m22;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m23  = {0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.205625};
        coefficients[i] = m23;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m24  = {0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, -1, 1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 1, -1, 1, -1, -1, 1, 0};
        coefficients[i] = m24;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m25  = {0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.621944};
        coefficients[i] = m25;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m26  = {0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.764385};
        coefficients[i] = m26;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m27  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0, 1, 0, -1, 0};
        coefficients[i] = m27;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m28  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.432572};
        coefficients[i] = m28;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m29  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.480762};
        coefficients[i] = m29;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m30  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0};
        coefficients[i] = m30;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m31  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.055983};
        coefficients[i] = m31;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m32  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.11378};
        coefficients[i] = m32;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m33  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0};
        coefficients[i] = m33;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m34  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.009607};
        coefficients[i] = m34;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m35  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.057797};
        coefficients[i] = m35;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m36  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 1, -1, 0};
        coefficients[i] = m36;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m37  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.407308};
        coefficients[i] = m37;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m38  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.452749};
        coefficients[i] = m38;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m39  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 0};
        coefficients[i] = m39;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m40  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.269677};
        coefficients[i] = m40;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m41  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.321806};
        coefficients[i] = m41;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m42  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 1, 0};
        coefficients[i] = m42;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m43  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.049232};
        coefficients[i] = m43;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m44  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.06902};
        coefficients[i] = m44;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m45  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0};
        coefficients[i] = m45;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m46  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0};
        coefficients[i] = m46;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m47  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.028754};
        coefficients[i] = m47;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m48  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, -1, 1, -1, -1, 1, 0};
        coefficients[i] = m48;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m49  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.484254};
        coefficients[i] = m49;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m50  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.524607};
        coefficients[i] = m50;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m51  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, -1, 0, 1, 0, -1, 0, 1, 0, 1, 0, -1, 0};
        coefficients[i] = m51;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m52  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.385492};
        coefficients[i] = m52;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m53  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.430134};
        coefficients[i] = m53;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m54  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0, 0, -1, 1, 0, 0, 1, -1, 0};
        coefficients[i] = m54;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m55  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.34983};
        coefficients[i] = m55;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m56  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.375781};
        coefficients[i] = m56;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m57  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 1, 0};
        coefficients[i] = m57;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m58  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.254028};
        coefficients[i] = m58;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m59  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.281308};
        coefficients[i] = m59;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m60  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, 0, 0, 0, 0, -1, 1, 1, -1, 0};
        coefficients[i] = m60;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m61  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.304995};
        coefficients[i] = m61;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m62  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.345347};
        coefficients[i] = m62;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m63  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 0, 0, -1, 0, 1, 0};
        coefficients[i] = m63;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m64  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.288899};
        coefficients[i] = m64;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m65  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.332212};
        coefficients[i] = m65;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m66  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, 0, 0, 0, 0, -1, 1, 0};
        coefficients[i] = m66;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m67  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.14351};
        coefficients[i] = m67;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m68  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.17057};
        coefficients[i] = m68;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m69  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1, 0};
        coefficients[i] = m69;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m70  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -0.129826};
        coefficients[i] = m70;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m71  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, -0.157435};
        coefficients[i] = m71;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m72  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, -1, 1, 1, -1, 0};
        coefficients[i] = m72;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m73  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -0};
        coefficients[i] = m73;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m74  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1};
        coefficients[i] = m74;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m75  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0, -1, 0, 1, 0};
        coefficients[i] = m75;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m76  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -0.141071};
        coefficients[i] = m76;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m77  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, -0.232574};
        coefficients[i] = m77;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m78  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0, 0, -1, 1, 0};
        coefficients[i] = m78;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m79  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -0};
        coefficients[i] = m79;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m80  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, -1};
        coefficients[i] = m80;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m81  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1, 0};
        coefficients[i] = m81;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m82  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -0.009607};
        coefficients[i] = m82;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m83  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, -0.057797};
        coefficients[i] = m83;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m84  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, -1, 1, 0};
        coefficients[i] = m84;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m85  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -0};
        coefficients[i] = m85;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m86  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, -1};
        coefficients[i] = m86;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m87  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0};
        coefficients[i] = m87;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m88  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, -0.091644};
        coefficients[i] = m88;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m89  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, -0.203531};
        coefficients[i] = m89;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m90  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0};
        coefficients[i] = m90;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m91  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -0};
        coefficients[i] = m91;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m92  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1};
        coefficients[i] = m92;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m93  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0};
        coefficients[i] = m93;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m94  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0};
        coefficients[i] = m94;
        relationship[i] = Relationship.GEQ;
        value[i] = 0.0;
        i++;
        double[] m95  = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -0.028754};
        coefficients[i] = m95;
        relationship[i] = Relationship.LEQ;
        value[i] = 0.0;
        i++;
        double[] m96  = {0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
        coefficients[i] = m96;
        relationship[i] = Relationship.EQ;
        value[i] = 1.0;

ArrayList<LinearConstraint> constraints = new ArrayList<>(97);
                for (int j = 0; j < 97; j++) {
                    constraints.add(new LinearConstraint(coefficients[j], relationship[j], value[j]));
                }
                double[] fooc = new double[33];
                fooc[3] = 1;
                LinearObjectiveFunction foo = new LinearObjectiveFunction(fooc, 0);
                SimplexSolver solver = new SimplexSolver();
                LinearConstraintSet se = new LinearConstraintSet(constraints);
                PointValuePair res = solver.optimize(MaxIter.unlimited(), foo, se, new NonNegativeConstraint(true));",,,,,,,,,,,,MATH-819,,,,,,,,,17/Jan/13 12:04;konstantin.fro;exception.txt;https://issues.apache.org/jira/secure/attachment/12565313/exception.txt,17/Jan/13 12:04;konstantin.fro;program.7z;https://issues.apache.org/jira/secure/attachment/12565312/program.7z,17/Jan/13 11:20;konstantin.fro;test.m;https://issues.apache.org/jira/secure/attachment/12565310/test.m,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2013-01-17 10:14:01.583,,,false,,,,,,,,,,,,,,304859,,,Sun Apr 07 09:18:50 UTC 2013,,,,,,0|i17wg7:,254226,,,,,,,,"17/Jan/13 10:14;tn;Hi Konstantin,

just as a hint, it is better to read if the code is in a separate attachment.

I validated your code with the latest release and I can not yet confirm the bug.
Running your test several times, I always received the same result (no exception), and the result looks fine:

 * a validation of the given solution is successful
 * a validation of the result from octave is wrong -> not a valid solution","17/Jan/13 11:09;tn;I have to correct myself:

the octave solution is correct if the used epsilon criteria is larger than 1e-6. As the given code does a minimization, the result is valid I would now assume that the CM solution is *better* than the one from octave. Can you attach the problem definition for octave so I can check if there is any difference between the CM and octave problem?",17/Jan/13 11:20;konstantin.fro;Octave version,"17/Jan/13 11:33;konstantin.fro;Sorry, I was incorrect. The solution is clearly optimal. But the exception keeps appearing. Sometimes it requires about 20 runs to appear (although in for-cycle it can appear only when the first iteration is running ). By ""latest release"" do you mean a 3.1.1-jar file from http://commons.apache.org/math/download_math.cgi ?","17/Jan/13 11:53;tn;Ok thanks for the file and the feedback. I did run the test (with the latest trunk which is ~= 3.1.1) a thousand times and always received the same output (and I would be surprised otherwise). Could you please attach the program you run where you get the exception?

Thanks, Thomas","17/Jan/13 12:09;konstantin.fro;I also noticed, that if epsilon == 1.0e-4 is used the exception disappear.","17/Jan/13 12:12;tn;Ok thanks, it must be something with the 3.1.1 jar. When I run the program with the latest trunk it always works, but with 3.1.1 it fails occasionally, I will look further into it.","17/Jan/13 12:35;tn;If I run the example from CM trunk with an execution environment J2SE-1.5 it never fails.

With an environment JavaSE-1.6 or JavaSE-1.7 (from eclipse) it fails with the same exception quite often.","17/Jan/13 13:25;tn;The problem is related to the LinearConstraintSet, which stores the constraints internally in a HashSet. Thus the iteration order is quasi-random, and also different with a different JRE.

Changing this to a normal list solves the problem. Tbh I do not know why this is implemented as a Set and will raise the issue on the mailinglist.","18/Jan/13 12:07;erans;I tried to override ""hashCode"" in class ""Relationship"" but it fails to compile (method is _final_ in ""Enum"").

bq. Tbh I do not know why this is implemented as a Set

Because it would prevent adding the same constraint twice.

bq. Changing this to a normal list solves the problem.

Only apparently.
If you record in which order the constraint are iterated when the problem shows up; then change the implementation to a ""List"" and pass the constraints in that order, does the problem still show up? I would think so (otherwise, that would be really strange unless I'm missing something). Then, changing to a ""List"" actually does little but hiding the problem (for one specific ordering of the constraints).
","19/Jan/13 13:48;tn;I have added more elaborate information about convergence criteria to the class javadoc of SimplexSolver and added an additional constructor to override the epsilon value.

As your problem seems to work fine with an epsilon value of 1e-4, no further changes are required imho.

After discussion on the mailinglist, the non-deterministic behavior, resulting in different results / exceptions is actually good and should give the user a hint that the current convergence criteria may be too strict.

Thanks for the report, your problem has been added as a unit test.","20/Jan/13 09:25;tn;With an epsilon value of 1e-4, I did not get any exceptions anymore with Java 5 & OpenJDK 6. But with OpenJDK 7 I get the same exception on a regular basis.

Reopened for further investigation.","20/Jan/13 10:13;tn;In r1435810, I also added the possibility to override the cut-off value.
This value has been introduced in 3.1 to improve the numerical stability with very tight problems. The default value has been set to 1e-12, which in your case was too small.

The idea of the cut-off value is to zero out entries in the Simplex tableau that get too small, as small values may lead to numerical problems due to the nature of the Simplex algorithm: the pivot element is used as a denominator, thus if a very small element is selected as pivot element, the resulting values will be very large, ....

Right now, the simplex solver is implemented in a very low-level way, which means you need to adjust the convergence criteria yourself to your problem. It would be nice to have a wrapper that adjusts these criteria in an adaptive way, depending on the problem at hand.","20/Jan/13 10:14;tn;I forgot to add, for your problem I have chosen the following parameters:

 * epsilon: 1e-4
 * maxUlps: 10
 * cutOff: 1e-6

Tested with Java 5, OpenJDK 6 and 7 and looks good so far.",07/Apr/13 09:18;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd,MATH-929,12627633,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,wydrych,wydrych,15/Jan/13 11:45,07/Apr/13 09:18,07/Apr/19 20:38,15/Jan/13 12:20,3.1.1,,,,,,,3.2,,,0,,,,,,,,"To reproduce:
{code}
Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);
{code}",,,,,,,,,,,,,,,,,,,,,15/Jan/13 11:46;wydrych;MultivariateNormalDistribution.java.patch;https://issues.apache.org/jira/secure/attachment/12564913/MultivariateNormalDistribution.java.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-01-15 12:20:21.821,,,false,,,,,,,,,,,,,,304417,,,Sun Apr 07 09:18:39 UTC 2013,,,,,,0|i17mdj:,252594,,,,,,,,"15/Jan/13 12:20;erans;Corrected in revision 1433367.

Thanks a lot for the report and fix!

A few weeks, Thomas wondered why using ""0.5 * x"" instead of ""x / 2"". That's an excellent reason... Unfortunately that instance slipped through my scanner :(.",07/Apr/13 09:18;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GammaDistribution cloning broken,MATH-927,12626005,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,dhendriks,dhendriks,04/Jan/13 11:36,04/Mar/13 18:53,07/Apr/19 20:38,13/Jan/13 18:02,3.1,,,,,,,3.1.1,,,0,,,,,,,,"Serializing a GammaDistribution and deserializing it, does not result in a cloned distribution that produces the same samples.

Cause: GammaDistribution inherits from AbstractRealDistribution, which implements Serializable. AbstractRealDistribution has random, in which we have a Well19937c instance, which inherits from AbstractWell. AbstractWell implements Serializable. AbstractWell inherits from BitsStreamGenerator, which is not Serializable, but does have a private field 'nextGaussian'.

Solution: Make BitStreamGenerator implement Serializable as well.

This probably affects other distributions as well.",,,,,,,,,,,,,,,,,,,,,04/Jan/13 11:38;dhendriks;DistributionCloneTests.java;https://issues.apache.org/jira/secure/attachment/12563281/DistributionCloneTests.java,04/Jan/13 11:40;dhendriks;bits-stream-generator-serializable.patch;https://issues.apache.org/jira/secure/attachment/12563282/bits-stream-generator-serializable.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2013-01-04 12:31:57.347,,,false,,,,,,,,,,,,,,302587,,,Fri Jan 04 12:31:57 UTC 2013,,,,,,0|i1744n:,249636,,,,,,,,04/Jan/13 11:38;dhendriks;Unit test to show the problem.,04/Jan/13 11:40;dhendriks;Proposed patch.,"04/Jan/13 12:31;erans;Fixed in revision 1428822.

I've slightly modified the unit test and included it in ""RealDistributionAbstractTest"" so that all current, and future, distribution implementations are automatically checked.

Thanks for the report and fix.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
new multivariate vector optimizers cannot be used with large number of weights,MATH-924,12625438,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,luc,luc,28/Dec/12 19:12,04/Mar/13 18:53,07/Apr/19 20:38,15/Jan/13 11:56,,,,,,,,3.1.1,,,0,,,,,,,,"When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large.

This happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.",,,,,,,,,,,,,,,,,,,,,29/Dec/12 02:20;erans;MATH-924;https://issues.apache.org/jira/secure/attachment/12562639/MATH-924,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-29 02:20:49.385,,,false,,,,,,,,,,,,,,301980,,,Sat Dec 29 13:27:16 UTC 2012,,,,,,0|i16xtj:,248612,,,,,,,,28/Dec/12 20:18;luc;Fixed in subversion repository as of r1426616.,"29/Dec/12 02:20;erans;Keeping the matrix concept, by representing uncorrelated observations with a diagonal weight matrix (instead of an array), allows to solve this issue with minimal changes to the code:
# The optimizer's API is untouched.
# The possibility to have correlated observations is kept.

The attached patch contains a minimal ""DiagonalMatrix"" implementation, needing overview (ant unit tests).
","29/Dec/12 13:18;erans;Changes committed in revision 1426758.

The same problem would occur int the deprecated ""o.a.c.m.optimization"" package. The same changes must be ported there.
","29/Dec/12 13:27;erans;bq. The same changes must be ported there.

Done in revision 1426759.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""getCovariances"" is broken (""o.a.c.m.optimization.general.LevenbergMarquardtOptimizer"")",MATH-922,12624504,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Blocker,Fixed,erans,erans,erans,18/Dec/12 17:58,04/Mar/13 18:53,07/Apr/19 20:38,18/Dec/12 18:03,,,,,,,,3.1,,,0,,,,,,,,"Method ""getCovariances"" (defined in base class ""AbstractLeastSquaresOptimizer"") returns incorrect values when called from subclass ""LevenbergMarquardtOptimizer"".
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,300325,,,Tue Dec 18 21:56:51 UTC 2012,,,,,,0|i16787:,244303,,,,,,,,18/Dec/12 18:03;erans;Fixed in revision 1423555.,"18/Dec/12 21:53;erans;Same problem in ""GaussNewtonOptimizer""...","18/Dec/12 21:56;erans;Fixed in revision 1423687.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Backward compatibility broken in ""ValueServer""",MATH-916,12623778,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Blocker,Fixed,erans,erans,erans,13/Dec/12 16:32,04/Mar/13 18:53,07/Apr/19 20:38,14/Dec/12 15:00,,,,,,,,3.1,,13/Dec/12 00:00,0,,,,,,,,"Issue similar to MATH-915.
",,,,,,,,,,,,,,,,,,,,,13/Dec/12 21:49;erans;MATH-916.diff;https://issues.apache.org/jira/secure/attachment/12560856/MATH-916.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,297478,,,Fri Dec 14 15:00:18 UTC 2012,,,,,,0|i14oyv:,235510,,,,,,,,13/Dec/12 21:49;erans;Patch that makes the Clirr error disappear.,14/Dec/12 15:00;erans;Compatibility restored in revision 1421911.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Backward compatibility broken in ""EmpiricalDistribution""",MATH-915,12623768,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Blocker,Fixed,,erans,erans,13/Dec/12 15:45,04/Mar/13 18:53,07/Apr/19 20:38,14/Dec/12 14:58,,,,,,,,3.1,,13/Dec/12 00:00,0,,,,,,,,"There is a binary-compatibility problem in {{o.a.c.m.random.EmpiricalDistribution}} (cf. ""Clirr"" report).

Usage of ""RandomDataImpl"" has been replaced by ""RandomDataGenerator"".
However, unless I'm mistaken, none of those is actually necessary. Moreover, the ""randomData"" field in this class ""shadows"" the (deprecated) protected field in the super class. Also, it duplicates functionality (RNG) already present in the super class (through the the ""random"" protected field).",,,,,,,,,,,,,,,,,,,,,13/Dec/12 16:30;erans;MATH-915.diff;https://issues.apache.org/jira/secure/attachment/12560801/MATH-915.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-12-14 06:19:14.714,,,false,,,,,,,,,,,,,,297468,,,Fri Dec 14 14:58:48 UTC 2012,,,,,,0|i14owf:,235499,,,,,,,,"13/Dec/12 16:30;erans;Here is a patch that restores backward-compatibility of ""EmpiricalDistribution"".

OK to apply?
","14/Dec/12 06:19;psteitz;I am sorry I screwed this up in my last commit.  The patch has the right idea, but is not quite correct.  The constructor that takes a RandomGenerator should not be deprecated, but it should call the superclass constructor, which means its null tolerance is going to have to be dropped (which seems to have been broken anyway in 3.0 or before).  What we might consider deprecating are constructors that take RandomDataImpl (incorrectly dropped in my last commit) and RandomDataGenerator (i.e. don't add that at all).  The RandomDataImpl constructors predate RandomGenerator, which is really the underlying source of randomness and the only random data constructor argument really needed.  I will fix this or roll back the last commit so backward compat is restored both here and in ValueServer.

One thing I am not sure I handled best is RandomDataGenerator itself as just a renamed version of RandomDataImpl.  The awkwardness of removing the deprecations makes me think that I did not do a good job designing the replacement - RandomDataGenerator.  This class is new for 3.1, so any comments on how to better accomplish the simple goal of renaming RandomDataImpl would be appreciated.  One thing that I have been experimenting with is having a package-scoped getDelegate() method added to RandomDataImpl.  That will allow RandomDataImpl instances to be replaced by RandomDataGenerators, so there is no need to maintain instances of both.","14/Dec/12 14:25;erans;bq. The constructor that takes a RandomGenerator should not be deprecated

It is not deprecated.

bq. but it should call the superclass constructor

From what I see, in ""EmpiricalDistribution"", the source of randomness is ""RandomDataGenerator"".
I indeed thought that it was redundant and first tried removing all reference to ""RandomDataGenerator"" in ""EmpiricalDistribution"". But then it did not work with ""ValueServer"" since that class internally uses an instance of ""EmpiricalDistribution"" created with a ""RandomDataGenerator"" argument!

Thus although I would gladly remove all usage of ""RandomDataGenerator"", it is not obvious to do it in a clean yet backward-compatible way...
In the meantime, calling the superclass's constructor is not done on purpose: to avoid that some methods use the superclass's (protected) ""random"" field while others use this class's ""randomData"" (or the superclass's deprecated ""randomData"").

bq. What we might consider deprecating are constructors that take RandomDataImpl

This is exactly what I've done in the patch.

bq. any comments on how to better accomplish the simple goal of renaming RandomDataImpl would be appreciated.

IMO, the problem is not with the renaming, it is having protected fields (instances of ""RandomDataImpl"", ""RandomDataGenerator"", and ""RandomGenerator"") which prevent an easy transition path.
For one thing, in 4.0, the ""random"" field in ""AbstractRealDistribution"" should become private.

Until we can start making incompatible changes, I think that the current patch is probably as good as any other half-baked solution: ""Old"" usage of ""RandomDataImpl"" will be delegated to the new ""RandomDataGenerator"".
As I said above further cleanup should remove usage of _both_ classes in CM. IMHO, ""RandomDataGenerator"" should only be a _user_ tool (syntactic sugar to bypass direct instantiation of a ""distribution"" object).



","14/Dec/12 14:58;erans;Compatibility restored in revision 1421910.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent multi-start randomization (optimizers),MATH-914,12623747,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,erans,erans,13/Dec/12 13:28,07/Apr/13 09:17,07/Apr/19 20:38,09/Mar/13 17:38,3.0,3.1,,,,,,3.2,,,0,,,,,,,,"In class ""o.a.c.m.optim.BaseMultiStartMultivariateOptimizer"", the ""starting points"" generator is passed at construction. But random initial guesses must fulfill the bound constraint and be somehow related to the user-supplied initial guess; and those are passed to the ""optimize"" method and thus can change from one call to the other, leading to inconsistent (and probably useless) multi-starts.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-09 17:38:35.575,,,false,,,,,,,,,,,,,,297446,,,Sun Apr 07 09:17:22 UTC 2013,,,,,,0|i14oqn:,235473,,,,,,,,"13/Dec/12 13:33;erans;In revision 1421287, I've marked (with ""XXX"") the code that should be revised.
I don't think that it is a blocking issue (it is also present in the ""optimization"" package), but anyone is welcome to provide a patch if a fix is quickly found.
",09/Mar/13 17:38;luc;Fixed in subversion repository as of r1454746.,07/Apr/13 09:17;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Front page 3.0 Javadoc link actually goes to 3.1 snapshot,MATH-912,12623363,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,peterfine,peterfine,11/Dec/12 11:49,11/Dec/12 13:52,07/Apr/19 20:38,11/Dec/12 13:52,3.0,,,,,,,,,,0,documentation,,,,,,,"On the front page of the math site [http://commons.apache.org/math/], the link on the left titled ""Javadoc (3.0 release)"" actually leads to [http://commons.apache.org/math/apidocs/index.html] with the title ""Commons Math 3.1-SNAPSHOT API"".

It should actually lead to the 3.0 docs. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-12-11 13:52:48.729,,,false,,,,,,,,,,,,,,297060,,,Tue Dec 11 13:52:48 UTC 2012,,,,,,0|i14l3r:,234882,,,,,,,,"11/Dec/12 13:52;erans;I added the missing ""api-3.0"" directory.
Thanks for the report.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FDistribution NoBracketingException in BrentSolver,MATH-909,12617545,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Cannot Reproduce,,meyerjp,meyerjp,25/Nov/12 23:38,26/Nov/12 19:31,07/Apr/19 20:38,26/Nov/12 19:31,3.0,,,,,,,,,,0,,,,,,,,"I get an exception when running the code below. the exception is 

{code}
function values at endpoints do not have different signs, endpoints: [0, 1.002], values: [-0.025, -∞]
{code}

The problematic code:

{code}
double df1 = 10675;
double df2 = 501725;
FDistribution fDist = new FDistribution(df1, df2);
System.out.println(fDist.inverseCumulativeProbability(0.025));//NoBracketingException
{code}

However, R returns the value 0.9733505. The R code is:
{code}
qf(p=.025, df1=10675, df2=501725)
{code}

I don't know enough about the FDistribution class to know the solution to the exception, but I thought I would report it.
",,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-11-26 09:10:49.939,,,false,,,,,,,,,,,,,,292045,,,Mon Nov 26 19:31:16 UTC 2012,,,,,,0|i0rorz:,159658,,,,,,,,"26/Nov/12 09:10;tn;I tested it with the latest trunk version, and the result is: 0.9730779455126357

The FDistribution relies on the Beta function, which got several improvements in terms of accuracy since the 3.0 release.
Could you please test yourself with a more recent version of CM?

Thanks,

Thomas","26/Nov/12 09:57;celestin;bq. The FDistribution relies on the Beta function, which got several improvements in terms of accuracy since the 3.0 release.

Actually, in the lastest trunk {{r1413531}}, {{Beta}} hasn't been changed yet (ongoing work in MATH-738). Accuracy of {{Gamma.logGamma}} has improved a lot (see MATH-849) which might explain why this bug no longer shows up.

Even if this should not be considered as a bug, I propose we include this simple test in our unit tests, as {{Beta}} will undergo some changes in the coming days, hopefully for the best, but it never hurts to check non-regression, does it?","26/Nov/12 10:02;tn;Ah ok my fault, but adding more regression tests is definitely useful.
Will add the test case later on.","26/Nov/12 13:17;meyerjp;I tested it with the most recent version and I also got 0.9730779455126357. The exception no longer occurs, but the result still seems to be too different from the value reported by R.","26/Nov/12 14:17;celestin;Patrick, the arguments of the incomplete beta function are large, so a loss of accuracy is to be expected. We are working on it (see MATH-738), but there is still a lot to do.

Meanwhile, I've used MAXIMA to compute in multiple precision the result you are asking for. Here is the script I wrote

{code}
kill(all);
load(newton1);
fpprec : 128;
d1 : 10675;
d2 : 501725;

p : 0.025b0;

F(x) := block(
  y : d1 * x / (d1 * x + d2),
  beta_incomplete(0.5b0 * d1, 0.5b0 * d2, y) / beta(0.5b0 * d1, 0.5b0 * d2)
  );

x0 : 0.9733505b0;
x1 : newton(F(x) - p, x, x0, 10**(-fpprec+1));
print(x1);
{code}

Basically, {{F(X)}} is the cumulative distribution function, and {{p}} is the target. I then look for an approximate solution of {{F(X) == p}}, starting from {{x0 = 0.9733505b0}} which is the value returned by R.

Here is the result I get
{code}
9.730779455235159798387713198169726833852358770410804399004797817813867760559669310857118225960097152689836562586854107391295331b-1
{code}

It seems to me that the value returned by CM is more accurate than the one returned by R. Could you carry out an independent check on that?

NOTA: for some reason, I had to compute the regularized incomplete beta function as the ratio of the incomplete beta function and the beta function, as the function {{beta_incomplete_regularized (a, b, z)}} led to errors.","26/Nov/12 14:56;meyerjp;Ha, you're right! R is less accurate. I checked the value with Stata (code listed below) and the result was 0.97307795. I'm satisfied. CM returns a more accurate value.

{code}
display invF(10675, 501725, 0.025)
{code}","26/Nov/12 15:09;celestin;Excellent! I'm glad CM turns out to be more accurate. Be cautious, though, since we are still working on the incomplete beta function (MATH-738).

Just out of curiosity: I Haven't R installed on my computer. Could you check the doc for the incomplete beta function, and let us know on which implementation it is based?
If I remember correctly, it's slatec. I've noticed that slatec is in fact *not very accurate* for the gamma and beta functions, so I ruled it out for MATH-738.

Patrick: are you satisfied? Can we consider this issue as solved? If that's OK with you, I suggest we keep it open until the unit test has been implemented. Then, we will close it. What do you think?","26/Nov/12 15:52;erans;bq. I suggest we keep it open until the unit test has been implemented.

As this test would not demonstrate a current buggy behaviour of CM, it should not be linked to this issue (i.e. not need to create a ""testMath909()"" method).
I think that the issue can be resolved now as ""Cannot reproduce"".

Of course, unit tests are always welcome. But in this case, your approach of fairly exhaustively checking the result of the underlying functions by comparison with ""maxima"" makes it superfluous to separately test the ""client"" (i.e. ""FDistribution"").
","26/Nov/12 17:21;meyerjp;According to the R documentation, the gamma and beta functions are C translations of the SLATEC Fortran subroutines, as you suspected. The incomplete gamma appears to have a different origin. According to the R documentation, the pbeta function is related to the incomplete beta function of Abramowitz and Stegun. They cite two different sources for the function depending on whether it is a central or non-central pbeta.

Central pbeta:

Didonato, A. and Morris, A., Jr, (1992) Algorithm 708: Significant digit computation of the incomplete beta function ratios, ACM Transactions on Mathematical Software, 18, 360–373. (See also
Brown, B. and Lawrence Levy, L. (1994) Certification of algorithm 708: Significant digit computation of the incomplete beta, ACM Transactions on Mathematical Software, 20, 393–397.)

Non-central pbeta:

Lenth, R. V. (1987) Algorithm AS226: Computing noncentral beta probabilities. Appl. Statist, 36, 241–244, incorporating
Frick, H. (1990)'s AS R84, Appl. Statist, 39, 311–2, and
Lam, M.L. (1995)'s AS R95, Appl. Statist, 44, 551–2.

As far as test cases go, I think we should include a test case, given the proposed work on the underlying incomplete beta function. The test case does not have to be specific to this issue, but it would be safe to include a test.




","26/Nov/12 18:12;celestin;bq. Of course, unit tests are always welcome. But in this case, your approach of fairly exhaustively checking the result of the underlying functions by comparison with ""maxima"" makes it superfluous to separately test the ""client"" (i.e. ""FDistribution"").

I wouldn't say superfluous. Maybe (maybe) redundant. But remember that the incomplete beta function has three arguments. Exhaustive checking of this function is going to be difficult...

On the whole, I agree with you. This test would seem a bit weird. I'll make sure that comparison with Maxima are ""exhaustive enough"" to include a test equivalent to the values proposed by Patrick in this issue.

I agree this issue should be tagged as ""cannot reproduce"".","26/Nov/12 19:27;celestin;bq. According to the R documentation, the gamma and beta functions are C translations of the SLATEC Fortran subroutines, as you suspected. The incomplete gamma appears to have a different origin. According to the R documentation, the pbeta function is related to the incomplete beta function of Abramowitz and Stegun. They cite two different sources for the function depending on whether it is a central or non-central pbeta.

Thank you Patrick for checking the references. Didonato and Morris (1992) is actually the reference I used for our new implementation of {{Gamma.logGamma}} and {{Gamma.gamma}}, as well as {{Beta.logBeta}} (not yet committed).

I'm worried about the incomplete beta function, as R uses this paper, and the accuracy seems to be not so good. I will look into it.","26/Nov/12 19:31;celestin;New implementation of {{Gamma.logGamma}} seems to solve this issue.
A test equivalent to the extreme test proposed by Patrick will be included in the unit tests of the incomplete beta function.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.commons.math3.special.Gamma comments have bad characters ,MATH-907,12617121,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,kberlin,kberlin,21/Nov/12 15:42,20/Jan/13 05:40,07/Apr/19 20:38,20/Jan/13 05:40,,,,,,,,,,,0,,,,,,,,"    [javac] /math/src/main/java/org/apache/commons/math3/special/Gamma.java:708: warning: unmappable character for encoding ASCII
    [javac]      * Returns the value of log ??(a + b) for 1 ??? a, b ??? 2.","OS 10.8.2, Java 7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-11-21 16:18:45.761,,,false,,,,,,,,,,,,,,259321,,,Sun Jan 20 00:15:21 UTC 2013,,,,,,0|i0locf:,124594,,,,,,,,"21/Nov/12 16:18;erans;It could be an environment problem.

Why is the encoding assumed to be ASCII? It's right that those are definitely not ASCII characters.
In ""emacs"", I have a problem with the visualization of the uppercase gamma greek letter, but it appears in ""gedit"".
In any case, compilation and HTML generation from the Javadoc proceed fine.
","21/Nov/12 16:37;kberlin;If that is so, you should modify the ant file. Things should compile without warnings on all platforms.","21/Nov/12 23:24;erans;bq. you should modify the ant file

Do you actually use ""ant"" to compile?
If so, please signal this issue on the ""dev"" ML.
","22/Nov/12 08:19;celestin;@Konstantin: I've recently modified the pom.xml to correctly process UTF-8 chars. I forgot to update the build.xml.

@Gilles: in emacs, M-x set-buffer-file-coding-system, then utf-8 should do the work. You can set utf-8 as your default encoding.","22/Nov/12 10:36;erans;bq. M-x set-buffer-file-coding-system, then utf-8 should do the work.

That does not seem to be the problem (I'm in utf-8 by default). It's just that the _visual_ output for ""Γ"" does not work: I get a rectangle instead (whereas the ""≤"" shows fine).
","22/Nov/12 10:43;erans;bq. I forgot to update the build.xml

The problem might be that there is no reference to an encoding in the ""build.xml"".

I think that the ""build.xml"" has quietly become unsupported... Please raise the issue on the ""dev"" ML, so that we can finally decide whether we should remove this file (unless someone will step forward to keep it in sync with the ""maven"" configuration).

","22/Nov/12 12:22;celestin;For some reason, I can't reproduce the problem on my computer (the whole system must be set to utf-8 by default).
Could someone try to add the following line to {{build.xml}}, and let us know if that solves the problem?
{code:xml}
<property name=""file.encoding"" value=""utf-8""/>
{code}

The Javadoc should also be checked to see if it comes out right. Thanks!","23/Nov/12 21:02;psteitz;I committed what should be a fix for this issue in r1413060.  I was not able to reproduce the compile failure; but I did see failure in javadoc correctness, which the committed change fixed.  The suggestion above did not seem to work for me - i.e., the jvm file.encoding property did not appear to be inherited properly by the the javac and javadoc tasks.  The change I committed defined a new local property and explicitly set it in both tasks.  If the OP can test and verify this works, we can resolve this as fixed.","19/Jan/13 14:48;tn;It looks like that Konstantin was using the ant build.xml file to compile CM. Why is it there anyway, as nobody of the developers seems to use it anymore, and it may be out-of-date?","19/Jan/13 17:13;psteitz;The Ant build is there because not all users use maven.  A working Ant build is a good thing to have and I am willing to do the (minimal) work to maintain it.  If Konstantin or another user having experienced the same problem can confirm the fix, we can resolve this.  I think what I committed in r1413060 should have fixed the problem.",20/Jan/13 00:15;kberlin;I can confirm that it compiles without warnings and passes all the unit tests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FastMath.[cosh, sinh] do not support the same range of values as the Math counterparts",MATH-905,12617008,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tn,tn,20/Nov/12 20:54,04/Mar/13 18:53,07/Apr/19 20:38,26/Nov/12 13:25,3.0,,,,,,,3.1,,,0,,,,,,,,"As reported by Jeff Hain:

cosh(double) and sinh(double):
Math.cosh(709.783) = 8.991046692770538E307
FastMath.cosh(709.783) = Infinity
Math.sinh(709.783) = 8.991046692770538E307
FastMath.sinh(709.783) = Infinity
===> This is due to using exp( x )/2 for values of |x|
above 20: the result sometimes should not overflow,
but exp( x ) does, so we end up with some infinity.
===> for values of |x| >= StrictMath.log(Double.MAX_VALUE),
exp will overflow, so you need to use that instead:
for x positive:
double t = exp(x*0.5);
return (0.5*t)*t;
for x negative:
double t = exp(-x*0.5);
return (-0.5*t)*t;",,,,,,,,,,,,,,,,,,,,,23/Nov/12 14:28;erans;MATH-905.diff;https://issues.apache.org/jira/secure/attachment/12554729/MATH-905.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-11-23 14:28:51.706,,,false,,,,,,,,,,,,,,258925,,,Mon Nov 26 13:25:55 UTC 2012,,,,,,0|i0l5sv:,121590,,,,,,,,"23/Nov/12 14:28;erans;Here is a patch for this issue.

The additional test methods ""testMath905..."" explore the range where the previous code would overflow (around [709, 710] and [-710, -709]).

Please note that the tolerance for those tests had to be increased to ""3"" (whereas it was ""2"" for the range [-30, 30]).

OK to commit?
","25/Nov/12 21:32;tn;Looks good to me.

Any specific reason why you prefer multiplication by 0.5 over division by 2, or just personal preference?
Searching in the FastMath source, there are several occurrences of both variants.","26/Nov/12 00:03;erans;bq. Any specific reason why you prefer multiplication by 0.5 over division by 2, or just personal preference?

I've doing that since having read that multiplication is simpler (thus potentially faster) than division.
Never tried to benchmark.
",26/Nov/12 13:25;erans;Committed revision 1413600.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53 ",MATH-904,12617007,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tn,tn,20/Nov/12 20:51,04/Mar/13 18:53,07/Apr/19 20:38,26/Nov/12 23:03,3.0,,,,,,,3.1,,,0,,,,,,,,"As reported by Jeff Hain:

pow(double,double):
Math.pow(-1.0,5.000000000000001E15) = -1.0
FastMath.pow(-1.0,5.000000000000001E15) = 1.0
===> This is due to considering that power is an even
integer if it is >= 2^52, while you need to test
that it is >= 2^53 for it.
===> replace
""if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)""
with
""if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)""
and that solves it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-11-21 21:48:21.357,,,false,,,,,,,,,,,,,,258924,,,Mon Nov 26 23:03:35 UTC 2012,,,,,,0|i0l5sn:,121589,,,,,,,,"21/Nov/12 18:42;tn;Does anyone know the reason why numbers > 2^53 are assumed to be even?

Looking at the javadoc of Math.pow, this is not mentioned, only the general case:

{noformat}
If the first argument is finite and less than zero
    if the second argument is a finite even integer, the result is equal to the result of raising the absolute value of the first argument to the power of the second argument
    if the second argument is a finite odd integer, the result is equal to the negative of the result of raising the absolute value of the first argument to the power of the second argument
    if the second argument is finite and not an integer, then the result is NaN. 
{noformat}","21/Nov/12 21:48;luc;bq. Does anyone know the reason why numbers > 2^53 are assumed to be even?

Because the mantissa of a double number encoding using IEEE-754 cannot handle a sufficient number of digits. If the Most Significant Bit is large enough, the Least Significant Bit becomes equal to 2.0 (and later when you still increase the MSB, then the LSB will become 4.0, and after that 8.0...

This is the essence of ""floating"" in floating point numbers. The decimal separator ""floats"", up to the end of the number than it slips out of the number.","22/Nov/12 09:44;tn;ah ok, thanks a lot, I found also an explanation here [http://en.wikipedia.org/wiki/Double-precision_floating-point_format]",26/Nov/12 23:03;erans;Fixed in revision 1413916.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UTF-8 extended characters are not rendered correctly in the javadoc,MATH-903,12616687,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,celestin,celestin,celestin,19/Nov/12 06:39,04/Mar/13 18:53,07/Apr/19 20:38,19/Nov/12 07:00,3.1,,,,,,,3.1,,,0,Javadocs,pom,UTF-8,,,,,"It was recently agreed on the [mailing list|http://markmail.org/thread/bnnjyakdhx7icsj7] that UTF-8 extended characters should be allowed in the Javadoc comments (in place of {{&...;}} HTML special characters), in order to increase readability.

The {{pom.xml}} states
{code:xml}
  <properties>
    ...
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
    ...
  </properties> 
{code}

which, according to [this FAQ|http://maven.apache.org/plugins/maven-javadoc-plugin/faq.html#What_are_the_values_of_encoding_docencoding_and_charset_parameters], should be enough to accept UTF-8 encoding in the Javadoc. However, UTF-8 extended characters are not rendered correctly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,258544,,,Mon Nov 19 07:00:28 UTC 2012,,,,,,0|i0kuh3:,119755,,,,,,,,"19/Nov/12 06:47;celestin;The problem seems to come from the parent {{pom}}, where two properties are defined
{code:xml}
  <properties>
    ...
    <commons.encoding>iso-8859-1</commons.encoding>
    <commons.docEncoding>${commons.encoding}</commons.docEncoding>
    ...
{code}
which are then used
{code:xml}
  <build>
    ...
    <pluginManagement>
      <plugins>
        ...
        <plugin>
          <groupId>org.apache.maven.plugins</groupId>
          <artifactId>maven-javadoc-plugin</artifactId>
          <version>${commons.javadoc.version}</version>
          <configuration>
            <!-- keep only errors and warnings -->
            <quiet>true</quiet>
            <encoding>${commons.encoding}</encoding>
            <docEncoding>${commons.docEncoding}</docEncoding>
            ...
{code}
Therefore, to fix this bug, it should be sufficient to override {{commons.encoding}} in our {{pom}}, and remove the following lines
{code:xml}
<properties>
    ...
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
    ...
</properties>
{code}",19/Nov/12 07:00;celestin;Changes committed in {{r1411084}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Font problem in LocalizedFormatsTest.java header,MATH-900,12616427,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,kberlin,kberlin,16/Nov/12 16:16,04/Mar/13 18:53,07/Apr/19 20:38,17/Nov/12 00:22,3.1,,,,,,,3.1,,,0,,,,,,,,"Non-standard charecters in the comment header of the file prevents compiling of the class with error
    [javac] /Users/kberlin/Dropbox/Projects/math/src/test/java/org/apache/commons/math3/exception/util/LocalizedFormatsTest.java:21: error: unmappable character for encoding ASCII
    [javac]  * Copyright 2010 CS Communication & Syst??mes","OS X 10.8.2, Java 7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-11-16 16:24:57.33,,,false,,,,,,,,,,,,,,258268,,,Fri Nov 16 17:16:50 UTC 2012,,,,,,0|i0kr0v:,119195,,,,,,,,"16/Nov/12 16:24;erans;This license text has been moved out of the way in revision 1410433.
Please check that it solves your problem.
","16/Nov/12 16:31;kberlin;Issue also present in HermiteInterpolator.java. The other file is now fixed.

  [javac] /Users/kberlin/Dropbox/Projects/math/src/main/java/org/apache/commons/math3/analysis/interpolation/HermiteInterpolator.java:48: warning: unmappable character for encoding ASCII
    [javac]  * is: Copyright 2002-2012 CS Syst??mes d'Information.
    [javac]                                   ^","16/Nov/12 16:50;erans;Text removed in revision 1410460.
",16/Nov/12 17:16;kberlin;Fixed now. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A random crash of MersenneTwister random generator,MATH-899,12616365,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,darksnake,darksnake,16/Nov/12 08:08,04/Mar/13 18:53,07/Apr/19 20:38,18/Nov/12 21:40,3.0,,,,,,,3.1,,,0,,,,,,,,"There is a very small probability that MersenneTwister generator gives a following error: 
java.lang.ArrayIndexOutOfBoundsException: 624
in MersenneTwister.java line 253
The error is completely random and its probability is about 1e-8.

UPD: The problem most probably arises only in multy-thread mode.","Windows 7, JDK 1.7.05",,,,,,,,,,,,,,,,,,,,18/Nov/12 01:07;erans;SynchronizedRandomGenerator.java;https://issues.apache.org/jira/secure/attachment/12553933/SynchronizedRandomGenerator.java,18/Nov/12 01:07;erans;SynchronizedRandomGeneratorTest.java;https://issues.apache.org/jira/secure/attachment/12553934/SynchronizedRandomGeneratorTest.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-11-16 08:45:01.418,,,false,,,,,,,,,,,,,,258143,,,Sun Nov 18 22:50:42 UTC 2012,,,,,,0|i0kn9r:,118587,,,,,,,,"16/Nov/12 08:45;tn;Hi,

Looking at the code, I do not understand how this can happen:

{noformat}
        if (mti >= N) { // generate N words at one time
            ...

            mti = 0;
        }

        y = mt[mti++];
{noformat}

So first we check if mti is gte to N, reset it in this case, and then use the mti value and increment afterwards.
Could it be that you use the MersenneTwister in a multi-threading environment, where the nextXXX() methods are called in a concurrent manner?","16/Nov/12 08:55;luc;Doing exactly the same analysis and doing somes tests (still running), I came to the same conclusion.

The mti index is not random at all (it is an index running through the pool and wrapping at the end). Only the content of the pool is random.

The MersenneTwister class is not thread safe, so I also guess there is a multi-threading issue there.","16/Nov/12 09:51;darksnake;Yes, i was running some tests in multi-thread mode. So I guess the problem is in concurrency. In this case it should be marked as non thread safe in the Docs.","16/Nov/12 12:07;erans;I propose to add the ""volatile"" keyword for variable ""mti"".
I also think that the ""synchronized"" keyword must be added to the ""setSeed"" methods.

Alexander,
Would you be willing to stress-test the codes before and after this modification to ensure to it makes the class thread-safe?
","16/Nov/12 12:19;luc;Adding synchronized to setSeed is not sufficient.
The part of the next(int bytes) method that uses mti (i.e. the part shown in Thomas comment above) should also be synchronized.
","16/Nov/12 12:23;tn;Indeed, I think it is a more general problem, as many of the RNGs are not thread-safe (if not all of them).
It would be nice to have annotations for thread-safe code, so a user would not have to dig into code to be sure.

For now we could also document in the class javadoc whether an implementation is thread-safe or not (like we did in commons-codec).",16/Nov/12 12:47;darksnake;I always can insert new version of the generator in the test program where I have found the problem in the first place. Still I don't know what conditions are required to error for occur. For now I'm using a JDKRandomGenerator and it works just fine so far.,"16/Nov/12 13:24;erans;I'm no expert in thread-safety but ...

bq. The part of the next(int bytes) method that uses mti [...]

... I don't think so; or more exactly, it can be that synchronization of the whole method is more than strictly necessary, or that it should be refactored into blocks of statements that must be synchronized and blocks that don't need to.
I think that it is one of the important collateral issues about making a code thread-safe: not overdoing it to avoid too much efficiency loss.

bq. For now we could also document in the class javadoc whether an implementation is thread-safe or not

To do that is a huge effort. For now, users must assume that no class (except immutable ones) is thread-safe.
","16/Nov/12 13:35;tn;Regarding sync (see embedded code from next(int) method).

This part has to be sync'ed, as it not only relies on the internal variable mti, it also modifies the internal array mt:

{noformat}
        if (mti >= N) { // generate N words at one time
            int mtNext = mt[0];
            for (int k = 0; k < N - M; ++k) {
                int mtCurr = mtNext;
                mtNext = mt[k + 1];
                y = (mtCurr & 0x80000000) | (mtNext & 0x7fffffff);
                mt[k] = mt[k + M] ^ (y >>> 1) ^ MAG01[y & 0x1];
            }
            for (int k = N - M; k < N - 1; ++k) {
                int mtCurr = mtNext;
                mtNext = mt[k + 1];
                y = (mtCurr & 0x80000000) | (mtNext & 0x7fffffff);
                mt[k] = mt[k + (M - N)] ^ (y >>> 1) ^ MAG01[y & 0x1];
            }
            y = (mtNext & 0x80000000) | (mt[0] & 0x7fffffff);
            mt[N - 1] = mt[M - 1] ^ (y >>> 1) ^ MAG01[y & 0x1];

            mti = 0;
        }

        y = mt[mti++];
{noformat}

This part does not need to be sync'ed:

{noformat}
        // tempering
        y ^=  y >>> 11;
        y ^= (y <<   7) & 0x9d2c5680;
        y ^= (y <<  15) & 0xefc60000;
        y ^=  y >>> 18;

        return y >>> (32 - bits);
{noformat}

So, have a more fine-grained sync would not be of much benefit imho. Maybe adding a sync'ed wrapper of RandomGenerator makes sense, which takes another RandomGenerator as input?

{quote}
To do that is a huge effort. For now, users must assume that no class (except immutable ones) is thread-safe.
{quote}

for all of CM indeed, for just the RNGs this would be feasible. I think the problem here also comes from the fact that the JDK RNG is thread-safe, so people may tend to assume the same is true for CM RNGs.","16/Nov/12 15:11;erans;bq. a more fine-grained sync would not be of much benefit

Maybe, maybe not.
Another approach (and preferrable in my non-expert opinion) is to use the JDK's [atomic utilities|http://docs.oracle.com/javase/1.5.0/docs/api/java/util/concurrent/atomic/package-summary.html].

bq. for just the RNGs this would be feasible

I agree, modulo the above remark.
","16/Nov/12 15:40;erans;Referring to the quoted code block.

Only the block that will be executed if the condition is true must synchronized (assuming that ""mti"" is to be ""volatile"").
{code}
if (mti >= N) {
 updateMtArray();
 mti = 0;
}
{code}
where ""updateMtArray()"" would be ""synchronized"".
We thus have a finer-grained thread-safety (locking occurs every N calls).
","16/Nov/12 15:46;tn;The moment you did the check ""if (mti >= N)"" another thread could already have updated mti, invalidating your program flow.","16/Nov/12 16:33;erans;Oh, yes. :(

Another related issue is that this implementation is a translation from the original algorithm.
Can we make it thread-safe and still make that claim?

To make it thread-safe but not too inefficient, can we depart from the standard implementation?

How much inefficiency is acceptable from such a utility? If efficiency is not critical, your suggestion of a wrapper is possibly the right answer.
","16/Nov/12 18:00;luc;bq. Can we make it thread-safe and still make that claim?

Yes, just do what was said a few comments above: synchronize the block identified above.
","16/Nov/12 19:15;psteitz;I am -1 on attempting to make this class threadsafe.  Each generating thread should have its own generator.  If users really need a shared singleton generator, they should do as Gilles suggests, which is to wrap and then only they will pay the sync overhead cost.  ","17/Nov/12 00:46;erans;Actually, it was Thomas's suggestion that we could provide an implementation of ""RandomGenerator"" that, IIUC, would wrap another instance and override all the methods with the ""synchronized"" keyword.
{code}
/**
 * Any ""RandomGenerator"" can be thread-safe if it is used through
 * an instance of this class.
 */
public class SynchronizedRandomGenerator {
  private final RandomGenerator wrapped;

  /**
   * @param rng Generator whose methods will be called through
   * their corresponding overridden synchronized version.
   * To ensure thread-safety, the wrapped generator <em>must</em>
   * not be used directly.
   */
  public SynchronizedRandomGenerator(RandomGenerator rng) {
    wrapped = rng;
  }

  public synchronized void setSeed(int seed) {
    wrapped.setSeed(seed);
  }

  // [Similarly for all methods.]
}
{code}

Any caveat with that solution?
",17/Nov/12 01:13;psteitz;Sounds reasonable to me.,"18/Nov/12 01:04;erans;I've implemented that class.
But I have a hard time implementing a unit test that would consistently reproduce the error reported here: It does not necessarily happen even with a fairly large number of threads.

The unit test with the synchronized wrapper passes, but it would have been more convincing to have both, showing that the wrapper indeed solves the problem.
","18/Nov/12 01:07;erans;Here are the Java files.
Ideas to improve the unit tests welcome.
","18/Nov/12 07:12;darksnake;The synchronized wrapper works fine. I can not guarantee that it does not produce error at all because I made only finite number of runs, but no errors so far.","18/Nov/12 10:35;tn;{quote}
But I have a hard time implementing a unit test that would consistently reproduce the error reported here: It does not necessarily happen even with a fairly large number of threads.
{quote}

hmm, I am not sure if this is necessary or achievable. When looking at comparable things (e.g. commons-collections, or openjdk), there also do not exist specific tests to prove that the synchronization prevents a race condition in not thread-safe code.

I think in this case it would be sufficient to do a proof by induction (sort of):

 * we know that e.g. MersenneTwister is not thread-safe
 * by providing a fully synchronized wrapper we make it thread-safe

btw. in the test you refer to MATH-900 while its actually MATH-899.","18/Nov/12 17:44;erans;bq. it would be sufficient to do a proof by induction (sort of): [...]

Well, this could be said for any piece of code: we must have property <x>, so we write code that implements <x>. ;)
We write unit tests that actually show that the code behaves as expected.

After spending quite some time on the attached code, I of course agree that it is complicated in some situations. In this case, the code is simple enough as to not necessitates a unit test.
But the issue will crop up again if we start introducing more ""complicated"" code (like the utilities in the ""java.util.concurrent"" package).

So do I commit just the class and no unit test, or just the unit test of the synchronizing wrapper?
","18/Nov/12 18:35;tn;You could and should provide a test that shows that the wrapper provides the same result as a wrapped RandomGenerator (using the same seed), imho.","18/Nov/12 19:38;psteitz;I think the reason that the unit test does not work is that Junit does not actually see the exception.  The Executor will not propagate it.  Changing the body of the core method (and other sigs, etc) to the following, I can get consistent failures when I execute with -Dtest=SynchronizedRandomGenratorTest

{code}
final RandomGenerator rng = new MersenneTwister();
final RandomGenerator wrapper = sync ? new SynchronizedRandomGenerator(rng) : rng;
final AtomicBoolean failed = new AtomicBoolean(false);
final ExecutorService exec = Executors.newFixedThreadPool(numThreads);
for (int i = 0; i < numGenerators; i++) {
    exec.execute(new Runnable() {
        public void run() {
            for (int j = 0; j < numSamples; j++) {
                try {
                    wrapper.nextGaussian();   
                } catch (ArrayIndexOutOfBoundsException ex) {
                    failed.getAndSet(true);
                    break;
                }
             }
         }
         });
        }
        exec.shutdown();
        exec.awaitTermination(100, TimeUnit.SECONDS);
        Assert.assertTrue(failed.get());
{code}

What I don't understand is why this consistently succeeds when executed as a single test, but usually fails when executed as part of the full test suite with
{code}
 mvn clean test
{code}

I would say in any case there is no need to add a unit test to show the non-thread-safety of MersenneTwister or to verify that adding synchronization does what it says it does, i.e., I would say go ahead and commit the wrapper with no test class.","18/Nov/12 20:54;erans;bq. I think the reason that the unit test does not work is that Junit does not actually see the exception.

That's not the problem. The test code works correctly and failures _are_ reported (and seen by Junit). Just they do not occur consistently, as must be expected, I assume, because the execution itself becomes random (subject to the threads scheduling by the system).

bq. commit the wrapper with no test class.

I'll do that then.

bq. You could and should provide a test that shows that the wrapper provides the same result as a wrapped RandomGenerator (using the same seed)

I don't understand.
",18/Nov/12 21:01;erans;Wrapper committed in revision 1410990.,"18/Nov/12 21:40;erans;""MersenneTwister"" not being thread-safe, if an instance is to be accessed from multiple threads, it must be through the wrapper proposed in this discussion.
","18/Nov/12 22:07;tn;thanks!

{quote}
I don't understand.
{quote}

Nevermind, I just had something like this in mind:

{noformat}
  RandomGenerator orig = new MersenneTwister(1234l);
  RandomGenerator wrapper = new SynchronizedRandomGenerator(new MersenneTwister(1234l));

  for (i = 0; i < 100; i++) {
     assertEquals(orig.nextGaussian(), wrapper.nextGaussian());
  }
  ... // similar for the other methods
{noformat}","18/Nov/12 22:50;erans;bq. I just had something like this in mind [...]

I'm ashamed to admit that you are perfectly right... Cf. revision 1411009.
",,,,,,,,,,,,,,,,,
BrentSolver : function value accuracy convergence criterion,MATH-896,12615727,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Not A Problem,,vruch,vruch,12/Nov/12 13:48,04/Mar/13 18:58,07/Apr/19 20:38,12/Nov/12 15:24,3.0,,,,,,,,,,0,,,,,,,,"Hi everybody,

In the current implementation of the Brent solver, it seems that attribute ""functionValueAccuracy"" is only used in the ""doSolve"" method, before entering the iteration loop in ""brent"" method. 

Shouldn't it be a convergence test on the current function value to ensure that function value accuracy is reached? 

This is the first time I'm posting here, so maybe I have missed something. Sorry if it's the case.

Thanks for your time,

Vincent",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-11-12 14:48:06.464,,,false,,,,,,,,,,,,,,257035,,,Mon Nov 12 15:24:00 UTC 2012,,,,,,0|i0j1l3:,109187,,,,,,,,"12/Nov/12 14:48;luc;Yes, this value is used only for an initial check in this algorithm.

The implementation of the loop corresponds to the original Brent algorithm, not to something specific created for the Apache Commons Math library. Our implementation of a classical algorithm should be as close as possible to the reference. Any departure from it would prevent comparison with other implementations and would only confuse users.

The value is inherited from an higher level base class and other implementations with different algorithms exist, which do use this parameter in the convergence loop. Examples are the secant based solvers and also BracketingNthOrderBrentSolver.

For most purposes, BracketingNthOrderBrentSolver is the recommended solver, it is often the most efficient one (typically with a setting for order 5) and has interesting features (bracketing).

Also note that the JIRA issue tracker is not the place for questions, it is a place for bug reports and features requests. Usage questions should be asked on the user list (see [http://commons.apache.org/mail-lists.html]). So if you agree, I will close this report as NOTAPROBLEM.","12/Nov/12 15:13;vruch;Thanks for your answer. 
And this was obviously not the right place to post, so of course this report can be closed as NOTAPROBLEM.","12/Nov/12 15:24;luc;As explained in the first comments, this behaviour is the expected one for the Brent algorithm.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SpearmansCorrelation fails when using NaturalRanking together with NaNStrategy.REMOVED,MATH-891,12615360,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tn,tn,08/Nov/12 18:31,07/Apr/13 09:17,07/Apr/19 20:38,27/Mar/13 19:44,3.0,,,,,,,3.2,,,0,,,,,,,,"As reported by Martin Rosellen on the users mailinglist:

Using a NaturalRanking with a REMOVED NaNStrategy can result in an exception when NaN are contained in the input arrays.

The current implementation just removes the NaN values where they occur, without taken care to remove the corresponding values in the other array.",,,,,,,,,,,,,,,,,MATH-958,,,,18/Feb/13 13:22;tn;MATH-891.patch;https://issues.apache.org/jira/secure/attachment/12569809/MATH-891.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2013-03-27 17:04:24.713,,,false,,,,,,,,,,,,,,256499,,,Sun Apr 07 09:17:10 UTC 2013,,,,,,0|i0hd67:,99396,,,,,,,,"08/Nov/12 18:38;tn;Test case:

{noformat}
        double[] column1 = new double[] { Double.NaN, 1, 2 };
        double[] column2 = new double[] { 10, 2, 10 };

        Array2DRowRealMatrix mydata = new Array2DRowRealMatrix(column1.length, 2);
        for (int i = 0; i < column1.length; i++) {
            mydata.addToEntry(i, 0, column1[i]);
            mydata.addToEntry(i, 1, column2[i]);
        }

        // compute correlation
        NaturalRanking ranking = new NaturalRanking(NaNStrategy.REMOVED);
        SpearmansCorrelation spearman = new SpearmansCorrelation(mydata, ranking);
        double correlations = spearman.correlation(column1, column2);
        System.out.println(""correlations "" + correlations);
{noformat}","18/Feb/13 13:22;tn;The attached patch does the following:

 * checks if the ranking algorithm is NaturalRanking with NaNStrategy set to REMOVED
 ** finds all indices with NaNs in the input arrays
 ** removes the corresponding rows in *all* input data
 ** passes the modified data to the ranking algorithm

This should ideally be handled by the RankingAlgorithm, but updating the interface would break compatibility afaik.

The SpearmanCorrelation class is the only one in CM that uses a RankingAlgorithm to rank correlated data, so it is a kind of compromise imho.","27/Mar/13 17:04;psteitz;I am OK with committing this patch, but lets keep the issue open, or open another one for handling missing data in multivariate stats.  I think it is OK to leave the RankingAlgorithm interface and impls as is - they are doing what they should be doing by contract.  I think multivariate stats should just not allow the REMOVED NAN strategy (i.e. throw in this case).  Once we agree on how to implement and represent missing data strategies at least just for this class, the Spearman's constructor should then be modified to include specification of missing data strategy.

I think it is better to commit the workaround now, since behavior is currently broken; but note in the javadoc and release notes that as of 4.0, the constructor will throw on REMOVED NaNStrategy and NANs should not be used to represent missing data.  Practical advice to users is to preprocess data to remove / replace / impute missing data in preparation for this.",27/Mar/13 19:44;tn;Applied changes in r1461822 and created MATH-958.,07/Apr/13 09:17;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GammaDistribution returns infinite density when cdf = 1,MATH-888,12614053,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,zhochberg,zhochberg,30/Oct/12 17:08,02/May/13 02:29,07/Apr/19 20:38,23/Nov/12 11:42,3.0,,,,,,,3.1,,,0,,,,,,,,"When GammaDistribution.cumulativeProbability = 1, then GammaDistribution.density may return Infinite - but it should return 0.
Here is sample java code which demonstrates this issue, with results. Note that when pass the point at which cumulativeProbability = 1, the density stays > 0, though very small (pdf should be 0 when cdf = 1) - but as continue, then cdf stays 1, but pdf becomes infinite. This is seen most clearly in the second set of data below.


package ApacheTester;

import org.apache.commons.math3.distribution.GammaDistribution;

public class GammaDistributionTester
{
    public static void main(String[] args)
    {
        double mean = 10.118627813856065;
        double std  = 0.8934747186204953;
        double alpha = (mean*mean)/(std*std);
        double beta = (std*std)/mean;
        System.out.println(""alpha: "" + alpha);
        System.out.println(""beta: "" + beta);
        
        GammaDistribution gd = new GammaDistribution(alpha, beta);
        
        for (int i=0; i<25; i++)
        {
            double pdf = gd.density(i);
            double cdf = gd.cumulativeProbability(i);
            System.out.println(""i="" + i + ""; pdf="" + pdf + ""; cdf="" + cdf);            
        }
        System.out.println();
        
        for (int i=0; i<20; i++)
        {
            double x = 19.0 + (.1 * i);
            double pdf = gd.density(x);
            double cdf = gd.cumulativeProbability(x);
            System.out.println(""x="" + x + ""; pdf="" + pdf + ""; cdf="" + cdf);            
        }
    }
}

Here's results of running above test:

alpha: 128.25629996917283
beta: 0.07889380729280462
i=0; pdf=0.0; cdf=0.0
i=1; pdf=8.663477060371509E-79; cdf=7.488555717749243E-81
i=2; pdf=5.5062704634862175E-46; cdf=1.0676382015037016E-47
i=3; pdf=4.413483488439264E-29; cdf=1.4607150065658644E-30
i=4; pdf=1.0945225959157887E-18; cdf=5.599255664602861E-20
i=5; pdf=7.35929647596093E-12; cdf=5.590805261026197E-13
i=6; pdf=2.7437881760964153E-7; cdf=3.0732983691238435E-8
i=7; pdf=2.8376132639470645E-4; cdf=4.7826733420766874E-5
i=8; pdf=0.021281179160656388; cdf=0.005695853765034336
i=9; pdf=0.2151187723352627; cdf=0.10175065472250033
i=10; pdf=0.44751658755472723; cdf=0.4586536845007119
i=11; pdf=0.259114059558244; cdf=0.8385256983930766
i=12; pdf=0.05218255030958943; cdf=0.9781383380332821
i=13; pdf=0.004329365023697574; cdf=0.9986058018679428
i=14; pdf=1.6878342979402664E-4; cdf=0.9999549514385939
i=15; pdf=3.431947471368873E-6; cdf=0.9999992047520807
i=16; pdf=3.9588967227014906E-8; cdf=0.9999999917873332
i=17; pdf=2.7752325350928474E-10; cdf=0.9999999999473412
i=18; pdf=1.2515801802459923E-12; cdf=0.9999999999997794
i=19; pdf=3.808710263354571E-15; cdf=0.9999999999999993
i=20; pdf=8.143130333262255E-18; cdf=1.0
i=21; pdf=Infinity; cdf=1.0
i=22; pdf=Infinity; cdf=1.0
i=23; pdf=Infinity; cdf=1.0
i=24; pdf=Infinity; cdf=1.0

x=19.0; pdf=3.808710263354571E-15; cdf=0.9999999999999993
x=19.1; pdf=2.0912827149653448E-15; cdf=0.9999999999999997
x=19.2; pdf=1.144280754920214E-15; cdf=0.9999999999999998
x=19.3; pdf=6.239549206021272E-16; cdf=0.9999999999999999
x=19.4; pdf=3.3907057624200243E-16; cdf=1.0
x=19.5; pdf=1.8363629496980252E-16; cdf=1.0
x=19.6; pdf=9.912278358035803E-17; cdf=1.0
x=19.7; pdf=5.332732516340122E-17; cdf=1.0
x=19.8; pdf=2.8595785047900534E-17; cdf=1.0
x=19.9; pdf=1.5284263208928833E-17; cdf=1.0
x=20.0; pdf=8.143130333262255E-18; cdf=1.0
x=20.1; pdf=4.324705899342647E-18; cdf=1.0
x=20.2; pdf=2.2895693394216285E-18; cdf=1.0
x=20.3; pdf=1.2083606178165914E-18; cdf=1.0
x=20.4; pdf=6.357672846520565E-19; cdf=1.0
x=20.5; pdf=Infinity; cdf=1.0
x=20.6; pdf=Infinity; cdf=1.0
x=20.7; pdf=Infinity; cdf=1.0
x=20.8; pdf=Infinity; cdf=1.0
x=20.9; pdf=Infinity; cdf=1.0

",Windows 7,,,,,,,,,,,,,,,,,,,MATH-849,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-11-09 20:59:20.724,,,false,,,,,,,,,,,,,,253177,,,Fri Nov 23 11:42:39 UTC 2012,,,,,,0|i0dc8n:,75912,,,,,,,,"09/Nov/12 20:59;tn;Several improvements to the Gamma function have already been implemented in trunk (see MATH-849).

Running your test program with the latest trunk gives the following results:

{noformat}
alpha: 128.25629996917283
beta: 0.07889380729280462
i=0; pdf=0.0; cdf=0.0
i=1; pdf=8.663477060372497E-79; cdf=7.488555717749243E-81
i=2; pdf=5.506270463486845E-46; cdf=1.0676382015037016E-47
i=3; pdf=4.4134834884397667E-29; cdf=1.4607150065658644E-30
i=4; pdf=1.0945225959159135E-18; cdf=5.599255664602861E-20
i=5; pdf=7.359296475961767E-12; cdf=5.590805261026197E-13
i=6; pdf=2.7437881760967277E-7; cdf=3.0732983691238435E-8
i=7; pdf=2.837613263947388E-4; cdf=4.7826733420766874E-5
i=8; pdf=0.021281179160658817; cdf=0.005695853765034336
i=9; pdf=0.21511877233528723; cdf=0.10175065472250033
i=10; pdf=0.4475165875547783; cdf=0.4586536845007119
i=11; pdf=0.25911405955827355; cdf=0.8385256983930767
i=12; pdf=0.052182550309595385; cdf=0.9781383380332821
i=13; pdf=0.004329365023698068; cdf=0.9986058018679428
i=14; pdf=1.6878342979404588E-4; cdf=0.9999549514385939
i=15; pdf=3.431947471369265E-6; cdf=0.9999992047520807
i=16; pdf=3.958896722701942E-8; cdf=0.9999999917873332
i=17; pdf=2.775232535093164E-10; cdf=0.9999999999473412
i=18; pdf=1.2515801802472468E-12; cdf=0.9999999999997794
i=19; pdf=3.808712290749614E-15; cdf=0.9999999999999993
i=20; pdf=8.266386718783014E-18; cdf=1.0
i=21; pdf=1.2660914223523828E-20; cdf=1.0
i=22; pdf=1.4746029859879733E-23; cdf=1.0
i=23; pdf=1.320017264979955E-26; cdf=1.0
i=24; pdf=9.28776702592028E-30; cdf=1.0

x=19.0; pdf=3.808712290749614E-15; cdf=0.9999999999999993
x=19.1; pdf=2.091282124070138E-15; cdf=0.9999999999999997
x=19.2; pdf=1.1442807873491891E-15; cdf=0.9999999999999998
x=19.3; pdf=6.239511895039934E-16; cdf=0.9999999999999999
x=19.4; pdf=3.3905614583343015E-16; cdf=1.0
x=19.5; pdf=1.8362803668472756E-16; cdf=1.0
x=19.6; pdf=9.92380512634114E-17; cdf=1.0
x=19.7; pdf=5.314805483145551E-17; cdf=1.0
x=19.8; pdf=2.8759196357817904E-17; cdf=1.0
x=19.9; pdf=1.528822526909328E-17; cdf=1.0
x=20.0; pdf=8.266386718783014E-18; cdf=1.0
x=20.1; pdf=7.797057331677706E-18; cdf=1.0
x=20.2; pdf=0.0; cdf=1.0
x=20.3; pdf=0.0; cdf=1.0
x=20.4; pdf=0.0; cdf=1.0
x=20.5; pdf=0.0; cdf=1.0
x=20.6; pdf=0.0; cdf=1.0
x=20.7; pdf=0.0; cdf=1.0
x=20.8; pdf=0.0; cdf=1.0
x=20.9; pdf=2.4498611525242454E-20; cdf=1.0
{noformat}

Could you please test yourself and report if the result is as expected?

Thanks,

Thomas","14/Nov/12 19:12;zhochberg;I've built trunk, and I do get the results Thomas Neidhart shows below - problem is fixed.

One odd thing: for all unit tests to run correctly, I needed to download trunk to root of c:\ drive.
If I copied elsewhere, I would get a unit test failure - once, for example, of FixedElapsedTimeTest.java in genetics package.","14/Nov/12 19:14;tn;Hi Zev,

thanks for the feedback.

Could you attach a log of the mentioned test failure?

Thanks,

Thomas","14/Nov/12 19:42;zhochberg;Here's a unit test error log, when I copy trunk to c:\ACM\commons-math3, rather than to c:\commons-math3
There was also a unit test error when trunk was much ""deeper"" in the drive; in that case failed to open a file, which was in fact present.
I'll try and find my notes so I reproduce that one.

-------------------------------------------------------------------------------
Test set: org.apache.commons.math3.genetics.FixedElapsedTimeTest
-------------------------------------------------------------------------------
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.488 sec <<< FAILURE!
testIsSatisfied(org.apache.commons.math3.genetics.FixedElapsedTimeTest)  Time elapsed: 3.487 sec  <<< FAILURE!
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:92)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertTrue(Assert.java:54)
	at org.apache.commons.math3.genetics.FixedElapsedTimeTest.testIsSatisfied(FixedElapsedTimeTest.java:72)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:236)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:134)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:113)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:103)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:74)
","14/Nov/12 20:10;tn;ok, the test should be fixed, the sleep was larger than the test condition which could lead to a race condition.","23/Nov/12 11:42;erans;Resolving, as the reporter confirmed that the issue had already been fixed in ""trunk"".
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SingularValueDecomposition fails to calcuate matrix rank,MATH-886,12613909,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,albert_triv,albert_triv,29/Oct/12 15:43,23/May/13 09:42,07/Apr/19 20:38,10/Nov/12 17:55,2.2,,,,,,,3.0,,,0,,,,,,,,"for the matrix {{0.0,1.0,0.0,1.0,1.0,1.0,0.0,0.0},
 {0.0,0.0,1.0,0.0,0.0,0.0,1.0,1.0},
 {1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0},
 {1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0}}, SVD return rank=4, but it is 3.",jdk1.6 on windows machine,,,,,,,,,,,,MATH-611,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-10-29 16:05:38.472,,,false,,,,,,,,,,,,,,252796,,,Sat Nov 10 17:55:05 UTC 2012,,,,,,0|i0d8af:,75119,,,,,,,,"29/Oct/12 16:05;erans;Did you try with Commons Math v3.0?
","29/Oct/12 16:19;psteitz;This looks like a duplicate to MATH-465, which was resolved in 3.0","09/Nov/12 21:03;tn;Tested with latest trunk:

{noformat}
        double[][] data = {
                {0.0,1.0,0.0,1.0,1.0,1.0,0.0,0.0},
                {0.0,0.0,1.0,0.0,0.0,0.0,1.0,1.0},
                {1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0},
                {1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0}
        };
        
        RealMatrix m = MatrixUtils.createRealMatrix(data);
        SingularValueDecomposition d = new SingularValueDecomposition(m);
        System.out.println(""Rank="" + d.getRank());
{noformat}

Result is: rank=3","10/Nov/12 17:55;tn;The SVD implementation has been completely rewritten for the 3.0 release (see MATH-611).

The given test case work with the latest version of CM. It is advised to upgrade to the latest version of CM as there will be no further bugfix release for the 2.x branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Eliminate meaningless properties in multivariate distribution classes,MATH-881,12612484,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,18/Oct/12 19:57,04/Mar/13 18:53,07/Apr/19 20:38,19/Oct/12 11:09,3.0,,,,,,,3.1,,,0,,,,,,,,"The MultivariateRealDistribution interface includes the following properties which make no sense for multivariate distributions:
getSupportLowerBound, getSupporUpperBound, isSupportLowerBoundInclusive, isSupportUpperBoundInclusive
In addition, the following property makes sense, but is unlikely to be useful:
isSuportConnected

All of these properties should be deprecated in 3.1 and dropped in 4.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-10-18 21:05:01.197,,,false,,,,,,,,,,,,,,249662,,,Fri Oct 19 09:33:12 UTC 2012,,,,,,0|i0ag8f:,58903,,,,,,,,"18/Oct/12 21:05;erans;bq. All of these properties should be deprecated in 3.1 and dropped in 4.0

This interface and implementing classes are post 3.0.
No need to wait for 4.0 to clean up. :)
","18/Oct/12 21:27;erans;All above methods removed in revision 1399864.
","18/Oct/12 21:33;erans;Shall I also remove this method:
{code}
/**
 * For a random variable {@code X} whose values are distributed according to
 * this distribution, this method returns {@code P(X = x)}. In other words,
 * this method represents the probability mass function (PMF) for the
 * distribution.
 *
 * @param x Point at which the PMF is evaluated.
 * @return the value of the probability mass function at point {@code x}.
 */
double probability(double[] x);
{code}
?

We already discussed that all the (currently implemented) {{RealDistribution}} classes return zero. I guess that is is even less likely that an exotic {{MultivariateRealDistribution}} will soon be implemented...
","18/Oct/12 23:17;psteitz;Sweet!  I did not realize this was post-3.0 stuff.  Yes, I would also nuke the pmf method.","19/Oct/12 09:33;erans;bq. [...] nuke the pmf method.

Removed in revision 1400010.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Polygon difference produces erronious results in some cases,MATH-880,12611561,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,curtis,curtis,12/Oct/12 17:31,04/Mar/13 18:53,07/Apr/19 20:38,22/Oct/12 19:28,3.0,,,,,,,3.1,,,0,,,,,,,,"The 2D polygon difference method is returning incorrect
results.  Below is a test case of subtracting two polygons (Sorry,
this is the simplest case that I could find that duplicates the
problem).  

There are three problems with the result. The first is that the first
point of the first set of vertices is null (and the first point of the
second set is also null).  The second is that, even if the first null
points are ignored,  the returned polygon is not the correct result.
The first and last points are way off, and the remaining points do not
match the original polygon boundaries.  Additionally, there are two
holes that are returned in the results.  This subtraction case should
not have holes.

{code:title=""Complex Polygon Difference Test""}
public void testComplexDifference() {
        Vector2D[][] vertices1 = new Vector2D[][] {
            new Vector2D[] {
                    new Vector2D( 90.08714908223715,  38.370299337260235),
                    new Vector2D( 90.08709517675004,  38.3702895991413),
                    new Vector2D( 90.08401538704919,  38.368849330127944),
                    new Vector2D( 90.08258210430711,  38.367634558585564),
                    new Vector2D( 90.08251455106665,  38.36763409247078),
                    new Vector2D( 90.08106599752608,  38.36761621664249),
                    new Vector2D( 90.08249585300035,  38.36753627557965),
                    new Vector2D( 90.09075743352184,  38.35914647644972),
                    new Vector2D( 90.09099945896571,  38.35896264724079),
                    new Vector2D( 90.09269383800086,  38.34595756121246),
                    new Vector2D( 90.09638631543191,  38.3457988093121),
                    new Vector2D( 90.09666417351019,  38.34523360999418),
                    new Vector2D( 90.1297082145872,  38.337670454923625),
                    new Vector2D( 90.12971687748956,  38.337669827794684),
                    new Vector2D( 90.1240820219179,  38.34328502001131),
                    new Vector2D( 90.13084259656404,  38.34017811765017),
                    new Vector2D( 90.13378567942857,  38.33860579180606),
                    new Vector2D( 90.13519557833206,  38.33621054663689),
                    new Vector2D( 90.13545616732307,  38.33614965452864),
                    new Vector2D( 90.13553111202748,  38.33613962818305),
                    new Vector2D( 90.1356903436448,  38.33610227127048),
                    new Vector2D( 90.13576283227428,  38.33609255422783),
                    new Vector2D( 90.13595870833188,  38.33604606376991),
                    new Vector2D( 90.1361556630693,  38.3360024198866),
                    new Vector2D( 90.13622408795709,  38.335987048115726),
                    new Vector2D( 90.13696189099994,  38.33581914328681),
                    new Vector2D( 90.13746655304897,  38.33616706665265),
                    new Vector2D( 90.13845973716064,  38.33650776167099),
                    new Vector2D( 90.13950901827667,  38.3368469456463),
                    new Vector2D( 90.14393814424852,  38.337591835857495),
                    new Vector2D( 90.14483839716831,  38.337076122362475),
                    new Vector2D( 90.14565474433601,  38.33769000964429),
                    new Vector2D( 90.14569421179482,  38.3377117256905),
                    new Vector2D( 90.14577067124333,  38.33770883625908),
                    new Vector2D( 90.14600350631684,  38.337714326520995),
                    new Vector2D( 90.14600355139731,  38.33771435193319),
                    new Vector2D( 90.14600369112401,  38.33771443882085),
                    new Vector2D( 90.14600382486884,  38.33771453466096),
                    new Vector2D( 90.14600395205912,  38.33771463904344),
                    new Vector2D( 90.14600407214999,  38.337714751520764),
                    new Vector2D( 90.14600418462749,  38.337714871611695),
                    new Vector2D( 90.14600422249327,  38.337714915811034),
                    new Vector2D( 90.14867838361471,  38.34113888210675),
                    new Vector2D( 90.14923750157374,  38.341582537502575),
                    new Vector2D( 90.14877083250991,  38.34160685841391),
                    new Vector2D( 90.14816667319519,  38.34244232585684),
                    new Vector2D( 90.14797696744586,  38.34248455284745),
                    new Vector2D( 90.14484318014337,  38.34385573215269),
                    new Vector2D( 90.14477919958296,  38.3453797747614),
                    new Vector2D( 90.14202393306448,  38.34464324839456),
                    new Vector2D( 90.14198920640195,  38.344651155237216),
                    new Vector2D( 90.14155207025175,  38.34486424263724),
                    new Vector2D( 90.1415196143314,  38.344871730519),
                    new Vector2D( 90.14128611910814,  38.34500196593859),
                    new Vector2D( 90.14047850603913,  38.34600084496253),
                    new Vector2D( 90.14045907000337,  38.34601860032171),
                    new Vector2D( 90.14039496493928,  38.346223030432384),
                    new Vector2D( 90.14037626063737,  38.346240203360026),
                    new Vector2D( 90.14030005823724,  38.34646920000705),
                    new Vector2D( 90.13799164754806,  38.34903093011013),
                    new Vector2D( 90.11045289492762,  38.36801537312368),
                    new Vector2D( 90.10871471476526,  38.36878044144294),
                    new Vector2D( 90.10424901707671,  38.374300101757),
                    new Vector2D( 90.10263482039932,  38.37310041316073),
                    new Vector2D( 90.09834601753448,  38.373615053823414),
                    new Vector2D( 90.0979455456843,  38.373578376172475),
                    new Vector2D( 90.09086514328669,  38.37527884194668),
                    new Vector2D( 90.09084931407364,  38.37590801712463),
                    new Vector2D( 90.09081227075944,  38.37526295920463),
                    new Vector2D( 90.09081378927135,  38.375193883266434)
            }
        };
        PolygonsSet set1 = buildSet(vertices1);

        Vector2D[][] vertices2 = new Vector2D[][] {
            new Vector2D[] {
                    new Vector2D( 90.13067558880044,  38.36977255037573),
                    new Vector2D( 90.12907570488,  38.36817308242706),
                    new Vector2D( 90.1342774136516,  38.356886880294724),
                    new Vector2D( 90.13090330629757,  38.34664392676211),
                    new Vector2D( 90.13078571364593,  38.344904617518466),
                    new Vector2D( 90.1315602208914,  38.3447185040846),
                    new Vector2D( 90.1316336226821,  38.34470643148342),
                    new Vector2D( 90.134020944832,  38.340936644972885),
                    new Vector2D( 90.13912536387306,  38.335497255122334),
                    new Vector2D( 90.1396178806582,  38.334878075552126),
                    new Vector2D( 90.14083049696671,  38.33316530644106),
                    new Vector2D( 90.14145252901329,  38.33152722916191),
                    new Vector2D( 90.1404779335565,  38.32863516047786),
                    new Vector2D( 90.14282712131586,  38.327504432532066),
                    new Vector2D( 90.14616669875488,  38.3237354115015),
                    new Vector2D( 90.14860976050608,  38.315714862457924),
                    new Vector2D( 90.14999277782437,  38.3164932507504),
                    new Vector2D( 90.15005207194997,  38.316534677663356),
                    new Vector2D( 90.15508513859612,  38.31878731691609),
                    new Vector2D( 90.15919938519221,  38.31852743183782),
                    new Vector2D( 90.16093758658837,  38.31880662005153),
                    new Vector2D( 90.16099420184912,  38.318825953291594),
                    new Vector2D( 90.1665411125756,  38.31859497874757),
                    new Vector2D( 90.16999653861313,  38.32505772048029),
                    new Vector2D( 90.17475243391698,  38.32594398441148),
                    new Vector2D( 90.17940844844992,  38.327427213761325),
                    new Vector2D( 90.20951909541378,  38.330616833491774),
                    new Vector2D( 90.2155400467941,  38.331746223670336),
                    new Vector2D( 90.21559881391778,  38.33175551425302),
                    new Vector2D( 90.21916646426041,  38.332584299620805),
                    new Vector2D( 90.23863749852285,  38.34778978875795),
                    new Vector2D( 90.25459855175802,  38.357790570608984),
                    new Vector2D( 90.25964298227257,  38.356918010203174),
                    new Vector2D( 90.26024593994703,  38.361692743151366),
                    new Vector2D( 90.26146187570015,  38.36311080550837),
                    new Vector2D( 90.26614159359622,  38.36510808579902),
                    new Vector2D( 90.26621342936448,  38.36507942500333),
                    new Vector2D( 90.26652190211962,  38.36494042196722),
                    new Vector2D( 90.26621240678867,  38.365113172030874),
                    new Vector2D( 90.26614057102057,  38.365141832826794),
                    new Vector2D( 90.26380080055299,  38.3660381760273),
                    new Vector2D( 90.26315345241,  38.36670658276421),
                    new Vector2D( 90.26251574942881,  38.367490323488084),
                    new Vector2D( 90.26247873448426,  38.36755266444749),
                    new Vector2D( 90.26234628016698,  38.36787989125406),
                    new Vector2D( 90.26214559424784,  38.36945909356126),
                    new Vector2D( 90.25861728442555,  38.37200753430875),
                    new Vector2D( 90.23905557537864,  38.375405314295904),
                    new Vector2D( 90.22517251874075,  38.38984691662256),
                    new Vector2D( 90.22549955153215,  38.3911564273979),
                    new Vector2D( 90.22434386063355,  38.391476432092134),
                    new Vector2D( 90.22147729457276,  38.39134652252034),
                    new Vector2D( 90.22142070120117,  38.391349167741964),
                    new Vector2D( 90.20665060751588,  38.39475580900313),
                    new Vector2D( 90.20042268367109,  38.39842558622888),
                    new Vector2D( 90.17423771242085,  38.402727751805344),
                    new Vector2D( 90.16756796257476,  38.40913898597597),
                    new Vector2D( 90.16728283954308,  38.411255399912875),
                    new Vector2D( 90.16703538220418,  38.41136059866693),
                    new Vector2D( 90.16725865657685,  38.41013618805954),
                    new Vector2D( 90.16746107640665,  38.40902614307544),
                    new Vector2D( 90.16122795307462,  38.39773101873203)
            }
        };
        PolygonsSet set2 = buildSet(vertices2);
        PolygonsSet set  = (PolygonsSet) new
RegionFactory<Euclidean2D>().difference(set1.copySelf(),

              set2.copySelf());

        Vector2D[][] verticies = set.getVertices();
        Assert.assertTrue(verticies[0][0] != null);
        Assert.assertEquals(1, verticies.length);
    }
{code}",,,,,,,,,,,,,,,,,,,,,12/Oct/12 17:37;curtis;PolygonDiffAll.png;https://issues.apache.org/jira/secure/attachment/12548927/PolygonDiffAll.png,12/Oct/12 17:37;curtis;PolygonDiffResults.png;https://issues.apache.org/jira/secure/attachment/12548928/PolygonDiffResults.png,12/Oct/12 17:35;curtis;PolygonInputs.png;https://issues.apache.org/jira/secure/attachment/12548926/PolygonInputs.png,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-10-17 14:43:46.056,,,false,,,,,,,,,,,,,,248021,,,Tue Oct 30 19:54:41 UTC 2012,,,,,,0|i09ahz:,52134,,,,,,,,12/Oct/12 17:35;curtis;Plot of the input polygons.,12/Oct/12 17:37;curtis;Attached images of difference results (by themselves and overlayed on the inputs),"12/Oct/12 17:37;curtis;FYI:  I have discovered, that if I change the decimal precision of the
input polygons, I get a much better result (no nulls, and it looks
almost correct).  I changed the inputs to have only 7 digits after the
decimal place.  Though, it is still the incorrect results (just much
closer).","12/Oct/12 17:37;curtis;I've also discovered  that if I translate the points to left by 89
units (I changed 90 to 1), that the results are even better.","17/Oct/12 14:43;luc;I confirm the issue.

The null points probably come from the algorithm thinking some boundaries are not closed, i.e. they extend to infinity. This is the way they are represented when transforming from BSP to boundary representation. So the problem is not the nul points by themselves, but trather why the algorithm fails to see the boundary should remain closed here.

There is probably a numerical problem here. It will require some investigations.","17/Oct/12 15:37;luc;I have reduced the problem to the following case with fewer points so it is easier to debug:

{code}
    @Test
    public void testIssue880() {

        Vector2D[][] vertices1 = new Vector2D[][] {
            new Vector2D[] {
                    new Vector2D( 90.13595870833188,  38.33604606376991),
                    new Vector2D( 90.14047850603913,  38.34600084496253),
                    new Vector2D( 90.11045289492762,  38.36801537312368),
                    new Vector2D( 90.10871471476526,  38.36878044144294),
                    new Vector2D( 90.10424901707671,  38.374300101757),
                    new Vector2D( 90.0979455456843,   38.373578376172475),
                    new Vector2D( 90.09081227075944,  38.37526295920463),
                    new Vector2D( 90.09081378927135,  38.375193883266434)
            }
        };

        Vector2D[][] vertices2 = new Vector2D[][] {
            new Vector2D[] {
                    new Vector2D( 90.13067558880044,  38.36977255037573),
                    new Vector2D( 90.1342774136516,   38.356886880294724),
                    new Vector2D( 90.13090330629757,  38.34664392676211),
                    new Vector2D( 90.15508513859612,  38.31878731691609),
                    new Vector2D( 90.15919938519221,  38.31852743183782),
            }
        };
        PolygonsSet set  =
                (PolygonsSet) new RegionFactory<Euclidean2D>().difference(buildSet(vertices1),
                                                                          buildSet(vertices2));

        Vector2D[][] verticies = set.getVertices();
        Assert.assertTrue(verticies[0][0] != null);
        Assert.assertEquals(1, verticies.length);

    }
{code}","17/Oct/12 16:49;luc;The problem does not lie in the polygon difference, but in the creation of the first polygon, the one created using buildset(vertices1).
Instead of being a simple closed shape, it extends to infinity in two branches. This is easily seen by using simple points checks to see which points are inside as follows:

{code}
        for (double x = 90.0; x < 90.15; x += 0.0005) {
            for (double y = 38.33; y < 38.4; y += 0.0005) {
                Vector2D p = new Vector2D(x, y);
                if (set1.checkPoint(p) == Location.INSIDE) {
                    System.out.println(x + "" "" + y);
                }
            }
        }
{code}

So we have to fix the polygon building from boundary representation.","18/Oct/12 13:41;luc;Here are some news about the investigations done on this issue.

The problem is indeed due to some numerical issues. During the conversion from boundary representation to BSP tree, we build edges from successive pair of points {p0, p1}, {p1, p2}, {p2, p3} ... These edges belong to hyperplanes (i.e. lines) h01, h12, h23 ... These hyperplanes are then inserted into a BSP tree which is built top down. What happens here is that as h12 is inserted in a tree which already contains h01, we compute the intersection of the two hyperplanes and find a point p1' slightly different from p1 due to numerical considerations. This points extends the edge very slightly, and it is considered afterwards to extend on both sides of h01 instead of one side only. So both sides of hyperplane h01 are split by hyperplane h12. The side that should not have been split now contains an additional artificial cut.

I am still looking for a solution to this. It seems doubtful to be able to completely avoid numerical problems and avoid this extra splitting. However, BSP trees allow extra cut hyperplanes to appear in the tree without belonging to the boundary (this happens for example with non-convex shapes). These extra cut sub-hyperplane simply define two neighboring cells which are both inside or outside the polygon. I am looking at a way to detect this case more efficiently and avoid creating an artificial ""inside"" cell.

This is a difficult problem.","18/Oct/12 15:14;curtis;Thanks for looking into.  We were also seeing problems with the inside detection and figured that there may be some connection between the contains and the difference problems.  We looked through the code to attempt to find the problem.  We soon realized that we would need to appeal to outside help.  So, again, thanks for looking at the problem.","18/Oct/12 15:57;luc;Curtis, I was wondering if the API we propose for building the polygons is a good one.
It was created this way because it is dimension-independent and is in fact implemented at a rather high level (in the AbstractRegion class).

Perhaps we should provide you an additional way to build polygons, which would be 2D-specific and would not lose topologic information like the general case obviously does now. When you build your polygons, do you have some additional knowledge about it? It seems you already have continuous loops and have organized your points in a logical way. Is this always the case? Could we assume that you know how the various segments are connected together, could we assume you always have closed shapes, could we assume you loops around the shapes with the inside on your left? If so, do you think having a dedicated API for 2D-polygons would be useful to you?","21/Oct/12 10:04;luc;Here is an update about this issue.

I have created a new constructor using only a simple boundary loop as a sequence of vertices:
{code}
    /** Build a polygon from a simple list of vertices.
     * <p>The boundary is provided as a list of points considering to
     * represent the vertices of a simple loop. The interior part of the
     * region is on the left side of this path and the exterior is on its
     * right side.</p>
     * <p>This constructor does not handle polygons with a boundary
     * forming several disconnected paths (such as polygons with holes).</p>
     * <p>For cases where this simple constructor applies, it is expected to
     * be numerically more robust than the {@link #PolygonsSet(Collection) general
     * constructor} using {@link SubHyperplane subhyperplanes}.</p>
     * <p>If the list is empty, the region will represent the whole
     * space.</p>
     * @param hyperplaneThickness tolerance below which points are consider to
     * belong to the hyperplane (which is therefore more a slab)
     * @param vertices vertices of the simple loop boundary
     */
    public PolygonsSet(final double hyperplaneThickness, final Vector2D ... vertices) {
      ...
    }
{code}

This constructor avoid the two way conversion: vertices -> hyperplanes -> rebuilt vertices. This two way conversion induces the problem as the vertices rebuilt from hyperplanes intersections are numerically slightly different from the original vertices. It does so by preserving the initial vertices all the way through, so it uses only the the first part of the conversion: vertices -> hyperplanes and never comes backs from hyperplanes to vertices.

As explained in the javadoc, this constructor does not handle boundaries in several paths. So if for example a polygon with a hole is to be built, then one polygon without the hole and hole itself can be built separately from this constructor, and the final polygon with a hole would be created by subtracting the hole from the polygon. I don't think the limitation is too cumbersome for users, so I think I will let the API as is.

This works well on the reduced case I presented in earlier comments above, but currently fails miserably on the initial bug report.
I am working on it.","21/Oct/12 20:11;luc;A partial fix for the problem has been committed in subversion as of r1400717.

There are still some problems with the test case, as shown by two TODO markers in the unit tests.

Could you check if this fix at least partially works for you?","22/Oct/12 14:38;luc;Well, the remaining problem is worse than expected. The current version even fails on the simplistic following case:

{code}
Vector2D[] vertices = new Vector2D[] {
    new Vector2D(-6, -4), new Vector2D(-8, -8), new Vector2D(  8, -8),
    new Vector2D( 6, -4), new Vector2D(10,  4), new Vector2D(-10,  4)
};
PolygonsSet set = new PolygonsSet(1.0e-10, vertices);
{code}

Still working on it :-(","22/Oct/12 19:28;luc;Fixed in subversion repository as of r1401022.

Note that the unit test corresponding to the issue uses a value equal to 1.0e-8 for the hyperplaneThickness parameter. Smaller values do not work due to numerical issues. This new parameter is therefore an important one for such problems and too small values should be avoided.

There are no definitive value that can be suggested and would work in every case. The appropriate value depends on the polygon (does it have thin pikes or dents, does it have vertices with almost flat angles, does it have independent edges that happen to be almost aligned despite they are not directly connected ..).

Thanks for reporting the problem.","30/Oct/12 19:54;curtis;The new PolygonsSet constructor fixes the problems we have been seeing.  So far 1.0e-8 works for all of our cases.  If it ever doesn't work, we'll come up with a heuristic for setting the thickness.

To answer the earlier questions about how we organize our data, we have created a layer on top of Apache Math.  We have developed our own internal API for different general euclidean representations and operations that we use on different projects.   We work both in 3D and 2D coordinate spaces.  Our polygon classes contain a clockwise ordered array of points.  In all cases, our polygons are sequentially connected, fully continuous, and closed (nothing fancy).  We keep track of holes as polygons defined in the same way.  We have methods to convert and order points in many different fashions.  Our 3D space is almost exclusively topographical in nature, so we generally reduce to 2D for most geometric calculations.  In addition to a topographical third dimension we do sometimes keep track of other information on those points (such as a altitude or measurement error value).  The PolygonsSet constructor you have created should work nicely for us.  If there was a way to build 2D polygons but keep the topological information (or other information), that would be nice; though, we are able to work with whatever you provide.

Because we have an abstraction layer on top of Apache Math, we should be able to adapt to any API that is provided.  Before Apache Math 3, we had a combination of our home grown code and Sun's Vecmath (Thanks for Apache Math 3; it fixes some limitations we had previously).  In that regard, the API for building polygons is ""a good one.""  It does what we need.  As long as there are examples, test cases, and documentation we'll figure it out.  The Apache Math API has many capabilities we don't need and so the building of Polygons is probably not as simple as it could be for what we are doing.  Now that you know roughly how we are using it, I'll leave it up to as to how ""good"" the API is for building polygons. One limitation I would point out is the absence of non-polygonal shapes like circles/spheres, ellipses/ellipsoids, cylinders, etc.  All in all, we are happy with it and very appreciative.

Thanks again, and Happy Halloween
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""CMAESOptimizer"" silently changes invalid input",MATH-879,12611338,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,erans,erans,erans,11/Oct/12 12:33,20/Aug/15 20:04,07/Apr/19 20:38,20/Aug/15 20:04,3.0,,,,,,,4.0,,,0,,,,,,,,"The ""lambda"" input parameter must be strictly positive. But when it's not the case, an undocumented default is used (cf. line 526).
When a precondition is not satisfied, the code must throw an exception.
Instead of the code unknowingly changing the input, it is rather the documentation that should suggest a good default.

This change would allow to make ""lambda"" a constant (final) field.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,247321,,,Thu Aug 20 20:04:27 UTC 2015,,,,,,0|i089vb:,46198,,,,,,,,"18/Oct/12 13:56;erans;The code is:
{code}
if (lambda <= 0) {
  lambda = 4 + (int) (3 * Math.log(dimension));
}
{code}

This implies that the default is dependent on the number of optimized parameters. Hence, I would suggest that ""lambda"" be specified at the call to ""optimize"", as kind of ""OptimizationData"" (with a more suggestive name, such as ""PopulationSize""), with no default (the value currently used in the code could appear as a _suggestion_ in the documentation).
And passing this parameter through the constructor will be deprecated.

What do you think?
","19/Oct/12 14:23;erans;""PopulationSize"" created in revision 1400108. Old code deprecated.

Further changes postponed until preparation of 4.0 (see code marked with ""XXX"").
","20/Aug/15 20:04;erans;Probably fixed some time ago, as there is no more code marked with ""XXX""...
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In v3, Bundle-SymbolicName should be org.apache.commons.math3 (not org.apache.commons.math as currently)",MATH-876,12610805,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mwebber,mwebber,08/Oct/12 09:22,04/Mar/13 18:53,07/Apr/19 20:38,15/Jan/13 12:23,3.0,,,,,,,3.1,,,0,build,,,,,,,"In Commons Math 3.0, all package names start with {{org.apache.commons.math3}}, to distinguish them from packages in the previous (2.2) - issue MATH-444.

However, the name of the bundle itself was not similarly changed - in the MANIFEST.MF from 3.0.0, we have this line:
{{Bundle-SymbolicName: org.apache.commons.math}}

This should be changed in 3.1 to:
{{Bundle-SymbolicName: org.apache.commons.math3}}

As an example, Apache Commons Lang changed their bundle name when they moved from v2 to v3 - exactly what I am proposing for Commons Math.

For various reasons, the existing plugin naming is a problem for us in our environment, where our code uses a mixture of 2.2 and 3.0 classes (there are too many references to quickly change).

",,,,,,,,,,,,,,,,,,,,,06/Jan/13 16:47;niallp;MATH876-bundle-name.patch;https://issues.apache.org/jira/secure/attachment/12563490/MATH876-bundle-name.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-10-08 13:18:03.721,,,false,,,,,,,,,,,,,,245519,,,Sun Jan 06 22:48:28 UTC 2013,,,,,,0|i06bdz:,34778,,,,,,,,"08/Oct/12 13:18;erans;Property ""commons.componentid"" changed to ""math3"" in revision 1395545.
",08/Oct/12 13:52;mwebber;Brilliant - thanks for the prompt fix. I look forward to the 3.1 release!,"08/Oct/12 14:27;erans;You could already test with the snapshot that will be generated tonight:

https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/","18/Oct/12 10:43;mwebber;We are now building against a 3.1 snapshot (with the new bundle name), and have not had any issues so far (based on limited testing).

Many thanks for promptly fixing this ticket.","18/Oct/12 11:01;erans;Thanks for the feedback.
I didn't think using the new ""MANIFEST.MF"" would create any problem; it's rather the release process which might be impacted, so I leave this open until we try it out (hopefully pretty soon).
",06/Jan/13 16:47;niallp;It would be better to override the commons.osgi.symbolicName property rather than changing the componentid to do this - attaching a patch,06/Jan/13 17:48;psteitz;+1 to apply Niall's patch,06/Jan/13 22:48;erans;Changed in revision 1429607.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
math3 SecantSolver can return Double.INFINITE,MATH-871,12609396,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Cannot Reproduce,,anthonymaidment,anthonymaidment,27/Sep/12 14:19,15/Oct/12 13:23,07/Apr/19 20:38,15/Oct/12 13:23,3.0,,,,,,,,,,0,,,,,,,,"In SecantSolver.doSolve(), I had a situation in which f0 and f1, from computeObjectiveValue() on lines 77 & 78, were the same value.

Then when it calculates the next appromixation at line 101:
final double x = x1 - ((f1 * (x1 - x0)) / (f1 - f0));
The denominator is then zero, and the next approximation is Double.INFINITE.

I was able to work around this in this particular instance by relaxing the accuracy requirements of the solver, although I haven't yet fully tested the downstream implications of this change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-09-28 11:23:44.534,,,false,,,,,,,,,,,,,,241564,,,Mon Oct 15 13:23:27 UTC 2012,,,,,,0|i028of:,11007,,,,,,,,"28/Sep/12 11:23;erans;Thanks for the report. Could you provide a unit test showing the failure?

At first sight, this seems the result of a inherent weakness of the algorithm, not a bug in the implementation.
With your particular use-case (i.e. a code excerpt), it will be useful to raise the issue on the ""dev"" in order to discuss whether to introduce a check to detect this problem.
","01/Oct/12 06:18;dhendriks;bq. final double x = x1 - ((f1 * (x1 - x0)) / (f1 - f0));

BaseSecantSolver (used for IllinoisSolver, PegasusSolver, and RegulaFalsiSolver), has the same line of code, at line 162 (CM 3.0 release).","01/Oct/12 09:25;erans;bq. In SecantSolver.doSolve(), I had a situation in which f0 and f1, from computeObjectiveValue() on lines 77 & 78, were the same value.

My first comment was a bit hasty.
""f0"" and f1"" cannot have the same value, as that would mean that there is no bracketing, a condition that is checked and, if not satisfied, raises an exception.

The line numbers you refer to seems to indicate that I do not look at the same code: Did you test with a recent snapshot of the library?
Alternately, please provide the use case.

",15/Oct/12 13:23;erans;Please reopen with more precise information.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default sigma for CMAESOptimizer is wrong when using bounds,MATH-868,12608714,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Done,,fhess,fhess,21/Sep/12 23:47,07/Jun/16 20:50,07/Apr/19 20:38,07/Jun/16 20:50,,,,,,,,,,,0,,,,,,,,"The documentation suggests setting inputSigma to 1/3 the range you are fitting over.  However, in CMAESOptimizer.initializeCMA() if boundaries are specified the sigmaArray is by default assigned a value of 0.3 divided by the range.  If the user had specified the inputSigma to be 0.3 of the range (as suggested by the docs) then sigmaArray would have been assigned the value of 0.3.  Thus, it looks like the 0.3 should not be divided by the range, only a user-specified inputSigma should get divided by the range.",,,,,,,,,,,,,,,,,MATH-872,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-09-22 11:51:04.813,,,false,,,,,,,,,,,,,,241567,,,Tue Jun 07 20:50:16 UTC 2016,,,,,,0|i028p3:,11010,,,,,,,,"22/Sep/12 11:51;erans;IIUC, the ""inputSigma"" should never be divided by the ""range"" but rather should be enforced to lie in the (0, 1) interval (and the documentation should suggest that ""0.3"" is a good default, and the value that is used when the user did not pass an ""inputSigma"" argument).

What do you think?
","22/Sep/12 23:48;erans;While running the code which you provided in MATH-867, I noticed that having an ""inputSigma"" either much larger or much smaller than 0.3 gives even worse results for that test (both when the optimum is near the lower bound and whne it's near the upper bound).
From this, I'd suspect that the code is correct (always divided by the range), but that the documentation is wrong to suggest multiplying 0.3 by the range as a good value for ""inputSigma"".

I propose to modify the doc of ""inputSigma"", indicating that it should be about 0.3.
That looks strange, and the problem may well lie deeper (in how constraints are taken into account). Unless you have a test case, there isn't much more I can do, unfortunately.
","24/Sep/12 12:54;fhess;I found an older ticket on this: MATH-702.  Apparently, the intent is for inputSigma to be in the same units as the other parameters (not normalized to the interval zero to one).  I can confirm when I play with the MATH-867 test it does worse with sigma that are much less than or greater than 1, I'm not sure what that means.","24/Sep/12 13:03;fhess;Also, in the Matlab code from http://www.lri.fr/~hansen I found a place where he defaults the sigma to 0.3 times the range:

insigma = 0.3*(ubounds-lbounds);","29/Sep/12 22:52;erans;The fix for MATH-867 probably fixes this issue too.
What do you think?
","01/Oct/12 14:19;fhess;After the MATH-867 fix, it defaults inputSigma to 0.3 regardless of boundaries.  This is effectively the same as the behavior I originally reported in this bug.  I'm okay with that though, as the simpler code no longer makes it look like it is trying to do something fancier.  I suggest the class be given a public DEFAULT_INPUTSIGMA constant equal to 0.3 (like the class's other DEFAULT_XXX constants) to make it explicit and documented what the default is.
","01/Oct/12 16:26;erans;Quoting Nikolaus Hansen (from issue MATH-867):

bq. Points beyond, say, startpoint + 10*sigma are not likely to be sampled in the beginning.

and

bq. Codes I write typically don't accept a default value for inputSigma, [...]

I wonder whether it wouldn't be better to not have a default, i.e. deprecate the constructors that do not require the values to be specified.

The situation is even worse, I see now, in that ""inputSigma"" should not be passed at all at construction because it depends on the number of parameters and on their scale (values of the upper and lower bounds), whereas the interface to the CM optimizers was designed to allow multiple ""optimize"" calls to the _same_ optimizer with
* different functions (even with a different number of parameters),
* different start points,
* different boundaries.

All these arguments are passed with the call to ""optimize"". Ideally the ""inputSigma"" argument should be specified at that level too. But it can't because it's not (and cannot be) part of the general interface.
A solution to this API problem would be welcome for 4.0.
","01/Oct/12 17:43;fhess;Well, the SimplexOptimizer handles this issue by having a ""setSimplex"" method which can be called before each call to optimize.  CMAESOptimizer could have a ""setInputSigma"" method.","01/Oct/12 21:51;erans;I think that it is the best we can do for now.
","15/Oct/12 12:22;erans;Actually, MATH-874 provides a solution to make the API uniform.
","18/Oct/12 11:53;erans;Coming back to the issue the default value for sigma.
Currently, it is thus still 0.3 but IIUC, this might be a very bad default if the range of the parameters is large and the initial guess is far from the optimum (cf. Nikolaus's advice that it should be about one third of the initial search volume).
Nikolaus also mentioned that providing a default might simply not be a good idea at all.

Hence I'd suggest that if ""inputSigma"" is null, an exception be thrown (instead of making up a possibly meaningless default).
Unfortunately, that will break some applications (if they use the constructors without the ""inputSigma"" argument). Could the current behaviour (not specifying ""inputSigma"") be considered buggy enough to warrant a compatibility breaking?
","19/Oct/12 14:24;erans;Code and unit tests updated in revision 1400108.

Further changes postponed until preparation of 4.0 (see code marked with ""XXX"").
","07/Jun/16 20:50;erans;There is no code marked with ""XXX"" anymore.
Thus, I guess that the issue should have been closed earlier.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CMAESOptimizer with bounds fits finely near lower bound and coarsely near upper bound. ,MATH-867,12608709,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,fhess,fhess,21/Sep/12 22:52,04/Mar/13 18:53,07/Apr/19 20:38,01/Oct/12 16:09,,,,,,,,3.1,,,0,,,,,,,,"When fitting with bounds, the CMAESOptimizer fits finely near the lower bound and coarsely near the upper bound.  This is because it internally maps the fitted parameter range into the interval [0,1].  The unit of least precision (ulp) between floating point numbers is much smaller near zero than near one.  Thus, fits have much better resolution near the lower bound (which is mapped to zero) than the upper bound (which is mapped to one).  I will attach a example program to demonstrate.",,,,,,,,,,,,,,,,,,,,,28/Sep/12 15:42;fhess;MATH867_patch;https://issues.apache.org/jira/secure/attachment/12547005/MATH867_patch,21/Sep/12 23:00;fhess;Math867Test.java;https://issues.apache.org/jira/secure/attachment/12546110/Math867Test.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-09-23 00:27:57.766,,,false,,,,,,,,,,,,,,292266,,,Wed Oct 03 18:10:25 UTC 2012,,,,,,0|i0rsxb:,160330,,,,,,,,21/Sep/12 23:00;fhess;Attached a test program.,"23/Sep/12 00:27;erans;bq. This is because it internally maps the fitted parameter range into the interval [0,1]

If this is indeed the reason, not much can be done short of modifying the algorithm. The implementation we have in CM was ported from the original code under the supervision of the original author. Maybe you should ask him this question.
","24/Sep/12 13:28;fhess;I've sent an email to the original author.  I've copied what I wrote to him below for reference:
{quote}
Hi,

I was wondering if you have any input on this bug in the Apache Commons implementation of your algorithm:

https://issues.apache.org/jira/browse/MATH-867

It seems to be due to their attempt to follow your ""encoding of variables"" hint.  They map parameters on to the interval [0,1] when boundaries are provided, which can have a bad interaction with how floating point variable ULP are distributed.  Since your hint seems to suggest the main point of this encoding is just make the width of the intervals uniform, it seems to me they could just scale the boundaries onto a interval of unit width without offseting the interval to start at zero.  This would get rid of the mismatch in ULP distribution between the scaled interval and the original boundaries.
{quote}
","24/Sep/12 13:36;fhess;I tried changing the FitnessFunction.encode/decode methods so they don't offset and that causes my test program to fit well near both bounds.  That is, I changed encode from:

res[i] = (x[i] - boundaries[0][i]) / diff;

to: 

res[i] = x[i] / diff;

and changed decode from:

res[i] = diff * x[i] + boundaries[0][i];

to:

res[i] = diff * x[i];
  ","24/Sep/12 15:18;jolievie;I don't see anything wrong with the new version (the original version better facilitates the display of the evolution of variables in a single picture). It seems also clear where the original version fails: taking the difference in the above computation leads to a loss of significant digits if x[i] and boundaries[0][i] largely differ, that is, if the solution is far away from the lower bound. 

However the use of boundaries for a range like [0, 5e16] seems not reasonable to me and it was not meant to be used like that. More specifically, I don't see a good reason to set an upper bound of 5e16, in particular when the initial point is 1. I would expect a reasonable initial point to lie roughly in the middle of the search interval. If the variable is supposed to be as large as 5e16, it is likely advisable to apply a non-linear transformation, e.g. to optimization its logarithm. More general, when searching in an interval of size 1e16 using double precision, one can, in principle, hardly expect to get a solution with a precision better than, say, 10 in which case one has identified the optimum with 15 digits of precision. 

","24/Sep/12 15:42;fhess;If I'm fitting a peak width, for example, I only want to fit positive widths.  So specifying a range with a bound on only one end like [0,+Infinity] would be natural.  Now the CMAESOptimizer doesn't accept infinite bounds, so the best I could do would be something like [0, VeryLargeValue].  And I might reasonably use an initial peak width of 1.  My expectation as a user is that specifying this bound actually helps the optimizer.  Instead what happens is the entirely the opposite (well, actually it works fine when the lower bound is zero but would blow up if you were setting a finite upper bound instead of a finite lower bound).

As a user the encoding of my fitted parameters to the interval 0 to 1 is entirely invisible and internal to the library.  So my expectation is the optimizer would give results with a precision matching the precision of the parameter being fitted (in the form I passed it in, not in some internal normalized form I know nothing about).","24/Sep/12 16:09;fhess;To elaborate on my previous point, the CMAESOptimizer also doesn't allow mixing of bounded and unbounded parameters.  So, if I only want to apply a bound to one parameter of a multi-parameter fit, then the best I can do is set the bounds of the ""unbounded"" parameters to be [-VeryLargeValue, +VeryLargeValue].  This causes the fit precision around zero for the ""unbounded"" parameters to be much worse than when no bounds are specified at all.","24/Sep/12 16:26;jolievie;It seems important to point out that a parameter transformation or scaling and variable boundaries are two different things. That they are mixed in the code/interface I would indeed consider as a bug. Generally, the boundary handling can be done without any variable transformation and therefore does not need to effect precision. 

If we want positive variable values only, the code should IMHO support only applying a lower bound in some way. 

Maybe it is still important to mention: a relevant initial parameter to CMA-ES is an initial step-size (a standard deviation) possibly in each coordinate. You might think of this step-size as similar to the width of the initial simplex in Nelder&Mead. Choosing this step-size is equivalent to choosing a different ""diff""-factor in the encoding as suggested above. 
","24/Sep/12 18:06;fhess;bq. It seems important to point out that a parameter transformation or scaling and variable boundaries are two different things. That they are mixed in the code/interface I would indeed consider as a bug. Generally, the boundary handling can be done without any variable transformation and therefore does not need to effect precision.

That's good news, I think things would work a lot closer to my expectations if the boundary handling was done without a transformation.

bq.  If we want positive variable values only, the code should IMHO support only applying a lower bound in some way. 

AFAICT the variable transformation is the main reason CMAESOptimizer doesn't let you use infinite values as bounds.  So if the transformation is dropped, we can have infinity as a bound value, which gives support for only specifying a bound on one side, plus mixing bounded and unbounded parameters.","26/Sep/12 08:44;erans;bq. That they are mixed in the code/interface I would indeed consider as a bug.

Is this ""bug"" in the original code?

bq. Generally, the boundary handling can be done without any variable transformation [...]

But isn't this variable transformation part of the original code? At least, that's how it looked like from my perspective since [~docdwo] contributed the port while in contact with you.
Do you mean that the ""encode"" and ""decode"" steps can be simply dropped from the code without any ill side-effects?

Another issue (MATH-868) also seems related to the default transformation.

","26/Sep/12 11:29;jolievie;the ""bug"" is not in the original code. In some CMA-ES codes, variable transformations are provided as an option (not in the one which was translated). This is however just meant as a convenience feature, as it could be equivalently implemented as part of the objective function (and should IMHO preferably viewed as such). This should IMHO be a general feature implemented in the optimization library, as the benefits of variable transformations are not tightly linked to any specific optimization method. 

It should be easily possible to drop the transformation in CMAESOptimizer, in which case 0 and 1 must be replaced by the lower and upper boundary values in some parts of the code. 
","26/Sep/12 12:24;jolievie;Why are variables transformations generally desirable? Because, for example, if a parameter x_i should only have positive values, the transformation x_i -> x_i^2 makes the objective function compatible to any unbounded optimizer. Or, for example, rescaling of variables (e.g. unit m in km etc) can change an ill-conditioned problem to a well-conditioned one. ","28/Sep/12 11:14;erans;I've tried Frank's proposal [above|https://issues.apache.org/jira/browse/MATH-867?focusedCommentId=13461788&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13461788]; in the code, I only modified the ""encode"" and ""decode"" functions.
This made one unit test fail (""testConstrainedRosen"").
I think that the test is partly incorrect in the tolerance setting and in the selection of the starting point, but even with those changed, I get critically different behaviours due to the ""isActiveCMA"" flag.
If ""true"", the solution is found with pretty good accuracy:
{noformat}
sol=[1.0000000029406888, 1.0000000045238486, 0.9999999996025084, 0.9999999959542569, 0.9999999988066054, 0.9999999956724651, 1.0000000008220962, 1.0000000011037358, 1.0000000004144547, 0.9999999946437816, 0.9999999977923537, 1.0000000007816154, 1.0000000164552258]
{noformat}

But when set to ""false"", the result is:
{noformat}
sol=[0.997107074864516, 0.9942080214735094, 0.9884131718553784, 0.9768835748661846, 0.954143705394098, 0.910067297297918, 0.8275510138614142, 0.6833931486612853, 0.4636505565068948, 0.20554769008446425, 0.0, 0.009899135523990096, 0.0]
{noformat}

Is this expected? If so, wouldn't it be safer to always set this flag to ""true"" (thus removing it as a user input)?
If unexpected, does it indicate that other things must be changed in addition to ""encode""/""decode""?
","28/Sep/12 12:33;fhess;I think you should also modify isFeasible() and initializeCMA().  isFeasible should use the bounds instead of the unit interval.  initializeCMA should initialize sigmaArray to to be equal to inputSigma if inputSigma is not null, or to 0.3 times the range (as long as the boundaries are not such that the range is calculated to be infinity).","28/Sep/12 14:16;erans;In revision 1391477, I've added a unit test (""testFitAccuracyDependsOnBoundary"") based on your attached file. But it is disabled since it fails with the current implementation (as this was the reason for this report).

I changed ""isFeasible()"" but it was not enough to make the ""testConstrainedRosen"" pass; modifying ""sigmaArray"" as per your last comment entails that the above unit test fails again, though it passed with all but the ""sigmaArray"" changes. :(

Could you please provide a patch against the latest revision, once you are sure that all unit test pass (also removing the ""@Ignore"" annotation on ""testFitAccuracyDependsOnBoundary"")? Thanks.
","28/Sep/12 15:42;fhess;Attached a patch.  It makes encode/decode identities and removes the restrictions on infinite ranges.  The test on fit resolution near upper/lower bounds fails.  I believe the reason is that defaulting sigma to 0.3 times the range causes the fitter to jump from the initial value (which is order of 1 away from the target) to some huge value on the order of 1e16 on the first step.  The fitter then has to work its way all the way back to the target.  It doesn't reach the target exactly due to one of the many stop conditions hard-coded into the generationLoop which decide that the fit is ""good enough"" and quit.  That should probably be entered as a different ticket though, in that there is no way to override the internal stop conditions and make the fitter try harder.  The ConvergenceChecker which can be passed into the optimizer only has the ability to terminate the optimizer earlier than it normally would quit, it doesn't suppress the other stop conditions which can't be directly controlled by the user.","28/Sep/12 15:51;jolievie;{quote}
But when set to ""false"", the result is:
sol=[0.997107074864516, 0.9942080214735094, 0.9884131718553784, 0.9768835748661846, 0.954143705394098, 0.910067297297918, 0.8275510138614142, 0.6833931486612853, 0.4636505565068948, 0.20554769008446425, 0.0, 0.009899135523990096, 0.0]
Is this expected?
{quote}

No! The code must pass this test with high probability. 
","28/Sep/12 16:38;jolievie;{quote}
I believe the reason is that defaulting sigma to 0.3 times the range causes the fitter to jump from the initial value (which is order of 1 away from the target) to some huge value on the order of 1e16 on the first step.
{quote}
Right. I suggest to make sigmaArray independent of the boundary values. Otherwise setting boundaries like [1e-15,1e15] still will most likely lead to an unexpected behavior: the user just does not want to exceed this value, while the algorithm interprets the interval as being reasonable values to be checked out. Still it makes perfectly sense to check consistency between the initial guess, sigmaArray and boundaries, in that, say, 

{code}
fitfun.encode(guess)[i] - sigmaArray[i]/2. > fitfun.encode(boundaries[0])[i] 
{code}

in case of a lower bound. 

{quote}
It doesn't reach the target exactly due to one of the many stop conditions hard-coded into the generationLoop which decide that the fit is ""good enough"" and quit. 
{quote}
more specifically, stopTolX is defined relative to the initial value of sigmaArray. This specific problem should therefore go away if stopTolX is defined as an absolute value like 1e-11. I think absolute and relative definition of stopTolX are both justified. 

Generally, the patch becomes buggy, when encode and decode are changed/reused. In this case, where solutions are compared with boundaries, like 

{code}
if(x[i] < boundaries[0][i])
{code}

the comparison must be done with encoded boundaries: 
{code}
encLboundaries = fitfun.encode(boundaries[0]); 
if (x[i] < encLboundaries[i])
{code}","28/Sep/12 23:04;erans;bq. Attached a patch. It makes encode/decode identities and removes the restrictions on infinite ranges.

Sorry but it is not good: You are removing the fix to your other issue (MATH-865). Even if it won't be necessary anymore when this issue is fixed, it cannot be reverted as part of this issue. Let's leave the possibility to have infinite bounds for later, and focus on making the code work with finite bounds.

bq. The test on fit resolution near upper/lower bounds fails.

This is what must be fixed by the patch, without any side-effects (i.e. other tests failing).

bq. I believe the reason is that defaulting sigma to 0.3 times the range [...]

In MATH-868, I noted that this does not work, since using this value makes the ""testFitAccuracyDependsOnBoundary"" fail. From this I conclude, that if the doc for ""inputSigma"" is correct, then there is something else to be changed in the code, or that the doc must be changed to indicate that ""inputSigma"" is ""relative"".

Nikolaus seems to confirm the latter (IIUC):
{noformat}
fitfun.encode(guess)[i] - sigmaArray[i]/2. > fitfun.encode(boundaries[0])[i]
{noformat}
","29/Sep/12 12:28;jolievie;I second to make encode/decode the identity to address the bug. I don't even see a different way to address it. Then, inputSigma/sigmaArray/insigma should become independent of the boundary values (which seems consistent with the doc, as long as encode/decode remains the identity). 

my previous snippet must read 
{code}
guess[i] - sigmaArray[i]/2. > fitfun.encode(boundaries[0])[i]
{code}
because we have
{code}
final double[] guess = fitfun.encode(getStartPoint());
{code}
i.e. guess is an encoded initial guess. ","29/Sep/12 13:05;erans;I don't understand. This is the documentation:
{code}
/**
 * Individual sigma values - initial search volume. inputSigma determines
 * the initial coordinate wise standard deviations for the search. Setting
 * SIGMA one third of the initial search region is appropriate.
 */
private double[] inputSigma;
{code}

AFAIUC, this says that sigma is _not_ independent on the boundary values.

bq. I second to make encode/decode the identity to address the bug.

I also don't understand this. Referring to the code of ""decode"":
{code}
public double[] decode(final double[] x) {
  if (boundaries == null) {
    return x;
  }
  double[] res = new double[x.length];
  for (int i = 0; i < x.length; i++) {
    double diff = boundaries[1][i] - boundaries[0][i];
    // res[i] = diff * x[i] + boundaries[0][i]; // XXX orig
    // res[i] = diff * x[i]; // XXX v1
    res[i] = x[i]; // XXX v2
  }
  return res;
}
{code}

_This_ issue's bug is solved by normalizing the variables (line marked with ""XXX v1"" in the above snippet). The downside is that ""testConstrainedRosen"" fails.

When ""decode"" is made the identity (line marked with ""XXX v2""), ""testConstrainedRosen"" passes but ""testFitAccuracyDependsOnBoundary"" fails, as with the original code (line marked with ""XXX orig"").

Since in some previous comments, you indicated that boundaries do not necessarily need to be taken into account inside the CMAES algorithm, a possibility is to review the entire code, and remove all code related to boundaries.

Would you (or Frank) be willing to do that? In the affirmative, a new issue must be created for that task.

Then, the ""encode""/""decode"" functions will disappear (by design) and so will the issue about ""inputSigma"".
Unit tests ""testConstrainedRosen"" and ""testFitAccuracyDependsOnBoundary"" will also be removed since the internal support form boundaries won't exist anymore.
All issues solved in one fell swoop! :)
","29/Sep/12 13:45;jolievie;This issue's bug is not solved by v1 alone. The way how to check the boundaries in isFeasible and the method repair must be adapted to the encode/decode function, otherwise a more severe bug has been introduced (even if it would pass all tests). I agree that v1, along with corrected boundary checks, can solve the issue. However v2 makes the remaining modifications simpler. 

Besides, the testConstrainedRosen must not fail, definitely not! ","29/Sep/12 13:54;jolievie;{quote}
Since in some previous comments, you indicated that boundaries do not necessarily need to be taken into account inside the CMAES algorithm, a possibility is to review the entire code, and remove all code related to boundaries.
{quote}
I believe that this must be a misunderstanding. We do not need the transformation into [0,1] (or any other transformation for that matter) to take into account boundaries. But we need to possibly take into account the boundaries within CMAES, if the returned solution is supposed to be in the bounds. 

","29/Sep/12 13:59;jolievie;regarding the documentation of inputSigma: I don't see in what sense the doc says that it depends on the bounds. The doc says how it should be set (but this is a bit blurry, to the extend of how ""initial search volume"" and ""initial search region"" are interpreted)","29/Sep/12 14:36;erans;bq. This issue's bug is not solved by v1 alone. The way how to check the boundaries in isFeasible and the method repair must be adapted to the encode/decode function, otherwise a more severe bug has been introduced (even if it would pass all tests).

This was assumed; I only copied the ""decode"" function here, but the rest was changed accordingly, and the result is as I described (and the same as obtained by Frank, see above).

bq. However v2 makes the remaining modifications simpler.

The problem is that we don't know what are ""the remaining modifications"": we never got beyond to the point were both ""testFitAccuracyDependsOnBoundary"" and ""testConstrainedRosen"" pass.

bq. But we need to possibly take into account the boundaries within CMAES, if the returned solution is supposed to be in the bounds.

There are two different things:
1. Does the existence of constraints modify the search procedure is some way (i.e. CMAES must ""know"" that it deals with boundaries)?
2. Alternately, is it possible to pass a modified objective function (in which the allowed range of the original objective function has been mapped to the [-inf, +inf] interval) and have CMAES behave the same (i.e. find the same solution)?

In the second alternative, CM has adapter classes (e.g. ""MultivariateFunctionPenaltyAdapter"") that handle the mapping (and re-implementing it within each optimizer is an unnecessary source of bugs).

bq. regarding the documentation of inputSigma: I don't see in what sense the doc says that it depends on the bounds.

Then what is ""initial search volume""?
I interpret the doc as roughly saying ""0.3 times the range"". Perhaps this is wrong, in which case it should be made clearer...
We noticed that very small or very large values for ""sigma"" did not work; so maybe we should say ""inputSigma must be of order 1"" (?).","29/Sep/12 15:30;jolievie;{quote}
However v2 makes the remaining modifications simpler.
The problem is that we don't know what are ""the remaining modifications"": 
{quote}
I thought I knew... 
{quote}we never got beyond to the point were both ""testFitAccuracyDependsOnBoundary"" and ""testConstrainedRosen"" pass.
{quote}
the former, because inputSigma was not adapted appropriately, the latter because the test against boundaries was not adapted appropriately. 

{quote}
There are two different things:
1. Does the existence of constraints modify the search procedure is some way (i.e. CMAES must ""know"" that it deals with boundaries)?
{quote}
yes. Otherwise it will sample and evaluation points outside the boundaries. 
{quote}
2. Alternately, is it possible to pass a modified objective function (in which the allowed range of the original objective function has been mapped to the [-inf, +inf] interval) and have CMAES behave the same (i.e. find the same solution)?
{quote}
probably not the same, but possibly reasonably well, unless the boundary is mapped to (or close to) inf, which is likely to lead to unexpected results, if the optimum is on (or close to) the boundary. 

checking MultivariateFunctionPenaltyAdapter (which does not map the parameters, rather computes a penalty), the answer is likely to be no, if the optimal solution happens to be on (or very close to) the bound. The methods developed for CMA-ES should work much better in this case. 

{quote}
regarding the documentation of inputSigma: I don't see in what sense the doc says that it depends on the bounds.
Then what is ""initial search volume""?
{quote}
the volume/region where points are likely to be sampled in the beginning of the search. Points beyond, say, startpoint + 10*sigma are not likely to be sampled in the beginning. 

{quote}
I interpret the doc as roughly saying ""0.3 times the range"". Perhaps this is wrong, in which case it should be made clearer...
{quote}
I agree, it is not clear (I guess it was taken from another doc and slightly changed context). 

{quote}
We noticed that very small or very large values for ""sigma"" did not work; so maybe we should say ""inputSigma must be of order 1"" .
{quote}
no, it entirely depends on how far we expect the optimal solution to be from the start solution. Putting it differently: one could rescale parameters by a factor 1000 (say in one case it is km, in the other m), then one would need to rescale inputSigma accordingly (which would not be in the order of one). 

Codes I write typically don't accept a default value for inputSigma, basically for this reason. I agree however that a value of 1 might often turn out OK. 
 
","29/Sep/12 15:56;jolievie;just another motivation: inputSigma is related to the uncertainty on getStartPoint(), as we cannot intrinsically know whether this is in the order of 1 or 1e-4 or 1e4 or 10e23. ","29/Sep/12 17:13;erans;Revision 1391840 contains modified ""encode"" and ""decode"" functions. Both unit tests now pass (for ""testConstrainedRosen"" I had to move the initial guess closer to the solution).

No change was required for ""inputSigma""; I still do not understand why it works as is (cf. lines 588, 589). And I have no idea how to improve the documentation...


bq. yes. Otherwise it will sample and evaluation points outside the boundaries.

No, because the modified objective function wouldn't have boundaries.

bq. probably not the same, but possibly reasonably well, unless the boundary is mapped to (or close to) inf, which is likely to lead to unexpected results, if the optimum is on (or close to) the boundary. 

In such a case, I imagine that a ""penalty"" adapter would work better than a ""mapping"" adapter. Both are available.

bq. The methods developed for CMA-ES should work much better in this case. 

Hence, unless someone wants to try it out, we'll of course trust you :) and leave the internal boundary handling in place.
","29/Sep/12 19:21;jolievie;{quote}
Revision 1391840 contains modified ""encode"" and ""decode"" functions. Both unit tests now pass (for ""testConstrainedRosen"" I had to move the initial guess closer to the solution).
{quote}
if the test does not pass with initial point at 0.1, something is wrong (and it doesn't look like a good idea to change the test to make the code pass). 

I found at least one problem:

{code}
        private double[] repair(final double[] x) {
            double[] repaired = new double[x.length];
            for (int i = 0; i < x.length; i++) {
                if (x[i] < 0) {
                    repaired[i] = 0;
                } else if (x[i] > 1.0) {
                    repaired[i] = 1.0;
                } else {
                    repaired[i] = x[i];
                }
            }
            return repaired;
        }

{code}
must read 
{code}
        private double[] repair(final double[] x) {
            double[] repaired = new double[x.length];
            if (boundaries == null) {
                for (int i = 0; i < x.length; i++) {
                    repaired[i] = x[i];
                }
            } else {

                final double[] bLoEnc = encode(boundaries[0]);
                final double[] bHiEnc = encode(boundaries[1]);

                for (int i = 0; i < x.length; i++) {
                    if (x[i] < bLoEnc[i]) {
                        repaired[i] = bLoEnc[i];
                    } else if (x[i] > bHiEnc[i]) {
                        repaired[i] = bHiEnc[i];
                    } else {
                        repaired[i] = x[i];
                    }
                }
            }
            return repaired;
    }


{code}
I am not sure whether or not this is the reason why the test fails. ","29/Sep/12 20:48;jolievie;{quote}
Revision 1391840 contains modified ""encode"" and ""decode"" functions. Both unit tests now pass (for ""testConstrainedRosen"" I had to move the initial guess closer to the solution).
No change was required for ""inputSigma""; I still do not understand why it works as is (cf. lines 588, 589). 
{quote}
to me it makes perfectly sense: luckily enough line 589 performs the same transformation on inputSigma as the encode function on getStartPoint() (maybe this should be mentioned in a comment?). As the transformation is linear, the situations before and after the transformations are mathematically equivalent (the bug came from loosing digits due to a subtraction, which is now omitted). As said before it would simplify the code if both transformations were omitted (but we could leave this to another issue). 

{quote}
And I have no idea how to improve the documentation...
{quote}
I'll think about it. 
","29/Sep/12 20:55;jolievie;{quote}
Hence, unless someone wants to try it out, we'll of course trust you  and leave the internal boundary handling in place.
{quote}
I spent several hundreds of hours (really!) to investigate (and try to understand) this with the greatest care I was able to. That of course doesn't mean there could be still surprises out there...
","29/Sep/12 21:00;erans;Thanks for looking into this. Unfortunately, a copy/paste of your version of ""repair"" (which I totally agree with) makes the tests fail again!

And we can also worry that the tests pass with the wrong version...

bq. if the test does not pass with initial point at 0.1, something is wrong (and it doesn't look like a good idea to change the test to make the code pass).

I certainly agree. But since there was something wrong before in the code, one could also imagine that it was because of the problem that it passed before...
","29/Sep/12 21:05;erans;bq. As said before it would simplify the code if both transformations were omitted (but we could leave this to another issue). 

I would like to, but as I said, the identity transformation makes one of the tests fail.

It must be because something else in the code assumes that the parameters have been restricted into the [0, 1] interval, which is not true anymore.

I wonder whether the various matrices (around lines 380) are computed based on this assumption...
","29/Sep/12 21:11;jolievie;I have corrected the repair: the last assignment 
{code}
                    } else {
                        repaired[i] = x[i];
{code}
was missing (sorry). Now there is some chance that it might pass...

{quote}
I certainly agree. But since there was something wrong before in the code, one could also imagine that it was because of the problem that it passed before...
{quote}

that seems somehow irrelevant: it is an absolute statement about the algorithm, that an implementation of it must pass this test. It's not related to the question when and why it has already been past or not. 

otherwise, the code was, AFAICS, not wrong before unless with boundary values as large as 1e16 in absolute value. 
","29/Sep/12 21:21;jolievie;{quote}
I would like to, but as I said, the identity transformation makes one of the tests fail.
It must be because something else in the code assumes that the parameters have been restricted into the [0, 1] interval, which is not true anymore.
{quote}
The reason is IMHO line 589, which does need to be changed according to the change in encode/decode. 

{quote}
I wonder whether the various matrices (around lines 380) are computed based on this assumption...
{quote}
I would not worry much, I am pretty sure they are not. 
","29/Sep/12 21:34;erans;bq. to me it makes perfectly sense: luckily enough line 589 performs the same transformation on inputSigma as the encode function on getStartPoint() [...]

I may be missing something (and I can just make wild guesses since I have no clue about the CMAES algorithm) but I would be expecting that the code behaves the same way without boundaries as with boundaries that become arbitrarily large (i.e. when the [loBound, hiBound] interval becomes [-inf, +inf]).
The line that uses ""inputSigma"" does not behave that way since the ""range"" becomes arbitrarily large as the bounds grow although when there is no boundaries it is set 1.0.

This is also shown by some unit tests which I've just set up, by copying existing ones which minimized a function without constraint and specifying a very large allowed interval (e.g. [-1e20, 1e20]): those tests fail.
Intuitively, when the solution is far from the bounds (and the initial point also), whether there are bounds or not should not matter. But with the current implementation that's clearly not the case.
","29/Sep/12 21:42;erans;bq. The reason is IMHO line 589, which does need to be changed according to the change in encode/decode. 

That cannot be the end of the story; I've just tried:
* remove the division by ""range"" on that line
* ""encode"" and ""decode"" are the identity

And the two tests still fail.
","29/Sep/12 21:42;jolievie;{quote}
I may be missing something (and I can just make wild guesses since I have no clue about the CMAES algorithm) but I would be expecting that the code behaves the same way without boundaries as with boundaries that become arbitrarily large (i.e. when the [loBound, hiBound] interval becomes [-inf, +inf]).
The line that uses ""inputSigma"" does not behave that way since the ""range"" becomes arbitrarily large as the bounds grow although when there is no boundaries it is set 1.0.
{quote}
as both, x and sigma are transformed in the same way it does not matter mathematically. If you choose boundaries as large as (close to) maxdouble, it will matter numerically. 

{quote}
This is also shown by some unit tests which I've just set up, by copying existing ones which minimized a function without constraint and specifying a very large allowed interval (e.g. [-1e20, 1e20]): those tests fail.
{quote}
did you correct the mistake in the repair method I proposed above? 

{quote}
Intuitively, when the solution is far from the bounds (and the initial point also), whether there are bounds or not should not matter. But with the current implementation that's clearly not the case.
{quote}
I agree. I am confident we will be able to sort this out. 
","29/Sep/12 21:45;jolievie;{quote}
And the two tests still fail.
{quote}
did you correct the bug in the repair method that I proposed above? ","29/Sep/12 21:47;erans;bq. I have corrected the repair: the last assignment [...]

Oops, I had missed that comment.
That's much better now. :)
","29/Sep/12 22:28;erans;Revision 1391908 contains the cleaned up code:
* All limitations concerning boundary specifications removed. This supersedes the fix for MATH-865.
* Methods ""encode"" and ""decode"" removed.
* ""sigmaArray"" is assigned the values from ""inputSigma"" (or 0.3 if ""inputSigma"" is null). This probably also solves MATH-868.

Please test.
","29/Sep/12 22:32;erans;bq. did you correct the bug in the repair method that I proposed above?

Yes. There was some unnecessary fiddling because of asynchronous updates of this page and the email notifications of those updates...","01/Oct/12 14:01;fhess;I looked at the commits, and ran the tests locally.  FWIW, everything looks fine to me now.","01/Oct/12 16:09;erans;Thanks; I'll thus set this issue as ""resolved"".
","02/Oct/12 14:46;jolievie;{quote}
And I have no idea how to improve the documentation...
{quote}

Here are my suggestions: Replace (several times) 
{code}
     * @param inputSigma Initial search volume; sigma of offspring objective variables.
{code}
with 

{code}
     * @param inputSigma Initial standard deviations to sample new points from startPoint
{code}

and

{code}
    /**
     * Individual sigma values - initial search volume. inputSigma determines
     * the initial coordinate wise standard deviations for the search. Setting
     * SIGMA one third of the initial search region is appropriate.
     */
{code}

with 

{code}
    /**
     * Values in inputSigma define the initial coordinate-wise 
     * standard deviations for sampling new search points about 
     * startPoint. 
     * Setting inputSigma roughly to the predicted distance of 
     * startPoint to the actually desired optimum is appropriate. 
     * Small values for inputSigma induce the search to be more local
     * and very small values are more likely to find a local optimum 
     * close to startPoint. 
     * Extremely small values will however lead to early termination. 
     */
{code}
","03/Oct/12 18:10;erans;Thanks. The improved Javadoc is in revision 1393641.
"
Wide bounds to CMAESOptimizer result in NaN parameters passed to fitness function,MATH-865,12608333,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,fhess,fhess,19/Sep/12 21:10,04/Mar/13 18:53,07/Apr/19 20:38,22/Sep/12 10:04,3.0,,,,,,,3.1,,,0,,,,,,,,"If you give large values as lower/upper bounds (for example -Double.MAX_VALUE as a lower bound), the optimizer can call the fitness function with parameters set to NaN.  My guess is this is due to FitnessFunction.encode/decode generating NaN when normalizing/denormalizing parameters.  For example, if the difference between the lower and upper bound is greater than Double.MAX_VALUE, encode could divide infinity by infinity.",,,,,,,,,,,,,,,,,,,,,19/Sep/12 22:16;fhess;Math865Test.java;https://issues.apache.org/jira/secure/attachment/12545815/Math865Test.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-21 15:33:23.163,,,false,,,,,,,,,,,,,,292268,,,Sat Sep 29 22:50:47 UTC 2012,,,,,,0|i0rsxr:,160332,,,,,,,,"19/Sep/12 21:33;fhess;Also, if the first call to the fitness function returns NaN and gets stored in the bestValue local variable in doOptimize() before the generationLoop, then the optimization will completely fail.  This is because the later comparison if(bestValue > bestFitness) will always return false if bestValue is NaN and no more optimal result will ever be found.",19/Sep/12 22:16;fhess;Attached test program that demonstrates the problem.,"19/Sep/12 22:41;fhess;Another issue is, if you use bounds then the bounded range is mapped onto the interval [0,1] where the fit is performed, and then blown back up to your specified bounds after getting a result.  This means if you use a large range, the discreteness of Double variables becomes very noticeable in the fit results.  For example, if you set the boundsMagnitude to 5e15 (approx. 1./Math.ulp(1.)) in the Math865Test program I attached, the fitter is unable to fit anything to a precision smaller than 1.","21/Sep/12 15:33;erans;bq. My guess is this is due to FitnessFunction.encode [...] generating NaN

Good guess.

I've introduced a check on the bounds that now throws an exception in case of overflow.
Please test revision 1388552.

Thanks for the report.
","21/Sep/12 22:47;fhess;Yes, it throws the exception for me now (revision 1388555), thanks.  The secondary issue i mentioned in the comment is still a problem, but I'll open a new bug for that.","29/Sep/12 22:50;erans;This issue is superseded by MATH-867 whose fix entails that an overflow cannot happen anymore.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CMAESOptimizer does not enforce bounds,MATH-864,12608315,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,fhess,fhess,19/Sep/12 19:32,04/Mar/13 18:53,07/Apr/19 20:38,22/Sep/12 10:05,3.0,,,,,,,3.1,,,0,,,,,,,,"The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.",,,,,,,,,,,,,,,,,,,,,19/Sep/12 22:07;fhess;Math864Test.java;https://issues.apache.org/jira/secure/attachment/12545812/Math864Test.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-19 21:14:22.681,,,false,,,,,,,,,,,,,,292269,,,Fri Sep 21 22:41:05 UTC 2012,,,,,,0|i0rsxz:,160333,,,,,,,,"19/Sep/12 20:08;fhess;Some more info: I see now that FitnessFunction.value will ""repair"" the parameters to be in-bounds before passing them to computeObjectiveValue.  However, the generationLoop does not ""repair"" the parameters when storing the optimum parameter values, it saves the unrepaired values (which may be outside of bounds).","19/Sep/12 21:14;erans;bq. I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.

It would be very helpful if you could set up a unit test (i.e. a minimal example) showing the failure.
Thanks.

bq. the generationLoop does not ""repair"" the parameters when storing the optimum parameter values

That looks suspicious indeed.
",19/Sep/12 22:07;fhess;Test program that shows bug.,"21/Sep/12 14:21;erans;Thanks for the report, the show-case and the hint toward solving this problem.
Fix committed in revision 1388517. Please test.
","21/Sep/12 22:41;fhess;Thanks, seems to be fixed testing with revision 1388555.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix and then deprecate isSupportXxxInclusive in RealDistribution interface,MATH-859,12606729,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,08/Sep/12 18:15,24/Feb/15 22:09,07/Apr/19 20:38,24/Feb/15 22:09,3.0,,,,,,,4.0,,,0,,,,,,,,"The conclusion from [1] was never implemented. We should deprecate these
properties from the RealDistribution interface, but since removal
will have to wait until 4.0, we should agree on a precise
definition and fix the code to match it in the mean time.

The definition that I propose is that isSupportXxxInclusive means
that when the density function is applied to the upper or lower
bound of support returned by getSupportXxxBound, a finite (i.e. not
infinite), not NaN value is returned.

[1] http://markmail.org/message/dxuxh7eybl7xejde
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2013-03-10 17:14:19.246,,,false,,,,,,,,,,,,,,241570,,,Tue Feb 24 22:09:34 UTC 2015,,,,,,0|i028pr:,11013,,,,,,,,08/Sep/12 22:17;psteitz;Fixes and deprecation done in r1382380.,"10/Mar/13 17:14;tn;Can this issue be resolved?
It is already contained in the 3.0 release notes.","10/Mar/13 17:21;psteitz;The methods can't actually be removed until 4.0, so this should stay open until we cut 4.0 with them removed.","10/Mar/13 17:30;tn;But I thought it is common practice to only remove deprecated methods/classes in the next major release, and there is no need to keep the corresponding issue open till then?","11/Mar/13 04:51;psteitz;Personally, I think it is better to put fix version at the release that will actually resolve the issue by removing the methods, as it is in this case (fix version 4.0).  When we decide to remove or refactor something, I think it is best to keep the issue open until it is fully resolved, which to me means deprecated methods have been removed.  This helps us remember to actually do it when we cut the major release and jogs memory when prepping that release what exactly we had in mind / why methods were deprecated.",24/Feb/15 22:09;tn;Removed deprecated methods in commit ece7c6fc67c0d584f4884c5b17ddf491a397fdfe.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlockRealMatrix java.beans.IntrospectionException,MATH-858,12606711,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,mdongi,mdongi,08/Sep/12 09:56,10/Sep/12 12:26,07/Apr/19 20:38,10/Sep/12 12:26,3.0,,,,,,,,,,0,BlockRealMatrix,,,,,,,"When I try to read properties (like getColumnDimension) of an instance of class BlockRealMatrix (org.apache.commons.math3.linear.BlockRealMatrix), system raise this exception:

Class - java.beans.IntrospectionException
Message - type mismatch between indexed read and indexed write methods: columnMatrix

=================
Code

import org.apache.commons.math3.stat.correlation.PearsonsCorrelation
import strategoianalysis.util.math.matrix.MatrixStatutils
import org.apache.commons.math3.linear.*

class JohnsonRWAController {

    private double[] computeRWA(double[][] data) {

BlockRealMatrix correlationMatrix = new PearsonsCorrelation().computeCorrelationMatrix(data)

        int rowDim = correlationMatrix.getRowDimension() 
        
      //EXCEPTION HERE


.... etc.
",MacOSX 10.8.1 (Macbook Air 4Gb RAM)- IntelliJ IDEA 11 - Grails 2.10 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-09-08 12:06:42.719,,,false,,,,,,,,,,,,,,292272,,,Mon Sep 10 12:26:20 UTC 2012,,,,,,0|i0rsyn:,160336,,,,,,,,"08/Sep/12 12:06;erans;Which version of Commons Math are you using?

In the current development version, this code:
{noformat}
BlockRealMatrix correlationMatrix = new PearsonsCorrelation().computeCorrelationMatrix(data);
{noformat}
does not even compile.

When you report a bug, please provide a fully working minimal code example.

Could you also provide the stack trace of the exception?

Moreover I cannot find any Commons Math error message that contains the string ""mismatch between indexed read""...
","08/Sep/12 16:45;mdongi;Yes sorry, the right line of code is this:
{noformat}RealMatrix correlationMatrix = (new PearsonsCorrelation()).computeCorrelationMatrix(data);{noformat}

When I try to access to getRowDimension() method, the exception occurs.

I'm working on Grails 2.10 platform with apache commons math3 3.0 libraries.

After some other tests, it seems to be a spring/groovy trouble. Error messages comes from Groovy compiler, which raise an org.codehaus.groovy.runtime.InvokerInvocationException.

So it's not a commons math bug. Really thanks for quick response Gilles, and sorry for this _off topic_ issue.
I'm going to close this issue.",10/Sep/12 12:26;mdongi;The problem is not related to apache commons math library. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""BrentOptimizer"" not always reporting the best point",MATH-855,12605932,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,erans,erans,02/Sep/12 23:52,04/Mar/13 18:53,07/Apr/19 20:38,05/Sep/12 14:23,3.0,,,,,,,3.1,,,0,,,,,,,,"{{BrentOptimizer}} (package ""o.a.c.m.optimization.univariate"") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.",,,,,,,,,,,,,,,,,,,,,03/Sep/12 00:00;erans;MATH-855.patch;https://issues.apache.org/jira/secure/attachment/12543502/MATH-855.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,292274,,,Sun Sep 09 10:41:46 UTC 2012,,,,,,0|i0rsz3:,160338,,,,,,,,"03/Sep/12 00:00;erans;Proposed patch.

The (somewhat contrived) unit test fails with the current version of the algorithm.
","03/Sep/12 00:01;erans;OK to apply?
",05/Sep/12 14:23;erans;Committed in revision 1381195.,"07/Sep/12 15:45;erans;That was not it yet; further improvement committed in revision 1382070, together with a Javadoc update explaining some change wrt the original version of the algorithm.
","09/Sep/12 10:41;erans;Yet another small change in revision 1382441.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EigenDecomposition fails for certain matrices,MATH-848,12603779,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,tn,tn,tn,16/Aug/12 20:09,04/Mar/13 18:53,07/Apr/19 20:38,23/Sep/12 19:35,3.1,,,,,,,3.1,,,0,,,,,,,,"The Schurtransformation of the following matrix fails, which is a preliminary step for the Eigendecomposition:

RealMatrix m = MatrixUtils.DEFAULT_FORMAT.parse(""{{0.184944928,-0.0646971046,0.0774755812,-0.0969651755,-0.0692648806,0.3282344352,-0.0177423074,0.206313634},{-0.0742700134,-0.028906303,-0.001726946,-0.0375550146,-0.0487737922,-0.2616837868,-0.0821201295,-0.2530000167},{0.2549910127,0.0995733692,-0.0009718388,0.0149282808,0.1791878897,-0.0823182816,0.0582629256,0.3219545182},{-0.0694747557,-0.1880649148,-0.2740630911,0.0720096468,-0.1800836914,-0.3518996425,0.2486747833,0.6257938167},{0.0536360918,-0.1339297778,0.2241579764,-0.0195327484,-0.0054103808,0.0347564518,0.5120802482,-0.0329902864},{-0.5933332356,-0.2488721082,0.2357173629,0.0177285473,0.0856630593,-0.35671263,-0.1600668126,-0.1010899621},{-0.0514349819,-0.0854319435,0.1125050061,0.006345356,-0.2250000688,-0.220934309,0.1964623477,-0.1512329924},{0.0197395947,-0.1997170581,-0.1425959019,-0.274947791,-0.0969467073,0.060368852,-0.2826905192,0.1794315473}}"");
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,292275,,,Sun Sep 23 19:35:20 UTC 2012,,,,,,0|i0rszb:,160339,,,,,,,,23/Sep/12 19:35;tn;Fixed in revision 1389129.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using Rotation to convert Euler angles to Quaternions produces wrong results,MATH-847,12603769,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,,dunmatt,dunmatt,16/Aug/12 18:57,04/Mar/13 18:53,07/Apr/19 20:38,17/Aug/12 06:17,3.0,,,,,,,3.1,,,0,,,,,,,,"import org.apache.commons.math3.geometry.euclidean.threed.Rotation;
import org.apache.commons.math3.geometry.euclidean.threed.RotationOrder;

public class temp {
  public static void main(String args[]) {
    Rotation r = new Rotation(RotationOrder.XYZ, -Math.PI / 2d, 0, 0);
    System.out.println(""("" + r.getQ0() + "" "" + r.getQ1() + "" "" + r.getQ2() + "" "" + r.getQ3() + "")"");
  }
}

Prints (0.707 0.707 0.0 0.0) (in-sig-figs elided), but when I type the same thing into Wolfram Alpha I get (.707 -.707 0 0) (note the negative)  see: http://www.wolframalpha.com/input/?i=euler+angles&a=*C.euler+angles-_*Formula.dflt-&a=*FP.EulerRotation.EAS-_e123&f3=-pi%2F2+rad&f=EulerRotation.th1_-pi%2F2+rad&f4=0&f=EulerRotation.th2_0&f5=0&f=EulerRotation.th3_0

One of the guys in the lab suggested that if Rotation is assuming the Euler angle is in a left-handed coordinate space this is an expected result, but if that's the case the question is, why is the less popular coordinate system the only option?",1.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-08-16 19:33:35.816,,,false,,,,,,,,,,,,,,292276,,,Fri Aug 17 06:17:54 UTC 2012,,,,,,0|i0rszj:,160340,,,,,,,,"16/Aug/12 19:33;luc;This is a question rather than a bug report, so it should be on the users mailing list, not on Jira.

Nevertheless, the answer to this question is not that we use a left handed coordinate system, it is that the angles are counted from a vectorial operator point of view, not from a frame conversion point of view. Please read the javadoc for all methods and for the class for an explanation.",16/Aug/12 21:48;erans;Can we close this report?,16/Aug/12 21:58;dunmatt;Sure.  Can we ammend it to ask that the part of the javadoc that describes this be moved from the one constructor to the whole class since it applies no matter how the rotation is constructed?,"17/Aug/12 06:17;luc;Sure, we will try to improve the javadoc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rotation constructor does wrong calculation,MATH-846,12603667,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,fhess,fhess,15/Aug/12 23:07,04/Mar/13 18:53,07/Apr/19 20:38,16/Aug/12 16:11,3.0,,,,,,,3.1,,,0,,,,,,,,"The following code produces the wrong result.  The resulting Rotation is does not even hold a normalized quaternion:

final Vector3D u1 = new Vector3D(1.0, 0.0, 0.0);
final Vector3D u2 = new Vector3D(1.0, -1.0, 0.0);
final Vector3D v1 = new Vector3D(0.9999999, 0., 0.0);
final Vector3D v2 = new Vector3D(0., 1., 0.0);
final Rotation rot = new Rotation(u1, u2, v1, v2);
System.err.println(""rot quaternion: "" + rot.getQ0() + "" "" + rot.getQ1() + "" "" + rot.getQ2() + "" "" + rot.getQ3());


For me it outputs:

rot quaternion: 0.0 0.0 0.0 -7.450580596923828E-9

The correct output should have been:

rot quaternion: 0.0 1.0 0.0 0.0

The constructor seems to be hitting some kind of numerical instability.",Windows java 1.7r5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-08-16 11:58:19.667,,,false,,,,,,,,,,,,,,292277,,,Thu Aug 16 16:11:39 UTC 2012,,,,,,0|i0rszr:,160341,,,,,,,,"16/Aug/12 11:58;luc;I am not able to reproduce this behavior with the current development version.

I think the problem has already been solved on June 6th as part of MATH-801.

Could you check with the development version from our subversion repository?",16/Aug/12 13:43;fhess;I think you are right.  I downloaded svn HEAD (r1373782) and the problem seems to be gone.  Thanks for your help.,16/Aug/12 16:11;luc;This issue was fixed as part of fixing MATH-801,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""HarmonicFitter.ParameterGuesser"" sometimes fails to return sensible values",MATH-844,12603145,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,erans,erans,12/Aug/12 21:03,04/Mar/13 18:53,07/Apr/19 20:38,16/Aug/12 20:45,3.0,,,,,,,3.1,,,0,,,,,,,,"The inner class ""ParameterGuesser"" in ""HarmonicFitter"" (package ""o.a.c.m.optimization.fitting"") fails to compute a usable guess for the ""amplitude"" parameter.
",,,,,,,,,,,,,,,,,,,,,12/Aug/12 21:09;erans;MATH-844.test.patch;https://issues.apache.org/jira/secure/attachment/12540577/MATH-844.test.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-08-15 12:02:58.036,,,false,,,,,,,,,,,,,,292278,,,Thu Aug 16 20:45:55 UTC 2012,,,,,,0|i0rszz:,160342,,,,,,,,"12/Aug/12 21:09;erans;I've attached a unit test demonstrating the problem reporting in [this thread|http://markmail.org/message/42yuec5lk5wr6gbo].
","14/Aug/12 23:03;erans;I propose to check whether the values are sensible, and when they are not, either
# throw an appropriate exception (i.e. signal that the guesser fails), or
# return arbitrary values that would allow the optimizer to proceed.

Any preference?
","15/Aug/12 12:02;luc;In this case, the guesser fails because the function in really far from an harmonic function. It is a triangular periodic function with amplitude +/-3 and period 12, and all sample points are taken as integer abscissa, so values all belong to the integer subset {-3, -2, -1, 0, 1, 2, 3}.

This is an (interesting) ill-conditioned case. As small integers are represented exactly even as primitive doubles, all computations are exact and one of the intermediate parameters (c2 in the source code) is a perfect 0 (there are no approximation here, the result is exact), and we divide by c2.

I would suggest to raise an exception here, and to store this as a junit test for failure mode.","16/Aug/12 20:45;erans;As of revision 1374046, the code will generate an exception.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Precision.EPSILON: wrong documentation,MATH-843,12602075,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,dgruntz,dgruntz,07/Aug/12 19:13,04/Mar/13 18:53,07/Apr/19 20:38,20/Aug/12 07:56,3.0,,,,,,,3.1,,,0,documentation,,,,,,,"The documentation of the Field {{EPSILON}} in class {{org.apache.commons.math3.util.Precision}} states, that {{EPSILON}} is the smallest positive number such that {{1 - EPSILON}} is not numerically equal to 1, and its value is defined as 1.1102230246251565E-16.

However, this is NOT the smallest positive number with this property.

Consider the following program:
{code}
public class Eps {
  public static void main(String[] args) {
    double e = Double.longBitsToDouble(0x3c90000000000001L);
	double e1 = 1-e;
	System.out.println(e);
	System.out.println(1-e);
	System.out.println(1-e != 1);
  }
}
{code}
The output is:
{code}
% java Eps
5.551115123125784E-17
0.9999999999999999
true
{code}

This proves, that there are smaller positive numbers with the property that 1-eps != 1.

I propose not to change the constant value, but to update the documentation. The value {{Precision.EPSILON}} is 
an upper bound on the relative error which occurs when a real number is
rounded to its nearest Double floating-point number. I propose to update 
the api docs in this sense.",,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-08-07 21:42:09.3,,,false,,,,,,,,,,,,,,292279,,,Mon Aug 20 07:56:07 UTC 2012,,,,,,0|i0rt07:,160343,,,,,,,,"07/Aug/12 21:42;erans;Thanks for the report.

I've updated the documentation in revision 1370547. Do you agree with the new version?
","07/Aug/12 22:50;dgruntz;yes, the new documentation in rev 1370547 is correct; 2^(-53) has that property (i.e. it is the largest double eps such that 1+eps = 1):
{code}
scala> math.pow(2, -53)
res1: Double = 1.1102230246251565E-16

scala> 1+res1 == 1
res2: Boolean = true

scala> java.lang.Double.longBitsToDouble(java.lang.Double.doubleToLongBits(res1)+1)
res3: Double = 1.1102230246251568E-16

scala> 1+res3
res4: Double = 1.0000000000000002
{code}

So I agree, but I do not like it. I would not define this constant over 1+eps == 1 (as if executed in extended precision the result may be different), but I would define it as upper bound on the relative error due to rounding real numbers to {{double}} floating-point numbers. ","08/Aug/12 09:49;erans;bq. [...] but I do not like it.

I like it because it is more straightforward to understand while I must admit that I don't fully understand your definition...
I've added a trivial unit test (""testMath841"") that verifies the claim made in the documentation.

Please provide a unit test that makes your point clear, and I'll see no problem adding your proposed comment in the Javadoc.

bq. [...] if executed in extended precision the result may be different [...]

I don't understand: The Javadoc now explicitly states ""double-precision"".
Let me know whether it can be made clearer.
","08/Aug/12 16:16;dgruntz;{quote}
I don't understand: The Javadoc now explicitly states ""double-precision"".
Let me know whether it can be made clearer.
{quote}
The computation may be done in the registers of your processor and these registers may provide more accuracy (typically 80bits). In Java this extended precision can be prevented by using the {{strictfp}} keyword.

The same holds for your test. It should be marked with {{strictfp}} as well. On some machines, the expression {{1 + Precision.EPSILON}} in the test {{1 + Precision.EPSILON == 1}} might be hold in a register with extended precision and thus may be greater than {{1}}. The expression then returns false.

Regarding the other definition: This is the one which can be found in [Wikipedia|http://en.wikipedia.org/wiki/Machine_epsilon] and in the [Computing Surveys article by Goldberg|http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html]. The machine epsilon is defined as upper bound of the relative error which may occur when a real number is rounded to its closest floating-point approximation. This upper bound is {{B^(-(p-1))/2}} where {{B}} is the base and {{p}} is the precision or length of the mantissa. For IEEE754 64-bit numbers (e.g. for {{double}}) this value is {{2^(-53)}}.","09/Aug/12 12:14;erans;bq. On some machines, the expression 1 + Precision.EPSILON in the test 1 + Precision.EPSILON == 1 might be hold in a register with extended precision and thus may be greater than 1. The expression then returns false.

Did you actually see that?
I would (maybe naively) think that ""1 + ESPILON"" would be cast to a 64-bit double before the comparison. If so, wouldn't the equality still hold?

bq. Regarding the other definition: [...]

No problem to add this comment if you provide it here in a ""noformat"" block with all the references as HTML links (""<a href=""..."">...</a>"") which I can copy/paste into the code.
Thanks.
","17/Aug/12 18:50;celestin;Dominik,
could you please review {{r1374395}} before we resolve this issue?
Thanks","20/Aug/12 07:51;dgruntz;looks good to me, thanks. Or is there another place for reviewing?
Dominik
","20/Aug/12 07:56;celestin;Thanks, Dominik, that will do!
I'm resolving this issue right now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Failures in ""FastMathTestPerformance"" when testRuns >= 10,000,002",MATH-840,12601475,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,erans,erans,05/Aug/12 02:35,04/Mar/13 18:53,07/Apr/19 20:38,05/Aug/12 02:38,3.0,,,,,,,3.1,,,0,test,,,,,,,"Tests for methods ""asin"" and ""acos"" fail because they use
{code}
i / 10000000.0
{code}
as the argument to those methods, where ""i"" goes from 0 to the value of ""testRuns"" minus one (if ""testRuns"" is defined).

A solution is to replace the above with
{code}
i / (double) RUNS
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,292281,,,Sun Aug 05 02:38:14 UTC 2012,,,,,,0|i0rt0n:,160345,,,,,,,,05/Aug/12 02:38;erans;Fixed in revision 1369514.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fraction(double, int) constructor strange behaviour",MATH-836,12600886,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,snalin,snalin,31/Jul/12 17:04,04/Mar/13 18:53,07/Apr/19 20:38,04/Aug/12 16:27,3.0,,,,,,,3.1,,,0,Fraction,,,,,,,"The Fraction constructor Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction. When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100'000s), two distinct bugs can manifest:

1: the constructor returns a positive Fraction. Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value

2: the constructor does not manage to reduce the Fraction properly. Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831.

I have, as of yet, not found a solution. The constructor looks like this:

public Fraction(double value, int maxDenominator)
        throws FractionConversionException
    {
       this(value, 0, maxDenominator, 100);
    }

Increasing the 100 value (max iterations) does not fix the problem for all cases. Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest. 

The problem is not neccissarily that the algorithm is unable to approximate a fraction correctly. A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find.

This bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html). Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.

* It is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that ""since fractions are always in lowest terms, numerators and can be compared directly for equality"", so it seems like this is the intention. ",,,,,,,,,,,,,,,,,,,,,31/Jul/12 17:05;snalin;FractionTestByAxiom.java;https://issues.apache.org/jira/secure/attachment/12538573/FractionTestByAxiom.java,31/Jul/12 17:06;snalin;value-maxDenominator_pairs_that_fails;https://issues.apache.org/jira/secure/attachment/12538574/value-maxDenominator_pairs_that_fails,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-08-01 21:14:45.678,,,false,,,,,,,,,,,,,,292283,,,Sun Aug 05 20:14:29 UTC 2012,,,,,,0|i0rt13:,160347,,,,,,,,"31/Jul/12 17:05;snalin;FractionTestByAxiom.java added: Tests the Fraction class through axioms. Not intended to be a part of a finished product, but to explore the class, looking for bugs.","31/Jul/12 17:06;snalin;value-maxDenominator_pairs_that_fails added: pairs of doubles and ints that makes the constructor Fraction(double, int) return faulty Fractions.","01/Aug/12 21:14;tn;The overflow check in Fraction does not take negative values into account, it has to be changed to the following:

{noformat}
  if (FastMath.abs(a0) > overflow) {
      throw new FractionConversionException(value, a0, 1l);
  }

...

  if ((FastMath.abs(p2) > overflow) || (FastMath.abs(q2) > overflow)) {
      throw new FractionConversionException(value, p2, q2);
  }
{noformat}

In that case your examples fail correctly with a FractionConversionException and all unit tests run through successfully.

I would be interested how you generated the test cases. Are these auto-generated?

Thanks,

Thomas",01/Aug/12 21:25;tn;Committed the changes in r1368253.,"01/Aug/12 21:52;tn;While looking into this issue, I realized that there is a very similar class in commons-lang, which also does a reduction when creating a Fraction object from a double value. We should consider also doing this for commons-math.","02/Aug/12 09:50;snalin;Thomas - the test cases are not auto-generated, they are hand-written. We're working on a tool - JaxT2 -  that interprets axioms for classes and auto-generates unit tests through reflection, to improve on the old JaxT tool (http://www.ii.uib.no/mouldable/testing/jaxt/index.html).

Good job with the overflow.","04/Aug/12 16:27;tn;Thanks for the info. This work looks really interested. If you need any support in further testing commons-math or any other commons component, do not hesitate to ask.

I resolve this issue now, as it seems to fix your problems.

Thomas",05/Aug/12 20:14;psteitz;JaxT definitely looks very interesting and broadly useful for us.  Looking forward to more applications to commons math and other commons components.  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fraction percentageValue rare overflow,MATH-835,12600843,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,snalin,snalin,31/Jul/12 13:09,04/Mar/13 18:53,07/Apr/19 20:38,31/Jul/12 15:01,3.0,,,,,,,3.1,,,0,Fraction,,,,,,,"The percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value.

The patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.",,,,,,,,,,,,,,,,,,,,,31/Jul/12 13:09;snalin;percentageValueOverflow.patch;https://issues.apache.org/jira/secure/attachment/12538547/percentageValueOverflow.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-31 15:01:02.099,,,false,,,,,,,,,,,,,,292284,,,Tue Jul 31 15:01:02 UTC 2012,,,,,,0|i0rt1b:,160348,,,,,,,,"31/Jul/12 15:01;erans;Fixed, as suggested, in revision 1367593.
Thanks for the report.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brent solver calculates incorrect root (namley Double.MAX_VALUE),MATH-832,12600065,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,alex87,alex87,24/Jul/12 20:56,04/Mar/13 18:57,07/Apr/19 20:38,27/Jul/12 15:10,3.0,,,,,,,,,,0,,,,,,,,"*Wolfram-Alpha-Solution:*
[http://www.wolframalpha.com/input/?i=min+100*sqrt%28x%29%2B1000000%2Fx%2B10000%2Fsqrt%28x%29+with+x%3E0]
{code:borderStyle=solid}min{100 sqrt(x)+1000000/x+10000/sqrt(x)|x>0}~~4431.94 at x~~804.936{code}

*Java-Input:*{code:borderStyle=solid}
int startValue1 = 100 + 1000000 + 10000;
int startValue2 = 100;

UnivariateFunction uf = new UnivariateFunction() {
    @Override
    public double value(double x) {
        return 100/(2*Math.sqrt(x)) - 1000000/Math.pow(x,2) - 10000/(2*Math.pow(x,(double) 3/2));
    }
};

System.out.println(
    (new BrentSolver()).solve(Integer.MAX_VALUE, uf, 1/Double.MAX_VALUE, Double.MAX_VALUE, startValue1)
);
System.out.println(
    (new BrentSolver()).solve(Integer.MAX_VALUE, uf, 1/Double.MAX_VALUE, Double.MAX_VALUE, startValue2)
);{code}

*Java-Output:*{code:borderStyle=solid}
804.9355821866686
1.7976931348623157E308 (= Double.MAX_VALUE){code} ","Netbeans 7.1.2
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-07-24 22:33:45.567,,,false,,,,,,,,,,,,,,292286,,,Fri Jul 27 15:10:32 UTC 2012,,,,,,0|i0rt1r:,160350,,,,,,,,"24/Jul/12 22:33;erans;bq. min{100 sqrt(x)+1000000/x+10000/sqrt(x)|x>0}

Are you using the right class?
Optimizing (a.k.a. finding the minimum) and solving (a.k.a. finding the root) are not the same thing.

Please try this:
{code}
public void testMath832() {
    final UnivariateFunction f = new UnivariateFunction() {
            public double value(double x) {
                final double sqrtX = FastMath.sqrt(x);
                final double a = 1e2 * sqrtX;
                final double b = 1e6 / x;
                final double c = 1e4 / sqrtX;

                return a + b + c;
            }
        };

    UnivariateOptimizer optimizer = new BrentOptimizer(1e-10, 1e-8);
    final double result = optimizer.optimize(1483,
                                             f,
                                             GoalType.MINIMIZE,
                                             Double.MIN_VALUE,
                                             Double.MAX_VALUE).getPoint();

    Assert.assertEquals(804.935582, result, 1e-6);
}
{code}
","26/Jul/12 09:25;alex87;I tried testMath832() and there was no error (although i'm not sure if i did it right - i never worked with JUnit before). But now i think i know where the problem is: It isn't the difference between optimizing and solving (minimizing a function (that derivative has only one root) => finding the root of the derivative) but a numerical issue:

http://www.wolframalpha.com/input/?i=Plot(100*sqrt%28x%29%2B1000000%2Fx%2B10000%2Fsqrt%28x%29%2C+{x%2C+0%2C+1000})
http://www.wolframalpha.com/input/?i=Plot(d%2Fdx+100*sqrt%28x%29%2B1000000%2Fx%2B10000%2Fsqrt%28x%29%2C+{x%2C+0%2C+1000})
[http://www.wolframalpha.com/input/?i=Plot(d%2Fdx+100*sqrt%28x%29%2B1000000%2Fx%2B10000%2Fsqrt%28x%29%2C+{x%2C+10^10%2C+10^100})]

An upper bound for the interval sufficiently smaller than infinity causes no problems:{code:borderStyle=solid}
System.out.println(
        (new BrentSolver()).solve(Integer.MAX_VALUE, uf, 1/Double.MAX_VALUE, 1.0e10, startValue2)
);{code}","26/Jul/12 11:30;erans;bq. [...] But now i think i know where the problem is [...]

I do not. Is there a problem in Commons Math?

bq. An upper bound for the interval sufficiently smaller than infinity causes no problems

The initial report seems to indicate that you wanted to obtain the minimum of ""uf"". Then, in your last comment, you still use ""BrentSolver"" which will search for the root (of that same ""uf"").
Which is it?

Also, please correct the links in your previous comment; they seem broken (parts not being interpreted as links). Even better would be to upload files (either figures, or source code to reproduce the problem) to this page, instead of those unreadable links!
Thanks.
","26/Jul/12 13:38;erans;""BrentSolver"" (on the derivative of ""uf"") and ""Brentoptimizer"" (on ""uf"") both return the same value.

What you encountered is probably a limitation of finite precision: large values (on the right of the search interval) caused the solver to stop iterating too early: You might want to try with other values of the tolerances (arguments to the constructor).
","27/Jul/12 15:10;erans;If you still consider that there is a problem, please file another report, with a more precise description of the issue.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not expected UnboundedSolutionException,MATH-828,12599477,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,tn,alexeyslepov,alexeyslepov,19/Jul/12 15:07,04/Mar/13 18:53,07/Apr/19 20:38,05/Aug/12 16:33,3.0,,,,,,,3.1,,,0,linear,math,programming,,,,,"SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables.

In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions.
First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result.

The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values.

What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem.

The problem is formulated as
min(1*t + 0*L) (for every r-th subject)
s.t.
-q(r) + QL >= 0
x(r)t - XL >= 0
L >= 0
where 
r = 1..R, 
L = {l(1), l(2), ..., l(R)} (vector of R rows and 1 column),
Q - coefficients matrix MxR
X - coefficients matrix NxR ",Intel Core i5-2300 Windows XP SP3,,,,,,,,,,,,,,,,MATH-842,,,,19/Jul/12 15:09;alexeyslepov;ApacheSimplexWrapper.java;https://issues.apache.org/jira/secure/attachment/12537184/ApacheSimplexWrapper.java,19/Jul/12 15:09;alexeyslepov;ApacheSimplexWrapperTest.java;https://issues.apache.org/jira/secure/attachment/12537185/ApacheSimplexWrapperTest.java,19/Jul/12 15:09;alexeyslepov;Entity.java;https://issues.apache.org/jira/secure/attachment/12537183/Entity.java,19/Jul/12 15:09;alexeyslepov;commons-math3-3.0.jar;https://issues.apache.org/jira/secure/attachment/12537186/commons-math3-3.0.jar,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2012-07-19 17:24:24.445,,,false,,,,,,,,,,,,,,292289,,,Sun Aug 05 16:39:47 UTC 2012,,,,,,0|i0rt2f:,160353,,,,,,,,"19/Jul/12 15:09;alexeyslepov;ApacheSimplexWrapperTest.java is the entry point
ApacheSimplexWrapper.java is the main class

commons-math3-3.0.jar is that same Apache Math 3 Lib I use

Source code contatins come auxiliary method, almost all for printing. Just run ApacheSimplexWrapperTest.java with a debugger and run straight forward to the line 160 in ApacheSimplexWrapper.java where things happen","19/Jul/12 15:19;alexeyslepov;And there is one more strange thing I see. Solutions of the problem that are not unbounded give value of t = 1 every single time while they are supposed to be within [0, 1]. E.g. in the predifend variables values case in the first iteration t1 is 1 and t2 is 0.25 (see Test #1)","19/Jul/12 17:24;erans;Did you test with a recent development snapshot?
","19/Jul/12 19:18;tn;Hi Alexey,

I have looked at your updated test case, and my observation is as follows:

You create lots of constraints (L >= 0) that are unnecessary as the solver is already configured to restrict variables to non-negative values.

I also think you use the objective function in a wrong way. It is defined as:

{noformat}
c1*x1 + ... cn*xn + d
{noformat}

so at index 0 you have the coefficient for the first variable, .... and the last index is for the constant term. Now you use something called theta, which you put on index 0 which is wrong imho.

If I remove all the unnecessary constraints, and move the theta variable to the end of the objective function vector, the tests run through successfully.

Thomas","19/Jul/12 19:20;tn;I close this issue also as invalid, as there is nothing wrong with the solver itself. You may also ask questions regarding the use of the simplex solver on the commons-user mailinglist.

Thomas","20/Jul/12 09:00;alexeyslepov;Thomas, thanks for reference that the solver is already configured to restrict variables to non-negative values. That signally decreased time the solver needed to find solution as much as decreased the number of UnboundedSolutionExceptions.

As to the objective, there is a misunderstanding I believe.
I use the definition of the objective as it's declared in http://commons.apache.org/math/apidocs/org/apache/commons/math/optimization/linear/LinearObjectiveFunction.html
and theta is x1 and there are R+1 variables. R is the number of objects the omtimum is to find for (ENTITIES_COUNT) that is equal to the number of lambdas l(i). And as it shown in the very first test of the JUnit test where Q = [1,2], X = [2,1] and L = T[l1, l2] the solver gives an expected result. That is the indicator that equations are written properly. In other words for every single of N entities there has to be an objective with N + 1 variables. For the case of 2 entities the 3 dimension space is used to build a surface that has it's theta or x(1) coordinate set to minimum, for the case of 3 entities the 4 dimension space is used to build a shape that has it's theta or x(1) coordinate set to minimum and etc.

Here is how it looks for the simplest case (JUnit test #0)

Model name: DEA problem
                 t       L1       L2 
Minimize         1        0        0 
R1               0        2        1 >=        1
R2               2       -1       -2 >=        0
R3               0        1        0 >=        0
R4               0        0        1 >=        0
Type          Real     Real     Real 
upbo           Inf      Inf      Inf 
lowbo            0        0        0 

the objective is to be 0.25 and theta = 0.25 and L1 = 0.5, L2 = 0
The solver gives the same result for the case.
But only I add more entities to find minimum for as the same add more lambdas the solver gives back wrong answer, unbounded solution or theta greater than 1 (that is wrong due to the problem condition)

I'm sure it's been really too early to close the issue :( ","20/Jul/12 19:37;tn;Hi Alexey,

you are right, I was too quick to draw a conclusion, the way you setup the problem is indeed correct.

What I have seen is that you use a very small maxUlps setting in your solver. The default it 10 and should work better atm. I will further look into it, it seems to be related to numerical instabilities.

Solving the same problems with glpk seems to be more robust, which maybe due to the scaling that is applied there to improve numerical properties of the constraint matrix.","23/Jul/12 09:12;alexeyslepov;Thank you Thomas for reopenning and your confirmation that I use math3 properly. I'll play around with maxUIps setting and will try GLPK that I've heard about but haven't tryed yet.

Gilles am I right thinking that I need to use SVN to get 3.1 (r12317576)
svn checkout http://svn.apache.org/repos/asf/commons/proper/math/trunk commons-math3
?
or if I'm wrong please refer me to the proper link.
Thanks","23/Jul/12 09:34;erans;bq. [...] I need to use SVN to get 3.1 [...]

The link refers to the development branch (it's not yet 3.1).
Yes, that's the way to get the up-to-date version of the code.

If you don't want to compile the code (which requires the ""maven"" software), you could also download a snapshot JAR:
 https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/

","23/Jul/12 10:02;alexeyslepov;Now with 3.1-SNAPSHOT things are going better. Number of unbounded exceptions and unexpected theta values (> 1) are fewer versus math 3.0.
Now the picture of failures and successes for
25 entities (i.e. 25 + 1 variables) with 7 constraint equations, 3 for -q(r) + QL >= 0 and 4 for x(r)*t - XL >= 0
looks like

Iteration 1 of 64
Iteration 2 of 64
Iteration 3 of 64
Iteration 4 of 64
Iteration 5 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 6 of 64
Iteration 7 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 8 of 64
Iteration 9 of 64
Iteration 10 of 64
Iteration 11 of 64
Iteration 12 of 64
Iteration 13 of 64
Iteration 14 of 64
Iteration 15 of 64
Iteration 16 of 64
Iteration 17 of 64
Iteration 18 of 64
Iteration 19 of 64
Iteration 20 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 21 of 64
Iteration 22 of 64
Iteration 23 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 24 of 64
Iteration 25 of 64
Iteration 26 of 64
EXCEPTION: unbounded solution
Iteration 27 of 64
Iteration 28 of 64
Iteration 29 of 64
Iteration 30 of 64
Iteration 31 of 64
Iteration 32 of 64
Iteration 33 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 34 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 35 of 64
Iteration 36 of 64
Iteration 37 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 38 of 64
Iteration 39 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 40 of 64
Iteration 41 of 64
Iteration 42 of 64
Iteration 43 of 64
Iteration 44 of 64
Iteration 45 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 46 of 64
Iteration 47 of 64
Iteration 48 of 64
Iteration 49 of 64
Iteration 50 of 64
Iteration 51 of 64
Iteration 52 of 64
Iteration 53 of 64
Iteration 54 of 64
Iteration 55 of 64
Iteration 56 of 64
Iteration 57 of 64
Iteration 58 of 64
Iteration 59 of 64
Iteration 60 of 64
Iteration 61 of 64
Iteration 62 of 64
EXCEPTION: illegal state: maximal count (32,768) exceeded
Iteration 63 of 64
Iteration 64 of 64

for the code
SimplexSolver solver = new SimplexSolver(epsilon, 15);
try
{
	solver.setMaxIterations(32768);
	PointValuePair optimum = solver.optimize(objectiveFunction, constraints, GoalType.MINIMIZE, true);
...

It's much better but it's still to risky to use math 3 to solve problems of this kind in real-world projects.","25/Jul/12 10:46;alexeyslepov;I've made several launches of the program with different values. Here is the result
These
final int INPUT_ARGUMENTS_COUNT = 4;
final int OUTPUT_ARGUMENTS_COUNT = 3;
final int MIN_ARGUMENT_VALUE = 1;
final int MAX_ARGUMENT_VALUE = 100;
final int ITERATIONS_COUNT = 512;
and
maxIterationsCount = 65536;
stay the same over all experiments


Experiment 1
final int ENTITIES_COUNT = 20;
final double EPSILON = 1E-5;
final boolean IS_INTEGER = false;
/*
IS_INTEGER is using in the source code as
value = MIN_ARGUMENT_VALUE + rand.nextInt(MAX_ARGUMENT_VALUE);
if(!IS_INTEGER){
     value += rand.nextDouble();
}
where value is an entity' coefficient value that is a cell of Q and X matrices
*/
gives
13 unbounded solutions of 512 iterations
0 nofeasible solutions of 512 iterations
34 maxcount exceeded exception of 512 iterations
Total 47.0 failures of 512 iterations ( = 0.091796875 of 1)

Experiment 2
final int ENTITIES_COUNT = 20;
final double EPSILON = 1E-8;
final boolean IS_INTEGER = false;
gives
13 unbounded solutions of 512 iterations
0 nofeasible solutions of 512 iterations
37 maxcount exceeded exception of 512 iterations
Total 50.0 failures of 512 iterations ( = 0.09765625 of 1)

Experiment 3
final int ENTITIES_COUNT = 20;
final double EPSILON = 1E-8;
final boolean IS_INTEGER = true;
gives
11 unbounded solutions of 512 iterations
3 nofeasible solutions of 512 iterations
33 maxcount exceeded exception of 512 iterations
Total 47.0 failures of 512 iterations ( = 0.091796875 of 1)

Experiment 4
final int ENTITIES_COUNT = 15;
final double EPSILON = 1E-8;
final boolean IS_INTEGER = false;
gives
10 unbounded solutions of 512 iterations
0 nofeasible solutions of 512 iterations
18 maxcount exceeded exception of 512 iterations
Total 28.0 failures of 512 iterations ( = 0.0546875 of 1)

Experiment 5
final int ENTITIES_COUNT = 10;
final double EPSILON = 1E-8;
final boolean IS_INTEGER = false;
gives
7 unbounded solutions of 512 iterations
1 nofeasible solutions of 512 iterations
16 maxcount exceeded exception of 512 iterations
Total 24.0 failures of 512 iterations ( = 0.046875 of 1)

Experiment 6
final int ENTITIES_COUNT = 5;
final double EPSILON = 1E-8;
final boolean IS_INTEGER = false;
gives
3 unbounded solutions of 512 iterations
0 nofeasible solutions of 512 iterations
0 maxcount exceeded exception of 512 iterations
Total 3.0 failures of 512 iterations ( = 0.005859375 of 1)


As you can see the most influence to the amount of failures gives the number of variables. When there are 5 of them the amount of failure is about a half of a precent which is satisfyingly. When there are 10 or more variables the amount of failures becomes unacceptable.

Please pay attention to the dependence of the amount of failures on the number of variables that is shown through experiments 3, 4, 5, 6
variables     failures
20            47
15            28
10            24
5             3
These failures numbers would change from one experiment launch to another of course but not much, +/- 2 failures","27/Jul/12 13:47;alexeyslepov;Another parameter I've playd around with is maxUlps
As far as I see it's Precision.equals(double x, double y, int maxUlps) where maxUlps is used

  public static boolean equals(double x, double y, int maxUlps) {
        long xInt = Double.doubleToLongBits(x);
        long yInt = Double.doubleToLongBits(y);

        // Make lexicographically ordered as a two's-complement integer.
        if (xInt < 0) {
            xInt = SGN_MASK - xInt;
        }
        if (yInt < 0) {
            yInt = SGN_MASK - yInt;
        }

        final boolean isEqual = FastMath.abs(xInt - yInt) <= maxUlps;

        return isEqual && !Double.isNaN(x) && !Double.isNaN(y);
    }

I've made several more experiments. Notice that on each iteration 

SimplexSolver solver = new SimplexSolver(epsilon, maxUlps);
		try
		{
			solver.setMaxIterations(maxIterationsCount);
			PointValuePair optimum = solver.optimize(objectiveFunction, constraints, GoalType.MINIMIZE, true);

code is called for each entity. I.e. when 1024 iterations there are 1024*ENTITIES_COUNT = 1024*15 calls of new instances of SimplexSolver
Sum of input and output arguments is equal to number of variables - 1. In this case there were 8 variables for the objective function.

These below are results of experiments with different maxUlps values

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 0
8 unbounded solutions of 1024 iterations ( = 0.0078125 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
48 maxcount exceeded exception of 1024 iterations ( = 0.046875 of 1)
Total 56.0 failures of 1024 iterations ( = 0.0546875 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 2
13 unbounded solutions of 1024 iterations ( = 0.0126953125 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
67 maxcount exceeded exception of 1024 iterations ( = 0.0654296875 of 1)
Total 80.0 failures of 1024 iterations ( = 0.078125 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 4
16 unbounded solutions of 1024 iterations ( = 0.015625 of 1)
1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)
45 maxcount exceeded exception of 1024 iterations ( = 0.0439453125 of 1)
Total 62.0 failures of 1024 iterations ( = 0.060546875 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 6
21 unbounded solutions of 1024 iterations ( = 0.0205078125 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
50 maxcount exceeded exception of 1024 iterations ( = 0.048828125 of 1)
Total 71.0 failures of 1024 iterations ( = 0.0693359375 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 8
13 unbounded solutions of 1024 iterations ( = 0.0126953125 of 1)
1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)
39 maxcount exceeded exception of 1024 iterations ( = 0.0380859375 of 1)
Total 53.0 failures of 1024 iterations ( = 0.0517578125 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 25
24 unbounded solutions of 1024 iterations ( = 0.0234375 of 1)
1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)
44 maxcount exceeded exception of 1024 iterations ( = 0.04296875 of 1)
Total 69.0 failures of 1024 iterations ( = 0.0673828125 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 30
19 unbounded solutions of 1024 iterations ( = 0.0185546875 of 1)
2 nofeasible solutions of 1024 iterations ( = 0.001953125 of 1)
43 maxcount exceeded exception of 1024 iterations ( = 0.0419921875 of 1)
Total 64.0 failures of 1024 iterations ( = 0.0625 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 35
15 unbounded solutions of 1024 iterations ( = 0.0146484375 of 1)
1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)
36 maxcount exceeded exception of 1024 iterations ( = 0.03515625 of 1)
Total 52.0 failures of 1024 iterations ( = 0.05078125 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8, maxUlps = 40
33 unbounded solutions of 1024 iterations ( = 0.0322265625 of 1)
1 nofeasible solutions of 1024 iterations ( = 9.765625E-4 of 1)
44 maxcount exceeded exception of 1024 iterations ( = 0.04296875 of 1)
Total 78.0 failures of 1024 iterations ( = 0.076171875 of 1)

It seems that maxUlps gives not much to the state of the problem.
I used https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/commons-math3-3.1-20120726.144152-154.jar SNAPSHOT for this expirement set","27/Jul/12 14:09;erans;Lines 74-78 in {{SimplexSolver}} look strange:
{code}
if (Precision.compareTo(entry, minValue, maxUlps) < 0) {
    minValue = entry;
    minPos = i;
}
{code}

Its seems like the ""minValue"" will not really be most negative negative coefficient, as claimed in the doc.
The larger ""maxUlps"", the most likely it will not be, in line with what you observe...
","28/Jul/12 16:42;tn;In r1366707, I committed the following changes to the SimplexSolver:

 * do not use maxUlps in getPivotColumn and getPivotRow when trying to find a minimum
   this is contra-productive, and the check should be strict

 * implement Bland's rule to prevent cycling when selecting the pivot row in case of multiple candidates

 * to improve numerical stability, introduce a CUTOFF_THRESHOLD (currently 1e-12) to zero-out
   values that are smaller than this threshold in SimplexTableau.subtractRow

With these changes your tests run through without any errors. Could you please verify yourself and confirm.

Thanks, 

Thomas
","30/Jul/12 08:53;alexeyslepov;Thank you very much, Thomas! Now it works well! 

5 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)
Total 0.0 failures of 1024 iterations ( = 0.0 of 1)

10 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)
Total 0.0 failures of 1024 iterations ( = 0.0 of 1)

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)
Total 0.0 failures of 1024 iterations ( = 0.0 of 1)

20 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
2 maxcount exceeded exception of 1024 iterations ( = 0.001953125 of 1)
Total 2.0 failures of 1024 iterations ( = 0.001953125 of 1)

25 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
2 maxcount exceeded exception of 1024 iterations ( = 0.001953125 of 1)
Total 2.0 failures of 1024 iterations ( = 0.001953125 of 1)

30 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-6, maxUlps = 10
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
3 maxcount exceeded exception of 1024 iterations ( = 0.0029296875 of 1)
Total 3.0 failures of 1024 iterations ( = 0.0029296875 of 1)

https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/commons-math3-3.1-20120729.124827-157.jar","30/Jul/12 13:24;tn;Thanks for the feedback, I will further investigate the remaining maxcount exceptions.
In my dev environment I mainly tested with 15 entities.","30/Jul/12 19:45;tn;I further investigated the remaining problems and came up with an empirical heuristic:

 * If we have not found a solution after half of maxIterations, ignore Bland's rule and revert to the top-most row

I did extensive tests with your provided test case and did not receive any exceptions anymore.","31/Jul/12 09:38;alexeyslepov;I confirm that those new improvements erased errors were remaining.
Tests of
https://repository.apache.org/content/repositories/snapshots/org/apache/commons/commons-math3/3.1-SNAPSHOT/commons-math3-3.1-20120730.205101-159.jar
gives

15 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)
Total 0.0 failures of 1024 iterations ( = 0.0 of 1)

25 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)
Total 0.0 failures of 1024 iterations ( = 0.0 of 1)

35 entities, 4 input arguments, 3 output arguments, epsilon = 1.0E-8
0 unbounded solutions of 1024 iterations ( = 0.0 of 1)
0 nofeasible solutions of 1024 iterations ( = 0.0 of 1)
0 maxcount exceeded exception of 1024 iterations ( = 0.0 of 1)
Total 0.0 failures of 1024 iterations ( = 0.0 of 1)

Excellent! Thank you, Thomas!","01/Aug/12 21:38;tn;Hmm, I am no so sure if the last fix is the right way to go. I will leave this issue open and will investigate more on the topic.",02/Aug/12 09:45;alexeyslepov;What can go wrong with the last fix?,"02/Aug/12 11:52;tn;The first fix introduced Bland's rule to prevent cycling. In fact cycling still occurs, thats why I added the additional heuristic.
We need to better understand why cycling still occurs with Bland's rule and better fix this problem, than trying to circumvent it with a rule like it is implemented now.

It should hopefully not affect you, as with the latest version your problems seem to work pretty well, but I want a more general solution. The current fix may break for other type of problems.","02/Aug/12 12:51;erans;Hi Thomas.

Wouldn't it be clearer (cf. description and subject of this issue) to open a new issue for the problem which you identified (cycling) together with a test case that induces it?
The new issue can reference this report so that the history can be recovered easily.

This will allow to resolve this issue, which accurately reflects its state from the perspective of the problem raised.
","03/Aug/12 09:01;tn;Hi Gilles,

the cycling problem has been fixed (+ test case) but in a way that may be very specific to the problem definition of Alexey.
But I am fine with closing this issue and creating a new one to further investigate the issue.

Thomas","03/Aug/12 09:17;alexeyslepov;Thomas, when you'll open a new thread for further investigation of the issue, post here a link to that thread, please","05/Aug/12 16:39;tn;Hi Alexey,

I linked the newly created issue to this one.

Thomas",,,,,,,,,,,,,,,,,,,,,
"public method ""laguerre"" should be private",MATH-825,12598934,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,erans,erans,16/Jul/12 09:16,16/Feb/15 23:04,07/Apr/19 20:38,16/Feb/15 23:04,3.0,,,,,,,4.0,,,0,,,,,,,,"In class ""LaguerreSolver"" (package ""o.a.c.m.analysis.solvers""), the method ""laguerre"" is public. However, it doesn't make any sense to call it from outside the class (because its argument list does not contain the function whose roots must be computed).
The method should be made private.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2015-02-16 23:04:05.37,,,false,,,,,,,,,,,,,,241586,,,Mon Feb 16 23:04:05 UTC 2015,,,,,,0|i028tb:,11029,,,,,,,,"16/Jul/12 09:19;erans;Since the change is not backward-compatible, the method should be marked as deprecated in 3.1.","16/Jul/12 09:57;erans;bq. [...] the method should be marked as deprecated [...]

Done in revision 1361956.",16/Feb/15 23:04;tn;Fixed in commit 5f47ad718e0ffcbbe6cd2f8d32b2e345c4af3a48.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparseRealVectorTest.testMap and testMapToSelf fail because zero entries lose their sign,MATH-821,12598560,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,celestin,celestin,12/Jul/12 14:01,19/May/14 15:13,07/Apr/19 20:38,20/Feb/14 16:17,3.0,,,,,,,3.3,,,0,linear,sparse,,,,,,"Mapping {{Inverse}} to an {{OpenMapRealVector}} can lead to wrong answers, because {{1.0 / 0.0}} should return {{+/-Infinity}} depending on the sign of the zero entry. Since the sign is lost in {{OpenMapRealVector}}, the answer must be wrong if the entry is truly {{-0.0}}.

This is a difficult bug, because it potentially affects any function passed to {{OpenMapRealVector.map()}} or {{mapToSelf()}}. I would recommend we relax the requirements in the unit tests of this class, and make people aware of the issue in the class documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-07-13 11:22:56.681,,,false,,,,,,,,,,,,,,241587,,,Mon May 19 15:13:35 UTC 2014,,,,,,0|i028tj:,11030,,,,,,,,"13/Jul/12 11:22;erans;In revision 1361164, I disabled the two unit tests that demonstrate this bug.
When the bug is fixed, the tests should be made active again.
","22/Oct/12 13:51;erans;I guess that this will be automatically fixed when the sparse vector implementation is deleted. :)
","20/Feb/14 16:17;luc;Unit tests have been relaxed.

Comments have been added in the javadoc for SparseVector interface and OpenMapRealVector class.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver - InfeasibleSolution when feasible,MATH-819,12598390,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Implemented,tn,vidyaraghu,vidyaraghu,11/Jul/12 15:21,07/Apr/13 09:16,07/Apr/19 20:38,10/Mar/13 17:29,3.1,,,,,,,3.2,,,0,,,,,,,,"I am seeing an odd behavior with the latest code in the main trunk (Directory revision: 1358535). The solver throws ""NoFeasibleSolutionException"" for a problem which has a feasible solution. Just by commenting out the last constraint, we get a feasible solution. And for that solution, the constraint in question does not seem to be playing a role. ","Windows 7, JDK 1.7.0_03",,,,,,,,,,,,,,,,,,,,11/Jul/12 15:21;vidyaraghu;CommonsSolver2.java;https://issues.apache.org/jira/secure/attachment/12536042/CommonsSolver2.java,12/Jul/12 20:13;tn;lp-octave.txt;https://issues.apache.org/jira/secure/attachment/12536273/lp-octave.txt,13/Jul/12 10:34;vidyaraghu;test.log;https://issues.apache.org/jira/secure/attachment/12536366/test.log,13/Jul/12 10:34;vidyaraghu;test.mod;https://issues.apache.org/jira/secure/attachment/12536367/test.mod,13/Jul/12 10:34;vidyaraghu;test.out;https://issues.apache.org/jira/secure/attachment/12536368/test.out,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-07-11 19:03:45.659,,,false,,,,,,,,,,,,,,241589,,,Sun Apr 07 09:16:12 UTC 2013,,,,,,0|i028tz:,11032,,,,,,,,"11/Jul/12 19:03;tn;Hi Raghu,

thanks for the report and the test case. There seems to be a bug in case the solver is not restricted to negative values. So when you call it the following way you get correct results:

{noformat}
    PointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);
{noformat}

I will dig further into it, and solve it asap.

Thomas","12/Jul/12 19:32;tn;I looked further into it, and I am not sure anymore that it is related to unrestricted variables. In fact the defined problem is over-constrained with constraints containing very large coefficients / values. If you disable them you will get a valid solution.

Now, the solver should find a solution anyway, but I guess the large coefficients lead to numerical stability problems, which would also explain the odd behavior when disabling the last constraint: disabling the constraint alters the tableau which changes the selection of pivot columns/rows.

I need to do more research on such cases where there are very large coefficients.","12/Jul/12 20:13;tn;an octave version of the same problem.

Octave uses glpk which does not find a solution either. glpk does a scaling of the coefficient matrix by default, which would be a nice addition to commons-math too imho.","13/Jul/12 10:37;vidyaraghu;Hi Thomas,

I will check out Octave. But glpk is able to solve this problem with those large coeffs. I used the standalone solver (glpsol) that comes with glpk. I have attached the input, output and solution files (test.*) for the same problem. Please look into it. 

I guess I can skip specifying such wide bounds when we don't really know the bounds. I will try it out and get back if I see any more problems. 

Again, thanks for looking into this issue.

--
Raghu","14/Jul/12 21:09;tn;Hi Raghu,

I tried to check if glpk would be able to solve the very same problem that you formulated using commons-math. This seems to fail too. I think the additional bounds are not necessary and lead to the problems that we have seen. By default, the variables may take values in the range [0, infinity), or better: number which can be represented as double, and you only need such bounds if you want to limit this to smaller numbers. Such high bounds seem to lead to numerical problems (especially when setting the restrictToNonNegative to false) which need to be further investigated, but may not be solved easily.

Now, I do not know what glpsol does internally and which problem it formulates based on your input, but my guess would be that the bounds are treated differently (not as explicit constraints).

Thomas","15/Jul/12 12:24;vidyaraghu;Hi Thomas,

I will try to skip adding such constraints as much as possible. 

You mention that variables without any specified bounds may take values in the range [0, infinity]. Am I correct to assume that such variables could take values from -infinity to infinity if I call the SimplexSolver.optimize method with restrictToNonNegative set to false? Sorry if I am asking for clarification on something that is obvious. Thanks.

--
Raghu","15/Jul/12 15:52;tn;Hi Raghu,

the short answer is yes. In theory it will be in the range [Double.MIN_VALUE, Double.MAX_VALUE], but in practice it will be limited by many more things and depend on your problem definition of course.

Thomas","15/Jul/12 16:29;vidyaraghu;Hi Thomas,

OK. I will try few more test cases and open a new defect if I see more issues. I will leave it to you if you want to keep this defect open until it gets fixed. Thanks.

--
Raghu","10/Mar/13 17:29;tn;In MATH-930, an additional cutOff parameter has been added to the SimplexSolver constructor in the reorganized optim package, allowing to zero-out very small numbers.

With the following values, the problem can be reliably solved:

{noformat}
new SimplexSolver(1e-6, 10, 1e-5)
{noformat}

By default the cutoff is set to 1e-12, but to get valid solutions in this case, a larger value is needed.","10/Mar/13 23:29;vidyaraghu;Hi Thomas,

Thanks. I will try it when I find some time.


 
--
Raghu


","11/Mar/13 09:11;tn;Ok, but keep in mind that you have to use the SimplexSolver in the new optim package. The whole optimization package has been refactored so a few things have changed, but the principle is the same of course.

Also this change will only be available from the 3.2 release, or you can test with the latest trunk of course.",07/Apr/13 09:16;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver bug?,MATH-813,12598195,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,vidyaraghu,vidyaraghu,10/Jul/12 14:58,04/Mar/13 18:53,07/Apr/19 20:38,10/Jul/12 20:05,3.0,,,,,,,3.1,,,0,,,,,,,,"I am trying to use the SimplexSolver in commons-math3-3.0 and am getting unpredictable results. I am pasting the problem code below. Basically swapping the sequence of the last two constraints results in two different results (of which one is pure sub-optimal). Am I not using the solver correctly?

------------------------------
import java.util.ArrayList;
import java.util.Collection;

import org.apache.commons.math3.optimization.*;
import org.apache.commons.math3.optimization.linear.*;

public class Commons_Solver {
  public static void main(String[] args) {

 // describe the optimization problem
    
    LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 1, 1, 1, 1, 1, 1, 0, 0 }, 0);
    Collection <LinearConstraint>constraints = new ArrayList<LinearConstraint>();
    
    //variables upper bounds
    constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0, 0, 0 }, Relationship.LEQ, 38));
    constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 0, 0, 0, 0, 0 }, Relationship.LEQ, 34));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 1, 0, 0, 0, 0, 0 }, Relationship.LEQ, 1));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 1, 0, 0, 0, 0 }, Relationship.LEQ, 6));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0, 0, 0 }, Relationship.LEQ, 17));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 1, 0, 0 }, Relationship.LEQ, 11));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 0, 1, 0 }, Relationship.LEQ, 101));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 0, 0, 1 }, Relationship.LEQ, 1e10));

    //variables lower bounds
    constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0, 0, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 0, 0, 0, 0, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 1, 0, 0, 0, 0, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 1, 0, 0, 0, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0, 0, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 1, 0, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 0, 1, 0 }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 0, 0, 1 }, Relationship.GEQ, 0));


    constraints.add(new LinearConstraint(new double[] { -1,-1, -1, -1, -1, -1, 1, 0 }, Relationship.EQ, 0));

    constraints.add(new LinearConstraint(new double[] { -1, -1, -1, -1, -1, -1,0 , 1 }, Relationship.EQ, 0));

    constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0, 0, -0.2841121495327103  }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 0, 0, 0, 0, -0.25420560747663556  }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 1, 0, 0, 0, -0.04485981308411215 }, Relationship.GEQ, 0));
    
    /*---------------
    Swapping the sequence of the below two constraints produces two different results 
    ------------------*/
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0, 0, -0.12710280373831778  }, Relationship.GEQ, 0));
    constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 0, 1, 0, -0.08224299065420561  }, Relationship.GEQ, 0));
    /*------------------*/
    
    PointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, false);

    // get the solution
    for (int i = 0 ; i < solution.getPoint().length; i++)      
      System.out.println(""x["" + i + ""] = "" +  solution.getPoint()[i]);
    
    System.out.println(""value = "" + solution.getValue());
  }
}
----------------------------------","Windows 7, JDK 1.7.0_03",,,,,,,,,,,,MATH-781,,,,,,,,10/Jul/12 14:59;vidyaraghu;Commons_Solver.java;https://issues.apache.org/jira/secure/attachment/12535845/Commons_Solver.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-10 17:59:38.444,,,false,,,,,,,,,,,,,,292294,,,Tue Jul 10 20:06:48 UTC 2012,,,,,,0|i0rt3j:,160358,,,,,,,,"10/Jul/12 17:59;tn;Hi,

I quickly checked your test case with the latest trunk and I get exactly the same result. Most likely your observed problem has been fixed already as part of MATH-781. Could you please check yourself with the latest trunk and confirm that the problem is not existent anymore?

Thanks,

Thomas","10/Jul/12 19:58;vidyaraghu;Hi Thomas,

I too am able to get the right results with the latest trunk code. Thanks for looking into this. 

Raghu",10/Jul/12 20:05;tn;Duplicate of MATH-781,"10/Jul/12 20:06;tn;Thanks for the report anyway!

I marked the issue as a duplicate of MATH-781 and changed the fix version to 3.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In RealVector, dotProduct and outerProduct return wrong results due to misuse of sparse iterators",MATH-812,12597900,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,celestin,celestin,celestin,07/Jul/12 12:01,04/Mar/13 18:53,07/Apr/19 20:38,12/Jul/12 13:38,3.0,,,,,,,3.1,,,0,linear,sparse,,,,,,"In class {{RealVector}}, the default implementation of {{RealMatrix outerProduct(RealVector)}} uses sparse iterators on the entries of the two vectors. The rationale behind this is that {{0d * x == 0d}} is {{true}} for all {{double x}}. This assumption is in fact false, since {{0d * NaN == NaN}}.

Proposed fix is to loop through *all* entries of both vectors. This can have a significant impact on the CPU cost, but robustness should probably be preferred over speed in default implementations.

Same issue occurs with {{double dotProduct(RealVector)}}, which uses sparse iterators for {{this}} only.

Another option would be to through an exception if {{isNaN()}} is {{true}}, in which case caching could be used for both {{isNaN()}} and {{isInfinite()}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-07-07 12:30:01.698,,,false,,,,,,,,,,,,,,292295,,,Thu Jul 12 13:38:05 UTC 2012,,,,,,0|i0rt3r:,160359,,,,,,,,"07/Jul/12 12:30;erans;bq. [...] robustness should probably be preferred over speed in default implementations.

+1
","12/Jul/12 13:19;celestin;In {{r1360662}}, {{RealVector.dotProduct(RealVector)}} now loops through all entries of the vectors.","12/Jul/12 13:33;celestin;In {{r1360668}}, {{RealVector.outerProduct(RealVector)}} now loops through all entries of the vectors.","12/Jul/12 13:38;celestin;Unit tests now pass, at the price of a performance loss.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver returns values out of constraints bounds,MATH-808,12596501,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,tn,alexeyslepov,alexeyslepov,30/Jun/12 11:41,04/Mar/13 18:53,07/Apr/19 20:38,18/Jul/12 21:05,3.0,,,,,,,3.1,,,0,constraints,math,SimplexSolver,unexpected,,,,SimplexSolver gives back result that doesn't match constraints,"win7 64, eclipse 3.7, Apache Math 3.0",,,,,,,,,,,,,,,,,,,,30/Jun/12 11:44;alexeyslepov;InputOrientedDEAAlgorithm.java;https://issues.apache.org/jira/secure/attachment/12534102/InputOrientedDEAAlgorithm.java,30/Jun/12 11:45;alexeyslepov;InputOrientedDEAAlgorithmTest.java;https://issues.apache.org/jira/secure/attachment/12534103/InputOrientedDEAAlgorithmTest.java,30/Jun/12 11:42;alexeyslepov;InputOrientedDEAAlgorithm_12-6-30_15-32-52.test.txt;https://issues.apache.org/jira/secure/attachment/12534101/InputOrientedDEAAlgorithm_12-6-30_15-32-52.test.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2012-06-30 14:02:40.626,,,false,,,,,,,,,,,,,,292297,,,Thu Jul 19 15:27:42 UTC 2012,,,,,,0|i0rt47:,160361,,,,,,,,30/Jun/12 11:42;alexeyslepov;Test report file. Keep attention to 'EXCEPTION: Wrong equation' lines,"30/Jun/12 11:44;alexeyslepov;Source code file. Take a look into 
public PointValuePair findEntityOptimum(EntityArgument entity, GoalType goalType) throws Exception
method",30/Jun/12 11:45;alexeyslepov;Unit test to run to repeat the unexpected SimplexSolver behavior,"30/Jun/12 14:02;erans;Could you please simplify the test case? It should contain the bare minimum of code necessary to show the buggy behaviour. The unit test should also contain a method from the {{org.junit.Assert}} class in order to indicate which result is expected.
Thanks.
","01/Jul/12 13:41;tn;Hi,

I could not run your test code as some classes are missing.
Maybe the described behavior is related to an accuracy flaw that has been fixed in MATH-781 (see https://issues.apache.org/jira/browse/MATH-781). Could you please check if the same problem occurs with the latest trunk.

For the test case that Gilles mentioned you could take a look at the attached ones from MATH-781.

Thanks,

Thomas",16/Jul/12 23:26;erans;Has this been confirmed to still occur with the development version?,"17/Jul/12 08:14;tn;No, it has not been confirmed, but I will try to extract relevant information from the attached test case and run it on trunk.","17/Jul/12 18:42;tn;I further looked into the test case and finally was able to run it (create Algorithm interface myself and use MathArrays.distance to compute the euclidean distance).

Imho, the bug report is wrong, as the failing tests are due to an invalid result validation:

{noformat}
1. First equation set -q(r) + QL >= 0 <---> QL >= q(r) verifications: 
	1: 0.0000*t  + 1.0940*0.0592  + 1.8101*0.0000  + 2.7621*0.0000  + 2.1166*0.0674  + 3.5084*0.3067  + 2.1122*0.0919  + 1.2818*0.2595  >= 1.8101   <--------> 1.81011188 >= 1.81011200
EXCEPTION: Wrong equation
java.lang.Exception: Wrong equation
	at ru.hse.cst.algorithm.InputOrientedDEAAlgorithm.findEntityOptimum(InputOrientedDEAAlgorithm.java:378)
{noformat}

Actually this is based on the following check:

{noformat}
if(sum < 0) {
   throw new Exception(""Wrong equation"");
}
{noformat}

Now, the SimplexSolver is not a perfect algorithm and has to cope with the usual limitations of floating-point arithmetic. So the solution will only be accurate within a given epsilon. The default one is 1e-6, so the result has to be evaluated taking this epsilon into account:

{noformat}
if(Precision.compareTo(sum, 0, 1e-6) < 0){
   throw new Exception(""Wrong equation"");
}
{noformat}

With such a check the test cases run through successfully. I still encountered some ""No feasible solution found"" and ""Max count exceeded"" situations, which I have not further debugged and are most likely happening because of test setup (the coefficients are created randomly).","18/Jul/12 21:05;tn;Following my observations, I close this issue as invalid. If you encounter other problems with the simplex solver, feel free to create a new issue with test cases attached.

Thanks, Thomas",19/Jul/12 15:27;alexeyslepov;Thanks Thomas for you involvement in this particular case. Now I see that the problem was out of unproper using of Math3. But! Always there is but :) the problem with this LP problem moved to MATH-828 issue. I'll be appreciated to you if you look in there again (now the code is all-sufficient and easier to read),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bugs in RealVector.ebeMultiply(RealVector) and ebeDivide(RealVector),MATH-803,12560058,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,celestin,celestin,09/Jun/12 18:51,19/May/14 15:13,07/Apr/19 20:38,20/Feb/14 15:58,3.0,,,,,,,3.3,,,0,,,,,,,,"{{OpenMapRealVector.ebeMultiply(RealVector)}} and {{OpenMapRealVector.ebeDivide(RealVector)}} return wrong values when one entry of the specified {{RealVector}} is nan or infinity. The bug is easy to understand. Here is the current implementation of {{ebeMultiply}}

{code:java}
    public OpenMapRealVector ebeMultiply(RealVector v) {
        checkVectorDimensions(v.getDimension());
        OpenMapRealVector res = new OpenMapRealVector(this);
        Iterator iter = entries.iterator();
        while (iter.hasNext()) {
            iter.advance();
            res.setEntry(iter.key(), iter.value() * v.getEntry(iter.key()));
        }
        return res;
    }
{code}

The assumption is that for any double {{x}}, {{x * 0d == 0d}} holds, which is not true. The bug is easy enough to identify, but more complex to solve. The only solution I can come up with is to loop through *all* entries of v (instead of those entries which correspond to non-zero entries of this). I'm afraid about performance losses.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-06-10 14:01:18.224,,,false,,,,,,,,,,,,,,241596,,,Mon May 19 15:13:24 UTC 2014,,,,,,0|i028vj:,11039,,,,,,,,"09/Jun/12 19:03;celestin;In {{r1348485}}, {{RealVectorAbstractTest}} include unit tests illustrating this bug.","10/Jun/12 14:01;luc;Perhaps we should use a post-processing branch to handle infinite or spacial values:

{code}
// current implementation looping over non-zero instance elements goes here

if (v.isNaN() || v.isInfinite()) {
  // post-processing loop, to handle 0 * infinity and 0 * NaN cases
  for (int i = 0; i < v.getDimension(); ++v) {
    if (Double.isInfinite(v.getElement(i)) || Double.isNaN(v.getElement(i)) {
       res.setEntry(i, Double.NaN);
    }
  }
}
{code}

This could be fast only if isNaN() and isInfinite() results are cached and the same vector is reused, otherwise the outer if statement should be removed and the post-processing should be done in all cases.",11/Jun/12 05:12;celestin;I'm going to implement the suggested post-processing for the time being; this should solve the bug. Caching of isNaN() and isInfinite() is postponed.,"11/Jun/12 05:43;celestin;I think this does not work for {{ebeDivide(RealVector)}}. In this case, I suggest to revert to naive implementation (loop through all entries).","22/Jun/12 06:05;celestin;Changed the title of the ticket, since the scope of this bug is much broader. Indeed, it affects {{RealVector.ebeMultiply(RealVector)}} and {{RealVector.ebeDivide(RealVector)}} as soon as the {{RealVector}} passed as a parameter is sparse, see examples below
# for {{ebeMultiply()}}
{code:java}
final RealVector v1 = new ArrayRealVector(new double[] { 1d });
final RealVector v2 = new OpenMapRealVector(new double[] { -0d });
final RealVector w = v1.ebeMultiply(v2);
System.out.println(1d / w.getEntry(0));
{code}
prints {{Infinity}}, instead of {{-Infinity}} (because the sign is lost in {{v2}}). This means that {{w}} holds {{+0d}} instead of {{-0d}}.
# for {{ebeDivide()}}
{code:java}
final RealVector v1 = new ArrayRealVector(new double[] { 1d });
final RealVector v2 = new OpenMapRealVector(new double[] { -0d });
final RealVector w = v1.ebeDivide(v2);
System.out.println(w.getEntry(0));
{code}
prints {{Infinity}}, instead of {{-Infinity}}.
","22/Jun/12 06:09;celestin;According to this [thread|http://markmail.org/thread/4evd6dcyrh2yc2bs], {{ebeMultiply}} and {{ebeDivide}} are deprecated.","22/Jun/12 07:08;celestin;Methods are deprecated in {{r1352782}}, and unit tests are skipped (they are kept for reference). Issue is to remain open until the methods are removed in version 4.0.","20/Feb/14 15:58;luc;Methods undeprecated in r1570246.
The test cases have been adapted, and the fact we force 0 * x = 0 even for NaNs and infinities is considered acceptable now, as it is similar to standard practive for sparse linear algebra libraries.",19/May/14 15:13;luc;Closing all resolved issue now available in released 3.3 version.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RealVector.subtract(RealVector) returns wrong answer.,MATH-802,12560043,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,celestin,celestin,celestin,09/Jun/12 13:05,04/Mar/13 18:53,07/Apr/19 20:38,09/Jun/12 13:14,3.0,,,,,,,3.1,,,0,,,,,,,,"The following piece of code
{code:java}
import org.apache.commons.math3.linear.ArrayRealVector;
import org.apache.commons.math3.linear.OpenMapRealVector;
import org.apache.commons.math3.linear.RealVectorFormat;

public class DemoMath {

    public static void main(String[] args) {
        final double[] data1 = {
            0d, 1d, 0d, 0d, 2d
        };
        final double[] data2 = {
            3d, 0d, 4d, 0d, 5d
        };
        final RealVectorFormat format = new RealVectorFormat();
        System.out.println(format.format(new ArrayRealVector(data1)
            .subtract(new ArrayRealVector(data2))));
        System.out.println(format.format(new OpenMapRealVector(data1)
            .subtract(new ArrayRealVector(data2))));
    }
}
{code}

prints
{noformat}
{-3; 1; -4; 0; -3}
{3; 1; 4; 0; -3}
{noformat}

the second line being wrong. In fact, when subtracting mixed types, {{OpenMapRealVector}} delegates to the default implementation in {{RealVector}} which is buggy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,292300,,,Sat Jun 09 13:14:22 UTC 2012,,,,,,0|i0rt4v:,160364,,,,,,,,"09/Jun/12 13:14;celestin;This bug was revealed while working on MATH-795. Previous unit tests didn't show that bug, because in {{RealVectorTest}}, {{TestVectorImpl}} used to override the default implementation of {{RealVector.subtract(RealVector)}}.

This is now corrected in {{r1348396}} (tests to be updated following work on MATH-795).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quaternion not normalized after construction,MATH-801,12559346,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,romdicos,romdicos,05/Jun/12 09:31,04/Mar/13 18:53,07/Apr/19 20:38,05/Jun/12 18:29,3.0,,,,,,,3.1,,,0,,,,,,,,"The use of the Rotation(Vector3D u1,Vector3D u2,Vector3D v1,Vector3D v2) constructor with normalized angle can apparently lead to un-normalized quaternion.
This case appeared to me with the following data :
u1 = (0.9999988431610581, -0.0015210774290851095, 0.0)
u2 = (0.0, 0.0, 1.0)
and 
v1 = (0.9999999999999999, 0.0, 0.0)
v2 = (0.0, 0.0, -1.0)

This lead to the following quaternion :
q0 = 225783.35177064248
q1 = 0.0
q2 = 0.0
q3 = -3.3684446110762543E-9

I was expecting to have a normalized quaternion, as input vector's are normalized. Does the quaternion shouldn't be normalized ?
I've joined the corresponding piece of code as JUnit Test case",,,,,,,,,,,,,,,,,,,,,05/Jun/12 09:33;romdicos;RotationNotNormalised.txt;https://issues.apache.org/jira/secure/attachment/12530927/RotationNotNormalised.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-06-05 18:29:16.194,,,false,,,,,,,,,,,,,,292301,,,Tue Jun 05 18:29:16 UTC 2012,,,,,,0|i0rt53:,160365,,,,,,,,"05/Jun/12 18:29;luc;Fixed in subversion repository as of r1346513.

Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PolynomialFitter.fit() stalls,MATH-798,12558857,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,kpp,kpp,31/May/12 20:22,04/Mar/13 18:53,07/Apr/19 20:38,04/Jul/12 18:01,3.0,,,,,,,3.1,,,0,,,,,,,,"Hi, in certain cases I ran into the problem that the PolynomialFitter.fit() method stalls, meaning that it does not return, nor throw an Exception (even if it runs for 90 min). Is there a way to tell the PolynomialFitter to iterate only N-times to ensure that my program does not stall?",Mac OS 10.6 and Win XP,,,,,,,,,,,,,,,,,,,,04/Jul/12 02:57;kpp;PolynomialFitterTest.java;https://issues.apache.org/jira/secure/attachment/12535032/PolynomialFitterTest.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-07-04 12:57:24.223,,,false,,,,,,,,,,,,,,292303,,,Wed Jul 04 18:01:53 UTC 2012,,,,,,0|i0rt5j:,160367,,,,,,,,31/May/12 20:24;kpp;JUnit Test case which fits a PolynomialFitter using a GaussNewtonOptimiaer (stalls) and a LevenbergMarquardtOptimizer (works).,04/Jul/12 02:57;kpp;Test case for Math-798,"04/Jul/12 12:57;erans;Your use-case had already been included in the unit test suite (albeit in the ""CurveFitterTest"" class because I initially wanted to remove the ""PolynomialFitter"" class).

MATH-799 discusses that the problem you reported here happens because the default tolerances were much too small. This has been solved (cf. unit test method ""testMath798"" in ""CurveFitterTest"").

However, since the mistake of setting the tolerances at too low values could still happen, I'm going to add a new (overridden) method in ""PolynomialFitter"", where you can explicitly set the number of allowed evaluations of the polynomial during the fit process. This will make it fail early instead of running ""forever"" (not really: the default number of evaluation is ""Integer.MAX_VALUE"").
","04/Jul/12 18:01;erans;Changes performed in revision 1357353.
The unit test ""testMath798"" is now in ""PolynomialFitterTest"".
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LP on android devices,MATH-794,12558559,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,sylvain.rousse,sylvain.rousse,30/May/12 08:51,04/Mar/13 18:57,07/Apr/19 20:38,31/May/12 12:46,3.0,,,,,,,,,,0,android,,,,,,,"When I express a Linear Programming problem in a Pure Java environment, I get a solution A.

When I express thje same linear programming problem in a Java Android environment using the same problem modelling classes, I get a solution A', which doesn't match the constraints I express.

For instance, If I have a set of Xi variables, with each Xi >= 0, I can get some Xj < 0.

Or if I have Xi >= a, then in the solution, I can have Xi < a.

I noticed that for freechart library, they have made a special library for android (Jfreecharrtt for pure javan Afreechart for Android).

I have tried to compile the source with my android app, but the problem is the same.

Is it necessary to have a library built up for android devices ?

Is it possible to have commons-math for android ?

In advance thanks.

Regards.

Sylvain.",Android 2.2 JDK 1.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-30 11:37:12.163,,,false,,,,,,,,,,,,,,292307,,,Thu May 31 12:46:15 UTC 2012,,,,,,0|i0rt6f:,160371,,,,,,,,"30/May/12 11:37;luc;Apache Commons Math should run on Android, as long as you package it as a dex file.
There is an exemple here: [https://www.orekit.org/forge/projects/socis-2011/wiki/HowToBuildToAndroid],
based on another Java library (Orekit), which depends on Apache Commons Math.

Can you reproduce the error with a really simple (say less than 100 lines of code) standalone application that could be run both on Android from the command line with dalvikvm (look at the exemple in the wiki page above, in the ""easy way"" section) and on a desktop environment?","31/May/12 12:46;luc;According to user, a simpler problem provided the same result on Android and desktop environment. So the problem was elsewhere.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OrderedTuple#hashCode() uses casting instead of boxing,MATH-793,12558353,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,28/May/12 16:12,04/Mar/13 18:53,07/Apr/19 20:38,31/May/12 21:59,,,,,,,,3.1,,,0,,,,,,,,,"OrderedTuple#hashCode() is as follows:

{code}
public int hashCode() {
    return Arrays.hashCode(components)   ^
           ((Integer) offset).hashCode() ^
           ((Integer) lsb).hashCode()    ^
           ((Boolean) posInf).hashCode() ^
           ((Boolean) negInf).hashCode() ^
           ((Boolean) nan).hashCode();
}
{code}

This is not the correct way to convert primitives to their object equivalents.

{code}
((Integer) x).hashCode() == x

Boolean#hashCode() is defined as

value ? 1231 : 1237
{code}

Note that this error won't be found unless the compiler emits warnings for boxing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-29 09:21:20.412,,,false,,,,,,,,,,,,,,292308,,,Thu May 31 12:47:32 UTC 2012,,,,,,0|i0rt6n:,160372,,,,,,,,"29/May/12 09:21;luc;Fixed in subversion repository as of r1343616.

Thanks for reporting the issue.","29/May/12 19:01;luc;Another fix has been committed as of r1343920, using inline computation.","29/May/12 19:38;sebb@apache.org;The hash code calculation added in r1343920 is a bit strange.

The way it is normally done is as follows:

{code}
int oddMult = 37; // constant odd multiplier
int hashCode = 17; // seed (odd)

hashCode = hashCode * oddMult + field1.hashcode;
hashCode = hashCode * oddMult + field2.hashcode;
{code}",29/May/12 19:49;luc;OK. Perhaps should we even use prime numbers instead of simply odd numbers ?,"29/May/12 20:19;erans;There is also an example in the ""Pair"" class (package ""util""). That one has been changed recently as the previous code was certainly not good, but I don't guarantee it to be the best choice ;).","30/May/12 09:24;luc;Another attempt to compute hash has been committed as of r1344166.

Sebb, I let you decide if the issue is solved or not and set the status accordingly.",31/May/12 12:47;luc;Can we set this to fixed?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets,MATH-790,12556531,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,mikl,james_pic,james_pic,19/May/12 17:01,04/Mar/13 18:53,07/Apr/19 20:38,12/Jun/12 14:28,3.0,,,,,,,3.1,,,1,newbie,patch,,,,,,"When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations.

Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles","Ubuntu Linux x64, Sun Java 6",3600,3600,,0%,3600,3600,,,,,,,,,,,,,,19/May/12 17:02;james_pic;MannWhitnetUOVerflowPatch.diff;https://issues.apache.org/jira/secure/attachment/12528263/MannWhitnetUOVerflowPatch.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-19 19:49:05.513,,,false,,,,,,,,,,,,,,292311,,,Tue Jun 12 14:28:08 UTC 2012,,,,,,0|i0rt7b:,160375,,,,,,,,"19/May/12 17:02;james_pic;A patch, which adds a unit test for the issue, and a fix in the affected code",19/May/12 19:49;mikl;Thank you very much for reporting this. I will have a look at this ASAP.,08/Jun/12 11:04;mikl;This is fixed in SVN revision 1348024. Thanks again for reporting this issue.,12/Jun/12 09:18;mikl;Thomas Neidhart suspect an overflow issues is still present.,"12/Jun/12 11:50;tn;As discussed on the ML, there may be still a problem with integer overflow in the code fragment below:

{noformat}
final double n1n2prod = n1 * n2;

// http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U#Normal_approximation
final double EU = n1n2prod / 2.0;
final double VarU = n1n2prod * (n1 + n2 + 1) / 12.0;

final double z = (Umin - EU) / FastMath.sqrt(VarU);
{noformat}

The calculation of n1n2prod may still overflow if n1 and n2 are too big as it still does an int multiplication, so I would suggest to do it like that:

{noformat}
final long n1n2prod = (long) n1 * n2;

// http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U#Normal_approximation
final double EU = n1n2prod / 2.0;
final double VarU = n1n2prod * (n1 + n2 + 1) / 12.0;

final double z = (Umin - EU) / FastMath.sqrt(VarU);
{noformat}
","12/Jun/12 13:58;mikl;Thanks for the details. Why not use a double immediately as below? Is it to avoid precision loss?
{noformat}
final double n1n2prod = (double) n1 * n2;

// http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U#Normal_approximation
final double EU = n1n2prod / 2.0;
final double VarU = n1n2prod * (n1 + n2 + 1) / 12.0;

final double z = (Umin - EU) / FastMath.sqrt(VarU);
{noformat}","12/Jun/12 14:04;tn;Actually yes, this was my intention (and long multiplication should be faster too ;-), but in this case it may be negligible.",12/Jun/12 14:07;mikl;I agree - I just wanted to have it in writing :-). Thanks for this.,"12/Jun/12 14:28;mikl;This second issue is fixed in SVN revision 1349372. Thanks again for reporting this issue, too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correlated random vector generator fails (silently) when faced with zero rows in covariance matrix,MATH-789,12554463,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,gertvv,gertvv,09/May/12 10:47,04/Mar/13 18:53,07/Apr/19 20:38,13/Sep/12 15:19,3.0,,,,,,,3.1,,,0,,,,,,,,"The following three matrices (which are basically permutations of each other) produce different results when sampling a multi-variate Gaussian with the help of CorrelatedRandomVectorGenerator (sample covariances calculated in R, based on 10,000 samples):

Array2DRowRealMatrix{
{0.0,0.0,0.0,0.0,0.0},
{0.0,0.013445532,0.01039469,0.009881156,0.010499559},
{0.0,0.01039469,0.023006616,0.008196856,0.010732709},
{0.0,0.009881156,0.008196856,0.019023866,0.009210099},
{0.0,0.010499559,0.010732709,0.009210099,0.019107243}}

> cov(data1)
   V1 V2 V3 V4 V5
V1 0 0.000000000 0.00000000 0.000000000 0.000000000
V2 0 0.013383931 0.01034401 0.009913271 0.010506733
V3 0 0.010344006 0.02309479 0.008374730 0.010759306
V4 0 0.009913271 0.00837473 0.019005488 0.009187287
V5 0 0.010506733 0.01075931 0.009187287 0.019021483

Array2DRowRealMatrix{
{0.013445532,0.01039469,0.0,0.009881156,0.010499559},
{0.01039469,0.023006616,0.0,0.008196856,0.010732709},
{0.0,0.0,0.0,0.0,0.0},
{0.009881156,0.008196856,0.0,0.019023866,0.009210099},
{0.010499559,0.010732709,0.0,0.009210099,0.019107243}}

> cov(data2)
            V1 V2 V3 V4 V5
V1 0.006922905 0.010507692 0 0.005817399 0.010330529
V2 0.010507692 0.023428918 0 0.008273152 0.010735568
V3 0.000000000 0.000000000 0 0.000000000 0.000000000
V4 0.005817399 0.008273152 0 0.004929843 0.009048759
V5 0.010330529 0.010735568 0 0.009048759 0.018683544 

Array2DRowRealMatrix{
{0.013445532,0.01039469,0.009881156,0.010499559},
{0.01039469,0.023006616,0.008196856,0.010732709},
{0.009881156,0.008196856,0.019023866,0.009210099},
{0.010499559,0.010732709,0.009210099,0.019107243}}

> cov(data3)
            V1          V2          V3          V4
V1 0.013445047 0.010478862 0.009955904 0.010529542
V2 0.010478862 0.022910522 0.008610113 0.011046353
V3 0.009955904 0.008610113 0.019250975 0.009464442
V4 0.010529542 0.011046353 0.009464442 0.019260317


I've traced this back to the RectangularCholeskyDecomposition, which does not seem to handle the second matrix very well (decompositions in the same order as the matrices above):

CorrelatedRandomVectorGenerator.getRootMatrix() = 
Array2DRowRealMatrix{{0.0,0.0,0.0,0.0,0.0},{0.0759577418122063,0.0876125188474239,0.0,0.0,0.0},{0.07764443622513505,0.05132821221460752,0.11976381821791235,0.0,0.0},{0.06662930527909404,0.05501661744114585,0.0016662506519307997,0.10749324207653632,0.0},{0.13822895138139477,0.0,0.0,0.0,0.0}}
CorrelatedRandomVectorGenerator.getRank() = 5

CorrelatedRandomVectorGenerator.getRootMatrix() = 
Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.0},{0.07764443622513505,0.13029949164628746,0.0},{0.0,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.0},{0.13822895138139477,0.0,0.0}}
CorrelatedRandomVectorGenerator.getRank() = 3

CorrelatedRandomVectorGenerator.getRootMatrix() = 
Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.033913748226348225,0.07303890149947785},{0.07764443622513505,0.13029949164628746,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.11851573313229945,0.0},{0.13822895138139477,0.0,0.0,0.0}}
CorrelatedRandomVectorGenerator.getRank() = 4

Clearly, the rank of each of these matrices should be 4. The first matrix does not lead to incorrect results, but the second one does. Unfortunately, I don't know enough about the Cholesky decomposition to find the flaw in the implementation, and I could not find documentation for the ""rectangular"" variant (also not at the links provided in the javadoc).",JDK 1.6 / Eclipse Indigo on Ubuntu 10.04,,,,,,,,,,,,,,,,,,,,11/May/12 13:54;gertvv;MultivariateGaussianGeneratorTest.java;https://issues.apache.org/jira/secure/attachment/12526521/MultivariateGaussianGeneratorTest.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-10 22:02:51.197,,,false,,,,,,,,,,,,,,238705,,,Thu Sep 13 15:19:07 UTC 2012,,,,,,0|i0rt7j:,160376,,,,,,,,"10/May/12 22:02;tn;Hi Gert,

thanks for the report. Could you please attach a test case for the described problem. This would really help investigating the problem.

Thanks,

Thomas",11/May/12 13:54;gertvv;Failing test case to reproduce bug (fails for covMatrix3).,"11/May/12 21:11;tn;Thanks for the test.

My first investigation is as follows:

in the RectangularCholeskyDecomposition class, the following code does not actually produce the maximal diagonal element:

{noformat}
   // find maximal diagonal element
   swap[r] = r;
   for (int i = r + 1; i < order; ++i) {
       int ii = index[i];
       int isi = index[swap[i]];
       if (c[ii][ii] > c[isi][isi]) {
         swap[r] = i;
       }
   }
{noformat}

thus the rank of the matrix is computed wrongly as the ordering of the columns is wrong and as a consequence the loop finishes too early. This can be fixed quite easily by changing index[swap[i]] to index[swap[r]].

The increment of r seems also to be wrong in the case the diagonal element is smaller than the user-defined limit.

When making the changes, the rank is correct, but the resulting root matrix is not very good (root * root.transpose() != covariance), thus the transformation of the matrix has to be further reviewed (I did not figure it out yet).

Unfortunately there is no unit test for the RectangularCholeskyDecomposition yet, so this should be added in the process of fixing this issue. ","13/Sep/12 15:19;luc;Fixed in subversion repository as of r1384363.

The problem was due to some missing permutations in the root matrix. It seems the old fix for MATH-226 was not good.

Thanks for the report, and thanks to Thomas for the identification of the wrong maximum element detection.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Complex bug with NaN: the operation new Complex(0.0).multiply(new Complex(Double.POSITIVE_INFINITY)) gives (Infinity, Infinity) instead of NaN",MATH-788,12554203,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Won't Fix,,poslavsky_sv,poslavsky_sv,07/May/12 18:32,04/Mar/13 18:57,07/Apr/19 20:38,22/Oct/12 14:18,3.0,,,,,,,,,,0,patch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-07 20:23:22.389,,,false,,,,,,,,,,,,,,238432,,,Mon Oct 22 14:18:05 UTC 2012,,,,,,0|i028vz:,11041,,,,,,,,"07/May/12 20:23;erans;Please have a look at the related issues.
We know about those behaviours. Some have called this ""bugs"", others ""inconsistencies"", yet others are satisfied with the current ""conventions"".
The accepted way to get out of this is to implement the proposal described in MATH-667.
Any volunteer?","22/Oct/12 14:18;erans;Older discussions concluded that this won't be fixed in the ""Complex"" class as it currently exists in Commons Math.
We are open to have other implementations (where the result of some operation may be more consistent with other conventions or standards), as outlined in MATH-667.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Numerical Underflow in ContinuedFraction,MATH-785,12553447,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,cjfuller,cjfuller,01/May/12 02:54,04/Mar/13 18:53,07/Apr/19 20:38,21/May/12 20:01,3.0,,,,,,,3.1,,,0,,,,,,,,"The ContinuedFraction calculation can underflow in the evaluate method, similar to the overflow case already dealt with.  I encountered this problem while trying to evaluate the inverse cumulative probability of an F distribution with a large number of degrees of freedom.

I would guess this has the same cause as MATH-718 and MATH-738, though I am not experiencing inaccurate results but rather an exception.

For instance, the following test case fails:

double prob = 0.01;
FDistribution f = new FDistribution(200000, 200000);
double fails = f.inverseCumulativeProbability(prob);

This produces a NoBracketingException with the following stack trace:

org.apache.commons.math3.exception.NoBracketingException: function values at endpoints do not have different signs, endpoints: [0, 1], values: [-0.01, -∞]
	at org.apache.commons.math3.analysis.solvers.BrentSolver.doSolve(BrentSolver.java:118)
	at org.apache.commons.math3.analysis.solvers.BaseAbstractUnivariateSolver.solve(BaseAbstractUnivariateSolver.java:190)
	at org.apache.commons.math3.analysis.solvers.BaseAbstractUnivariateSolver.solve(BaseAbstractUnivariateSolver.java:195)
	at org.apache.commons.math3.analysis.solvers.UnivariateSolverUtils.solve(UnivariateSolverUtils.java:77)
	at org.apache.commons.math3.distribution.AbstractRealDistribution.inverseCumulativeProbability(AbstractRealDistribution.java:156)

I could avoid the issue as in the comment to MATH-718 by relaxing the default value of epsilon in ContinuedFraction, although in my test case I can't see any reason the current default precision shouldn't be attainable.

I fixed the issue by implementing underflow detection in ContinuedFraction and rescaling to larger values similarly to how the overflow detection that is already there works.  I will attach a patch shortly.

One possible issue with this fix is that if there exists a case where there is a legitimate reason for p2 or q2 to be zero (I cannot think of one), it might break that case.","Issue seen in both 3.0 release binary version as well as a fresh checkout of the subversion trunk.

java -version output:

java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)

(On Ubuntu 12.04)




",,,,,,,,,,,,,,,,,,,,01/May/12 02:55;cjfuller;patch.txt;https://issues.apache.org/jira/secure/attachment/12525151/patch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-05-10 21:47:41.889,,,false,,,,,,,,,,,,,,237609,,,Mon May 21 20:01:03 UTC 2012,,,,,,0|i0rt87:,160379,,,,,,,,01/May/12 02:55;cjfuller;Patch to fix the numerical underflow problem in ContinuedFraction.,"10/May/12 21:47;tn;I have looked into this patch, and it looks very reasonable.

My original experiments with the epsilon were just scraping on the symptom but this seems to deal with the actual cause of the numerical instability problems.

Results of distributions using this fix also greatly improved to the situation before.","20/May/12 21:38;tn;I looked further into it and am not convinced anymore that this really to solve the numerical stability problems. In fact the results are pretty much random depending on the choice of the scaling factor.

In fact I implemented the modified Lentz-Thompson algorithm to do the continued fraction evaluation and the results are much much better. All the unit tests run through and the probability evaluations for the different distributions for large trials are stable and return correct values.","21/May/12 20:01;tn;The problem has fixed together with MATH-718. Instead of applying the attached patch, the evaluation of the continued fraction has been changed to the modified Lentz-Thompson algorithm which does not suffer from underflow/overflow problems as the original implementation. A test case for the described problem has been added too.

Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BrentOptimizer: User-defined check block is badly placed,MATH-782,12552120,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,erans,erans,23/Apr/12 12:36,04/Mar/13 18:53,07/Apr/19 20:38,27/Apr/12 23:31,3.0,,,,,,,3.1,,,0,,,,,,,,"The CM implementation of Brent's original algorithm was supposed to allow for a user-defined stopping criterion (in addition to Brent's default one).
However, it turns out that this additional block of code is not at the right location, implying an unwanted early exit.
",,,,,,,,,,,,,,,,,,,,,23/Apr/12 12:38;erans;MATH-782.patch;https://issues.apache.org/jira/secure/attachment/12523785/MATH-782.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-23 14:59:36.073,,,false,,,,,,,,,,,,,,237299,,,Fri Apr 27 23:31:21 UTC 2012,,,,,,0|i0rt8n:,160381,,,,,,,,23/Apr/12 12:38;erans;Please have a look at the proposed patch.,"23/Apr/12 14:59;luc;I have no specific comment about this fix, just a question.

The previous implementation, check user case after an if/else structure, so it was checked each time. Now it is checked only in one branch. Are we sure we would not always end up in the other branch and never user user check ?","23/Apr/12 18:20;erans;I saw that too. And I'm not sure.
Clearly, what was there before was wrong: If there is no update of ""x"" and ""fx"" before the user-defined check is performed, the ""previous"" and ""current"" will hold the same values, leading to an early exit. The test which I've added fails miserably with the check statement located at its original place.
I don't know the details of the algorithm, but there is only one block where ""x"" and ""fx"" are updated; thus it would seem safe that the user's check is placed there.
",25/Apr/12 14:50;erans;Change applied in revision 1330321.,"27/Apr/12 23:19;erans;Luc,

You were right, the check is still not at the right place.
I'll try to fix it shortly.
",27/Apr/12 23:31;erans;Hopefully fixed this time (revision 1331635).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver gives bad results,MATH-781,12551973,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,scheiber@unitbv.ro,scheiber@unitbv.ro,21/Apr/12 07:11,04/Mar/13 18:53,07/Apr/19 20:38,02/May/12 18:29,3.0,,,,,,,3.1,,,0,,,,,,,,"Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0
in a simple test problem. It works well in commons-math-2.2. ","Windows 7 (64), jdk1.7.0_03",,,,,,,,,,,,,,,,,,,,22/Apr/12 11:29;scheiber@unitbv.ro;ASF.LICENSE.NOT.GRANTED--LinearProgCM.java;https://issues.apache.org/jira/secure/attachment/12523679/ASF.LICENSE.NOT.GRANTED--LinearProgCM.java,22/Apr/12 11:29;scheiber@unitbv.ro;ASF.LICENSE.NOT.GRANTED--LinearProgCM2.java;https://issues.apache.org/jira/secure/attachment/12523680/ASF.LICENSE.NOT.GRANTED--LinearProgCM2.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-04-22 09:03:32.841,,,false,,,,,,,,,,,,,,236782,,,Wed May 02 18:28:52 UTC 2012,,,,,,0|i0rt8v:,160382,,,,,,,,"22/Apr/12 09:03;luc;Could you give some more information ?

What is the simple problem ? What are the expected results ? What are the results returned by Apache Commons Math 3.0 ?
Could we have some example code so we can reproduce the problem ?","22/Apr/12 11:29;scheiber@unitbv.ro;Hi,

I attached the test codes LinearProgCM.java for commons-math3-3.0 and 
LinearProgCM2.java for commons-math-2.2.

Best regards,
E.Scheiber



",22/Apr/12 14:48;tn;I did a quick check and found no obvious reason for the different behavior. I will do a more thorough check when I am back in 2 weeks.,"01/May/12 17:55;tn;in MATH-434, floating-point comparisons have been changed to use either an:

 * epsilon for any comparisons related to algorithm convergence
 * ulp for any other comparisons

Now, when dropping the objective function of the first phase, the comparison is done using ulp which is wrong imho as it is basically a convergence check. When changing this back to an epsilon check like before, the test runs through as expected.","02/May/12 08:06;tn;The algorithm flow is as follows:

 * perform phase 1:
 ** iterate while !optimal
 ** isOptimal uses epsilon
 * after phase 1, drop phase 1 objective function
 ** drop columns using the same criteria as in isOptimal but with ulp instead of epsilon
 * perform phase 2
 ** ..

After finishing phase 1, we end up dropping columns based on a different epsilon (ulp) as in the convergence check of the iteration for phase 1.",02/May/12 18:28;tn;Fixed in r1333146.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BSPTree class and recovery of a Euclidean 3D BRep,MATH-780,12551802,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,arwillis,arwillis,20/Apr/12 15:11,04/Mar/13 18:53,07/Apr/19 20:38,13/May/12 15:56,3.0,,,,,,,3.1,,,0,BSPTree,euclidean.threed,,,,,,"New to the work here. Thanks for your efforts on this code.

I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem.

Any ideas?
",Linux,,,,,,,,,,,,,,,,,,,,22/Apr/12 15:44;arwillis;BSPMesh2.java;https://issues.apache.org/jira/secure/attachment/12523688/BSPMesh2.java,22/Apr/12 15:38;arwillis;BSPMesh2.java;https://issues.apache.org/jira/secure/attachment/12523685/BSPMesh2.java,20/Apr/12 20:55;arwillis;BSPMesh2.java;https://issues.apache.org/jira/secure/attachment/12523563/BSPMesh2.java,20/Apr/12 20:52;arwillis;BSPMesh2.java;https://issues.apache.org/jira/secure/attachment/12523561/BSPMesh2.java,20/Apr/12 16:40;arwillis;BSPMesh2.java;https://issues.apache.org/jira/secure/attachment/12523518/BSPMesh2.java,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2012-04-20 18:16:12.575,,,false,,,,,,,,,,,,,,236609,,,Sun May 13 15:56:34 UTC 2012,,,,,,0|i0rt93:,160383,,,,,,,,"20/Apr/12 16:08;arwillis;Looks like I had an index flipped resulting in an improperly oriented face for the tetrahedron. I did get the tetrahedron to work. However, I am now struggling with getting a cube to work and I have verified the normals/orientations of the planes to be correct.","20/Apr/12 16:40;arwillis;The code in BSPMesh2.java produces the following error when I run it. If you comment in the line that re-assigns the coordinate data to be cubeCoords1, then the code works fine. The only difference in the two data sets is that one coordinate has changed by a small amount.

Exception in thread ""main"" java.lang.ClassCastException: org.apache.commons.math3.geometry.partitioning.BoundaryAttribute cannot be cast to java.lang.Boolean
	at org.apache.commons.math3.geometry.euclidean.twod_exact.PolygonsSet.computeGeometricalProperties(PolygonsSet.java:135)
	at org.apache.commons.math3.geometry.partitioning.AbstractRegion.getSize(AbstractRegion.java:380)
	at org.apache.commons.math3.geometry.euclidean.threed_exact.PolyhedronsSet$FacetsContributionVisitor.addContribution(PolyhedronsSet.java:171)
	at org.apache.commons.math3.geometry.euclidean.threed_exact.PolyhedronsSet$FacetsContributionVisitor.visitInternalNode(PolyhedronsSet.java:153)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:262)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:261)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:261)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:263)
	at org.apache.commons.math3.geometry.partitioning.BSPTree.visit(BSPTree.java:261)
	at org.apache.commons.math3.geometry.euclidean.threed_exact.PolyhedronsSet.computeGeometricalProperties(PolyhedronsSet.java:118)
	at org.apache.commons.math3.geometry.partitioning.AbstractRegion.getSize(AbstractRegion.java:380)
	at datastructures.j3d.bsptree.BSPMesh.<init>(BSPMesh.java:130)
	at datastructures.j3d.bsptree.BSPMesh2.main(BSPMesh2.java:206)
","20/Apr/12 18:16;luc;Hi Andrew,

The file BSPMesh2.java you attached to the issue is not sufficient to reproduce the error. It refers to a BSPMesh class and to a PolygonOfPoints class. It seems changing BSPMesh to BSPMesh2 is sufficient to solver the references in the main method (but I am not sure this class is a drop-in replacement for your issue), but I have no clue about the other class. Could you attach it to the issue too ?","20/Apr/12 20:52;arwillis;New version. Hopefully no missing references. I have removed references to the PolygonOfPoints class and fixed the constructor error. I have created a class specifically for the bug report and apologies for not making it cleaner/more clear.

I'm working on an exact arithmetic version of BSPTrees for CSG operations. I hope to contribute the code if it's of value to the project. Initially i'll use Java's BigDecimal classes for arbitrary arithmetic but I plan on performance enhancements per JR Shewchuk's work (orient3d() etc., http://www.cs.cmu.edu/~quake/robust.html).

hope this code (latest version of BSPMesh2.java) helps.","22/Apr/12 13:10;luc;Thanks for the new version. I was able to reproduce the problem. However, I don't understand what you are trying to achieve here.

In the double loop that creates the 2D SubLine instances, I have added some print statement to see the indices and the start and end points of the built lines. Here is the output I get:

{noformat}
idxA = 0, idxB = 2, idxC = 4
   adding SubLine: {1; -1} {-1; 1}
   adding SubLine: {-1; 1} {-1; 1}
   adding SubLine: {-1; 1} {1; -1}
idxA = 2, idxB = 4, idxC = 6
   adding SubLine: {-1; 1} {-1; 1}
   adding SubLine: {-1; 1} {-1; -1}
   adding SubLine: {-1; -1} {-1; 1}
idxA = 8, idxB = 14, idxC = 12
   adding SubLine: {1; -1} {-1; 1}
   adding SubLine: {-1; 1} {1; 1}
   adding SubLine: {1; 1} {1; -1}
idxA = 14, idxB = 12, idxC = 10
   adding SubLine: {-1; 1} {1; 1}
   adding SubLine: {1; 1} {-1; -1}
   adding SubLine: {-1; -1} {-1; 1}
idxA = 0, idxB = 8, idxC = 10
   adding SubLine: {1; -1} {1; -1}
   adding SubLine: {1; -1} {-1; -1}
   adding SubLine: {-1; -1} {1; -1}
idxA = 8, idxB = 10, idxC = 2
   adding SubLine: {1; -1} {-1; -1}
   adding SubLine: {-1; -1} {-1; 1}
   adding SubLine: {-1; 1} {1; -1}
idxA = 2, idxB = 10, idxC = 12
   adding SubLine: {-1; 1} {-1; -1}
   adding SubLine: {-1; -1} {1; 1}
   adding SubLine: {1; 1} {-1; 1}
idxA = 10, idxB = 12, idxC = 4
   adding SubLine: {-1; -1} {1; 1}
   adding SubLine: {1; 1} {-1; 1}
   adding SubLine: {-1; 1} {-1; -1}
idxA = 4, idxB = 12, idxC = 14
   adding SubLine: {-1; 1} {1; 1}
   adding SubLine: {1; 1} {-1; 1}
   adding SubLine: {-1; 1} {-1; 1}
idxA = 12, idxB = 14, idxC = 6
   adding SubLine: {1; 1} {-1; 1}
   adding SubLine: {-1; 1} {-1; -1}
   adding SubLine: {-1; -1} {1; 1}
idxA = 8, idxB = 0, idxC = 6
   adding SubLine: {1; -1} {1; -1}
   adding SubLine: {1; -1} {-1; -1}
   adding SubLine: {-1; -1} {1; -1}
idxA = 0, idxB = 6, idxC = 14
   adding SubLine: {1; -1} {-1; -1}
   adding SubLine: {-1; -1} {-1; 1}
   adding SubLine: {-1; 1} {1; -1}
lines.size() = 36
{noformat}

Once the 36 sublines have been created, one polygon is created from them.
It seems for each indices triplets, 3 lines are created representing a semi-infinite stripe. As they are all packed togeteher before building the single polygon, I guess the algorithm gets lots in all redundant boundary lines.

Are you sure the inner loop should build only 3 Subline instance and not 4, and are you sure only one polygon should be built from all these lines ?","22/Apr/12 15:38;arwillis;By the output, the index values displayed are all factors of 2. This seems to indicate you are running the constructor for 2D shapes which is commented out with an if (false) {} statement in the main() function.
I you use the constructor:

public BSPMesh2(float[] coords, int[] indices)

Then your indices should be factors of 3.

If you run the 2D shape constructor on the 3D points (cubeIndices, cubeCoords) strange things will occur as you mention.

I've attached yet another BSPMesh2.java with all extraneous code stripped out.

The code constructs a cube from a set of triangles. Each face (having 4 vertices) is triangulated by adding an edge across the diagonal of the face, i.e., a quad face with vertex indices 0 1 2 3 becomes the following two triplets of indices {0 1 2} {2 1 3} as two triangles.
Hence each face (normally 4 SubLines has 6 subLines (one redundant across the diagonal and these redundant lines have opposing directions (subline 1 2 and subline 2 1)). I am working towards converting generic triangular meshes to BSPTrees (and back to meshes). This is a standard for triangular mesh representations.

I hope this helps clarify.","22/Apr/12 16:37;arwillis;Sorry, the indices should be {0 1 2} {1 2 3} above and the subline {1 2} will be redundant. I checked the orientation of the planes for the indices above and one of the two triangles is flipped orientation (incorrectly specified indices).","23/Apr/12 06:24;luc;Hi Andrew,

Thanks for the explanation, now I understand your code. It would be a nice enhancement to have this in Apache Commons Math!

I think there are still some indices issues. As far as I understand, for each facet of the cube, the current indices do not define two triangles sharing one diagonal, but rather two triangles with the crossing diagonals (i.e. they overlap on some part of the fact, and a quarter of the facet is missing. If I replace the indices array from:
{noformat}
{0, 1, 2, 1, 2, 3,
 4, 7, 6, 7, 6, 5,
 0, 4, 5, 4, 5, 1,
 1, 5, 6, 5, 6, 2,
 2, 6, 7, 6, 7, 3,
 4, 0, 3, 0, 3, 7}
{noformat}

to
{noformat}
{0, 1, 2, 2, 3, 0,
 4, 7, 6, 6, 5, 4,
 0, 4, 5, 5, 1, 0,
 1, 5, 6, 6, 2, 1,
 2, 6, 7, 7, 3, 2,
 4, 0, 3, 3, 7, 4}
{noformat}

it seems to work (I had no time for thorough testing though). Note the pattern of the change on the last three columns.

Could you tell me if this work for you ang give expected results ?
","24/Apr/12 02:14;arwillis;The indices you have above work. Thanks. 
Try these
{noformat}
            float[] coordVals = {
1.000000f, -1.000000f, -1.000000f, 
1.000000f, -1.000000f, 1.000000f, 
-1.000000f, -1.000000f, 1.000000f, 
-1.000000f, -1.000000f, -1.000000f, 
1.000000f, 1.000000f, -1f, 
0.999999f, 1.000000f, 1.000000f, 
-1.000000f, 1.000000f, 1.000000f, 
-1.000000f, 1.000000f, -1.000000f};
int[] coordIdxs = {
0, 1, 2, 0, 2, 3, 
4, 7, 6, 4, 6, 5, 
0, 4, 5, 0, 5, 1, 
1, 5, 6, 1, 6, 2, 
2, 6, 7, 2, 7, 3, 
4, 0, 3, 4, 3, 7};
{noformat}
Then change the coord 0.999999f to 1.0f as follows:
{noformat}
float[] coordVals = {
1.000000f, -1.000000f, -1.000000f, 
1.000000f, -1.000000f, 1.000000f, 
-1.000000f, -1.000000f, 1.000000f, 
-1.000000f, -1.000000f, -1.000000f, 
1.000000f, 1.000000f, -1.000000f, 
1.000000f, 1.000000f, 1.000000f, 
-1.000000f, 1.000000f, 1.000000f, 
-1.000000f, 1.000000f, -1.000000f};
coordIdxs = {
0, 1, 2, 0, 2, 3, 
4, 7, 6, 4, 6, 5, 
0, 4, 5, 0, 5, 1, 
1, 5, 6, 1, 6, 2, 
2, 6, 7, 2, 7, 3, 
4, 0, 3, 4, 3, 7};
{noformat}

I get an error on the first set of coordinates but not on the second. The indices are the same. This is the original data which gave rise to the bug report.

Let me know what you find.
thanks,
andrew
","29/Apr/12 20:09;luc;I have made some progress analyzing this issue.
First, I confirm there is a problem in the code.
It appears that during the build, a very thin triangle occurs on a facet. In the facet plane, the coordinates of the three vertices of this triangle are (0.9999999997583233 -0.999998986721039), (-1.0000000000000002 -0.9999989867210387) and (-1.0000000000000002 -1.0). While extracting the vertices of this triangle, the public getVertices method in PolygonsSet first build the segments and then calls the private followLoop method to identify their topology (i.e. how they connect to each other). The segments are properly built and identified, but the followLoop method fails to connect them. It hits a dedicated conditional statement considering the loop is below some predefined threshold (despite it really is above this threshold) and it ignores the triangle completely. Later, this breaks things as a node in the tree without a boundary must be a leaf node which must contain a boolean attribute. However, here we are not a a leaf node, we are at an internal node with an ignored boundary.

So I think there are two bugs here. One bug for not identifying correctly the triangle, and one bug for misclassifying an internal node as a leaf node.

The triangle identification bug will probably take some time to fix. The internal/leaf node bug will probably be easy to fix.

I'll look into that, hopefully in the next few days.
",03/May/12 14:53;arwillis;Thanks for your input. I am still very much interested in resolving this. I'll keep an eye out for your reply.,"13/May/12 15:56;luc;Fixed in subversion repository as of r1337929.

The fix is an ugly one. I have only prevented the wrong cast since boolean attributes occur only at leaf nodes in the tree (internal nodes have different attributes, related to the boundary).

The roots of the problem are much deeper than that, and need more thoughts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ListPopulation Iterator allows you to remove chromosomes from the population.,MATH-779,12550553,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,reidhoch,reidhoch,11/Apr/12 15:28,04/Mar/13 18:53,07/Apr/19 20:38,12/Apr/12 18:34,3.0,,,,,,,3.1,,,0,genetics,,,,,,,Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,11/Apr/12 15:29;reidhoch;MATH-779.txt;https://issues.apache.org/jira/secure/attachment/12522262/MATH-779.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-12 18:34:06.133,,,false,,,,,,,,,,,,,,235417,,,Thu Apr 12 18:34:06 UTC 2012,,,,,,0|i0rt9b:,160384,,,,,,,,11/Apr/12 15:29;reidhoch;Proposed patch.,"12/Apr/12 18:34;tn;Fixed in r1325427 with minor modifications (updated javadoc, use getChromosomes() instead of Collections.unmodifiableList).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dfp Dfp.multiply(int x) does not comply with the general contract FieldElement.multiply(int n),MATH-778,12550319,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,celestin,celestin,celestin,10/Apr/12 06:06,04/Mar/13 18:53,07/Apr/19 20:38,21/Oct/12 16:23,3.0,,,,,,,3.1,,,0,,,,,,,,"In class {{org.apache.commons.math3.Dfp}},  the method {{multiply(int n)}} is limited to {{0 <= n <= 9999}}. This is not consistent with the general contract of {{FieldElement.multiply(int n)}}, where there should be no limitation on the values of {{n}}.",,,,,,,,,,,,,,,,,,,,,23/Sep/12 20:26;tn;MATH-778.patch;https://issues.apache.org/jira/secure/attachment/12546220/MATH-778.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-09-23 20:26:44.629,,,false,,,,,,,,,,,,,,235184,,,Sun Oct 21 16:23:33 UTC 2012,,,,,,0|i028wf:,11043,,,,,,,,"23/Sep/12 20:26;tn;Hi,

I looked at this issue, and if I understand it correctly, the current multiply(int) method is using a performance shortcut for values of x between 0 and RADIX.

I did a very simple patch to implement the following logic:

 * if 0<=x<RADIX: call multiplyFast with x
 * otherwise create a new Dfp instance with x and call multiply(Dfp) with it","19/Oct/12 15:36;erans;Thomas, Sébastien,

Any reluctance to apply this patch?
","20/Oct/12 08:55;tn;I looked at it as it is one of the few open issues for 3.1. As I am no expert for the Dfp implementation, I did not want to commit it without at least a comment from Sebastien or somebody else who is more proficient in this area than I am.",20/Oct/12 09:08;luc;The patch seems good to me.,"21/Oct/12 09:06;celestin;Seems good to me too. Using shift-right operations would result in faster implementations. I remember having tried that at the time I created this issue, but it seems to me I met with an issue, I can't remember what...
So for the time being, let's stick with this patch, and once we get some time, we can look into optimizations.",21/Oct/12 16:23;tn;Applied patch in r1400671.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need range checks for elitismRate in ElitisticListPopulation constructors.,MATH-776,12549191,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,reidhoch,reidhoch,02/Apr/12 17:15,04/Mar/13 18:53,07/Apr/19 20:38,02/Apr/12 18:47,3.0,,,,,,,3.1,,,0,genetics,,,,,,,"There is a range check for setting the elitismRate via ElitisticListPopulation's setElitismRate method, but not via the constructors.",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,02/Apr/12 17:16;reidhoch;MATH-776.txt;https://issues.apache.org/jira/secure/attachment/12520993/MATH-776.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-02 18:47:29.342,,,false,,,,,,,,,,,,,,234182,,,Mon Apr 02 18:47:29 UTC 2012,,,,,,0|i0rt9r:,160386,,,,,,,,02/Apr/12 17:16;reidhoch;Proposed patch for MATH-776.,"02/Apr/12 18:47;tn;Patch applied with minor modification in r1308454.

Thanks for the contribution!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In the ListPopulation constructor, the check for a negative populationLimit should occur first.",MATH-775,12549190,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,reidhoch,reidhoch,02/Apr/12 17:09,04/Mar/13 18:53,07/Apr/19 20:38,12/Apr/12 18:19,3.0,,,,,,,3.1,,,0,genetics,,,,,,,"In the ListPopulation constructor, the check to see whether the populationLimit is positive should occur before the check to see if the number of chromosomes is greater than the populationLimit.",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,02/Apr/12 17:11;reidhoch;MATH-775.txt;https://issues.apache.org/jira/secure/attachment/12520992/MATH-775.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-04-04 18:31:36.962,,,false,,,,,,,,,,,,,,234181,,,Thu Apr 12 18:19:13 UTC 2012,,,,,,0|i0rt9z:,160387,,,,,,,,02/Apr/12 17:11;reidhoch;Proposed patch for MATH-775.,"04/Apr/12 18:31;tn;Good catch.
After looking at the class, I see even more problems:

 * setPopulationSize allows to change the population size, but does not check the chromosome list for compatible size
 * addChromosome does not check the population size at all
 * the chromosome list given as parameter to the ctor and setChromosomes is assigned directly, allowing the list to be changed from the outside later on 

From my understanding the following should happen:

 * setPopulationSize should shrink the existing chromosome list if it is larger than the new size (optionally we could also remove this method)
 * addChromosome should throw an exception if adding a chromosome would exceed the population size
 * when setting an externally provided chromosome list, the entries should be copied to a new internal list ","05/Apr/12 22:15;tn;The patch has been committed in r1310103, together with other changes that have been outlined before.

Additionally, I have added two new methods:

 * public void addChromosomes(Collection<Chromosome> c)
 * protected List<Chromosome> getChromosomeList()

and made setChromosomes deprecated.

Rationale:

The internal state of ListPopulation shall be protected, and shall not be changeable from the outside as it was possible before. When adding chromosomes, the entries are added to the internal list, instead of setting the internal list reference to the provided list.

Derived classes can get access to the internal list via getChromosomeList (we could also make the internal list protected, is there a policy in CM?).

The setters throw appropriate exceptions to keep the internal state consistent, and addChromosome also throws an exception if the population would exceed the population limit.",09/Apr/12 16:55;tn;I changed the internal list in ListPopulation to a protected member and removed the getChromosomeList again. This way it is cleaner imho.,"10/Apr/12 01:33;sebb@apache.org;Using a getter is better.
Exposing a field means that it is harder to make changes later, e.g. should there be a need to synch. the field or make it read-only.

As a general rule, mutable fields should never be exposed.","10/Apr/12 07:38;tn;hmm, I see your point. My concern was that there is already a public getChromosomes method that I have now changed to return an unmodifiable version of the internal list. As derived classes need access to the internal list (to alter it), I added a protected getChromosomeList method, but somehow this did not feel right.

Maybe a different name for the method would suffice?","12/Apr/12 18:19;tn;Fixed in r1325422.

I kept the name getChromosomeList for the getter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SymmLQ not tested in SymmLQTest,MATH-770,12547758,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,celestin,celestin,celestin,23/Mar/12 07:24,24/Mar/12 16:16,07/Apr/19 20:38,23/Mar/12 08:14,3.0,3.1,,,,,,3.1,,,0,linear,solver,,,,,,"In {{SymmLQTest}}, two test actually create instances of {{ConjugateGradient}} instead of {{SymmLQ}}. These tests are
* {{testUnpreconditionedNormOfResidual()}}
* {{testPreconditionedNormOfResidual()}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,232855,,,Fri Mar 23 08:14:24 UTC 2012,,,,,,0|i0rtav:,160391,,,,,,,,23/Mar/12 07:26;celestin;Typos corrected in {{r1304215}}; {{testPreconditionedNormOfResidual()}} now fails.,"23/Mar/12 07:53;celestin;Test corrected in {{r1304216}}. Failure was due to the fact that in {{SymmLQ}}, the residual for the preconditioned system is really {{P * r}}, where {{r = b - A * x}}, and {{P}} is the square root of the preconditioner.",23/Mar/12 08:14;celestin;There is a follow-up to this ticket in MATH-771.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DescriptiveStatistics.windowSize has a getter and setter, but is protected, so subclasses can bypass the validation check in the setter",MATH-760,12545019,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,sebb@apache.org,sebb@apache.org,03/Mar/12 01:20,16/Feb/15 23:03,07/Apr/19 20:38,16/Feb/15 23:03,,,,,,,,4.0,,,0,,,,,,,,"DescriptiveStatistics.windowSize has a setter which does validation and maintains the list if necessary.

However the field is protected, so classes can ignore the setter.

As it happens, this is exactly what the subclass ListUnivariateImpl.setWindowSize does.

The field should be made private.",,,,,,,,,,,,,,,,,MATH-759,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-05-04 22:07:45.186,,,false,,,,,,,,,,,,,,230205,,,Mon Feb 16 23:03:49 UTC 2015,,,,,,0|i028xj:,11048,,,,,,,,"04/May/12 22:07;tn;+1 for the change, but it would break source/binary compatibility thus postponing to 4.0",16/Feb/15 23:03;tn;Fixed in commit d0721feadc1e0cae6efa3e0d82f523debc5b8d97.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ListUnivariateImpl does not always access windowSize using synch.,MATH-759,12545016,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,sebb@apache.org,sebb@apache.org,03/Mar/12 01:16,04/Mar/13 18:53,07/Apr/19 20:38,22/Oct/12 18:48,,,,,,,,3.1,,,0,,,,,,,,"ListUnivariateImpl has two synch. methods:
getWindowSize
setWindowSize

However, windowSize is frequently referenced elsewhere without synch.

It's not clear why the methods need synch., but if there is a need, then windowSize should always be read using synch.

The code should use the super-class getter and setter.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-03-03 04:48:05.639,,,false,,,,,,,,,,,,,,230202,,,Mon Oct 22 18:48:38 UTC 2012,,,,,,0|i028xr:,11049,,,,,,,,"03/Mar/12 04:48;erans;This is class is defined in the ""test"" part of the source tree.","05/May/12 13:42;tn;Cleanup of the ListUnivariateImpl class in r1334421:

 * removed unnecessary synchronized keywords (unit tests are single-threaded)
 * get windowSize using getter instead of protected member (in preparation for MATH-760)",22/Oct/12 14:12;erans;It seems that this issue can be resolved. Please confirm.,22/Oct/12 18:48;tn;I can confirm that the issue can be closed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ResizableDoubleArray is not thread-safe yet has some synch. methods,MATH-757,12544996,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,02/Mar/12 22:09,10/Mar/15 22:18,07/Apr/19 20:38,10/Mar/15 22:18,,,,,,,,4.0,,,0,,,,,,,,"ResizableDoubleArray has several synchronised methods, but is not thread-safe, because class variables are not always accessed using the lock.

Is the class supposed to be thread-safe?

If so, all accesses (read and write) need to be synch.

If not, the synch. qualifiers could be dropped.

In any case, the protected fields need to be made private.
",,,,,,,,,,,,,,,,,,,,,27/Feb/15 15:36;tn;MATH-757.patch;https://issues.apache.org/jira/secure/attachment/12701373/MATH-757.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-03-02 22:20:52.875,,,false,,,,,,,,,,,,,,230182,,,Tue Mar 10 22:18:13 UTC 2015,,,,,,0|i028y7:,11051,,,,,,,,"02/Mar/12 22:20;tn;As far as I have seen, all methods that change an internal state use a lock in some way:

 * using synchronized modifier
 * synchronized(this) block

The only methods that are not synchronized just return some (immutable) parameters of the class wrt array expansion strategy. So imo the class should be thread-safe.","02/Mar/12 23:32;sebb@apache.org;I'm referring to safe publication: because of the Java memory model, changes to variables are not necessarily made visible (published) to other threads unless both the writer and reader use the same lock. [Or the variable is volatile]

Furthermore, because the variables are not private, subclasses can change any of the variables without using synch.

There seems to be no particular reason for the variables to be anything but private; that would at least prevent external code from changing them arbitrarily.

It also looks to me as though there is no need to make some of the variables mutable, as there is a ctor which provides them all. Why not just make them final?

This does not seem to be the sort of class that needs runtime config, e.g. via JavaBeans.
",03/Mar/12 05:17;erans;Instance variables are private as of r1296563.,"03/Mar/12 08:49;tn;Thanks for the explanation sebb, that made it clear. I will look out for such occurrences myself!","22/Oct/12 14:08;erans;What's the status of this issue?

Is the class in a consistent state?
Should this class really be advertized as thread-safe?

Primarily, it's a utility for CM's internal use; currently, it's used in
* ExponentialDistribution (as a local variable used only at class initialization)
* DescriptiveStatistics (as a private field)
with no need of thread-safety.

If the answer to the above is ""no"", could we just drop all ""synchronized"" keywords and be done with this issue?
",22/Oct/12 14:27;psteitz;+1 to just drop the syncs.   ,"23/Oct/12 12:10;erans;The class is also more complicated than necessary by having public and protected setters for variables that are only set at construction. Removing those would allow making the corresponding fields private (and thus getting closer to the goal of thread-safety, by immutability).
","23/Oct/12 12:35;erans;I find the ""addElement"" method a little strange:
{code}
public synchronized void addElement(double value) {
  numElements++;
  if ((startIndex + numElements) > internalArray.length) {
    expand();
  }
  internalArray[startIndex + (numElements - 1)] = value;
  if (shouldContract()) {
    contract();
  }
}
{code}

Why would we want to contract just after _adding_ an element?
This seems to be arguably useful only if the ""initial capacity"" was not set correctly. In fact, the current code always contracts the array (created with the default constructor) at the first call to ""addElement"" because the default initial capacity (16) is ""too large"" for an array that contains a single element...

The method would also probably be slightly more efficient if written as:
{code}
public synchronized void addElement(double value) {
  if (internalArray.length <= startIndex + numElements) {
    expand();
  }
  internalArray[startIndex + numElements++] = value;
}
{code}
","23/Oct/12 19:55;psteitz;Good catch on the extraneous should Contract above.  That only has relevance in the addElementRolling method, so I would see the above change as safe.

On the other point, I disagree.  This class is by nature mutable - it maintains a dynamic data structure.  To make it threadsafe, we would have to protect all of the data members.  The protected methods are there to allow subclasses to override specific behaviors.  Eliminating mutability of exposed properties limits the functionality of the class.  We don't use that mutability now in [math], but the class is public and others may use it.  The key point is that making things like expansion factor and expansion mode immutable does little / nothing to move toward threadsafety, while limiting functionality.","23/Oct/12 23:21;erans;Yes, the object is mutable; I actually meant to increase the degree of encapsulation by removing some bells and whistles that will hardly be used (what kind of situation would need a change of those ""contraction/expansion"" properties during the lifetime of the object?).
This class represents an array of primitive doubles that can adapt its size. Fine, but if CM does not use whatever refinement can be put into such a functionality, I don't see why we should maintain an overly complicated object. I'd guess that users would not look at CM for this kind of utility (which belongs to e.g. ""Commons Primitives""). I can understand the existence of this class in CM given the no-dependencies requirement, but that leads us back to my point (why maintain functionality beyond what is used internally?). Of course, I do not suggest to remove the methods right now; just starting a discussion for 4.0. At this time, I'm not even sure that the ""synchronized"" keywords can be removed (wouldn't it break compatibility?).
",23/Oct/12 23:27;sebb@apache.org;The synch. keyword does not affect binary or source compatibility.,"24/Oct/12 01:20;erans;Wouldn't removing it break _usage_: from supposedly thread-safe to definitely unsafe?
","24/Oct/12 12:20;erans;I'm also wary of
* the package-scoped ""getInternalLength"" method,
* the ""expansion mode"" being represented as an ""int"" (and being mutable).

To KISS, I think that we should mimic the standard ""Collections"" API (as was done in ""Commons Primitives"") and hide/encapsulate everything else (i.e. set the behaviour at construction time).

For better separation of concerns, I'd also suggest to move all ""rolling"" features to a new class that would inherit from a trimmed-down ""ResizableDoubleArray"" (i.e. only concerned with, hmm, resizable array features, à la ""Commons Primitives"").
","24/Oct/12 16:24;psteitz;Good point on breaking existing usage if we drop the syncs.  I think it is best to push this to 4.0, when we can also refactor the API.  The only reason this class exists is for the ""rolling"" behavior, which is all that needs to be retained.",19/Jul/13 00:14;sebb@apache.org;No need to wait for the next release if we create a class with a new name and the desired new functionality.,19/Jul/13 04:20;psteitz;Lets not do that.  That will force all users of the class to change to the new one.  We have done too much of that already.,"19/Jul/13 10:27;sebb@apache.org;Huh? I was not suggesting removing the old class; users can continue to use that.

Besides, if we break the API in 4.0, we'll need to change class or package names anyway.

It just seems to me we could get some experience with the new class before moving to 4.0.

We could even mark it - for internal use only - to allow the API to change as necessary in the initial stages.","19/Jul/13 11:27;erans;We could implement a clean, basic functionality i.e.
* dropping all ""synchronized"" keywords,
* removing everything marked as deprecated,
* only implementing a minimal API (perhaps similar to [Deque<Double>|http://docs.oracle.com/javase/6/docs/api/java/util/Deque.html]), with transparent resizing.

from which ""ResizableDoubleArray"" could inherit.","19/Jul/13 14:43;psteitz;Sebb - we already have too much duplicative / over-complicated clutter.  Yes, everyone has to change package names uniformly as we - once again - break compatibility throughout the library in 4.0.  At least keeping the same class makes it easier.  Just dropping the sync is sufficient in this case IMO.  This is a moderately useful class that others have used (myself included) outside of [math].  Lets try to maintain it simply.  The simplest way to fix this issue is to just drop the sync and not advertise the class as threadsafe.","20/Jul/13 11:41;erans;Phil,

Some time ago, you agreed to refactor this class as long as the ""rolling"" functionality remains (in this class, IIUC). A lot has already been done in that direction (final fields, deprecation of unsafe methods, ...).
I imagined that what I proposed in my previous comment can reconcile the initial goal (completing the refactoring prior to 4.0) and what Sebb would need prior to 3.3 (a new ""clean"" class, perhaps without ""rolling""). It remains to be defined what precise functionality is necessary in this base class.

Or do I miss something?
",20/Jul/13 14:07;psteitz;I agree with the first two of your bullets above.  The class has been improved.  I do not agree with the third bullet. My opinion is that we should just drop the sync and remove the deprecated methods in 4.0.  ,"27/Feb/15 15:36;tn;I have attached a patch with the following changes:

 * remove all uses of synchronized
 * remove deprecated methods
 * make configuration fields final
 * removed static copy method (did not make sense due to the final fields)
 * updated javadoc
 * formatting

If ok (see the remark about copy method), I will resolve this issue.",10/Mar/15 22:18;tn;Applied patch in commit 76b7413d2b1eb2dc22f05de5b76f9519be5142e0.,,,,,,,,,,,,,,,,,,,,,,,
Several Constructors for PoissonDistributionImpl are broken,MATH-747,12543153,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Won't Fix,,sappling,sappling,18/Feb/12 15:09,24/Mar/12 16:24,07/Apr/19 20:38,18/Feb/12 20:50,2.2,,,,,,,3.0,,,0,,,,,,,,"Only the constructor PoissonDistributionImpl(double) seems to actually work.  All of the others cause a NullPointerException.  The working one creates a default NormalDistribution, but all of the others just leave the member ""normal"" set to null and cause an NPE in the constructor.  

I can't even find a work around to construct a PoissonDistributionImpl with both a specified mean and an epsilon, since there is no way to set epsilon after creation and the constructor PoissonDistributionImpl(double p, double epsilon) does not work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-02-18 20:49:23.82,,,false,,,,,,,,,,,,,,228439,,,Sat Mar 24 16:24:10 UTC 2012,,,,,,0|i0rtd3:,160401,,,,,,,,"18/Feb/12 20:49;erans;You are right.
I suggest that you try the snapshot of the upcoming version 3.0 (there won't be a 2.2.1 release).
","18/Feb/12 20:51;erans;The code does not exist anymore in ""trunk"".
",24/Mar/12 16:24;luc;changing status to closed as 3.0 has been released,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BigFraction.doubleValue() returns Double.NaN for large numerators or denominators,MATH-744,12542291,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,thundre,thundre,12/Feb/12 16:11,24/Mar/12 16:16,07/Apr/19 20:38,14/Feb/12 13:31,2.2,,,,,,,3.0,,,0,,,,,,,,"The current implementation of doubleValue() divides numerator.doubleValue() / denominator.doubleValue().  BigInteger.doubleValue() fails for any number greater than Double.MAX_VALUE.  So if the user has 308-digit numerator or denominator, the resulting quotient fails, even in cases where the result would be well inside Double's range.

I have a patch to fix it, if I can figure out how to attach it here I will.",,,,,,,,,,,,,,,,,,,,,12/Feb/12 21:24;thundre;src.patch;https://issues.apache.org/jira/secure/attachment/12514286/src.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-02-12 19:34:31.629,,,false,,,,,,,,,,,,,,227578,,,Tue Feb 14 13:31:25 UTC 2012,,,,,,0|i0aonj:,60267,,,,,,,,12/Feb/12 16:12;thundre;Patch file for the fraction package.,"12/Feb/12 19:34;erans;Thanks for fixing this.
Would you mind adding a unit test that demonstrates the fix?
",12/Feb/12 21:12;thundre;I figured out my problem with junit.  Here's a test case.,12/Feb/12 21:16;thundre;I noticed that floatValue() has the same bug.  Stand by for more patches.,"12/Feb/12 21:24;thundre;Patch for fixes to BigFraction.doubleValue() and floatValue(), and junit tests.","12/Feb/12 22:12;erans;Sorry to be picky, but it is better to avoid clumping many tests in the same test function, especially if they check different things.
In this case, you could create a new test method, e.g. ""testDoubleValueForLargeNumeratorAndDenominator"".
Also, in addition to checking that the result is not NaN, you should check that the result is the expected value.
Thanks for your contribution.
","14/Feb/12 13:31;erans;Fix committed in revision 1243912.
Added unit tests.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Errors in BOBYQAOptimizer when numberOfInterpolationPoints is greater than 2*dim+1,MATH-728,12535776,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,bjohnson,bjohnson,20/Dec/11 02:05,24/Mar/12 16:16,07/Apr/19 20:38,11/Feb/12 23:15,3.0,,,,,,,3.0,,,0,,,,,,,,"I've been having trouble getting BOBYQA to minimize a function (actually a non-linear least squares fit) so as one change I increased the number of interpolation points.  It seems that anything larger than 2*dim+1 causes an error (typically at

line 1662
                   interpolationPoints.setEntry(nfm, ipt, interpolationPoints.getEntry(ipt, ipt));

I'm guessing there is an off by one error in the translation from FORTRAN.  Changing the BOBYQAOptimizerTest as follows (increasing number of interpolation points by one) will cause failures.

Bruce



Index: src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java
===================================================================
--- src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java	(revision 1221065)
+++ src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java	(working copy)
@@ -258,7 +258,7 @@
 //        RealPointValuePair result = optim.optimize(100000, func, goal, startPoint);
         final double[] lB = boundaries == null ? null : boundaries[0];
         final double[] uB = boundaries == null ? null : boundaries[1];
-        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 1);
+        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 2);
         RealPointValuePair result = optim.optimize(maxEvaluations, func, goal, startPoint, lB, uB);
 //        System.out.println(func.getClass().getName() + "" = "" 
 //              + optim.getEvaluations() + "" f("");
","Mac Java(TM) SE Runtime Environment (build 1.6.0_29-b11-402-11M3527)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-12-20 13:47:18.654,,,false,,,,,,,,,,,,,,221460,,,Sat Feb 11 23:15:55 UTC 2012,,,,,,0|i0aop3:,60274,,,,,,,,"20/Dec/11 03:22;bjohnson;Making this change fixes the problem and seems reasonable, but one of the original translators (from FORTRAN to Java) should look this section of code over.


@@ -1657,10 +1657,10 @@
                     final int tmp2 = jpt;
                     jpt = ipt - n;
                     ipt = tmp2;
-                    throw new PathIsExploredException(); // XXX
+                    //throw new PathIsExploredException(); // XXX
                 }
-                interpolationPoints.setEntry(nfm, ipt, interpolationPoints.getEntry(ipt, ipt));
-                interpolationPoints.setEntry(nfm, jpt, interpolationPoints.getEntry(jpt, jpt));
+                interpolationPoints.setEntry(nfm, ipt-1, interpolationPoints.getEntry(ipt, ipt-1));
+                interpolationPoints.setEntry(nfm, jpt-1, interpolationPoints.getEntry(jpt, jpt-1));

","20/Dec/11 13:47;erans;Thanks for spotting this. Revision 1221253 contains your fix, together with a unit test that exercises the setting of more interpolation points.
","20/Dec/11 15:15;bjohnson;Thanks for checking this in, and it's worth noting that my function now minimizes properly.  It seems that with a large number of interpolation points there was an out of bounds error, but even where that out of bounds error wasn't thrown, the interpolation was set up wrong for the last few points, causing a failure to converge.  Others who have had trouble with convergence would do well to recheck with this fix.","20/Dec/11 15:43;erans;Anyone who wishes to use the ""BOBYQAOptimizer"" class should have a look at the MATH-621 issue.
As you have figured out, the code is really not ready yet. Unfortunately, the implementation being not ""natural"" in Java, it is not easy to separate algorithm complexity from Fortran-driven optimizations (which should be removed).
The problem is all compounded by the fall-through switch-cases which should also be recoded properly.

We are still in the middle of the river: Many things have been improved structure-wise but bugs could have crept in while doing so. Bugs like the one you discovered.
And we don't have a thorough test suite to ensure that every code path works as in the original Fortran.

The code was checked in under the assumption that it would be converted into ""natural"" Java, so that people can maintain it.
I wonder whether it should not be removed for the upcoming release...
","20/Dec/11 15:55;bjohnson;I, for one, am very happy to see BOBYQA in CM and will continue with ""real world"" testing.  If I find more issues I'll certainly report them, but so far it's looking very promising with my applications.","23/Dec/11 13:40;erans;It would be very useful if you could provide unit tests that cover the still unexplored code paths (cf. lines containing ""throw new PathIsExploredException();"").
Thanks in advance for your contributions.
","08/Feb/12 12:14;erans;Hi Bruce.

Would you be interested in testing your code with a large number of ""additional"" interpolation points?
I'm referring to the unit test ""testConstrainedRosenWithMoreInterpolationPoints"" in ""BOBYQAOptimizerTest"", at lines 236-256. It would be nice to know whether the failures, for some values of the number of points, reveal yet other bugs. (Or whether they are expected; in which case, the reason would be a welcome addition to the documentation...)
","08/Feb/12 12:46;bjohnson;Hi Giles,

I'll try to do so over the next couple days.

cheers,

Bruce","11/Feb/12 22:11;bjohnson;I've been playing with BOBYQA (downloaded from svn repository today, and commenting out the PathNotExplored exceptions).  A couple observations.
1) I can't make it fail with large number of interpolation points (as long as you stay under the (2n+1)*(2n+2)/2 recommended max. So this issue is resolved.
2) With large number of interpolation points the algorithm is significantly slower.  I'm minimizing a function  with a 169 parameters.  The function evaluation takes ~50 msec.  With n+2 interpolation points, the additional time for each step is about 10 msec.  With 2*n+1 points, the additional time is about 50 msec, and with about 6*n, the additional time is 250 msec.  So with larger nInterpolation points a lot of time is spent in the algorithm, besides evaluating the function.   At some point I'll try to do some profiling of the code.
3) It makes a big difference to normalize the parameters as the initial search region is dependent on the point with the smallest boundary difference.  So it seems one shouldn't directly fit the ""natural"" parameters but normalized values.","11/Feb/12 23:15;erans;Although the bug that triggered this issue is fixed, failures of the unit test still miss an explanation...

The poor performance is to be expected given the current state of the code (e.g. many matrix calculations are done explicitly, with getters and setters, instead of calling methods of the matrix objects).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)",MATH-727,12535580,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,17/Dec/11 16:36,24/Mar/12 16:16,07/Apr/19 20:38,17/Dec/11 16:56,3.0,,,,,,,3.0,,,0,,,,,,,,"Adaptive step size integrators compute the first step size by themselves if it is not provided.
For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,221264,,,Sat Dec 17 16:56:04 UTC 2011,,,,,,0|i0aopb:,60275,,,,,,,,17/Dec/11 16:56;luc;Fixed in subversion repository as of r1215524.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomDataImpl.nextInt does not distribute uniformly for negative lower bound,MATH-724,12534797,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,dhendriks,dhendriks,12/Dec/11 15:03,24/Mar/12 16:16,07/Apr/19 20:38,20/Dec/11 21:14,1.0,1.1,1.2,2.0,2.1,2.2,,3.0,,,0,,,,,,,,"When using the RandomDataImpl.nextInt function to get a uniform sample in a [lower, upper] interval, when the lower value is less than zero, the output is not uniformly distributed, as the lowest value is practically never returned.

See the attached NextIntUniformTest.java file. It uses a [-3, 5] interval. For several values between 0 and 1, testNextIntUniform1 prints the return value of RandomDataImpl.nextInt (as double and as int). We see that -2 through 5 are returned several times. The -3 value however, is only returned for 0.0, and is thus under-respresented in the integer samples. The output of test method testNextIntUniform2 also clearly shows that value -3 is never sampled.",,,,,,,,,,,,,,,,,,,,,15/Dec/11 08:23;dhendriks;NextIntTest3.java;https://issues.apache.org/jira/secure/attachment/12507485/NextIntTest3.java,12/Dec/11 15:04;dhendriks;NextIntUniformTest.java;https://issues.apache.org/jira/secure/attachment/12507005/NextIntUniformTest.java,15/Dec/11 08:23;dhendriks;NextUniformTest3.java;https://issues.apache.org/jira/secure/attachment/12507486/NextUniformTest3.java,14/Dec/11 14:16;dhendriks;math-724-v2.patch;https://issues.apache.org/jira/secure/attachment/12507355/math-724-v2.patch,15/Dec/11 08:23;dhendriks;math-724-v3.patch;https://issues.apache.org/jira/secure/attachment/12507487/math-724-v3.patch,13/Dec/11 08:23;dhendriks;math-724.patch;https://issues.apache.org/jira/secure/attachment/12507147/math-724.patch,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,2011-12-12 16:49:17.093,,,false,,,,,,,,,,,,,,220481,,,Tue Dec 20 21:14:16 UTC 2011,,,,,,0|i0aopj:,60276,,,,,,,,12/Dec/11 15:04;dhendriks;NextIntUniformTest.java: see issue description,"12/Dec/11 16:49;psteitz;Thanks for reporting this. The problem is in the rounding, which does not work correctly for negative values.  My first inclination is to test for negative lower bound and just shift the interval in that case.  Any better ideas?","13/Dec/11 08:23;dhendriks;math-724.patch: it first scales the [0..1) interval to [0..length), then discretizes it, and finally shifts it to [lower, upper].

It may be a good idea to also add some tests for cases such as [0,3], [3,5], [-3,5], [-5, -3], and see if the distribution of sampled values is uniform. It seems RandomDataTest.testNextInt does this using chiSquare, but since I'm not familiar with that, I'm not sure how to add more tests for the other lower/upper bound pairs...","13/Dec/11 08:33;dhendriks;I just ran the unit tests with my patch applied, an the following test, in RandomDataTest:

{code:java}
    @Test
    public void testNextIntExtremeValues() {
        int x = randomData.nextInt(Integer.MIN_VALUE, Integer.MAX_VALUE);
        int y = randomData.nextInt(Integer.MIN_VALUE, Integer.MAX_VALUE);
        Assert.assertFalse(x == y);
    }
{code}

fails, as does testNextLongExtremeValues. Both x and y become equal to Integer.MIN_VALUE, making x == y to become true, causing the assertion to fail...",13/Dec/11 09:14;dhendriks;Also note that RandomDataImpl.nextUniform uses a similar scale/shift method to transform the range. It may thus suffer from the same failure in case of extreme values...,"14/Dec/11 14:16;dhendriks;math-724-v2.patch: 2nd patch.

 - I think all unit tests work now, including the ones for the Integer.MIN_VALUE to Integer.MAX_VALUE interval.
 - The original problem was that negative values were rounded up by the conversion from double to int, while positive numbers were rounded down. By using floor, we first round the numbers down, and then convert to integer, thus ensuring a proper uniform distribution.
 - Test cases for negative values are still missing... Could someone else add them?
 - RandomDataImpl.nextUniform: I haven't changed this, as the change that I used for integers does not have the desired effect for doubles... This may be caused by the fact that Double.MIN_VALUE is more negative than Double.MAX_VALUE is positive, but I'm not really sure. Maybe it is not even an issue for the nextUniform method?
","14/Dec/11 14:41;erans;bq. [...] the fact that Double.MIN_VALUE is more negative [...]

[Double.Min_VALUE|http://docs.oracle.com/javase/6/docs/api/java/lang/Double.html#MIN_VALUE] is a _positive_ number.
","15/Dec/11 08:23;dhendriks;bq. Double.Min_VALUE is a positive number.

Oops...

OK, I uploaded a third version of the patch (math-724-v3.patch), which also applies the new formula for nextUniform. I included two test files (NextUniformTest3.java and NextIntTest3.java), that show the results for nextInt and nextUniform, for both the old and new formulas. As for as I can see, the new formula works equally well or better in all cases. Also, all existing unit tests pass.
","20/Dec/11 21:14;psteitz;Thanks for reporting and diagnosing this, Dennis.

Slightly modified version of the third patch (just removing unecessary parens), along with tests, committed in r1221490.  The ""negativeToPositiveRange"" tests fail before the fix.  The change to nextUniform is also needed to prevent overflows. I changed the relevant test cases to use the TestUtils chisquare test, which is more straightforward and has better output.  This was added after the original versions of these tests were written.  Others in this class should be similarly updated.  Patches welcome to further tidy the tests, but this issue can be resolved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"BitStreamGenerators (MersenneTwister, Well generators) do not clear normal deviate cache on setSeed",MATH-723,12534683,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,psteitz,psteitz,11/Dec/11 21:03,24/Mar/12 16:16,07/Apr/19 20:38,11/Dec/11 21:59,2.0,2.1,2.2,,,,,3.0,,,0,,,,,,,,"The BitStream generators generate normal deviates (for nextGaussian) in pairs, caching the last value generated. When reseeded, the cache should be cleared; otherwise seeding two generators with the same value is not guaranteed to generate the same sequence.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,220404,,,Sun Dec 11 21:59:41 UTC 2011,,,,,,0|i0aopr:,60277,,,,,,,,11/Dec/11 21:59;psteitz;Fixed in r1213087.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] Complex Tanh for ""big"" numbers",MATH-722,12534547,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,celestin,jbbpopo,jbbpopo,09/Dec/11 21:34,24/Mar/12 16:16,07/Apr/19 20:38,27/Jan/12 07:01,2.2,,,,,,,3.0,,,0,patch,,,,,,,"Hi,

In Complex.java the tanh is computed with the following formula:

tanh(a + bi) = sinh(2a)/(cosh(2a)+cos(2b)) + [sin(2b)/(cosh(2a)+cos(2b))]i

The problem that I'm finding is that as soon as ""a"" is a ""big"" number,
both sinh(2a) and cosh(2a) are infinity and then the method tanh returns in
the real part NaN (infinity/infinity) when it should return 1.0.

Wouldn't it be appropiate to add something as in the FastMath library??:

if (real>20.0){
      return createComplex(1.0, 0.0);
}
if (real<-20.0){
      return createComplex(-1.0, 0.0);
}


Best regards,

JBB
","I'm working with Eclipse 3.6.2 on Windows XP, but the bug is Enviroment independent",900,900,,0%,900,900,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-12-11 08:35:52.772,,,false,,,,,,,,,,,,,,220269,,,Fri Jan 27 07:01:40 UTC 2012,,,,,,0|i0aopz:,60278,,,,,,,,"11/Dec/11 08:35;celestin;Even when a is not large enough to cause overflows, I'm not sure the general expression used is quite accurate (because {{cosh(2 * a)}} and {{sinh(2 * a)}} are very close, but not equal). I've often come accross this issue, and found that reverting to exponential representation (keeping only the exponentials with negative argument) was much better behaved.

In the present case, maybe the following representation of the real part
{{(1 - exp(-4 * a)) / (1 + 2 * exp(-2 * a) * cos(2 * b) + exp(-4 * a))}}
would avoid overflows (for {{a > 0}}, while remaining fairly accurate (otherwise, the threshold needs to be tuned).

Similar expressions can be derived for {{a < 0}}.

What do you think?
Sébastien","11/Dec/11 09:18;luc;Very good idea Sébastien. Using stable formulas according to argument size is surely a good thing. We can also use FastMath.expm1() both in numerator and denominator, it will improve accuracy when a is close to 0.","27/Jan/12 03:55;celestin;I've run some tests comparing the two formulas: I didn't notice any major difference. Therefore I suggest to stick with the existing formula.
Meanwhile, the fix proposed by Juan must be implemented. It can be proved mathematically that for any value of {{b}}, the value of {{tanh(a + i * b)}} is within one ulp of 1.0 for {{abs(a) > 18.9}}, so the proposed threshold (20.0) is fairly safe, and consistent with other parts of CM. Note that the same fix must be implemented for {{tan}} also.",27/Jan/12 07:01;celestin;Changes proposed by Juan committed in {{r1236548}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"building common-math on Solaris SPARC gives ""error: floating point number too small""",MATH-721,12534416,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,jprante,jprante,08/Dec/11 23:51,04/Mar/13 18:53,07/Apr/19 20:38,21/Apr/12 12:16,2.2,,,,,,,3.1,,,0,,,,,,,,"It seems the assumption in MathUtils.java 


    /** Safe minimum, such that 1 / SAFE_MIN does not overflow.
     * <p>In IEEE 754 arithmetic, this is also the smallest normalized
     * number 2<sup>-1022</sup>.</p>
     */
    public static final double SAFE_MIN = 0x1.0p-1022;

does not work on my openjdk 1.7.0_1 on Solaris SPARC, because I get the following build error:

[INFO] Compiling 457 source files to /opt/builder/projects/commons-math-2.2-src/target/classes
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/main/java/org/apache/commons/math/util/MathUtils.java:[42,42] error: floating point number too small

[INFO] 1error
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE

and also in the tests I encounter the error when parsing floating point constants

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.1:testCompile (default-testCompile) on project commons-math: Compilation failure: Compilation failure:
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/test/java/org/apache/commons/math/util/FastMathTest.java:[1050,69] error: floating point number too small
[ERROR] 
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/test/java/org/apache/commons/math/util/FastMathTest.java:[1055,28] error: floating point number too small
[ERROR] 
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/test/java/org/apache/commons/math/util/FastMathTest.java:[1055,69] error: floating point number too small
[ERROR] 
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/test/java/org/apache/commons/math/util/FastMathTest.java:[1056,28] error: floating point number too small
[ERROR] 
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/test/java/org/apache/commons/math/util/FastMathTest.java:[1062,70] error: floating point number too small
[ERROR] 
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/test/java/org/apache/commons/math/util/FastMathTest.java:[1063,70] error: floating point number too small
[ERROR] 
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/test/java/org/apache/commons/math/util/FastMathTest.java:[1068,70] error: floating point number too small
[ERROR] 
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/test/java/org/apache/commons/math/util/FastMathTest.java:[1069,70] error: floating point number too small
[ERROR] 
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/test/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizerTest.java:[503,51] error: floating point number too small
[ERROR] 
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/test/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizerTest.java:[503,66] error: floating point number too small
[ERROR] 
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/test/java/org/apache/commons/math/estimation/LevenbergMarquardtEstimatorTest.java:[583,52] error: floating point number too small
[ERROR] 
[ERROR] /opt/builder/projects/commons-math-2.2-src/src/test/java/org/apache/commons/math/estimation/LevenbergMarquardtEstimatorTest.java:[585,52] error: floating point number too small


I suggest using java.lang.Double.MIN_NORMAL for a platform normalized minimal floating point value.


A quick program for printing Double.MIN_NORMAL gives the following result.

class test {
   public static void main(String[] args) {
       System.out.println(""min float = "" + Double.MIN_NORMAL );
   }
}

java test
min float = 2.2250738585072014E-308

Could it be an openjdk glitch? Thank you for any comments and help how this could be fixed.
","Build environment is:
mvn --version
Apache Maven 3.0.3 (rNON-CANONICAL_2011-10-13_22-23_builder; 2011-10-13 22:23:49+0200)
Maven home: /usr/local/share/maven-3.0.3
Java version: 1.7.0-internal, vendor: Oracle Corporation
Java home: /usr/local/share/jdk1.7.0_1/jre
Default locale: de_DE, platform encoding: UTF-8
OS name: ""sunos"", version: ""5.10"", arch: ""sparc"", family: ""unix""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-12-09 08:03:48.582,,,false,,,,,,,,,,,,,,220138,,,Sat Apr 21 12:16:06 UTC 2012,,,,,,0|i0rten:,160408,,,,,,,,"09/Dec/11 08:03;luc;We can't use Double.MIN_NORMAL because it is only available since Java 6 and Apache Commons Math currently targets Java 1.5.
It may be an openjdk glitch as you say. This number is a perfectly regular number, and even smaller number exists and should be able to be used: the subnormal numbers.

Could you try using a conversion from hexadecimal encoding on your platform, using something along this line:

  public static final double SAFE_MIN = Double.longBitsToDouble(1L << 52);",23/Jan/12 11:24;erans;Any follow-ups on this issue?,"03/Mar/12 21:03;jprante;Replacing constant 2.2250738585072014E-308 (MIN_NORMAL) with java.lang.Double.longBitsToDouble(0x0010000000000000L)
is a workaround. Could be a Solaris SPARC compiler optimization issue when parsing float constants.",21/Apr/12 12:16;luc;Fixed in subversion repository as of r1328492,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RandomDataImpl uses both JDKRandomGenerator and Well19937c, while contract/javadoc only specifies Well19937c.",MATH-720,12534185,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,dhendriks,dhendriks,07/Dec/11 14:04,23/Jul/12 23:24,07/Apr/19 20:38,12/Dec/11 01:35,,,,,,,,3.0,,,0,random,seed,,,,,,"See attached unit test. I create a distribution, sample it (not printed), set the seed to 0, and then print the next sample. I also create the same distribution again, set the seed to 0, and then print the next sample. I expect the same sample, as in both cases the seed was set to 0, just before sampling. I however get this output:

{code}
5
4
{code}

The problem is in the org.apache.commons.math.random.RandomDataImpl class:
 - The RandomDataImpl(RandomGenerator rand) constructor states in javadoc: ""@param rand the source of (non-secure) random data (may be null, resulting in default JDK-supplied generator)""
 - reSeed(long seed) method does: if (rand == null) rand = new JDKRandomGenerator();
 - reSeed() method does: if (rand == null) rand = new JDKRandomGenerator();
 - class javadoc states: ""If no <code>RandomGenerator</code> is provided in the constructor, the default is to use a Well19937c generator.""
 - getRan() does: if (rand == null) rand = new Well19937c(System.currentTimeMillis() + System.identityHashCode(this));
 - getRan() states in javadoc: ""Creates and initializes a default generator if null. Uses a Well19937c generator with System.currentTimeMillis() + System.identityHashCode(this)) as the default seed.""

It seems that only Well19937c should be used, but the constructor javadoc, and the implementation of the reSeed methods was not updated to match this. I think the partial changes were done in MATH-701, more specifically, in commit [1197626] (and related commit [1197716]).",,,,,,,,,,,,,,,,,,,,,07/Dec/11 14:05;dhendriks;TestRandom.java;https://issues.apache.org/jira/secure/attachment/12506444/TestRandom.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-07 22:30:08.918,,,false,,,,,,,,,,,,,,219907,,,Mon Dec 12 01:35:37 UTC 2011,,,,,,0|i0aoq7:,60279,,,,,,,,"07/Dec/11 14:05;dhendriks;TestRandom.java: shows the problem, as described in the issue description.","07/Dec/11 22:30;erans;Good catch!

This bug shows how safer it would be to have the ""rand"" field ""final"" (no need to test for ""null"" all over the place).
",12/Dec/11 01:35;psteitz;Fixed in r1213130.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Strange deprecations in API,MATH-719,12534055,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Later,,pbloem,pbloem,06/Dec/11 17:07,24/Mar/12 16:16,07/Apr/19 20:38,23/Jan/12 11:28,2.0,2.1,2.2,,,,,,,,0,api-change,deprecated,,,,,,"Sorry if this doesn't belong here. I couldn't find any sort of mailing list or other feedback mechanism on the website.

RealMatrix has some very odd deprecations. In particular inverse(), getDeterminant() and isSingular(). The last has the message:

bq. Deprecated. as of release 2.0, replaced by the boolean negation of new LUDecompositionImpl(m).getSolver().isNonSingular()

That's an implementation, not an interface. The whole point of having an interface is that 
* I can query whether a matrix is singular withou having to know about LUDecompositions
* You guys can change the implementation of isSingular() if something better pops up without us guys having to change our code.

I'm not using these methods now, because they're deprecated, but I've basically recreated them in as static methods in a utility class. Wouldn't it be much better to just put code from the deprecation message into the method and remove the deprecation?

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-12-06 18:50:10.323,,,false,,,,,,,,,,,,,,219781,,,Mon Jan 23 11:28:07 UTC 2012,,,,,,0|i0rtev:,160409,,,,,,,,"06/Dec/11 18:50;erans;bq. Sorry if this doesn't belong here.

Indeed, you'd better bring this kind of issue to the ""dev"" ML. :)
The more so that there have been recent discussions about changing the matrix API and decisions ought to be made quite soon now.
","06/Dec/11 20:07;pbloem;Ah, so there is a mailing list. I guess I should have looked a little harder. I'll bring it up there.","23/Jan/12 11:28;erans;It is unlikely that we can come up with a new design before the release of v3.0.
This must be thoroughly discussed first on the ""dev"" ML, together with other matrix interface issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.,MATH-718,12533757,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,uchiyama,uchiyama,04/Dec/11 00:40,04/Mar/13 18:53,07/Apr/19 20:38,21/May/12 19:56,2.2,3.0,,,,,,3.1,,,1,,,,,,,,"The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem.

{{System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5));}}

This returns 499525, though it should be 499999.

I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.",,,,,,,,,,,,,,,,,MATH-785,,,,20/May/12 21:59;tn;MATH-718.diff;https://issues.apache.org/jira/secure/attachment/12528359/MATH-718.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-12-04 09:22:55.786,,,false,,,,,,,,,,,,,,219484,,,Mon May 21 19:56:36 UTC 2012,,,,,,0|i0rtf3:,160410,,,,,,,,"04/Dec/11 09:22;celestin;Hi Yuji,
thanks for reporting this. For version 3.0, we are currently reshaping the package distribution, and this will probably get resolved once we are over with MATH-692.
Best regards,
Sébastien",15/Dec/11 23:55;cwinter;There seem to be stability problems with Beta.regularizedBeta(...) when using extreme parameters. {{PascalDistribution.cumulativeProbability(Integer.MAX_VALUE)}} returns {{NaN}} instead of 1. We should look for a way to avoid both infinite values and NaNs in the implementation of the regularized beta function.,"04/Feb/12 06:33;celestin;As rightly pointed out by Christian, this issue is strongly related to MATH-738.","04/Feb/12 06:35;celestin;Sorry, I only meant to postpone this issue.",12/Apr/12 17:09;jhkauf;I just wanted to let you know that our open source project (http://www.eclipse.org/stem/) needs this function and we are eagerly awaiting the update. We are experiencing the issue of wrong values for large trials.,"12/Apr/12 18:48;celestin;Hi James,
thanks for your interest. STEM is a very interesting project!
I will try and find a fix for this issue as soon as possible. Any ideas are welcome!
Sébastien","13/Apr/12 13:20;tn;The problem Christian described wrt the PascalDistribution is a simple integer overflow in the class itself:

{noformat}
    public double cumulativeProbability(int x) {
        double ret;
        if (x < 0) {
            ret = 0.0;
        } else {
            ret = Beta.regularizedBeta(probabilityOfSuccess,
                    numberOfSuccesses, x + 1);
        }
        return ret;
    }
{noformat}

when x = Integer.MAX_VALUE, adding 1 to it will result in an overflow. As the parameter of regularizedBeta is anyway a double, so it should be changed to something like ""1L + x"" to enforce a long addition.

Edit: Similar things happen btw also in other Distribution implementations, so it should be fixed also there, e.g. BinomialDistribution","13/Apr/12 13:57;tn;The problem is not only related to the Beta function, also the ContinuedFraction.evaluate is numerically unstable.

The reason the cumulativeProbability returns infinity instead of NaN is because the evaluate return 0.0 when called from Beta.regularizedBeta, which leads to a division by zero. The used default epsilon of 10e-15 seems also quite restrictive, when relaxing the epsilon I got much better results (e.g. with 10e-5 I got a result of 499997).","20/May/12 21:42;tn;I further looked into this with relation to MATH-785. First of all, in the original bug report, the reporter mentions that the expected result should be 499999 which is wrong, imho it should be 500000.

After implementing the modified Lentz-Thompson algorithm, the results for the BinomialDistribution of large trials show correct results.","20/May/12 21:55;tn;The attached diff file shows the (preliminary) implementation of the modified Lentz-Thompson algorithm.

Edit: re-uploaded the diff file as it was broken.

Edit2: the failing unit tests I mentioned before were due to a wrong loop, the latest diff shows no unit test errors.",21/May/12 19:56;tn;Fixed in r1341171.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary,MATH-716,12533416,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,pparraud,pparraud,01/Dec/11 09:36,24/Mar/12 16:16,07/Apr/19 20:38,01/Dec/11 23:25,3.0,,,,,,,3.0,,,0,,,,,,,,"In some cases, the aging feature in BracketingNthOrderBrentSolver fails.
It attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket.
In the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-12-01 09:55:18.756,,,false,,,,,,,,,,,,,,219144,,,Thu Dec 01 23:25:58 UTC 2011,,,,,,0|i0aoqf:,60280,,,,,,,,"01/Dec/11 09:55;luc;The problem Pascal (who works with me) describes has already been encountered during the development of the algorithm. The solution found at that time seems to be insufficient. What happens is that in order to still gain a few digits while rebalancing the bracketing interval, we base our retargeting on the currently best solution. In this case, we set targetY = -yA/16 and fail to rebalance. Using the other bracket would improve rebalancing but waste evaluations as was observed during development. So its not a perfect solution either.

I think a compromise would be to attempt rebalancing with a progressively more aggressive target. We could start from the current setting (i.e -1/16 of the best bracket), and if we still update the same side, move towards larger targets.","01/Dec/11 22:15;luc;The following simple test reproduces the bad behavior with a trivial function:
{code}
    @Test
    public void testIssue716() {
        BracketingNthOrderBrentSolver solver =
                new BracketingNthOrderBrentSolver(1.0e-12, 1.0e-10, 0.0, 5);
        UnivariateFunction sharpTurn = new UnivariateFunction() {
            public double value(double x) {
                return (2 * x + 1) / (1.0e9 * (x + 1));
            }
        };
        double result = solver.solve(100, sharpTurn, -0.9999999, 30, 15, AllowedSolution.RIGHT_SIDE);
        Assert.assertEquals(0, sharpTurn.value(result), solver.getFunctionValueAccuracy());
        Assert.assertTrue(sharpTurn.value(result) >= 0);
        Assert.assertEquals(-0.5, result, 1.0e-10);
    }
{code}
The test fails with TooManyEvaluationsException. In fact, only the right side of the bracketing interval is updated and very slowly decreases from 15.0 to 14.999677603318897 while the left side of the bracketing interval is stuck at -0.9999999.","01/Dec/11 23:25;luc;Fixed in subversion repository as of r1209307.

Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PascalDistribution returns wrong values of mean and variance,MATH-715,12533064,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,celestin,celestin,celestin,29/Nov/11 03:56,18/Mar/12 09:26,07/Apr/19 20:38,06/Dec/11 08:19,,,,,,,,3.0,,,0,,,,,,,,"The header of the Javadoc states that the random variable (say X) being represented by this {{o.a.c.m.distribution.PascalDistribution}} is the number of *failures*. The current Javadoc is slightly confusing, because it refers to the Wikipedia website, where the opposite convention is adopted (X is the number of *successes*) : different formulas therefore apply for the mean and variance of X. The javadoc should be made clearer, for example by inclusion of full formulas. Also the parameters differing from the Wikipedia reference should not have the same name
  * {{p}} is the probability of success in both cases: OK,
  * {{r}} is the number of failures in Wikipedia, but the number of successes in CM. This could be renamed (say) {{s}}.

Finally, with the current notations of CM, the mean of X is given by {{mean(X) = r * (1 - p) / p}}, while the currently implemented formula is {{r * p / (1 - p)}}, which is actually the formula corresponding to the Wikipedia convention.

The following piece of code shows that the current implementation is faulty
{code:java}
public class PascalDistributionDemo {
   public static void main(String[] args) {
       final int r = 10;
       final double p = 0.2;
       final int numTerms = 1000;
       final PascalDistribution distribution = new PascalDistribution(r, p);
       double mean = 0.;
       for (int k = numTerms - 1; k >= 0; k--) {
           mean += k * distribution.probability(k);
       }
       // The following prints 40.00000000000012
       System.out.println(""Estimate of the mean = "" + mean);
       // The following prints 2.5
       System.out.println(""CM implementation = "" +
                          distribution.getNumericalMean());
       // The following prints 2.5
       System.out.println(""r * p / (1 - p) = "" + (r * p / (1 - p)));
       // The following prints 40.0
       System.out.println(""r * (1 - p) / p = "" + (r * (1 - p) / p));
   }
}
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,218792,,,Sun Mar 18 09:26:32 UTC 2012,,,,,,0|i0aoqn:,60281,,,,,,,,06/Dec/11 08:19;celestin;Fixed in {{r1210359}}.,18/Mar/12 09:26;celestin;Fixed in 3.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Negative value with restrictNonNegative,MATH-713,12532737,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mskrzypczak,mskrzypczak,25/Nov/11 16:35,24/Mar/12 16:16,07/Apr/19 20:38,28/Nov/11 20:17,2.2,,,,,,,3.0,,,0,nonnegative,simplex,solver,,,,,"Problem: commons-math-2.2 SimplexSolver.

A variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call:
SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true);

Function
1 * x + 1 * y + 0

Constraints:
1 * x + 0 * y = 1

Result:
x = 1; y = -1;

Probably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.",commons-math-2.2,10800,10800,,0%,10800,10800,,,,,,,,,,,,,,28/Nov/11 15:38;tn;MATH-713.patch;https://issues.apache.org/jira/secure/attachment/12505337/MATH-713.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-27 09:28:55.371,,,false,,,,,,,,,,,,,,218470,,,Mon Nov 28 20:17:46 UTC 2011,,,,,,0|i0aor3:,60283,,,,,,,,"27/Nov/11 09:28;luc;Could you please check this against the latest development version from the subversion repository ?
There have been several fixes concerning these coefficients in simplex solver since 2.2
","28/Nov/11 10:53;mskrzypczak;I'm quite busy at work now... When I'll find some time, I'll try to test it. Of course I'd be glad if one of developers would do it before :)","28/Nov/11 15:38;tn;I have tested this problem with the latest trunk, and the problem still remains.

The attached patch handles the case of unconstrained variables that still occur in the objective function. I have also added a unit test.","28/Nov/11 20:17;luc;Fixed in subversion repository as of r1207566.

Patch applied directly.

Thanks to Michal for the report and thanks to Thomas for the patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BigFraction percentageValue does BigInteger divide.,MATH-709,12531883,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,bmilton,bmilton,18/Nov/11 03:17,24/Mar/12 16:16,07/Apr/19 20:38,18/Nov/11 07:43,2.2,,,,,,,3.0,,,0,,,,,,,,integer divide of 1/2 returns 0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-11-18 07:43:31.402,,,false,,,,,,,,,,,,,,217619,,,Fri Nov 18 07:43:31 UTC 2011,,,,,,0|i0aorr:,60286,,,,,,,,"18/Nov/11 07:43;luc;Fixed in subversion repository as of r1203516.

I have also added the percentage method to Fraction.

Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad interaction between step handlers and events detectors that reset state in ODE integrators,MATH-706,12531198,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,11/Nov/11 23:33,24/Mar/12 16:16,07/Apr/19 20:38,11/Nov/11 23:40,3.0,,,,,,,3.0,,,0,,,,,,,,"During ODE integration, when en event detector attempts to reset a state, if there are also step handlers
associated with the integrator, the reset state is not called with the state at event time, but may be called with some other state. The time provided always correspond to the real event time (and hence is inconsistent with the state).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,216935,,,Fri Nov 11 23:40:33 UTC 2011,,,,,,0|i0aosf:,60289,,,,,,,,11/Nov/11 23:40;luc;Fixed in subversion repository as of r1201105.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DormandPrince853 integrator leads to revisiting of state events,MATH-705,12530878,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,dhendriks,dhendriks,09/Nov/11 13:29,24/Mar/12 16:16,07/Apr/19 20:38,27/Nov/11 14:34,3.0,,,,,,,3.0,,,0,,,,,,,,"See the attached ReappearingEventTest.java. It has two unit tests, which use either the DormandPrince853 or the GraggBulirschStoer integrator, on the same ODE problem. It is a problem starting at time 6.0, with 7 variables, and 1 state event. The state event was previously detected at time 6.0, which is why I start there now. I provide and end time of 10.0. Since I start at the state event, I expect to integrate all the way to the end (10.0). For the GraggBulirschStoer this is what happens (see attached ReappearingEventTest.out). For the DormandPrince853Integerator, it detects a state event and stops integration at 6.000000000000002.

I think the problem becomes clear by looking at the output in ReappearingEventTest.out, in particular these lines:

{noformat}
computeDerivatives: t=6.0                  y=[2.0                 , 2.0                 , 2.0                 , 4.0                 , 2.0                 , 7.0                 , 15.0                ]
(...)
g                 : t=6.0                  y=[1.9999999999999996  , 1.9999999999999996  , 1.9999999999999996  , 4.0                 , 1.9999999999999996  , 7.0                 , 14.999999999999998  ]
(...)
final result      : t=6.000000000000002    y=[2.0000000000000013  , 2.0000000000000013  , 2.0000000000000013  , 4.000000000000002   , 2.0000000000000013  , 7.000000000000002   , 15.0                ]
{noformat}

The initial value of the last variable in y, the one that the state event refers to, is 15.0. However, the first time it is given to the g function, the value is 14.999999999999998. This value is less than 15, and more importantly, it is a value from the past (as all functions are increasing), *before* the state event. This makes that the state event re-appears immediately, and integration stops at 6.000000000000002 because of the detected state event.

I find it puzzling that for the DormandPrince853Integerator the y array that is given to the first evaluation of the g function, has different values than the y array that is the input to the problem. For GraggBulirschStoer is can be seen that the y arrays have identical values.","Commons Math trunk, Java 6, Linux",,,,,,,,,,,,,,,,,,,,09/Nov/11 13:31;dhendriks;ReappearingEventTest.java;https://issues.apache.org/jira/secure/attachment/12503077/ReappearingEventTest.java,09/Nov/11 13:31;dhendriks;ReappearingEventTest.out;https://issues.apache.org/jira/secure/attachment/12503078/ReappearingEventTest.out,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-09 19:57:08.802,,,false,,,,,,,,,,,,,,216616,,,Sun Nov 27 14:34:04 UTC 2011,,,,,,0|i0aosn:,60290,,,,,,,,"09/Nov/11 13:31;dhendriks;The Java unit tests that show the problem, and the console output they give, as described in the issue description.","09/Nov/11 19:57;luc;The reason for this strange behavior is that g function evaluations are based on the integrator-specific interpolator.

Each integration method has its specific algorithm and preserve a rich internal data set. From this data set, it is possible to build an interpolator which is specific to the integrator and in fact share part of the data set (they reference the same arrays). So integrator and interpolator are tightly bound together.

For embedded Runge-Kutta methods like Dormand-Prince 8(5,3), this data set corresponds to one state vector value and several state vector derivatives sampled throughout the step. When the step is accepted after error estimation, the state value is set to the value at end of step and the interpolator is called. So the equations of the interpolator are written in such a way interpolation is backward: we start from the end state and roll back to beginning of step. This explains why when we roll all the way back to step start, we may find a state that is not exactly the one we started from, due to both the integration order and interpolation order.

For Gragg-Bulirsch-Stoer, the data set corresponds to one state vector value and derivatives at several orders, all taken at step middle point. When the step is accepted after error estimation, the interpolator is called before the state value is set to the value at end of step and. So the equations of the interpolator are written in such a way interpolation is forward: we start from the start state and go on towards end of step. This explains why when we go all the way to step end, we may find a state that is not exactly the one that will be used for next step, due to both the integration order and interpolation order.

So one integrator type is more consistent at step start and has more error at step end, while the other integrator has a reversed behavior.

In any case, the interpolation that is used (and in fact the integration data set it is based upon) are not error free. The error is related to step size.

We could perhaps rewrite some interpolators by preserving both start state s(t[k]) and end state s(t[k+1]) and switching between two hal model as follows:
  i(t) = s(t[k])   + forwardModel(t[k], t)    if t <= (t[k] + t[k+1])/2
and
  i(t) = s(t[k+1]) + backwardModel(t, t[k+1]) if t > (t[k] + t[k+1])/2

This would make interpolator more consistent with integrator at both step start and step end and perhaps reduce this problem. This would however not be perfect, as it will introduce a small error at junction point. I'm not sure if it would be easy or not, we would have to review all interpolators and all integrators for that. All models are polynomial ones.

Note that the problem should not appear for Adams methods (when they will be considered validated ...), because in this case, it is the interpolator that is built first and the integrator is in fact an application of the interpolator at step end! So interpolator and integrator are by definition always perfectly consistent with each other.

What do you think ?

Should we let this problem alone and consider we are in the grey zone of expected numerical inaccuracy due to integration/interpolation orders or should we attempt the two half-models trick ?
","10/Nov/11 14:31;dhendriks;bq. Should we let this problem alone and consider we are in the grey zone of expected numerical inaccuracy due to integration/interpolation orders or should we attempt the two half-models trick?

I consider it important that events are properly detected, and are detected exactly once (if they occur exactly once). Therefore, in general, I think the half-models trick would be a good idea, as it is more important (to me) to have higher accuracy at the end points than in the middle. For me, the DormandPrince853Integrator is now practically useless.
","20/Nov/11 21:44;luc;This issue should be fixed in subversion repository as of r1204270.

Could you check it works for you ? If so, I will use the same trick for other Runge-Kutta type step interpolators.","21/Nov/11 07:46;dhendriks;bq. This issue should be fixed in subversion repository as of r1204270. Could you check it works for you ? If so, I will use the same trick for other Runge-Kutta type step interpolators.

It seems to work perfectly. Thanks!","27/Nov/11 14:34;luc;Fixed in subversion repository as of r1206723.

Applied same method to all Runge-Kutta based integrators.

Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
One of Variance.evaluate() methods does not work correctly,MATH-704,12530782,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,lilyevsky,lilyevsky,08/Nov/11 23:08,24/Mar/12 16:16,07/Apr/19 20:38,30/Nov/11 06:25,2.2,,,,,,,3.0,,,0,,,,,,,,"The method org.apache.commons.math.stat.descriptive.moment.Variance.evaluate(double[] values, double[] weights, double mean, int begin, int length) does not work properly. Looks loke it ignores the length parameter and grabs the whole dataset.
Similar method in Mean class seems to work.
I did not check other methods taking the part of the array; they may have the same problem.

Workaround: I had to shrink my arrays and use the method without the length.",Java 1.6,,,,,,,,,,,,,,,,,,,,28/Nov/11 12:51;tn;MATH-704.patch;https://issues.apache.org/jira/secure/attachment/12505328/MATH-704.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-11-27 06:01:57.08,,,false,,,,,,,,,,,,,,216520,,,Wed Nov 30 06:25:55 UTC 2011,,,,,,0|i0aosv:,60291,,,,,,,,27/Nov/11 06:01;psteitz;I can't seem to make this fail and I have added tests that would indicate that array segments are being handled properly.  Can you provide an example showing a failure?,"28/Nov/11 12:51;tn;I have found a small bug wrt this bug report. The sum of the weights is calculated on the whole weight array rather than the specified sub-array [begin, begin + length).

See the attached patch for more details. Still need a testcase to verify that it finally addresses the original bug report.","30/Nov/11 06:25;psteitz;Good catch, Thomas.  Thanks!  Patch, with test case added, committed in r1208291.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CMA-ES optimizer input sigma should not be normalized by user,MATH-702,12530572,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,luc,luc,07/Nov/11 13:47,24/Mar/12 16:16,07/Apr/19 20:38,07/Nov/11 14:13,3.0,,,,,,,3.0,,,0,,,,,,,,"I am trying to use CMA-ES optimizer with simple boundaries.

It seems the inputSigma parameter should be normalized as it is checked against the [0 - 1] range in the checkParameters private method and as its value defaults to 0.3 if not not set in the initializeCMA private method.

I would have expected this value to be in the same units as the user parameters and to be normalized as part of an internal processing step instead of relying to the user doing this. I think the method need normalized values internally, as per the encode/decode methods in the inner class FitnessFunction suggest.

The optimizer should accept values in the same units as the other parameters and use ""encode"" (or a similar function) to do the normalization. This way, normalization is considered an internal implementation detail.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-11-07 19:46:51.517,,,false,,,,,,,,,,,,,,216310,,,Tue Nov 08 21:57:18 UTC 2011,,,,,,0|i0aotb:,60293,,,,,,,,07/Nov/11 14:13;luc;Fixed in subversion repository as of r1198741.,"07/Nov/11 19:46;Nikolaus.Hansen@lri.fr;good point. However, if the encoding/decoding methods can be defined
coordinate-wise differently or are even non-linear (I am not aware whether
they can be or not), I am not sure you can come up with a sufficiently
reasonable and comprehensible way to apply this transformation to sigma.
sigma is a scalar and positive, so it is a different object than the
encoding/decoding methods are operating on, right?

The interplay between sigma and an encoding is an unfortunately delicate
part in the user interface, but I don't really see a way to make it right
*and* look entirely obvious to the user.


On Mon, 07 Nov 2011 14:48:51 +0100, Luc Maisonobe (Created) (JIRA)



-- 
Science is a way of trying not to fool yourself.

The first principle is that you must not fool yourself, and you
are the easiest person to fool. So you have to be very careful
about that. After you've not fooled yourself, it's easy not to
fool other[ scientist]s. You just have to be honest in a
conventional way after that.
                                         -- Richard P. Feynman

Nikolaus Hansen
INRIA, Research Centre Saclay – Ile-de-France
Machine Learning and Optimization group (TAO)
University Paris-Sud (Orsay)
LRI (UMR 8623), building 490
91405 ORSAY Cedex, France
Phone: +33-1-691-56495, Fax: +33-1-691-54240
URL: http://www.lri.fr/~hansen
","07/Nov/11 19:59;luc;You are right.
for now, encoding/decoding is both liner only and hidden in a private inner class (FitnessFunction), so users only see simple bounds and inside these bounds the transform is linear.
For sure if we come up with a different mapping, we will need to come up with a way to define also the mapping of covariance. One way would be to rely on the Jacobian, but it would be strange to propose mapping function and requiring them to be smooth while the goal function by itself could be highly non-smooth.
So for now, the simple linear mapping seems sufficient to me, and would need improvement only if we change some internals of the class.","08/Nov/11 21:51;Nikolaus.Hansen@lri.fr;unfortunately, the problem already arises with a linear coordinate-wise  
mapping: which coordinate gives the unit where sigma is defined on?

 from a practical viewpoint it is important to consider coordinate-wise  
non-linear mappings. Multidimensional mappings (e.g. with ""covariance"") I  
have never been able to apply successfully.

Cheers,
Niko

On Mon, 07 Nov 2011 21:00:51 +0100, Luc Maisonobe (Commented) (JIRA)  



-- 
Science is a way of trying not to fool yourself.

The first principle is that you must not fool yourself, and you
are the easiest person to fool. So you have to be very careful
about that. After you've not fooled yourself, it's easy not to
fool other[ scientist]s. You just have to be honest in a
conventional way after that.
                                     -- Richard P. Feynman

Nikolaus Hansen
INRIA, Research Centre Saclay – Ile-de-France
Machine Learning and Optimization group (TAO)
University Paris-Sud (Orsay)
LRI (UMR 8623), building 490
91405 ORSAY Cedex, France
Phone: +33-1-691-56495, Fax: +33-1-691-54240
URL: http://www.lri.fr/~hansen
","08/Nov/11 21:57;luc;Does ""coordinate-wise"" means that each coordinate has its dedicated sigma ?
If so, this is what I have set up. The sigma vector was already an array with the same dimension as both the state vector and the lower/upper bounds, so I have simply used upper[i] - lower[i] as a multiplication factor for sigma[i].
Does what I did make sense ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Seeding a default RNG,MATH-701,12530041,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,erans,erans,03/Nov/11 13:26,24/Mar/12 16:16,07/Apr/19 20:38,05/Nov/11 06:50,,,,,,,,3.0,,,0,,,,,,,,"In ""RandomDataImpl"":
{code}
private RandomGenerator getRan() {
    if (rand == null) {
        rand = new JDKRandomGenerator();
        rand.setSeed(System.currentTimeMillis());
    }
    return rand;
}
{code}
The conditional branch is used by ""sample()"" in ""AbstractContinuousDistribution"".

When several ""...Distribution"" objects are instantiated in a short time interval, they are seeded with the same value.
",,,,,,,,,,,,,,MATH-720,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-11-03 16:31:22.02,,,false,,,,,,,,,,,,,,215900,,,Sat Nov 05 06:50:23 UTC 2011,,,,,,0|i0aotj:,60294,,,,,,,,"03/Nov/11 16:31;mikl;I see your point, but at the same time it might be difficult to deal with correctly. One work-around is to use the serialVersionUID field together with System.currentTimeMillis(), but I am not sure if that is a good idea nor how well it can be implemented.","03/Nov/11 16:43;erans;No, my point was that the seeding should just be removed: When several distribution objects are instantiated, we usually don't want the samples to be the same...
","03/Nov/11 17:09;psteitz;I agree that there is a problem here; but I would solve it in AbstractContinuousDistribution by passing a seeded generator to the RandomDataImpl constructor.  The code above is for the default case where no generator has been supplied or set.  For sampling, a Well generator would be a better choice.","03/Nov/11 22:10;erans;IIRC we discarded the solution of passing a RNG in the distribution's constructor because the ""sample"" methods was construed as syntactic sugar in order to access the ""RandomData"" functionality from within the ""...Distribution"" classes.
It was also argued that people who would use this API (instead of directly instantiating a ""RandomDataImpl"" object) would be satisfied with a default RNG, whatever was deemed the best choice by the CM developers. Thus, if another generator is indeed better, we should just plug it in instead of the current ""JDKRandomGenerator"".
In the same line of argument, the possibility of seeding was also supposed to be given up in exchange for a ""sample"" API at the distribution level.

In ""AbstractContinuousDistribution"", we have:
{code}
protected final RandomDataImpl randomData = new RandomDataImpl();
{code}
In effect, there is no reason not to replace that statement with:
{code}
protected final RandomDataImpl randomData = new RandomDataImpl(new Well44497a());
{code}

As as side note: I don't understand why there is a default constructor in ""RandomDataImpl""; since all the methods need a RNG, it does not make much sense to allow the object to be constructed without one!

Second side note: Why not separate the ""secure"" alternatives into another implementation? There would be ""RandomData"" (with a ""nextHexString"" method) and a ""SecureRandomData"" (also with a ""nextHexString"" method, instead of the current ""nextSecureHexString"").

Back to this report's issue: The parent class ""AbstractWell"" seems to suffer from the same problem because the default constructor calls {{System.currentTimeMillis()}}.
We should have a look at how ""java.util.Random"" seeds its default instances so that they can claim (excerpt form Javadoc):
{noformat}
This constructor sets the seed of the random number generator to a value very likely to be
distinct from any other invocation of this constructor.
{noformat}
","04/Nov/11 02:43;psteitz;+1 to changing the default in the RandomDataImpl to a Well generator (documenting this)
+1 as well to trying to find a better default seed (documenting how it works).  Harmony still belongs to us, so we can certainly look at / imitate the impl there.

The reason that the argumentless constructor exists is to allow users to accept the default generator. The lazy initialization idiom was chosen because in some cases, users may choose to invoke the setter instead of passing a generator as a constructor argument.  This is an older style no longer in use much in [math].  I would be OK getting rid of this - i.e., having the argumentless constructor take the hit to create a default instance.

-0 for separating out the secure stuff.  RandomDataImpl is an aggregate class that bundles a lot of commonly used random data generation methods.  Just as the random generator is pluggable, so is the secure random generator, which makes the class convenient to use for applications that require both kinds of random values.  I think its simpler and more convenient to bundle things as they are.  Of course, I have been using this class for a long time, so I may not be the best judge of what is simplest / easiest to use.","04/Nov/11 09:40;sebb@apache.org;If lazy init is dropped, then the rand field can be made final.

Can secRand be made final? This would mean dropping setSecureAlgorithm() in favour of an extra ctor.
Since secRand is not always needed, this is an argument for making the secure stuff a sub-class.
Alternatively, IODH could perhaps be used with secRand.","04/Nov/11 12:10;erans;* From Harmony's implementation of ""java.util.Random"":
{code}
public Random() {
    setSeed(System.currentTimeMillis() + hashCode());
}
{code}

* I'd also prefer setting both RNGs at construction.
A user would have to write
{code}
SecureRandom secRand = SecureRandom.getInstance(algorithm, provider);
{code}
in his application's code, and that would make it clearer for him why and how an exception can be generated (and that CM's code is not the cause of it).

* I also favour a new ""SecureRandomDataImpl"" as a subclass of ""RandomDataImpl""; the constructor would be something like:
{code}
public SecureRandomDataImpl(SecureRandom rng) {
    super(rng);
}
{code}
This would ensure that _all_ the {{next...}} methods use a secure RNG, whereas currently only some of the accessors have a secure alternative (e.g. ""nextGaussian"" is not using ""secRand""). This would be less confusing without requiring long explanations...
",04/Nov/11 16:23;psteitz;I just committed (r1197626) a) change to Well generator as default for non-secure generator b) seeding like the Harmony code.  Can we resolve this and take the refactoring discussion to the dev list?,"04/Nov/11 16:59;erans;Okay for the design discussion! ;)

But don't resolve just yet.
To avoid nasty surprises, I think that the ""AbstractWell"" (maybe others?) should be modified so that default constructors are all seeded similarly.
""RandomDataImpl"" as a ""user"" of the RNG should not have to worry how to best seed the generator.
","04/Nov/11 17:09;psteitz;I thought about changing the default seeding in AbstractWell and agree that would be a good idea.  I still like to supply the seed explicitly and document it in RandomDataImpl, though, so users of that class know exactly what they are getting by default.  There is a little wrinkle here, too that keeping the seeding expressed and documented in RandomDataImpl makes easier to keep track of.  If we ever implement hashcode in RandomDataImpl (or the Well generators), things could get messed up if it does not separate generator instances the way the system identity haschcode does.","05/Nov/11 06:50;psteitz;Changes to WELL, MersenneTwister committed in r1197896.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inverseCumulativeDistribution fails with cumulative distribution having a plateau,MATH-699,12529630,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,celestin,celestin,celestin,01/Nov/11 08:35,18/Mar/12 09:27,07/Apr/19 20:38,05/Dec/11 06:50,2.2,,,,,,,3.0,,,0,,,,,,,,"This bug report follows MATH-692. The attached unit test fails. As required by the definition in MATH-692, the lower-bound of the interval on which the cdf is constant should be returned. This is not so at the moment.",,,,,,,,,,,,,,,,,,,,,01/Nov/11 08:36;celestin;AbstractContinuousDistributionTest.java;https://issues.apache.org/jira/secure/attachment/12501747/AbstractContinuousDistributionTest.java,22/Nov/11 07:33;celestin;inv-cum-new-impl.zip;https://issues.apache.org/jira/secure/attachment/12504710/inv-cum-new-impl.zip,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-11-05 16:21:04.463,,,false,,,,,,,,,,,,,,215489,,,Sun Mar 18 09:27:52 UTC 2012,,,,,,0|i0aotr:,60295,,,,,,,,"04/Nov/11 19:13;celestin;h4. Should this be considered as an error?
Please have a look to lines [103-108|http://commons.apache.org/math/xref/org/apache/commons/math/distribution/AbstractContinuousDistribution.html#103] of {{AbstractContinuousDistribution}}.
I think the logics is flawed, I have the feeling that {{rootFindingFunction.value(lowerBound)}} should be compared with {{solver.getFunctionValueAccuracy()}}, and not with {{getSolverAbsoluteAccuracy()}}. The problem is that {{AbstractContinuousDistribution}} should then have a method called {{getSolverFunctionValueAccuracy()}}. Should I add one?
Thanks for your comments,
Sébastien","05/Nov/11 16:21;psteitz;I agree that the wrong solver property is being used. I would say yes, add the property, possibly protected.  Assuming protected access, it might even be better to just expose the solver itself.","05/Nov/11 17:46;cwinter;I also agree that the mentioned code lines use the wrong property. But I'm not sure whether the bracketing step is necessary and efficient at all. Maybe it's better to pass lowerBound and upperBound directly to the solving step because the solver will shrink the interval efficiently. The bracketing algorithm, however, is very inefficient in expanding the interval around the initial point to a bracket (At least the current implementation is inefficient as it makes linear steps. Geometrical steps would be better for distribution functions, but valid brackets might be missed for non-monotonic functions.). The only problem I see for the solver is if lowerBound or upperBound is infinite. The JavaDoc of getDomainLowerBound() and getDomainUpperBound() just could mention that an implementation must return a finite value.

I'm fine with protected access to the solver.","06/Nov/11 06:04;celestin;I think Christian has a point here. The bracketing step is superfluous. I'll propose a new impl getting rid of it, so that you can decide whether or not to restore it.

However, we cannot forbit getDomainLowerBound() and getDomainUpperBound() to return {{Double.POSITIVE_INFINITY}} (for {{p == 1}}) or {{Double.NEGATIVE_INFINITY}} (for {{p == 0}}). I will prepare something along these lines.","06/Nov/11 06:12;psteitz;Be careful eliminating the bracketing step.  IIRC it is in there to resolve a bug involving some corner case.  In theory, the unit tests should pick up any regressions, but it would be a good idea to review the commit logs and issue reports for this class before ripping out the bracketing step.","06/Nov/11 06:34;celestin;OK, thanks for the tip. I will do so.
Sébastien

Additional note: Phil, you are referring to r141391. This was 7 years, 3 months ago, you do have a good memory! Will look into that more closely.","07/Nov/11 04:04;celestin;Hi,
I thought I would keep you up to date with the problems I'm facing at the moment.

First of all, I see no smart way to check that for sure, {{inverseCumulativeProbability(0)}} (resp. {{inverseCumulativeProbability(1)}}) should return {{Double.NEGATIVE_INFINITY}} (resp. {{Double.POSITIVE_INFINITY}}). What could be done would be to test for a neighbouring value, and check that the cumulative probability is slightly above zero (resp. below one). But this makes no sense, because the only neighbouring value which would make sense would be {{+/-Double.MAX_VALUE}}, and this surely return 0.0 or 1.0. So this is my first problem.

The way I see things is as follows: users might have no clue about the inverse cumulative probability, but they *must* know the values of this inverse for p = 0 and p = 1. So I would suggest to change the contract of {{getInitialDomain(p)}}. I would make clear in the javadoc that {{getInitialDomain(p)}} should return {{Double.NEGATIVE_INFINITY}} *if, and only if* {{inverseCumulativeProbability(0) == Double.NEGATIVE_INFINITY}}. Similarly, {{getInitialDomain(p)}} should return {{Double.POSITIVE_INFINITY}} *if, and only if* {{inverseCumulativeProbability(1) == Double.POSITIVE_INFINITY}}.

My second problem is to check for the presence of plateaux, consistently with finite precision. Here is my initial idea. I first define the two absolute accuracies
* {{dx = getSolverAbsoluteAccuracy()}},
* {{dp = getSolverFunctionValueAccuracy()}}.

Then, if {{x}} is the root found by the solver: we do have {{cumulativeProbability( x ) == p}} (within a specified accuracy). The problem is to check whether there is a *smaller* value which also satisfies this requirement (in which case, the smallest such value must be returned).

My initial test was {{cumulativeProbability(x - dx) == p}}. Then, I tried {{FastMath.abs(cumulativeProbability(x - dx) - p) <= dp}}. Although more consistent with finite precision, this is not fully satisfactory, because in simple cases (where there is no plateau), it might lead to the solver moving to a somewhat less good point. At the very least, it would lead to additional iterations... to finally get back to the initial point {{x}}.

Then, I thought of checking for *exact* nullity of the pdf at x. The problem is that the pdf might have discontinuities, in which case, this simple test might fail (if {{x}} turns out to be the higher-end of the plateau, and there is a slope discontinuity here).

So, I'm now back to this test: {{cumulativeProbability(x - dx) == cumulativeProbability( x )}}. Please note
* *exact* equality test,
* I'm no longer testing for equality with the target value {{p}}, but with its estimate {{cumulativeProbability( x )}}.

I would be grateful for some feedback on these issues. Also, I think it is clear that
* my code will require careful reviewers!!!
* it won't be completely fool-proof.","07/Nov/11 15:11;psteitz;My inclination would be to keep the implementation in the base class as simple as possible, documenting what it does and pushing the responsibility for dealing with plateaus in the distribution to the implementations that have these.  I don't think any of the currently implemented real distributions have this problem.  Correct?  

The invariant you are proposing for when to return infinities for domain lower and upper bounds would make sense if these were intended to be the bounds of support for the distribution, but this is not what these properties represent.  They are initial guesses for where to start when trying to bracket a root.  That means they have to be values that can be fed into the cumulative probability function. I may be missing the point that you and Christian are making, but the basic problem is that as Christian points out, we always need to start with finite values and that is what led to the somewhat inelegant construct of the domain lower/upper bounds as guesses and the need to do the bracketing step.  The code you reference is trying to do the bracketing, starting with the domain upper and lower bounds as initial guesses.  

Remember to consider convergence problems when the actual parameter to inverse cum is close to or exactly equal to 0 or 1.  Per the comment in the code above the test that uses (or should use) the function value absolute accuracy, if the distribution has bounded support and the argument is 0 or 1, bracketing will fail.

","07/Nov/11 19:18;celestin;{quote}
I don't think any of the currently implemented real distributions have this problem. Correct?
{quote}
Maybe I misunderstood, but in MATH-692 asked for a more precise definition of {{inverseCumulativeProbability}} as inf{x in R | P(X<=x) >= p}. If we exclude distribution with plateaus, I think that the current implementation is  satisfactory (but for the use of the wrong tolerance I've already pointed out). But it was agreed that this implementation should be made more robust. So what was it exactly that needed improvement? What do you want me to do on this method?

{quote}
They are initial guesses for where to start when trying to bracket a root. That means they have to be values that can be fed into the cumulative probability function.
{quote}
I'm aware of that, but the current solver does fail when {{inverseCumulativeProbability(0)}} should return -inf, or {{inverseCumulativeProbability(1)}} should return +inf. See for example the implementation of {{NormalDistributionImpl}}.
{code:java}
    public double inverseCumulativeProbability(final double p)
     {
        if (p == 0) {
            return Double.NEGATIVE_INFINITY;
        }
        if (p == 1) {
            return Double.POSITIVE_INFINITY;
        }
        return super.inverseCumulativeProbability(p);
    }
{code}
So currently, people who want to implement a distribution must be aware of the fact that the default implementation of {{inverseCumulativeProbability}} *must* be overriden. This rather unusual fact should be made clear in the Javadoc, unless a workaround can be thought of. I agree the one I proposed was far from perfect.

{quote}
Remember to consider convergence problems when the actual parameter to inverse cum is close to or exactly equal to 0 or 1.
{quote}
Thank you for pointing this out earlier. I do keep this important point in mind. But again, if we do not widen the scope of this method, I don't see what is required of me (appart from some cosmetic alterations to the Javadoc).","08/Nov/11 03:22;psteitz;I am sorry, Sebastien.  I am not being very helpful here. As I dig deeper into the code and archaeology, I am having a hard time seeing how we can really improve the default impl without knowledge of the underlying distribution.  I agree with you that we should clearly document its limitations, though, and fix the javadoc contracts everywhere to be correct and consistent.  The default impl was never really intended to be universal - just a simple solver-based impl that would work for well-behaved distributions. The normal distribution example above points to a slight improvement that could now be done.  When that code was written, we had yet to define the supportLowerBound and supportUpperBound properties.  Now that we have those, the test in the normal dist case could be moved up into the default impl, using the getters on the distribution for the return values.

By all means if you can find a way to deal with plateaus or otherwise improve robustness of the default impl, go for it.  In particular, if you can come up with a better way to set up the solver, possibly eliminating the domainLower/upperBound methods, that would be great.  It has always seemed a little ugly to me that we had to implement these methods for every distribution just so we could get initial guesses for the inverse cum solver.

 ","08/Nov/11 06:54;celestin;I agree with you: however careful we are, the default implementation will never be perfect, and it should be clearly stated in the javadoc. I see (but haven't dug into it yet) there is an abstract test for continuous distributions. I'll try and make sure that most possible cases of failure of {{inverseCumulativeProbability}} are tested in this abstract framework, so that users could be *strongly* encouraged to implement this test for their very specific distribution.

I do think we can widen a little bit the scope of the default implementation. I actually worked towards eliminating the bracketing step, but you convinced me that it should be the other way round: can we eliminate the domainLower/upperBound methods (which I agree are very efficient, but a little bit of a pain in the neck...).

Here is an idea I'm going to check (keeping in mind the accuracy issues you've already pointed at)
* for distributions with bounded support: maybe the bounds (possibly shifted if the bounds are not inclusive) of the support itself could be used as an initial bracketting range.
* for distributions with unbounded support: start from *any* (? or a better guess? average value?) point in the support, and do bracketting. However, I do agree with Christian, we should use geometric progressions in this case, instead of arithmetic progressions. This would probably lead to a very large interval, but it would provide us an interval more quickly, and the solver itself would probably be quite good at narrowing it very quickly. I can try some monitoring on this issue.

As for plateaus, I think what I'm currently working on is not too inefficient. I'm sure it's not *the* answer (which is, anyway, 42 :)), but I'm struggling to fail this algo... Don't worry, I'll break it. I think that what I've done entails very little additional cost (maybe one more evaluation of cumulativeProbability) for distributions *without* plateau, but I need to check that (we do not want to pay a heavy price for plateaus, since most of our distributions do not have one, as you rightly pointed out).

I would be grateful for any idea on these topics, and will keep you informed. All the best for now,
Sébastien","08/Nov/11 22:38;cwinter;Much discussion has happened here in the recent days. Thanks!

Now I realize that it is hard to get rid of the bracketing step in the case of unbounded support. I guess a default bracket of {{[-Double.MAX_VALUE;+Double.MAX_VALUE]}} for  {{0<p<1}} isn't a good idea for such cases. Maybe we can move the bracketing step into a {{getBracket(double p)}} method (only {{0<p<1}} needs to be handled), which makes use of the support bounds where possible and performs an bracketing algorithm only as last option. For specific distributions this method can be overridden by providing brackets through precomputed cdf-values or clever estimations, which removes using an bracketing algorithm at all. Such a method allows to delete {{getDomainLowerBound}}/{{getDomainUpperBound}}/{{getInitialDomain}}. However, this replacement doesn't remove the pain caused by {{getDomainLowerBound}}/{{getDomainUpperBound}}/{{getInitialDomain}} completely. It is just a little mitigation to this pain as {{getBracket}} doesn't have to be overridden (though it should be overridden to increase performance unless {{inverseCumulativeProbability}} is overridden entirely).

For the plateau issue I unfortunately don't see a way to avoid implementing a modified solver so that it approximates the solution by moving together {{x0}} and {{x1}} satisfying {{P(X<=x0) < p}} and {{P(X<=x1) >= p}}.","08/Nov/11 22:56;cwinter;A different idea for solving the bracketing issue: We could solve the inverse cdf calculation by transforming the domain [-&infin;;+&infin;] to [-1;+1] with arctan. More precisely, we could do the first iterations of the solving step on the transformed domain, and we can go back to original domain as soon as +/-1 aren't interval limits any more.","09/Nov/11 03:06;psteitz;Interesting idea.  Worth playing with.  One more archaeological fact that occurred to me is that the current setup with domain upper / lower bounds used in bracketing was developed before we required either mean or variance from distributions.  We now have getNumericalMean and getNumericalVariance.  When these both exist and are finite, inequalities such as Chebyshev's might be useful in setting up brackets.","09/Nov/11 06:54;celestin;{quote}
A different idea for solving the bracketing issue: We could solve the inverse cdf calculation by transforming the domain [-∞;+∞] to [-1;+1] with arctan. More precisely, we could do the first iterations of the solving step on the transformed domain, and we can go back to original domain as soon as +/-1 aren't interval limits any more.
{quote}

Is it not a ""disguised"" bracketting step, with a somewhat more clever way of sampling the reals (as opposed to arithmetic progression)? In this case, maybe it would be just as simple to perform a bracketting step, with *geometric* progression (as proposed by Christian), when one of the bounds of the support is not finite.

{quote}
only 0<p<1 needs to be handled
{quote}
I guess you mean by that that what should be done is
* return {{getSupportLowerBound()}} when {{p == 0}},
* return {{getSupportUpperBound()}} when {{p == 1}}.

This means that the contract of {{getSupportLowerBound()}}/{{getSupportUpperBound()}} *requires* that the returned values are actually the *best* bounds on the support. While I have no problem with this sound requirement, I don't think this is currently stated clearly in the Javadoc, but I'm no native english speaker...","09/Nov/11 21:40;cwinter;The Javadoc of {{getSupportLowerBound()}}/{{getSupportUpperBound()}} in fact needs to be sharpened. I didn't see this before. If I didn't overlook anything, all current implementations already return the best bounds.

Regarding the bracketing issue there are now several ideas (thanks for the Chebyshev idea, Phil). But I don't have a preference for a particular solution which can be built from these ideas. Thus, Sébastien, just choose a strategy which you consider to be beneficial.","22/Nov/11 07:33;celestin;Hi,
please find attached a proposal for a new implementation of {{inverseCumulativeDistribution}}. For the time being, this new implementation is implemented as a static (utility) class, as I would like you to check the logics before committing it.
Highlights are
* use of Chebyshev's inequality (thanks Phil) to bracket the root
* in cases where the random variable has no mean or no variance, and is not compactly supported, the method falls back to geometric bracketing, instead of arithmetic bracketing.
* removal of calls to {{getDomainLowerBound(p)}}, {{getDomainUpperBound(p)}}, {{getInitialDomain(p)}}.

The attached class has a main() method which runs some performance checks (new impl vs old impl). The report on my machine (Intel Core i5 M540 @2.53GHz, 4Go RAM) tends to indicate that the new implementation is slightly quicker than the old one.

Commented out is also a plateau detection which proves fairly robust (although I'm sure we can fail it), but induces a slight decrease of the overall performances. I'm just wondering if we should set a flag in the constructor, to indicate whether or not plateaux should be detected ? Alternatively, we could remove plateau detection altogether, and clearly state in the javadoc that the default impl *fails* with plateaux.

Looking forward to reading your comments,
Sébastien","24/Nov/11 20:48;cwinter;Thanks for the patch, Sébastien.
There is no need for a plateaux detection flag in the constructor. While dealing with the distribution classes and interfaces, I saw the method isSupportConnected(). This method will tell whether plateaux might occur because a plateau in the cdf corresponds to a gap in the support.","25/Nov/11 21:43;psteitz;Wow, we can really improve the code when we put our heads together :)

Great observation by Christian - we can switch on plateau detection iff isSupportConnected is false.  Otherwise, I would say the ""new"" implementation is good to go.  I would also be happy to celebrate dropping all of the getXxx(p) methods, which are not needed any more.

We can all thank Mikkel, btw, for suggesting a while back that we add the numerical mean, variance and connected support properties.  Came in very handy here!
","26/Nov/11 05:18;celestin;{quote}
Great observation by Christian - we can switch on plateau detection iff isSupportConnected is false. Otherwise, I would say the ""new"" implementation is good to go.
{quote}
Yes, it is indeed a very neat way to proceed. I'll make the changes once Christian's patch for MATH-703 is committed.

{quote}
I would also be happy to celebrate dropping all of the getXxx(p) methods, which are not needed any more.
{quote}
Good! So I do not need to post on the mailing list on this issue?

{quote}
We can all thank Mikkel, btw, for suggesting a while back that we add the numerical mean, variance and connected support properties. Came in very handy here!
{quote}
I would also thank the guy who suggested Chebyshev's inequality (I wonder who that is ;)). Overall, I personnally enjoyed working on this issue, as it truly was team work.
Sébastien","03/Dec/11 16:28;celestin;New implementation (+ unit tests) committed in {{r1209942}}. {{getDomainLowerBound(double)}}, {{getDomainUpperBound(double)}} and {{getInitialDomain(double p)}} have now become superfluous: removed in {{r1209963}}.

The same kind of approach might probably be adopted for {{IntegerDistribution}}. I suggest we look into it before we resolve this issue.","04/Dec/11 20:14;cwinter;Yes, we can get rid of {{IntegerDistribution.getDomainLowerBound(double)}} and {{IntegerDistribution.getDomainUpperBound(double)}}. I will do this in the reimplementation of {{IntegerDistribution.inverseCumulativeProbability(double)}} based on MATH-692.

Using Chebyshev's inequality also can be adopted. But a plateau detection step is not needed there because {{IntegerDistribution}} requires its own solver for finding the inverse cumulative probability anyway. Thus this solver can apply the correct inequalities for finding the ""right"" quantile automatically. (Using the ""correct"" inequalies for the solver in {{RealDistribution}} wasn't an option because an existing general purpose solver is utilized there.)","05/Dec/11 06:50;celestin;Thanks Christian for your help: this issue can therefore be considered as ""solved"".","06/Dec/11 02:59;celestin;I've realized that boundary cases ({{p == 0}} and {{p == 1}}) are now handled correctly: concrete instances of {{AbstractRealDistribution}} no longer need to override this method, which are removed in rev {{1210756}}.
",18/Mar/12 09:27;celestin;Fixed in 3.0.,,,,,,,,,,,,,,,,,,,,,
Incomplete reinitialization with some events handling,MATH-695,12528715,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,pparraud,pparraud,25/Oct/11 16:52,24/Mar/12 16:16,07/Apr/19 20:38,26/Oct/11 09:09,3.0,,,,,,,3.0,,,0,,,,,,,,"I get a bug with event handling: I track 2 events that occur in the same step, when the first one is accepted, it resets the state but the reinitialization is not complete and the second one becomes unable to find its way.
I can't give my context, which is rather large, but I tried a patch that works for me, unfortunately it breaks the unit tests.",,,,,,,,,,,,,,,,,,,,,25/Oct/11 16:57;pparraud;events.patch;https://issues.apache.org/jira/secure/attachment/12500719/events.patch,26/Oct/11 08:24;luc;test-case-MATH-695.patch;https://issues.apache.org/jira/secure/attachment/12500839/test-case-MATH-695.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-25 19:26:20.212,,,false,,,,,,,,,,,,,,214575,,,Wed Oct 26 09:09:54 UTC 2011,,,,,,0|i0aoun:,60299,,,,,,,,25/Oct/11 16:57;pparraud;this patch resolves my problem but breaks some unit tests.,"25/Oct/11 19:26;luc;As I work with Pascal (we share the same office at work), I have seen how the bug occurs and am trying to set up a simplified test case that reproduces it. It seems that there is a combination of conditions that is not handled properly. We have seen the bug occurring when both following conditions are true:

* there are several events occurring in the same integration step
* when one of the earliest events occurrence is triggered, it returns RESET_DERIVATIVES

In this case, the acceptStep method in AbstractIntegrator returns early from inside the while loop and the remaining events that where expected to occur later on are left in an inconsistent state with respect to the integrator. The t0 and g0 fields in the corresponding EventState instance still contain values from the beginning of the step, they do not reflect the fact the event has been triggered. This implies that when next step is started with the updated derivatives, evaluateStep tries to catch up from t0 to current t and calls the g function at times that do not belong to the current step.

Up to now, I have not been able to set up a simplified test case that exhibits this, but I'm working on it.",26/Oct/11 08:24;luc;The attached test case reproduces the error.,"26/Oct/11 09:09;luc;Fixed in subversion repository as of r1189086.

The fix is different from the proposed patch, it directly updates the events when the step truncation occurs, thus preventing even transient inconsistency.

Thanks for the report and for the patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cumulative probability and inverse cumulative probability inconsistencies,MATH-692,12527632,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,cwinter,cwinter,18/Oct/11 18:01,23/Jul/12 23:18,07/Apr/19 20:38,02/Feb/12 06:45,1.0,1.1,1.2,2.0,2.1,2.2,3.0,3.0,,,0,,,,,,,,"There are some inconsistencies in the documentation and implementation of functions regarding cumulative probabilities and inverse cumulative probabilities. More precisely, '<' and '<=' are not used in a consistent way.

Besides I would move the function inverseCumulativeProbability(double) to the interface Distribution. A true inverse of the distribution function does neither exist for Distribution nor for ContinuosDistribution. Thus we need to define the inverse in terms of quantiles anyway, and this can already be done for Distribution.

On the whole I would declare the (inverse) cumulative probability functions in the basic distribution interfaces as follows:

Distribution:
- cumulativeProbability(double x): returns P(X <= x)
- cumulativeProbability(double x0, double x1): returns P(x0 < X <= x1) [see also 1)]
- inverseCumulativeProbability(double p):
  returns the quantile function inf{x in R | P(X<=x) >= p} [see also 2), 3), and 4)]

1) An aternative definition could be P(x0 <= X <= x1). But this requires to put the function probability(double x) or another cumulative probability function into the interface Distribution in order be able to calculate P(x0 <= X <= x1) in AbstractDistribution.
2) This definition is stricter than the definition in ContinuousDistribution, because the definition there does not specify what to do if there are multiple x satisfying P(X<=x) = p.
3) A modification could be defined for p=0: Returning sup{x in R | P(X<=x) = 0} would yield the infimum of the distribution's support instead of a mandatory -infinity.
4) This affects issue MATH-540. I'd prefere the definition from above for the following reasons:
- This definition simplifies inverse transform sampling (as mentioned in the other issue).
- It is the standard textbook definition for the quantile function.
- For integer distributions it has the advantage that the result doesn't change when switching to ""x in Z"", i.e. the result is independent of considering the intergers as sole set or as part of the reals.

ContinuousDistribution:
nothing to be added regarding (inverse) cumulative probability functions

IntegerDistribution:
- cumulativeProbability(int x): returns P(X <= x)
- cumulativeProbability(int x0, int x1): returns P(x0 < X <= x1) [see also 1) above]",,,,,,,,,,,,,,,,,,,,,19/Dec/11 22:47;cwinter;MATH-692_integerDomain_patch1.patch;https://issues.apache.org/jira/secure/attachment/12507992/MATH-692_integerDomain_patch1.patch,08/Nov/11 20:28;cwinter;Math-692_realDomain_patch1.patch;https://issues.apache.org/jira/secure/attachment/12502956/Math-692_realDomain_patch1.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-10-20 01:13:06.429,,,false,,,,,,,,,,,,,,88883,,,Thu Feb 02 06:45:59 UTC 2012,,,,,,0|i0aovb:,60302,,,,,,,,"20/Oct/11 01:13;psteitz;Thanks for raising this issue, Christian - especially now as we finalize the 3.0 API.

I am +1 for these changes.  I agree that the inf-based definition of inverse cum is more standard and we are in a position now make the change, so I say lets do it.  I am also +1 on the move of this up to the distribution interface.  The reason we did not include it there originally was that we thought we might implement distributions for which we could not define inverses.  That has not happened in the last 8 years, so I think its safe enough to push it up.

The code, test, user guide and doc changes for this have to be done carefully.  Patches most welcome.

Is everyone else OK with this change?","20/Oct/11 06:09;celestin;I have neither used nor developed this part of CM, so my view on this is of but little value. Having said that, anything improving consistency can only be desirable, especially at this stage. So I'm all for it, and will be soon available (when I'm done on SYMMLQ) for an (novice on these issues) help.

Sébastien",20/Oct/11 06:26;mikl;+1,"22/Oct/11 17:10;cwinter;Thanks for the feedback to all. Sébastien, thanks for offering your help. If you like and find time for it, you could implement AbstractDistribution.inverseCumulativeProbability(double p).

I will provide some patches next week, but adjusting AbstractContinuousDistribution.inverseCumulativeProbability(double p) will take some more time.

After thinking a little more about the structure of the interfaces, I'd like to put the function probability(double x) to Distribution anyway (independently of the thought in point 1) above).

Are there any preferences on P(x0 <= X <= x1) or P(x0 < X <= x1) for cumulativeProbability(double x0, double x1)?","22/Oct/11 20:24;psteitz;I am not sure it is really makes sense to add probability(double x) to the Distribution interface.  It would have to be defined as density (referring to the distribution function) to make sense in the continuous case, since defined as p(X = x) it would in most cases be identically 0 for continuous distributions.

Regarding the cum definition, I am fine with P(x0 < X <= x1).","23/Oct/11 08:39;celestin;Happy to help on the inverse cumulative probability. You will have to be patient and forgieving with me, though, as I discover this part of CM.

As for the definition, I think that one of the bounds should be excluded, so that these cumulative probabilities can be summed
P(a < X <= c) = P(a < X <= b) + P(b < X <= c),
even in the case of discrete PDFs.

Whether the lower or upper bound should be excluded is another matter. I usually work with continuous pdfs, so I don't know if there is a common practice in the probability community. If there is none, I would tend to chose the following definition
P(x0 <= X < x1)
(sorry Phil!), because it would be consistent with the way things are usually indexed in java (a[0].. a[a.length-1]). See also {{org.apache.commons.math.util.MultidimensionalCounter}}. Although this type of consistency is not an absolute requirement, I think it is nice for the user to have such simple principle: ""lower bound always included, upper bound always excluded"". Appart from this small point, I really have no objection to any choice.","23/Oct/11 15:22;psteitz;Have a look at the default implementation of cum(x0,x1) now in AbstractDistribution.  I think the incorrectness in the documentation there may have been what triggered Christian to raise this issue.  The equation cum(a,b) = F(b) - F(a) where F is the distribution function is natural and what the impl there is trying to do.  In the discrete case, this equation fails, however, unless you define the cum to exclude the *lower* endpoint.  That's why P(x0 < X <= x1) is a better definition.","23/Oct/11 15:31;celestin;OK, Phil, it makes perfect sense.","23/Oct/11 19:57;cwinter;Good, the definition of cum(x0,x1) will be P(x0 < X <= x1). Phil, you are right: cum(x0,x1) in AbstractDistribution was a reason for raising this issue. Another reason was cum(int x0, int x1) in AbstractIntegerDistribution.

The idea behind probability(double x) is in fact to define it as p(X = x) and to return 0 for continuous distributions. This function would be useful for discrete distributions not inheriting from IntergerDistribution and for distributions being composed of discrete and continuous parts.",23/Oct/11 20:59;psteitz;I guess I am OK with pushing p(x) up.  See related post to follow in commons-dev. ,"27/Oct/11 07:07;celestin;Hi Christian,
I've started looking into this issue. As I said, you will have to be patient with me ;).
I can see there already is a default implementation of {{AbstractContinuousDistribution.inverseCumulativeProbability}}. So what exactly would you like me to do? Is this implementation fragile? Would you like me to improve robustness? Provide full testing?

I think there might be issues when the PDF falls down to zero in a range (in which case the cum exhibits a plateau). The returned value might differ from the mathematical definition you proposed. Is this what you want me to work on? Have you already identified other issues?

Best regards,
Sébastien","28/Oct/11 21:36;cwinter;Hi Sébastien,

the problem with the plateau is indeed one issue which needs to be solved.

Additionally, AbstractDistribution will need an implementation of inverseCumulativeProbability. In fact both implementations should be the same except for the solver to be used. Thus inverseCumulativeProbability should be implemented just once in AbstractDistribution, and invoking the solver should be put to a separate procedure so that it can be overridden in AbstractContinuousDistribution.

A third point is the choice of the solvers. For AbstractDistribution we need a solver which works even for discontinuous cdfs (BisectionSolver can do the job, but maybe the implementations of the faster IllinoisSolver, PegasusSolver, BrentSolver, or another solver can cope with discontinuities, too). For AbstractContinuousDistribution it would be beneficial to use a DifferentiableUnivariateRealSolver. However, the NewtonSolver cannot be used due to uncertainty of convergence and an alternative doesn't seem to exist by now. So we have to choose one of the other solvers for now.

As all these points are interdependent, I guess it's best to solve them as a whole. If you like, you can do this.

Best Regards,
Christian","28/Oct/11 22:02;cwinter;Another point for discussion:
I'd like to introduce
getDomainBracket(double p): returns double[]
to AbstractDistribution as helper function for inverseCumulativeProbability. This allows to avoid searching a bracket where a bracket can be specified directly.
The function getDomainBracket could be made abstract (which means to remove getInitialDomain, getDomainLowerBound, and getDomainUpperBound as these functions aren't needed any more), or it could have a default implementation (according to the corresponding part of the current implementation of inverseCumulativeProbability) which uses getInitialDomain, getDomainLowerBound, and getDomainUpperBound. However, getInitialDomain, getDomainLowerBound, and getDomainUpperBound should not be abstract in the latter case. Otherwise a derived class would be forced to implement something it potentially doesn't use. Thus the functions getInitialDomain, getDomainLowerBound, and getDomainUpperBound should have default implementations which either return default values (0, -infinity, +infinity) or throw an exception saying something like ""has to be implemented"".","29/Oct/11 04:23;celestin;Hi Christian,

{quote}
Hi Sébastien,

the problem with the plateau is indeed one issue which needs to be solved.
{quote}
I'm working on it...

{quote}
Additionally, AbstractDistribution will need an implementation of inverseCumulativeProbability. In fact both implementations should be the same except for the solver to be used. Thus inverseCumulativeProbability should be implemented just once in AbstractDistribution, and invoking the solver should be put to a separate procedure so that it can be overridden in AbstractContinuousDistribution.
{quote}
OK, for now, I'm concentrating on making the current impl in {{AbstractContinuousDistribution}} more robust. The other impl should be easier.

{quote}
A third point is the choice of the solvers. For AbstractDistribution we need a solver which works even for discontinuous cdfs (BisectionSolver can do the job, but maybe the implementations of the faster IllinoisSolver, PegasusSolver, BrentSolver, or another solver can cope with discontinuities, too). For AbstractContinuousDistribution it would be beneficial to use a DifferentiableUnivariateRealSolver. However, the NewtonSolver cannot be used due to uncertainty of convergence and an alternative doesn't seem to exist by now. So we have to choose one of the other solvers for now.
{quote}
The current implementation uses a Brent solver. I think the solver itself is only one side of the issue. The other point is the algorithm used to bracket the solution, in order to ensure that the result is consistent with the definition of the cumprob. As for the {{DifferentiableUnivariateRealSolver}}, I'm not too sure. I guess it depends on what is meant by ""continuous distribution"". For me, it means that the random variable takes values in a continuous set, and possibly its distribution is defined by a density. However, in my view, nothing prevents occurences of Dirac functions, in which case the cum sum is only piecewise C1. It's all a matter of definition, of course, and I'll ask the question on the forum to check whether or not people want to allow for such a situation.

{quote}
As all these points are interdependent, I guess it's best to solve them as a whole. If you like, you can do this.

Best Regards,
Christian
{quote}
Yes, I'm very interested.

Best regards,
Sébastien","05/Nov/11 08:29;celestin;Please note that MATH-699 has been created specifically to handle plateaux.

Sébastien","08/Nov/11 20:28;cwinter;Here is the first patch for this issue (unfortunately with some delay). It adjusts the distributions with real domain to the definitions in this issue, and it mainly changes documentations.

I could not move inverseCumulativeProbability(double) up to Distribution because there would be a conflict with IntegerDistribution.inverseCumulativeProbability(double): This method returns int. This problem will be removed by solving issue MATH-703.

The implementation of inverseCumulativeProbability(double) is not changed as Sébastien is working on this.

I will provide the patch for the integer distributions as soon as I have adjusted the test data to the new inequalities and reverified the adjusted test data.","09/Nov/11 07:22;celestin;All,
since I'm already working on this package, I'm happy to commit the patch on behalf of Christian. However, since I'm a relatively new committer, I would feel more confident if one of the ""old, wise committers"" could double check the svn log afterwards.

Best,
Sébastien","09/Nov/11 15:48;psteitz;Hey, that's how it always works :)  

I don't know about ""wise"" but I certainly qualify as ""old"" by any standard, so will have a look once you have reviewed and committed.

Thanks!","10/Nov/11 06:23;celestin;Patch {{Math-692_realDomain_patch1.patch}} (20111108) applied in rev 1200179, with minor modifications (mostly checkstyle fixes).
Thanks Christian!
","04/Dec/11 21:02;cwinter;As mentioned by Sébastien in MATH-699, the implementation of {{IntegerDistribution.inverseCumulativeProbability(double p)}} can benefit from the ideas which came up for {{RealDistribution.inverseCumulativeProbability(double p)}} in that thread.

Thus I will remove {{getDomainLowerBound(double p)}} and {{getDomainUpperBound(double p)}} from the integer distributions. I checked that all current implementations of the lower/upper bound methods provide the whole support of the distribution as starting bracket. This means that using {{getSupportLowerBound()}} and {{getSupportUpperBound()}} for the starting bracket won't degrade the performance of the current distribution implementations. However, a user might want the improve the performance of his distribution implementations by providing a more targeted starting bracket for probability {{p}}. Thus I will swap the solving step to a protected function {{solveInverseCumulativeProbability(double p, int lower, int upper)}}, so that it gets easy to override {{inverseCumulativeProbability}} with an implementation which finds a better starting bracket.

Furthermore, Phil's idea with Chebyshev's inequality can be applied to the generic implementation of {{inverseCumulativeProbability}} in order to get a better starting bracket.","05/Dec/11 06:51;celestin;Hi Christian,
If you agree with that, I suggest that you also take care of MATH-718, as the two issues seem to be very much connected.
Sébastien","15/Dec/11 23:44;cwinter;Hi Sébastien,

my changes in the integer distributions don't solve MATH-718. Instead I found a probably related problem with the Pascal distribution.

The integer distribution patch for this issue still isn't ready. I will provide it next week.

Christian","19/Dec/11 22:47;cwinter;This is the patch which adjusts the integer distributions to the agreements above.

The changes to the test cases for the random generators may be unexpected. But these changes initially were triggered by adjusting {{RandomDataTest.checkNextPoissonConsistency(double)}} to the new contract for integer distributions. Then some random generator tests failed due to chance. While adjusting their seeds, I found some other tests with a high failure probability. Thus I also set some failure probabilities to 0.01 in order to find suitable seeds more quickly.

My next task on this issue is to adjust the user guid.","20/Dec/11 20:27;celestin;Hi Christian,
thanks for this contribution. I am away for a few days, but am very happy to commit this patch as soon as I am back, if you are not in too much of a hurry.
Thanks again,
Sébastien","31/Dec/11 05:25;celestin;Well, we've recently run into some troubles with SVN, but it seems everything is working fine again. Patch {{MATH-692_integerDomain_patch1.patch}} (with minor checkstyle changes) committed in revision {{1226041}}.

Please do not forget to run {{mvn clean; mvn site:site}} and check the reports (in particular, {{checkstyle}}) prior to submitting a patch!

Thanks for this contribution.",31/Dec/11 08:39;celestin;The committed patch actually causes failure of {{Well1024Test}} in {{o.a.c.m.random}}.,"31/Dec/11 17:01;cwinter;Thanks for committing the patch, Sébastien. I see you already changed the seed in {{Well1024aTest}}. This hopefully removes the failure.

I'll have a look into Maven to prepare a better patch next time. :-)","31/Dec/11 17:11;celestin;{quote}
I see you already changed the seed in Well1024aTest.
{quote}

Yes I did, but is this really how we want {{Well2004aTest}} to pass?","02/Jan/12 17:51;cwinter;I guess there is no alternative to this way of making probabilistic test cases pass. However, I understand your bad feeling with this kind of failure fixing. The problem is that probabilistic tests are quiet fuzzy: Neither a passed test nor a failed test provides a clear answer whether something is right or wrong in the implementation. There is just a high chance to pass such a test with a correct implementation. The chance for failure increases with an erroneous implementation due to systematic deviations in the generated data. These chances tell whether it is easy to find a seed which passes the tests or not. Thus difficulties in finding a suitable seed are an indicator for problems in the code.","02/Jan/12 18:53;celestin;{quote}
Thus difficulties in finding a suitable seed are an indicator for problems in the code.
{quote}

That's exactly the point I've raised on the mailing-list: out of three seeds (100, 1000 and 1001), only one works. Of course, I would not dare to call that representative statistics, but I'm wondering whether or not we should be worried...","02/Feb/12 06:45;celestin;The issue about selection of an appropriate seed has been raised elsewhere. No definitive answer has been provided so far, so I suggest we consider this issue as solved for the time being.",,,,,,,,,,,,,,,
Statistics.setVarianceImpl makes getStandardDeviation produce NaN,MATH-691,12527363,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,psteitz,warrentang,warrentang,16/Oct/11 17:18,24/Mar/12 16:16,07/Apr/19 20:38,27/Nov/11 05:20,2.2,,,,,,,3.0,,,0,,,,,,,,"Invoking SummaryStatistics.setVarianceImpl(new Variance(true/false) makes getStandardDeviation produce NaN. The code to reproduce it:

{code:java}
int[] scores = {1, 2, 3, 4};
SummaryStatistics stats = new SummaryStatistics();
stats.setVarianceImpl(new Variance(false)); //use ""population variance""
for(int i : scores) {
  stats.addValue(i);
}
double sd = stats.getStandardDeviation();
System.out.println(sd);
{code}

A workaround suggested by Mikkel is:
{code:java}
  double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());
{code}","Windows 7 64-bit, java version 1.6.0_23",18000,18000,,0%,18000,18000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-10-16 17:29:43.969,,,false,,,,,,,,,,,,,,87511,,,Sun Nov 27 05:20:47 UTC 2011,,,,,,0|i0aovj:,60303,,,,,,,,"16/Oct/11 17:29;psteitz;Thanks for reporting this.  Another workaround is to work with the default impl:
{code}
SummaryStatistics stats = new SummaryStatistics();
Variance variance = (Variance) stats.getVarianceImpl();
variance.setBiasCorrected(false);
{code}
and then just use the stats instance directly. 

The problem in the SummaryStatistics code is in addValue:
{code}
// If mean, variance or geomean have been overridden,
// need to increment these
if (!(meanImpl instanceof Mean)) {
    meanImpl.increment(value);
}
if (!(varianceImpl instanceof Variance)) {
    varianceImpl.increment(value);
}
if (!(geoMeanImpl instanceof GeometricMean)) {
    geoMeanImpl.increment(value);
}
{code}

The default impls get incremented via their embedded moments, so the code above skips incrementing them.  If, however, they have been overridden by instances of the same class (as in this bug report), this causes a problem.
","16/Oct/11 19:32;psteitz;Warren reported that the second workaround above does not work.  He is correct.  That is the result of yet another problem in this class.
{code}
public double getVariance() {
    if (varianceImpl == variance) {
        return new Variance(secondMoment).getResult();
    } else {
        return varianceImpl.getResult();
    }
}
{code}

In the first case, varianceImpl is not used, so setting its properties has no effect.  The mean has a similar problem.  The root cause of all of these problems is the reuse of momemts (i.e., just incrementing the moments instead of both them and the Mean and Variance instances).  We could either toss this (slight loss in performance, but likely trivial) or expose or allow moments to be attached to Mean, Variance instances.

",27/Nov/11 05:20;psteitz;Fixed in r1206666.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integer overflow in OpenMapRealMatrix,MATH-679,12525510,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,cberner,cberner,03/Oct/11 05:36,24/Mar/12 16:16,07/Apr/19 20:38,10/Oct/11 19:57,2.2,,,,,,,3.0,,,0,,,,,,,,"computeKey() has an integer overflow. Since it is a sparse matrix, this is quite easily encountered long before heap space is exhausted. The attached code demonstrates the problem, which could potentially be a security vulnerability (for example, if one was to use this matrix to store access control information).

Workaround: never create an OpenMapRealMatrix with more cells than are addressable with an int.",,,,,,,,,,,,,,,,,,,,,03/Oct/11 05:37;cberner;Flaw.java;https://issues.apache.org/jira/secure/attachment/12497426/Flaw.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-10-10 19:57:12.881,,,false,,,,,,,,,,,,,,43524,,,Mon Oct 10 19:57:12 UTC 2011,,,,,,0|i0aoxb:,60311,,,,,,,,03/Oct/11 05:37;cberner;Move code to an attachment,"10/Oct/11 19:57;luc;Fixed in subversion repository as of r1181181.

Thanks for the report and the workaround.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NonLinear Optimizers seem to have a hard time hitting NIST standards,MATH-678,12525387,Bug,Resolved,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Incomplete,,gsteri1,gsteri1,01/Oct/11 01:59,16/Feb/15 23:18,07/Apr/19 20:38,16/Feb/15 23:18,3.0,,,,,,,,,,0,NIST,NonLinear,Optimization,,,,,"As per a discussion on the mailing list, I am opening this ticket. In applying the nonlinear optimizers in commons, I noticed what I believe to be instability in the techniques. Further investigation investigation (both of my tests) and the code in prod is warranted. 

I will be pushing a first set of tests which should illustrate what I am seeing. 

 ",Java ,,,,,,,,,MATH-763,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-07-17 15:00:13.495,,,false,,,,,,,,,,,,,,41752,,,Mon Feb 16 23:18:01 UTC 2015,,,,,,0|i0290n:,11062,,,,,,,,"17/Jul/12 15:00;erans;IMHO, this issue is too vague to be useful.
The unit tests referred to have been removed from the CM test suite (cf. MATH-763) because they were all failing (although such big problems were not revealed by the other unit tests). Nobody seems eager to determine whether there could be a problem in the test methodology (e.g. too stringent tolerances) or in some way clean them up so that the potential problems can be sorted out.

I would thus suggest to resolve this issue as ""Won't fix"".
","22/Sep/12 18:34;erans;Greg Sterijevski does not seem to be around this forum anymore.
",16/Feb/15 23:18;tn;Closing as the mentioned instabilities have never been documented.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FieldLUDecomposition.Solver is missing appropriate testing,MATH-673,12523516,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,celestin,celestin,celestin,19/Sep/11 06:22,18/Mar/12 09:27,07/Apr/19 20:38,20/Sep/11 06:14,3.0,,,,,,,3.0,,,0,,,,,,,,I could not find any unit test for this class.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,60136,,,Sun Mar 18 09:27:11 UTC 2012,,,,,,0|i0aoyf:,60316,,,,,,,,"20/Sep/11 06:14;celestin;Revision r1172988 proposes {{FieldLUSolverTest}}, a unit test based on what was done with {{RealMatrix}} (see {{LUSolverTest}}). The entries are {{Fraction}}. The tests did not reveal any bug.",18/Mar/12 09:27;celestin;Fixed in 3.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnivariateRealIntegrator throws ConvergenceException,MATH-669,12523194,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,celestin,celestin,15/Sep/11 14:14,18/Mar/12 09:32,07/Apr/19 20:38,15/Sep/11 14:28,3.0,,,,,,,3.0,,,0,,,,,,,,"{{ConvergenceException}} is a checked exception, which goes against the developer's guide. It occurs in the {{throws}} clause of some methods in package o.a.c.m.analysis.integration. It seems that these occurences are remnants from previous versions, where exceptions were probably checked. This exception is actually never thrown : it is safe to remove it from the {{throws}} clause.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,60559,,,Sun Mar 18 09:32:51 UTC 2012,,,,,,0|i0aoyv:,60318,,,,,,,,15/Sep/11 14:28;celestin;Done in rev1171111.,18/Mar/12 09:32;celestin;Fixed in 3.0.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Polygon difference function produces erroneous results with certain polygons,MATH-668,12522754,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,luc,jensenc,jensenc,12/Sep/11 15:18,04/Mar/13 18:58,07/Apr/19 20:38,20/Apr/12 19:19,3.0,,,,,,,3.0,,,0,difference,"math,","polygon,",,,,,"For some polygons, the difference function produces erroneous results.  This appears to happen when one polygon is completely encompassed in another, and the outer has multiple concave sections.",,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,14/Sep/11 15:14;jensenc;PolygonsSetCircleTest.java;https://issues.apache.org/jira/secure/attachment/12494455/PolygonsSetCircleTest.java,12/Sep/11 15:32;jensenc;PolygonsSetTest.java;https://issues.apache.org/jira/secure/attachment/12494047/PolygonsSetTest.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2012-01-24 11:49:28.411,,,false,,,,,,,,,,,,,,2184,,,Fri Apr 20 19:19:52 UTC 2012,,,,,,0|i0rtg7:,160415,,,,,,,,"12/Sep/11 15:32;jensenc;There are two test cases here.  The first ""testdifference1"" produces 3 polygons, two of which are correct (one outer polygon and one inner hole polygon).  The third polygon is a rectangular region in the lower left corner of the outer polygon and should not be there.

The second test case ""testdifference2"" is a simpler example.  Technically, the results are not incorrect, but they are strange.  This example is included because it seems interesting and may help understand the problem.  The inner resultant polygon has two extra points that are collinear with two edges on the polygon.  The extra points don't change the shape of the polygon, but they don't need to be there.","12/Sep/11 15:38;jensenc;Never mind on testdifference1.  I think I see the problem (bottom left corner of outer polygon is ambiguous).  I'll work on creating a better example.  Though, testdifference2 is still an interesting case.","14/Sep/11 15:14;jensenc;This circle test case takes the difference of two concentric circles.  The results produces is an empty list of vertices, when it should be the outer circle with a hole where the inner circle is.","24/Jan/12 11:49;tn;I have checked the attached test cases:

 - circle test: you remove the outer circle from the inner one, resulting in an empty polygon set, which is correct behaviour. If you switch the two circles, the correct result is returned: a PolygonsSet with two loops: the two circles itself.

 - testdifference2: the described behaviour is implementation specific due to the way the hyperplane is cut when subtracting the two regions.","26/Jan/12 19:21;luc;The fix would probably not imply API change, so it can be delayed to 3.1.","20/Apr/12 19:19;luc;I agree with Thomas analyses.

Concerning the difference2 case, the two points are explained by the vertical line at x = 5.0 which comes from the outer shape. The internal representation is a BSP tree and one of this part of the outer boundary creates an hyperplane that splits the inner triangle. When the boundary representation is rebuilt, the two segments are glued together and the points appear there. There is no post-processing that simplifies the representation afterwards.

Concerning the circle test, I guess you mixed the arrays. What is really in the code is that the vertices2 array is build first from outer circle and the vertices1 array is built afterwards from inner circle. So you are really subtracting a big disk from a smaller one. As Thomas explained, computing set2 minus set1 give the expected two boundaries. Another possible change is to build the circles clockwise instead of counter-clockwise, and in this case the two regions are infinite wich a whole at the center, then subtracting set2 from set1 returns a disk with a hole.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QRDecomposition does not have a singularity threshold,MATH-665,12522260,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,08/Sep/11 13:54,24/Mar/12 16:16,07/Apr/19 20:38,23/Jan/12 10:40,,,,,,,,3.0,,,0,,,,,,,,QRDecompositionImpl tests elements of rDiag for exact equality to 0 in checking singularity.  A singularity threshold should be defined for this class and used in the singularity test.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-09-08 14:24:07.306,,,false,,,,,,,,,,,,,,2187,,,Mon Jan 23 10:40:18 UTC 2012,,,,,,0|i0aoz3:,60319,,,,,,,,08/Sep/11 14:24;celestin;I've looked into NR and [SLATEC|http://www.netlib.org/slatec/lin/dqrsl.f] (see above line 150). In both cases a similar test for strict equality is carried out. This is strange indeed; what would be a reasonable threshold?,08/Sep/11 19:29;luc;I guess MathUtils.SAFE_MIN would be an appropriate candidate. It was used in some LAPACK routines (for SVD if I remember correctly).,"23/Jan/12 10:40;erans;A threshold setting has been provided (defaulting to ""zero"" so as to retain the previous behaviour) as part of MATH-664 (r1230509).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Division by zero,MATH-657,12521240,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Won't Fix,erans,erans,erans,02/Sep/11 22:38,16/Feb/18 15:50,07/Apr/19 20:38,02/Oct/11 21:14,,,,,,,,3.0,,,0,,,,,,,,"In class {{Complex}}, division by zero always returns NaN. I think that it should return NaN only when the numerator is also {{ZERO}}, otherwise the result should be {{INF}}. See [here|http://en.wikipedia.org/wiki/Riemann_sphere#Arithmetic_operations].
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-09-03 04:06:02.749,,,false,,,,,,,,,,,,,,2189,,,Sat Mar 24 16:22:37 UTC 2012,,,,,,0|i0rth3:,160419,,,,,,,,"02/Sep/11 23:08;erans;{{divide}} methods fixed in revision 1164756.

However, the change leads to a bug in {{atan}}: See test method {{testAtanI()}}, at line 575 in ""ComplexTest.java"".
","03/Sep/11 04:06;psteitz;This is probably best discussed on the mailing list before making changes to contracts in the code.

There are two things going on here.  First, this - possibly - represents another case where the computational formula documented in the code returns NaN and it an argument could be made that another value would be better. The tradeoff is complexity in documentation and overhead in computation.  I would like to see a real practical use case justifying adding this overhead and added complexity in both the code and javadoc.

The second question is more interesting.  Again, decision should be based on practical use cases.  That question is, do we view our complex class as representing the compactified space, including a designated single point at infinity.  One could argue that the answer is yes already, because we have ""INF"" defined.  But to really do this, we need to identify all of the other infinite points (i.e., change equals) and also modify arithmetic operations uniformly.  I am not sure we really want to do that - first because of performance impacts and second because some users may in fact want to preserve directed infinities.  I would like to hear from users and see actual use cases justifying the changes before we walk down this path.
","03/Sep/11 09:30;erans;I've just posted a mail on ""dev"".

IMO, the main argument is consistency. Also with how reals (i.e. {{double}}) work; IIUC, MATH-164 triggered a change for that same reason.

Arne Plöse is a user and [reported|MATH-620] that the previous behaviour was not fine for him.

I don't think that this one change can have a discernible performance impact.
It might not be necessary to map all {{Complex}} instances that have an infinite component to a single object. I pointed it as a convenient justification for fixing a bug (and for not fixing the other two points reported by Arne in MATH-620).
","02/Oct/11 21:14;erans;An alternative solution has been proposed: MATH-667.
",24/Mar/12 16:22;luc;changing status to closed as 3.0 has been released,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ValueServer not deterministic for a fixed random number seed,MATH-654,12520774,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,d.james,d.james,30/Aug/11 17:23,24/Mar/12 16:16,07/Apr/19 20:38,01/Sep/11 00:14,1.1,1.2,2.0,2.1,2.2,,,3.0,,,0,,,,,,,,"I have built an agent-based model using the Apache Commons Math library, which has come in handy.

The ValueServer seemed particularly helpful, as explained at:
http://commons.apache.org/math/userguide/random.html

My simulation needs repeatable randomness, so I used this form of the ValueServer constructor:

    ValueServer(RandomData randomData) 
    Construct a ValueServer instance using a RandomData as its source of random data.
    // http://commons.apache.org/math/api-2.2/org/apache/commons/math/random/ValueServer.html

However, in my simulation, I found that the ValueServer did not act deterministically if I supplied the same random number seed.

I have not inspected the source code, but I suspect that the ValueServer is not using the `randomData` generator correctly. If it was, then it should be deterministic.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-08-30 19:00:38.698,,,false,,,,,,,,,,,,,,62977,,,Thu Sep 01 00:14:02 UTC 2011,,,,,,0|i0ap0f:,60325,,,,,,,,"30/Aug/11 19:00;psteitz;Thanks for reporting this.  I assume you are using DIGEST_MODE.  If this is the case and you are comfortable compiling the code in trunk, the fix for MATH-634 enables a workaround for this.  Using the reseed method added to EmpiricalDistributionImpl in trunk, you can use ValueServer's getEmpiricalDistribution to get the distribution and then invoke reseed.  Unfortunately, this method does not exist in any released version yet.

The problem is that ValueServer#getNextDigest (what it does for getNext in DIGEST_MODE) delegates to EmpiricalDistributionImpl#getNextValue.  EmpiricalDistributionImpl has its own RandomData instance.  To fix this issue, EmpiricalDistirbutionImpl should add a constructor taking a RandomData and ValueServer should provide this.",01/Sep/11 00:14;psteitz;Fixed in r1163875. ValueServer now exposes a reSeed method that when supplied a fixed seed will generate a fixed sequence in any stochastic mode. The RandomDataImpl that it uses internally is passed to the EmpiricalDistributionImpl it creates when used in DIGEST_MODE.  The changes for this issue include an incompatible (vs. 2.x) change: the constructor for EmpiricalDistributionImpl that previously took a RandomData now takes a RandomDataImpl.  The plan for 3.0 is to merge these.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tridiagonal QR decomposition has a faulty test for zero... ,MATH-652,12520042,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,gsteri1,gsteri1,25/Aug/11 02:17,24/Mar/12 16:17,07/Apr/19 20:38,22/Jan/12 11:20,3.1,,,,,,,3.0,,,0,TriDiagonalTransformer,,,,,,,"In the method getQT() of TriDiagonalTransformer we have:

    public RealMatrix getQT() {
        if (cachedQt == null) {
            final int m = householderVectors.length;
            cachedQt = MatrixUtils.createRealMatrix(m, m);

            // build up first part of the matrix by applying Householder transforms
            for (int k = m - 1; k >= 1; --k) {
                final double[] hK = householderVectors[k - 1];
                cachedQt.setEntry(k, k, 1);
                final double inv = 1.0 / (secondary[k - 1] * hK[k]);
                if (hK[k] != 0.0) {
                    double beta = 1.0 / secondary[k - 1];

The faulty line is : final double inv = 1.0 / (secondary[k - 1] * hK[k]);
It should be put after the test for the zero, eg:

    public RealMatrix getQT() {
        if (cachedQt == null) {
            final int m = householderVectors.length;
            cachedQt = MatrixUtils.createRealMatrix(m, m);

            // build up first part of the matrix by applying Householder transforms
            for (int k = m - 1; k >= 1; --k) {
                final double[] hK = householderVectors[k - 1];
                cachedQt.setEntry(k, k, 1);
                if (hK[k] != 0.0) {
                    final double inv = 1.0 / (secondary[k - 1] * hK[k]);
                    double beta = 1.0 / secondary[k - 1];


",JAVA,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,25/Aug/11 02:17;gsteri1;tridiagonal;https://issues.apache.org/jira/secure/attachment/12491580/tridiagonal,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-22 11:19:51.246,,,false,,,,,,,,,,,,,,2192,,,Sun Jan 22 11:19:51 UTC 2012,,,,,,0|i0ap0v:,60327,,,,,,,,22/Jan/12 11:19;tn;Patched in r1234486.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
eigendecompimpl allocates space for array imagEigenvalues when it is not needed,MATH-651,12520041,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Won't Fix,tn,gsteri1,gsteri1,25/Aug/11 01:59,04/Mar/13 18:57,07/Apr/19 20:38,23/Jul/12 20:21,3.1,,,,,,,,,,0,EIGENDECOMPOSITIONIMPL,,,,,,,The class variable imagEigenvalues is allocated even there is no use for it. I propose leaving the reference null. Patch will follow. ,JAVA,,,,,,,,,,,,,,,,,,,,25/Aug/11 02:08;gsteri1;eigendecompimpl;https://issues.apache.org/jira/secure/attachment/12491579/eigendecompimpl,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2012-01-22 11:34:05.479,,,false,,,,,,,,,,,,,,2193,,,Mon Jul 23 20:21:42 UTC 2012,,,,,,0|i0rthj:,160421,,,,,,,,25/Aug/11 02:08;gsteri1;The patch with proposed changes... ,"22/Jan/12 11:34;tn;The current version of EigenDecomposition only works for symmetric matrices which always results in real eigenvalues.

For non-symmetric matrices, there can also be complex eigenvalues, which is currently developed for issue MATH-235.

It is to be discussed how one can identify the result as real or complex:

 - test for null vector
 - test for zero vector
","18/Jul/12 21:01;tn;pushing to 4.0 as it would change the current behavior. As of version 3.1, also general asymmetric matrices are supported, thus there may be complex eigen values.

So the variable imagEigenvalues has some use now: it is used to distinguish between real and complex eigenvalues. In case of real eigenvalues, all entries are 0.","18/Jul/12 22:42;erans;bq. pushing to 4.0 [...]

Can't we resolve the issue instead?
The ""lost"" memory will be quite small compared to the other data stored.
","19/Jul/12 06:26;tn;That's right indeed. The reason I'd like to keep it open is to discuss how we want to distinguish the case of real / complex eigenvalues. Right now, a user has to make a zero check on the imagEigenvalues array to see if the result is maybe complex.

We could also solve it by adding a new method like ""hasComplexEigenvalues"" that does the trick.","21/Jul/12 01:27;erans;That might a useful addition. Maybe ask for confirmation on the ML.

We can nevertheless resolve this issue, since it was about allocation of an unused array that is now used (IIUC).
","23/Jul/12 20:21;tn;With the addition of support for general asymmetric matrices in EigenDecomposition, the field imagEigenvalues is actually used, thus the issue is obsolete.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleRegression has extraneous constructor,MATH-648,12519509,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,psteitz,psteitz,20/Aug/11 21:27,24/Mar/12 16:16,07/Apr/19 20:38,20/Aug/11 21:46,2.2,,,,,,,3.0,,,0,,,,,,,,"The SimpleRegression(int) constructor added in version 2.2 has no effect on any statistics computed by the class.  The private TDistributionImpl data member that this constructor initializes is no longer meaningful, as a new distribution instance is created each time it is needed.  The T distribution implementation used in computations is no longer meaningfully pluggable, so the instance field should be removed, along with this constructor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,64317,,,Sat Aug 20 21:46:40 UTC 2011,,,,,,0|i0ap1j:,60330,,,,,,,,20/Aug/11 21:46;psteitz;Fixed in r1159918.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MATH-449,MATH-647,12519125,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,,meyerjp,meyerjp,17/Aug/11 14:55,17/Aug/11 14:59,07/Apr/19 20:38,17/Aug/11 14:59,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,64799,,,Wed Aug 17 14:59:47 UTC 2011,,,,,,0|i0rthr:,160422,,,,,,,,17/Aug/11 14:59;meyerjp;Error. This issue should not have been created,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MathRuntimeException with simple ebeMultiply on OpenMapRealVector,MATH-645,12518787,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,celestin,celestin,13/Aug/11 16:18,24/Mar/12 16:16,07/Apr/19 20:38,02/Sep/11 20:54,3.0,,,,,,,3.0,,,0,linear,sparse,vector,,,,,"The following piece of code
{code:java}
import org.apache.commons.math.linear.OpenMapRealVector;
import org.apache.commons.math.linear.RealVector;

public class DemoBugOpenMapRealVector {
    public static void main(String[] args) {
        final RealVector u = new OpenMapRealVector(3, 1E-6);
        u.setEntry(0, 1.);
        u.setEntry(1, 0.);
        u.setEntry(2, 2.);
        final RealVector v = new OpenMapRealVector(3, 1E-6);
        v.setEntry(0, 0.);
        v.setEntry(1, 3.);
        v.setEntry(2, 0.);
        System.out.println(u);
        System.out.println(v);
        System.out.println(u.ebeMultiply(v));
    }
}
{code}
raises an exception
{noformat}
org.apache.commons.math.linear.OpenMapRealVector@7170a9b6
Exception in thread ""main"" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating
	at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373)
	at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564)
	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372)
	at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1)
	at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-08-13 18:12:31.058,,,false,,,,,,,,,,,,,,62820,,,Sat Aug 13 18:12:31 UTC 2011,,,,,,0|i0ap1r:,60331,,,,,,,,"13/Aug/11 18:12;erans;Probably fixed in revision 1157403.
Added a unit test for ""ebeMultiply"" and ""ebeDivide"".
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"for the class of hyper-geometric distribution, for some number the method ""upperCumulativeProbability"" return a probability greater than 1 which is impossible.  ",MATH-644,12518772,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,msayati,msayati,13/Aug/11 06:26,04/Mar/13 18:53,07/Apr/19 20:38,31/May/12 23:45,2.2,,,,,,,3.1,,,0,hypergeometric,probability,,,,,,"In windows 7, I used common.Math library. I used class ""HypergeometricDistributionImpl"" and method ""upperCumulativeProbability"" of zero for distribution and the return value is larget than 1. the following code is working error. 

HypergeometricDistributionImpl u = new HypergeometricDistributionImpl(14761461, 1035 ,1841 );
System.out.println(u.upperCumulativeProbability(0))

Thanks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-02-23 21:43:04.371,,,false,,,,,,,,,,,,,,2196,,,Thu May 31 23:45:25 UTC 2012,,,,,,0|i0rti7:,160424,,,,,,,,23/Feb/12 21:43;tn;Updated fix version as there will be no 2.2.1,"31/May/12 23:45;tn;Finally, I found the problem for the described behavior:

In the upperCumulativeProbability function there was a sanity check like this:

{noformat}
        if (x < domain[0]) {
            ret = 1.0;
        } else if (x > domain[1]) {
            ret = 0.0;
{noformat}

In fact to be correct, it has to be like this

{noformat}
        if (x <= domain[0]) {
            ret = 1.0;
        } else if (x > domain[1]) {
            ret = 0.0;
{noformat}

which is also symmetric to the case of the cumulativeProbability function.
It means that for values of x that are at the lower bound, the probability must be 1.0 as the upperCumulativeProbability is defined as P(X >= x).

Additionally, the duplicate probability mass functions have been cleaned up. After looking through the version history it became clear that initially there existed two methods, whereas the public one called only the private one (which contained the actual computation). Later on the public one got improved, whereas the private one was still called by the cumulativeProbability methods.

This has been fixed in the sense that only the public method (which also behaves better for large values of N, m, k) is used in the class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractRandomGenerator nextInt() and nextLong() default implementations generate only positive values,MATH-640,12517683,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,psteitz,psteitz,02/Aug/11 19:06,24/Mar/12 16:16,07/Apr/19 20:38,03/Aug/11 04:17,1.1,1.2,2.0,2.1,2.2,,,3.0,,,0,,,,,,,,The javadoc for these methods (and what is specified in the RandomGenerator interface) says that all int / long values should be in the range of these methods.  The default implementations provided in this class do not generate negative values.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,66450,,,Wed Aug 03 04:17:43 UTC 2011,,,,,,0|i0ap27:,60333,,,,,,,,03/Aug/11 04:17;psteitz;Fixed in r1153338,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
numerical problems in rotation creation,MATH-639,12515920,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,01/Aug/11 14:12,24/Mar/12 16:16,07/Apr/19 20:38,05/Aug/11 15:06,2.2,,,,,,,3.0,,,0,,,,,,,,"building a rotation from the following vector pairs leads to NaN:
u1 = -4921140.837095533, -2.1512094250440013E7, -890093.279426377
u2 = -2.7238580938724895E9, -2.169664921341876E9, 6.749688708885301E10
v1 = 1, 0, 0
v2 = 0, 0, 1

The constructor first changes the (v1, v2) pair into (v1', v2') ensuring the following scalar products hold:
 <v1'|v1'> == <u1|u1>
 <v2'|v2'> == <u2|u2>
 <u1 |u2>  == <v1'|v2'>

Once the (v1', v2') pair has been computed, we compute the cross product:
  k = (v1' - u1)^(v2' - u2)

and the scalar product:
  c = <k | (u1^u2)>

By construction, c is positive or null and the quaternion axis we want to build is q = k/[2*sqrt(c)].
c should be null only if some of the vectors are aligned, and this is dealt with later in the algorithm.

However, there are numerical problems with the vector above with the way these computations are done, as shown
by the following comparisons, showing the result we get from our Java code and the result we get from manual
computation with the same formulas but with enhanced precision:

commons math:   k = 38514476.5,            -84.,                           -1168590144
high precision: k = 38514410.36093388...,  -0.374075245201180409222711..., -1168590152.10599715208...

and it becomes worse when computing c because the vectors are almost orthogonal to each other, hence inducing additional cancellations. We get:
commons math    c = -1.2397173627587605E20
high precision: c =  558382746168463196.7079627...

We have lost ALL significant digits in cancellations, and even the sign is wrong!
",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,65996,,,Fri Aug 05 15:06:09 UTC 2011,,,,,,0|i0ap2f:,60334,,,,,,,,"01/Aug/11 14:27;luc;The expected result quaternion computed to high precision manually and checked afterwards is:
q0 =  0.62283703596082005783621150
q1 =  0.02577076214564987845778149
q2 = -0.00000000025030122695149900
q3 = -0.78192703908611094998656730","05/Aug/11 14:17;luc;The fact that c = <k | (u1^u2)> is really wrong is due to the fact the rotation axis is almost exactly in the (u1, u2) plane. In fact it is only 1.833e-8 degrees (i.e. 3.199e-10 radians) out of the plane!",05/Aug/11 15:06;luc;Fixed in subversion repository as of r1154257.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NaN: Method ""equals"" in Complex not consistent with ""=="" for ""double"" primitive type",MATH-632,12515203,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Not A Problem,,erans,erans,24/Jul/11 21:36,24/Mar/12 16:22,07/Apr/19 20:38,05/Aug/11 22:08,,,,,,,,3.0,,,0,,,,,,,,"The following tests show several contradictions:
{code}
final double a = Double.NaN;
final double b = Double.NaN;
Assert.assertFalse(""a == b"", a == b); // (1)
Assert.assertEquals(""a != b"", a, b, Double.MIN_VALUE); // (2)
Assert.assertFalse(""a == b"", MathUtils.equals(a, b, Double.MIN_VALUE)); // (3)
Assert.assertFalse(""a == b"", MathUtils.equals(a, b, Double.MIN_VALUE)); // (4)
final Double dA = Double.valueOf(a);
final Double dB = Double.valueOf(b);
Assert.assertFalse(""dA == dB"", dA.doubleValue() == dB.doubleValue()); // (5)
Assert.assertTrue(""!dA.equals(dB)"", dA.equals(dB)); // (6)
final Complex cA = new Complex(a, 0);
final Complex cB = new Complex(b, 0);
Assert.assertTrue(""!cA.equals(cB)"", cA.equals(cB));  // (7)
{code}

They all pass; thus:
# ""Double"" does not behave as ""double"": (1) and (5) vs (6)
# Two NaNs are almost equal for Junit: (2)
# Two NaNs are never equal for MathUtils: (3) and (4)
# Complex.NaN is consistent with Object ""Double.valueOf(NaN)"" (hence not with primitive ""Double.NaN""): (7)

This is quite confusing.

In MathUtils, we chose to follow IEEE754 (and Java for primitive ""double""), i.e. it is ""correct"" that assertion (1) is false. Do we want ""Complex"" to conform with this or with the inconsistent behaviour of ""Double""?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-07-24 22:11:25.735,,,false,,,,,,,,,,,,,,65967,,,Sat Mar 24 16:22:53 UTC 2012,,,,,,0|i0rtin:,160426,,,,,,,,"24/Jul/11 21:44;erans;Tests committed in revision 1150496.
","24/Jul/11 22:11;psteitz;Can anyone present a practical argument for changing the current documented behavior of Complex.equals?  There is no perfect solution here, given the way equals is defined for doubles in Java.  The current behavior is simple, well-documented and has been defined this way since version 1.0.  Changing it may break some code that depends on it, so we need to have good practical reasons to change. 

From my perspective, the current implementation of Complex equals, which is consistent with what we also do for ArrayRealVectors, is natural and convenient.  I don't see the examples as particularly relevant, since a Complex instance is not a pair of doubles, but an object that has two double-valued attributes. Once a Complex number has a NaN part, it is for all practical purposes NaN, so it makes sense to lump all instances with NaN parts into one equivalence class modulo equals.  IIRC, this was the rationale used to define the current implementation of Complex equals.   ","25/Jul/11 08:04;luc;I have tried to find again some external references from C++ standard and C99 standard. They seem to only specify behavior of == as a logical (a.real == b.real) && (a.imaginary == b.imaginary), which would lead to numbers with NaN never been equal to anything, including themselves.

We don't use complex internally in [math] yet (at least I am not aware of it). However, some existing users do (at least Arne seems to, as he asked for several changes). Perhaps we should ask on the users list (not dev) for users comments on this, as most users will not be aware of this Jira issue here.

We may need to use complex in [math] by ourselves for some algorithms we want to implement. I see at least to difference use cases: eigen value decomposition for non-symmetric matrices and root solvers. Do they use some standard definition ?","25/Jul/11 20:28;erans;The problem starts here:
{noformat}
 (x == x) is true, except when (x == NaN)
{noformat}

Some people advocate that this should not be so, i.e.
{noformat}
 (x == x) is true, always
{noformat}
but IEEE574 has chosen the former. And Java primitive ""double"" conforms to this.
I don't understand the rationale of having decided that ""Double"" should behave differently than ""double"", but this is beyond our reach anyway :). But, _if_ you'd have to define ""equals"" for ""Double"" wouldn't you check the equality of the ""double"" returned by ""doubleValue()""? In the example reported by Luc, this is done similarly and two complex NaNs won't be equal.

What bothers me, slightly [1], is that CM is not consistent with itself: A ""Complex"" instance is an abstraction/approximation of a complex number, in the same way that a ""double"" is an abstraction/approximation of a real number. So why would you consider that
{quote}
[...] it makes sense to lump all instances with NaN parts into one equivalence class modulo equals.
{quote}
and at the same time that CM should also follow IEEE754 for ""double"" (cf. ""MathUtils.equals""), which is the opposite of the previous statement?

[1] Personally, I never had to use NaN beyond being a signal that there was a bug in my code.
","25/Jul/11 20:44;psteitz;The difference between Double and double, and Complex vs double is that the things with capital letters are *objects*.  Jave *had* to define Double equals to make NaN equal to NaN because equals has to be an equivalence relation, which means it has to be reflexive.  Personally, I don't see any problem with Complex equals behaving differently from what we now have in MathUtils.equals, which is a specialized method not intended to represent an equivalence relation on objects.  If we did for some reason define our own ""real"" object, I would expect it to behave as Double does - making NaN equal to itself.  The only reasonable alternative to the current implementation of Complex equals (and ArrayRealVector equals, for that matter) is to isolate every NaN-infected instance into its own equivalence class.  That is less convenient when dealing with collections of results.  Note that, like Double, we will still in any case have to make equals reflexive, which will make it appear ""inconsistent"" with the (in my mind irrelevant) examples above.","25/Jul/11 21:24;erans;{quote}
[...] equals has to be an equivalence relation [...]
{quote}

""=="" _is_ an equivalence relation for real numbers. Nevertheless the IEEE574 deemed it important that it is broken when the floating point representation of a real number is NaN (which is not the representation of any real number).
IIUC correctly (and probably very partially), NaNs are supposedly useful in the implementations of algorithms (which is what CM does).

The difference between ""Object"" and primitive is not relevant for deciding which is more important:
* The purpose of NaNs and conformance with a standard for the representation of real numbers, or
* the equivalence relation of the representations of real numbers (""Double"", ""double"", ""Complex"").

To view it differently: ""double"" would/should not have existed in Java (as an OO language) if it had been possible to deal exclusively with objects without a significant loss of performance. CM uses (almost?) exclusively ""double"" and not ""Double""; so I think that it makes sense to ask whether or not ""Complex"" should be consistent with ""double"" (making the examples quite relevant).

I don't understand how collections interfere with this issue...
","26/Jul/11 20:13;psteitz;Complex is an object, so has to have an equals implementation that is reflexive, symmetric and transitive.  Do you have an alternative implementation in mind?  And an explanation of why the alternative is better?

My reference to collections comes from my own use in simulating complex dynamical systems.  Like floating point equals comparisons among doubles, I don't use Complex.equals much, and when I do it is either in test code or comparing results of divergent processes.  When processes diverge, it is convenient to be able to compare results wrt infinities and NaNs.  In that case, it is convenient to have equals return true for NaNs.","26/Jul/11 22:17;erans;The alternative implementation was reported by Luc from the C++ standard:
{code}
  (a.real == b.real) && (a.imaginary == b.imaginary)
{code}
It is better simply because it is consistent with whatever is defined by the constituents of the complex number.

As I said, I don't really care whether NaN == NaN or NaN != NaN. But, I repeat, I'm not knowledgeable enough in floating point intricacies to say that the standard is wrong or useless. Are you?

Maybe we shouldn't care about IEEE574, and decide to ban any algorithm that would rely on NaN != NaN. And in that case, to be consistent, I would also propose to change ""MathUtils.equals"" so that it treats two NaNs as equal (and do away with the ""equalsIncludingNaN"" methods).

Some time ago, we decided to conform with IEEE754; but if it is more convenient to not conform and if it doesn't hurt anyone...
","26/Jul/11 22:30;psteitz;The definition above is not reflexive, so can't be implemented as equals in Java.  We would have to add a reference equality check to make it a legitimate equals implementation, making every NaN-infected instance an equals singleton.  Unless someone can provide a practical reason why this is better in applications, we should not change it, since it will break any applications that depend on the current implementation.","27/Jul/11 00:28;psteitz;I forgot about a couple of things that we considered in relation to this in the past.  The first is the status of the relevant ""spec,"" which is Annex G of the C99x spec for the C language.  We decided not to try to strictly adhere to this spec for reasons documented in MATH-5: a) it is a C language spec b) it is not open or freely available (we could not even get blanket approval to quote from the spec in our javadoc) c) when we last checked it was still not normative and d) trying to adhere strictly to the spec for arithmetic operations would kill performance.  Search the commons-dev archives for discussion.

The second thing I forgot is that in this and some other decisions, we were influenced by Colt (the old com.imsl.math package), which took a practical approach.  The javadoc for equals in Colt's Complex (implemented the same way we do) points out that implementing equals this way makes Complex instances work correctly in hashtables.  This is another reason that we used the definition that we did.","27/Jul/11 09:27;sebb@apache.org;For future reference, we should mention these design decisions in the code comments.","27/Jul/11 09:43;erans;{quote}
The javadoc for equals in Colt's Complex (implemented the same way we do) points out that implementing equals this way makes Complex instances work correctly in hashtables.
{quote}

Actually they probably copied the similar comment from [here|http://download.oracle.com/javase/6/docs/api/java/lang/Double.html#equals(java.lang.Object)]. :)

I'm convinced that the ambiguity (Object/primitive) cannot be lifted, and I probably should not worry anymore that CM is not more consistent than Java itself...

As Sebb mentions, one should not search the archive to get an explanation for such seemingly contradictory behaviour. It suffices to say that we follow IEEE754 for ""MathUtils.equals"" and that we don't for _any_ ""Object"" even though it would represent a (tuplet of) real numbers, on a par with what Java does.
","05/Aug/11 22:08;erans;Clarifying note added in revision 1154392.
",24/Mar/12 16:22;luc;changing status to closed as 3.0 has been released,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""RegulaFalsiSolver"" failure",MATH-631,12515172,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,erans,erans,23/Jul/11 23:48,24/Mar/12 16:16,07/Apr/19 20:38,08/Sep/11 12:13,,,,,,,,3.0,,,0,,,,,,,,"The following unit test:
{code}
@Test
public void testBug() {
    final UnivariateRealFunction f = new UnivariateRealFunction() {
            @Override
            public double value(double x) {
                return Math.exp(x) - Math.pow(Math.PI, 3.0);
            }
        };

    UnivariateRealSolver solver = new RegulaFalsiSolver();
    double root = solver.solve(100, f, 1, 10);
}
{code}
fails with
{noformat}
illegal state: maximal count (100) exceeded: evaluations
{noformat}

Using ""PegasusSolver"", the answer is found after 17 evaluations.
",,,,,,,,,,,,,,,,,,,,,02/Sep/11 08:58;dhendriks;ticket631.patch;https://issues.apache.org/jira/secure/attachment/12492726/ticket631.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-08-07 13:03:23.313,,,false,,,,,,,,,,,,,,61924,,,Thu Sep 08 12:13:59 UTC 2011,,,,,,0|i0ap3z:,60341,,,,,,,,23/Jul/11 23:50;erans;Reported by Axel Kramer in MATH-599.,"06/Aug/11 22:44;erans;The problem was due to the fact that at some point, the update formula always gave the same value: Nothing was being updated and the loop went on until the number of evaluations was exhausted.

I've committed a tentative solution in revision 1154614.
However:
# I'm not sure that it doesn't have any adverse side-effects on the bracketing property.
# It is quite probably not a pristine ""regula falsi"" algorithm anymore.

Please review.

Anyways, for the function that triggered the problem (see ""testIssue631"" in ""RegulaFalsiSolverTest.java""), the (modified) {{RegulaFalsiSolver}} takes 3624 evaluations (versus 17 for {{PegasusSolver}}). We should probably add a word of warning in the class Javadoc.
","07/Aug/11 13:03;dhendriks;I just got back from a 3 week vacation, so I couldn't reply earlier.

The documentation for the RegulaFalsiSolver states: ""Unlike the Secant method, convergence is guaranteed by maintaining a bracketed solution."" While this is theoretically true, in this case it is not so, because (if I understand correctly) only a single bound is updated repeatedly, and the update is too small to matter (has no effect), due to the double representation.

The change you propose (which is difficult to see as you also change other things in the same commit) is to modify x0 and f0 if the new value of x and x1 are equal. As I see it, this changes the algorithm, and it is no longer the Regula Falsi method as known from literature. I'm therefore against this change.

The problem that is identified in this issue is very similar to the well-known problem of the Regula Falsi method: it converges very slowly for certain problems, due to one side being updated all the time, while the other one stays the same. The Illinois and Pegasus algorithms solve exactly this problem, and are well-documented in literature.

I therefore think it would be better if the RegulaFalsiSolver kept it's original implementation, and for this problem the Illinois or Pegasus method should then be used instead.

The other changes (if statements to switch with default, extracting bound switch statements, etc) can be kept, if you wish.

The suggestion to add a warning to the Secant and Regula Falsi solvers that this is a possible problem, and the solution (use Illinois or Pegasus), would indeed be a good idea. In general, adding a note that the Illinois and Pegasus algorithms perform better, would be a good idea regardless of this issue.

Once more, to be clear, I don't think this issue is a bug. It is a result of the limited convergence of the Regula Falsi method combined with the implications of limited double precision. The limited convergence of the algorithm is a property of the algorithm, and should in my opinion not be changed. I also don't think that trying to work around the limited double precision would be a good idea.","07/Aug/11 17:38;erans;> (which is difficult to see as you also change other things in the same commit)

Sorry, but I didn't hit the solution right away, i.e. before changing those two additional little things to make the code clearer (for me)...

The only actual change is that the {{REGULA_FALSI}} enum was not used (i.e. with the {{switch}} little change, the corresponding {{case}} would have been empty) whereas now it contains the update of x0 to avoid an infinite loop.

The other (cosmetic) change was to take these two statements
{code}
x1 = x;
f1 = fx;
{code}
out of the previous {{if}} and {{else}} blocks, as they were duplicated there (which made me wonder whether it was a bug that they were _not_ different).

You say
> [...] convergence is guaranteed [...]
> [...] it converges very slowly for certain problems, [...]
> [...] The limited convergence of the algorithm is a property of the algorithm, [...]

All the above imply that one expects that the algorithm _can_ find the solution.
However, in this implementation, it _can't_.
Therefore there is a bug, somewhere.

I agree that it is a limitation of double precision. But, IMHO, leaving the code as-is is not a good idea because because it leads to the impression that the ""Regula Falsi"" mathematical algorithm can fail to converge, which is not correct (IIUC).
Therefore, we could add a comment stating that the _implementation_ with limited precision can fail to converge but that would be akin to saying to users: ""Here is a code, but don't use it.""
Personally, I would prefer to say: ""Because of limited precision, the implementation can fail to converge. In those cases, we slightly modified the original algorithm in order to avoid failure.""
","07/Aug/11 19:06;luc;{quote}
All the above imply that one expects that the algorithm can find the solution.
However, in this implementation, it can't.
Therefore there is a bug, somewhere.
{quote}

Here, the bug is in the algorithm itself, not in the implementation.

{quote}
it leads to the impression that the ""Regula Falsi"" mathematical algorithm can fail to converge, which is not correct (IIUC).
{quote}

It is correct. Regula Falsi fails to converge, or rather it can take a too large number of iteration to converge. This is exactly this behavior that has lead to the construction of other algorithms like Illinois or Pegasus. These two algorithms try to detect the case when the same end of the interval is always updated, and the other end remains unchanged. Once they have detected this, they slightly change the value at one end to trick the linear evaluation into choosing a value that is very likely to have the required sign to update this other end. In fact, in many cases depending of the sign of the curvature near the root, as soon as one end is very close to the root the linear interpolation will always remain on the same side of the root and hence will update this end.

I agree with Dennis here, the change needed to ensure convergence is not tool long is to choose a better algorithm, such as Illinois, Pegasus ... or the nth order bracketing solver I recently added. Regula Falsi should remain the reference Regula Falsi, just as secant and Brent should remain the reference ones.
","07/Aug/11 20:28;erans;""fails to converge"" and ""large number of iteration to converge"" are completely different things.

The documentation says: ""convergence is guaranteed"". Is _that_ false?

Moreover, for the function reported in this issue, the problem is not that it takes a large number iterations, it is that the loop is _literally_ infinite because at some point, nothing changes anymore.

Stated otherwise: If implemented with larger/infinite precision, would it converge?
In the affirmative, then in my opinion it means that the plain ""Regula Falsi"" method cannot be implemented with double precision (or that its convergence properties are not as stated in the docs) or that there is a bug in the implementation.

In the former case, why keep something that will never be used (as we'll warn users that they should use ""Pegasus"" or ""Illinois"" but certainly not ""RegulaFalsi"")? IMHO, we could just state in the docs that ""RegulaFalsi"" was not implemented because it is demonstrably less efficient and sometimes fails to work.

A less radical alternative would be to keep the test I've inserted in the code (at line 186) and throw a {{MathIllegalStateException}} if it passes. The previous behaviour (infinite loop) is a bug in CM.
","07/Aug/11 21:00;luc;{quote}
The documentation says: ""convergence is guaranteed"". Is that false?
{quote}

It depends on what is called convergence.
If convergence is evaluated only as the best of the two endpoints (measured along y axis), yes convergence is guaranteed and in this case it is very slow. This is what appears in many analysis books.
If convergence is evaluated by ensuring the bracketing interval (measured along x axis) reduces to zero (i.e. both endpoints converge to the root), convergence is not guaranteed.

The first case is achieved with our implementation by using the function accuracy setting. The second case is achieved with our implementation by using relative accuracy and absolute accuracy settings, which both are computed along x axis.

I fear that there are several different references about convergence for this method (just as for Brent). So we already are able to implement both views.

Without any change to our implementation, we reach convergence for this example by setting the function accuracy to 7.4e-13 or above, and it is slow (about 3500 evaluations). The default setting for function accuracy is very low (1.0e-15) and in this case, given the variation rate of the function near the root, it is equivalent to completely ignore convergence on y on only check the convergence on the interval length along x. 
","07/Aug/11 21:41;psteitz;I think we should either stick with the standard implementation of Regula Falsi or drop the class altogether.  Different rootfinders are going to perform better / worse for different functions and parameter values and I don't think it is a good idea to try to modify our implementations of the algorithms to try to work around their shortcomings for problem instances for which they are not well-suited.  It is much better to stick with standard algorithms, document them, and leave it to users to choose among implementations.  

Regula Falsi is not a good general-purpose rootfinder, but it does perform well for some problems (or parts of problems) and the original submission was a working implementation, so I would say revert the changes and keep it.","07/Aug/11 21:42;erans;I understand what you say. But however you put it, there is a bug; if not in the implementation, then in the API. It is not expected behaviour that something which must be changed (function accuracy threshold) to ensure correct behaviour (avoid an undetected infinite loop) is not a mandatory parameter.
To debug this, I started by raising the absolute accuracy threshold (the first default parameter, thus the first obvious thing to do) to 1e-2 and was stunned that I couldn't get anything after 1000000 iterations!

Therefore I maintain that, at a minimum, we put a line that will detect the infinite loop and raise an exception identifying _that_ problem and not let the user wait for ""TooManyEvaluationsException"" to be raised, as that will induce the unaware (me) to just allow more evaluations and try again.

This solution does not corrupt the algorithm; it just adds protection.
","07/Aug/11 22:11;psteitz;I disagree with your statement about setting accuracy.  All of this is configurable and if not set, you get the (documented) defaults.  This is all documented.  If the documentation is not clear, then we can improve it.  A user who applies Regula Falsi to the problem instance being examined here will end up maxing iterations.  I see no problem with that and in fact I see it as *correct* behavior (given the documented execution context of the algorithm).  ","07/Aug/11 22:57;erans;How can it be correct to have an infinite loop?
The problem is not slow convergence, which you can overcome by allowing more iterations.
It is too low function value accuracy which you cannot overcome by allowing more iterations. Thus my point: We must raise the appropriate exception (the doc for which will state that it can happen if the function value accuracy is too low for the implementation to provide a result).
","07/Aug/11 23:06;erans;[My comment starting with ""I understand what you say."" was an answer to Luc. I hadn't read Phil's previous one which was posted while I was writing mine.]

I agree that it is better not to change the standard algorithm, as I indicated in my first comment.
The fix which I'm proposing is not an algorithm change, it is an implementation detail similar to the many hundreds checks performed in CM. Just it is not a precondition test. It adequately indicates that something went wrong and can help the user figure out what it was. It makes the implementation more robust.
","07/Aug/11 23:46;erans;The original implementation, for the ""problem instance being examined here"", would find the root with absolute accuracy lower than *10e-12* after 3560 evaluations (note: using the default value of *1e-6*).
In fact, the root was found, at the required accuracy, after around 2200 evaluations.

That does not sound like correct behavior.
The problem is that, ""x0"" never being updated, the convergence test always fails... until we reach the limitation of double precision, which entails an infinite loop.

In fact my fix should not be necessary, as things have gone awry before it would apply, but there is a bug to fix nonetheless.
","07/Aug/11 23:54;psteitz;Is there actually a possibility of an infinite loop in the code?  Looks to me like the max evaluations bound will stop the while loop, so there is no potential for an infinite loop.  Apologies if I am misreading the code and the loop can fail to terminate, in which case I agree this is a problem.  (As a side note, from a style perspective, I prefer to explicitly bound loops to avoid this kind of uncertainty.  The natural hard bound here is the evaluation count.)

Trying to detect when a sequence of iterates has gotten ""stuck"" and is destined to hit max iterations without converging is walking down a path that I think is unwise for us and users.  I see no reason not to stick with the standard impl here, which is nicely documented in the original submission.  Trying to workaround numerical problems in simple algorithms and change contracts to include these workarounds is asking for trouble - both for us and users.  In a simple case like this, it is much better to just stick with the documented algorithm, which should in this case (again unless I am missing something) end with max evaluations exceeded, which is the right exception to report. ","08/Aug/11 07:24;erans;I surely hope that your last post is not an answer to mine from 23:46.

I'll try to answer here in case it was in reply to my previous one (23:06).
Of course, the code will not run forever because of the ""maxeval"" bound.
But it will run for a time that depends on the value of ""maxeval"" *with no added benefit*! From a certain point, the loop is like
{code}
while (true) {
  // Do nothing useful, just count!
  ++count;
  if (count > maxeval) {
    throw new TooManyEvalutationsException(maxeval);
  }
}
{code}

{quote}
from a style perspective, I prefer to explicitly bound loops
{quote}

From an *OO* style perspective, the reuse of the ""Incrementor"" is better, and you don't have to rewrite the same ""test and throw exception if failed"" boiler plate code each time there is such a loop.

{quote}
Trying to detect when a sequence of iterates has gotten ""stuck"" and is destined to hit max iterations without converging is walking down a path that I think is unwise for us and users.
{quote}

Why?

{quote}
I see no reason not to stick with the standard impl here
{quote}

A busy idle loop is a compelling reason IMO.

{quote}
Trying to workaround numerical problems in simple algorithms and change contracts to include these workarounds is asking for trouble
{quote}

The trouble is there with the current implementation. I'm not criticizing the contribution but this issue shows that it should be made more robust.
Also, the documentation about ""convergence is guaranteed"" can lead to a false sense of security.
Moreover, is the ""regula falsi"" a mathematical algorithm (with a guaranteed converge property if computed with infinite precision) or a numerical one, which this issue proves that it cannot guarantee convergence? In the former case, CM's (numerical) implementation is not strictly ""regula falsi"" and there would be no such thing as respect for an original/standard implementation if we can make it more robust.

I've already indicated that the fix does *not* change the contract; it stops the busy idle loop as soon as it is detected and reports that it won't do any good to increase the number of iterations. That's _obviously_ more robust.

Now, if you were answering to my 23:46 post, I'd be glad to read an explanation of why the first paragraph describes expected behaviour.

","08/Aug/11 08:37;luc;I don't understand.

When it was created, the maxIteration threshold was exactly designed for this purpose: get out of infinite loops. It was later renamed maxEvaluation but the purpose is still the same: don't get stuck. The reason why we get stuck is irrelevant. This limit is simply a safety limit, not a tuning parameter that user are expected to raise once they hit it hoping they will converge later on. If they could raise it later, then they should set it to an appropriate value at once. Hitting it implies computation failed. Regula falsi just like any algorithm can fail if applied with the wrong parameters or to the wrong function (in fact, even with a good setting of function accuracy, it fails to converge if we require a bracket selection on the side that does not move).

Also detecting one bound is not updated is what Illinois and Pegasus are designed to do.

So I think we should completely get rid of regula falsi and only keep the better algorithms.","08/Aug/11 09:57;erans;{quote}
I think we should completely get rid of regula falsi and only keep the better algorithms.
{quote}

That was my first idea. And that would be the simplest one, the safest one, and the only viable one as I can't seem to state clearly enough that
* Problem 1: When the doc says ""guaranteed convergence"", the algorithm should provide the answer.
* Problem 2: When the (absolute) accuracy threshold is set to 1e-6, and the correct root *is* found (after 2200 iterations) within the requirements, it should be returned, instead running idle and finish with an exception

{quote}
The reason why we get stuck is irrelevant.
{quote}

But why? If we *can* be more precise on the cause of failure, why not do it?

{quote}
This limit is simply a safety limit, not a tuning parameter that user are expected to raise once they hit it hoping they will converge later on.
{quote}

In principle, some possible use would be to compare the efficiency of different methods where the main criterion would be a time limitation (assuming that the function evaluation time overwhelms the of the root solver algorithm time). Thus with the function that triggered this issue:
* If you set maxeval to ""3000"", then both ""Pegasus"" (17 evals) and (a fixed) ""RegulaFalsi"" (2200 evals) would fill the bill.
* If you set maxeval to ""1000"", then ""Pegasus"" will be the only winner.


Anyways:
+1 for removing it altogether, and include somewhere the reason for it not being implemented in CM.
","08/Aug/11 18:23;psteitz;I am OK removing Reg Falsi, but stand by comments above that it is a very bad idea to hack standard algorithms and agree with Luc that maxing iterations is the correct behavior in the case we have been discussing. It is kind of pathetic that the compromise is to drop the impl; but in this case I don't see it as a real loss, since I can't think of any examples where Reg Falsi would be preferable to one of the other solvers - other than for educational purposes.","08/Aug/11 19:36;erans;May I please know *why it is OK that a bit of code does loop counting and repeatedly computes the same thing*!

You insist that I'd be ""hacking"" whereas I've indicated 3 or 4 times that there is no hack: just a test that will exit the loop as soon as it detects that the algorithm has failed. Why is it not understandable that the busy loop could last for a long time? The function is potentially evaluated millions of times at the same point. What if the evaluation is costly? Imagine the computation running for days, only to discover that it could have been be stopped after a few seconds. Is that robust code and good advertising for a library? It is one thing to expect that there are unknown bugs in CM, but refusing to fix a known one is so obviously wrong...

And may I please know *why it is OK that an algorithm that finds the right result does not return it*.

I had been trying to provide alternatives to the removal, but I can't do much more if nobody answers the above two questions.
You just have to run the code and print ""x"" and ""x1"" to see what is going on!
","08/Aug/11 20:13;luc;{code}
May I please know *why it is OK that a bit of code does loop counting and repeatedly computes the same thing!*
{code}

We didn't say that. We said that regula falsi is a standard *bad* algorithm. We said that very smart people have enhanced it 40 years ago and the enhanced versions are known and already implemented in Commons Math. These algorithms are *not* blind loop counters and they insert smart target shifts that *prevent* the behavior we observe here. These algorithms not only detect the problem, they fix it! They allow convergence along x. They allow selection of the side of the root.

{code}
The function is potentially evaluated millions of times at the same point.
{code}

The maxEvaluations is already here to prevent this, and in fact now this max number is even mandatory in the solve method (you placed it if I remember correctly). So the function is called millions of time only if the users wishes so by setting the maxEvaluations to a number in the range of millions.

{code}
And may I please know *why it is OK that an algorithm that finds the right result does not return it.*
{code}

If the user asked for a convergence in x or for a convergence on y on the side that is stuck, then no, the algorithm did not find the right result. One of its bounds converged but the users asked for something else.

{code}
You just have to run the code and print ""x"" and ""x1"" to see what is going on!
{code}

We know exactly what is going on! We know the algorithm is stuck. We know why it is stuck. We know why it did not detect it is stuck. We know it will finally hit the safety maxEvaluation threshold that is just waiting for that. And we know that removing all these problems is done by using other algorithms which are already there.

Regula falsi is doomed. It is an algorithm used for educational purposes, or for comparison purposes, not something suited for production use. It is just like Euler for ODE (and by the way we did implement Euler for ODE and we don't recommend users to use it as we also did implement better algorithms that were also designed by smart mathematicians decades ago).","08/Aug/11 20:44;erans;{quote}
So the function is called millions of time only if the users wishes so by setting the maxEvaluations to 
a number in the range of millions.
{quote}

No, the user should not expect that any algorithm will go on a single iteration more than necessary.
This is a plain bug.

Why do you see that a test such as I proposed (exit the loop early) is wrong while CM (and any good program) is full of tests to ensure that you don't do useless computations?
This has nothing to do with ""regula falsi"", it is robustness in the face of limited precision.

However, if you insist that the bug (failing to detect that it is stuck) is really an integral part of the algorithm, then removing it is not a ""pathetic compromise"", it is the only right thing to do!

","08/Aug/11 21:00;psteitz;This is a pointless discussion.  Gilles, you obviously don't share the views that Luc and I have on implementing standard algorithms or even what the meaning of a numerical algorithm is. Some algorithms perform well for some classes of problems and not others.  There is an art to choosing the right algorithm for the problem instance at hand.  If we modify our implementations to try to work around shortcomings of the algorithms we implement, then we are not implementing the standard algorithms, and we need to document exactly what it is that we are implementing, because in this case we are actually making it harder for users to choose (because we are not longer advertising standard numerics).  This is what I meant when I said it is both harder for us (because we have to document the hacks and non-standard contracts) and users (because the standard numerical analysis theory that they may be using to choose among implementations will no longer apply).  It is, IMO, a ""pathetic compromise"" to drop the implementation because we can't agree on what it means to implement the algorithm. So be it. Lets drop it and resolve this issue as ""fixed.""","08/Aug/11 22:08;erans;{quote}
Gilles, you obviously don't share the views that Luc and I have on implementing standard algorithms 
{quote}

That's simply _not true_.
I was the one pointing out that standard algorithms should have precedence: Please recall that it was considered fine that ""Levenberg-Marquardt"" and ""Brent"" would be, unknowingly to the user, ""twisted"" to perform _non-standard_ convergences check.
In those cases, there was the risk that the result of the algorithm would not be the same as the reference implementation.

In this case, there is no such thing as deviating from standard numerics! It was just a matter of throwing the right exception. So: ""The algorithm fails? Let's tell it sooner rather than later.""

Very interesting question that you ask: ""what it means to implement the algorithm"". But please note that I asked it several posts ago[1], and an answer would have helped sort out this discussion. What is your definition?


[1] 08/Aug/11 07:24
","09/Aug/11 13:51;erans;Also:

Phil,

Could you please leave out dismissive qualifiers such as ""pointless"" and ""pathetic"" (and, elsewhere, ""silly"") and stick to more or less objective arguments?
That will certainly help keep the conversation tone to a courteous level.

Luc,

Thanks for stating in full details what you meant by ""convergence"" in this case. However, it is still a ""post-mortem"" description.
Do you really expect that the average user of the CM library (a.o. me and the original reporter of the issue) to be able to figure out that ""obvious"" explanation just by getting a ""TooManyEvalutationsException"", setting the along-x accuracy threshold to a ridiculously high value and still getting the same exception?
If just for educational purposes, don't you think that it is more instructive to get a specific hint that the algorithm is stuck, rather than hit the ultimate fail-safe barrier much much later, and then download the source code and sprinkle the code with ""println"" statements to do forensic analysis?

Phil,

I tried to handle this issue out of respect for a real user who reported an issue that would have looked suspicious to many CM users. [How many of them would be experts in numerical analysis?]
You do not do me a favour by removing this algorithm; I don't want it to be a _compromise_ (pathetic or not). If you prefer to keep it, I don't care anymore. But, in that case, _you_ should have answered to Axel Kramer to go and read some books on numerical analysis.
","10/Aug/11 07:27;psteitz;Gilles, I apologize for tone of comments.","12/Aug/11 13:29;dhendriks;The discussions for this issue have left me with a lack of overview, so I'll (try to) objectively summerize the discussions above:

The problems are:
 # Regula Falsi states it always converges, but the implementation doesn't.
 # The main loop may continue, even when it no longer makes any progress, and subsequently ends with a TooManyEvaluationsException exception.

The cause of both problems is:
 - The limited/finite precision of the Java double type.

Proposed solutions:
 # The patch from revision 1154614, which modifies the Regula Falsi algorithm.
   #- Consensus seems to be that this change, which modifies the algorithm, is undesireable. We should keep the original algorithm.
 # Detect that the algorithm no longer makes progress and throw an exception, instead of continuing the loop which no longer makes progress.
   #- This is just earlier detection of the algorithm getting stuck.
   #- We could throw the TooManyEvaluationsException exception, that continuing the loop would also get us.
     #-- The class only states ""Exception to be thrown when the maximal number of evaluations is exceeded."".
     #-- The exception message only states: ""illegal state: maximal count (100) exceeded: evaluations""
     #-- Both may benefit from more extended documentation/messages.
   #- We could also throw an other exception that more clearly states this issue (NoMoreProgressException, AlgorithmStuckException, ...?).
     #-- It could for instance mention that changing the function value accuracy may be a solution, or asking for a different kind of solution?
 # Add documentation to the Regula Falsi algorithm that it is not intended to be used for actual problems, but only to compare algorithms, for testing, educational purposes, etc.
 # Add documentation to the Regula Falsi algorithm that users should use Illinois or Pegasus instead, which should outperform the algorithm for most if not all problems.
 # Add documentation to the Regula Falsi algorithm that it theoretically converges, but the implementation may not, due to the limited/finite precision of Java's double type. This will result in an exception (or 2 if we also do solution number 2).
 # Remove the Regula Falsi algorithm, and document why it is not included/implemented.
   #- This seems to have been accepted as a last resort solution only.

Other notes:
 - The following problem was also indicated: a solution is found after a certain number of iterations, but the algorithm does not return the solution (it does not terminate)
   -- This should only happen if the user asked for a different solution. That is, there are several accuracy parameters, as well as an allowedSolution parameter.
   -- If the solution requested by the user is found, it should return the solution immediately, otherwise it is a bug.

New notes:
 - I think the Regula Falsi algorithm does not state a fixed convergence criteria: it is left to the user to decide on one.
   -- When I implemented the algorithm, I think I copied the convergence checks for Brent.
   -- I subsequently modified the convergence criteria when I added the allowedSolution parameter.

My personal opinions on the proposed solutions:
 - (1) Revert part of 1154614, so get the original algorithm back. The other changes of that commit, that don't change the actual algorith, can stay.
 - (2) If we keep the algorithm, earlier detection would be nice. Not sure which exception to throw in these cases.
   -- This would result in a single 'if' that detects that the new approximation is the same as the previous one, and we thus no longer make progress, in which case we throw the exception earlier, instead of later.
 - (3-5) If we keep the algorith, all 3 documentation extensions would be a good idea.
 - (6) If possible, keep the algorithm, and don't remove it.

New issue:
 - TooManyEvaluationsException currently seems to use LocalizedFormats.MAX_COUNT_EXCEEDED(""maximal count ({0}) exceeded""), but maybe should use LocalizedFormats.MAX_EVALUATIONS_EXCEEDED(""maximal number of evaluations ({0}) exceeded"") instead?
","12/Aug/11 16:51;erans;Thanks for the neat summary!

{quote}
* (1) Revert part of 1154614, so get the original algorithm back. The other changes of that commit, that don't change the actual algorith, can stay.
{quote}

Done in revision 1157185.

{quote}
* (2) If we keep the algorithm, earlier detection would be nice. Not sure which exception to throw in these cases.
** This would result in a single 'if' that detects that the new approximation is the same as the previous one, and we thus no longer make progress, in which case we throw the exception earlier, instead of later.
{quote}

+1 (my position in the ""07/Aug/11 20:28"" post)
As suggested there, the exception could be ""MathIllegalStateException"" but with a clear message stating that the algorithm is stuck. Or maybe a new subclass of it which we could call ""NumericalPrecisionException"" or even a general-purpose ""ImplementationException"".

{quote}
[...] all 3 documentation extensions would be a good idea.
{quote}

+1

About the ""new issue"", the message string:
{quote}
""illegal state: maximal count (100) exceeded: evaluations""
{quote}
contains everything:
# error type: illegal state
# failure description: maximal count (100) exceeded
# context: evaluations

I proposed to use this approach (combining message items with the ""addMessage"" method of ""ExceptionContext"") in order to reduce the number of messages in the ""LocalizedFormats"" enum. Too many of them are just slight variations on a same theme.
","12/Aug/11 23:12;dhendriks;bq. contains everything

I agree. I was just wondering why a message that seems to be exactly the same as the exception was not used, as it kind of looked like it was created just for this purpose...

bq. I proposed to use this approach (combining message items with the ""addMessage"" method of ""ExceptionContext"") in order to reduce the number of messages in the ""LocalizedFormats"" enum. Too many of them are just slight variations on a same theme.

Ah, so then the MAX_EVALUATIONS_EXCEEDED is just a remnant of the past that should be eliminated, by replacing it everywhere by the more general MAX_COUNT_EXCEEDED?","12/Aug/11 23:38;erans;Yes. In the file ""LocalizedFormats.java"", I've started to write
{noformat}
/* keep */
{noformat}
after each enum that is supposedly to be kept. All the others are still to be examined for redundancy with another one, or the possibility to create something close using the ""multi-item"" approach.
","02/Sep/11 08:58;dhendriks;The 'ticket631.patch' file is my attempt to resolve this issue with a solution (or maybe I should call it a compromise?) that is satisfactory for all people that participated in the discussions for this issue, without having to remove the Regula Falsi algorithm from Commons Math.

I changed the following:
 - Added early detection of no longer making progress ('getting stuck'), and documented it.
   -- I used ConvergenceException for this, as it seems to fit... Do we want a custom error message with it?
 - Extended RegulaFalsiSolver documentation to indicate:
   -- that the algorithm should not be used for actual problems.
   -- that Illinois and Pegasus are improved versions and should be prefered.
   -- that the implementation does not guarantee convergence, while the algorithm theoretically does.
 - Extended IllinoisSolver and PegasusSolver documentation to indicate that they don't suffer from the RegulaFalsiSolver's implementation/convergence issues.

Please comment on whether this patch is an acceptable solution/compromise, and if not, why it is not.","02/Sep/11 11:15;erans;Committed (with minor additional Javadoc fixes) in revision 1164474.

Leaving open until confirmation that {{ConvergenceException}} is the right one to use. I thought that we could make a difference between _theoretical_ and _implementation_ convergence failures. But it might not be worth introducing the distinction just for this one case, especially since it is quite clear clear now that the class should not be used.","08/Sep/11 12:13;erans;No objection raised; setting to ""Resolved"".",,,,,,,,,,,,,,
multiplication of infinity,MATH-620,12514033,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Won't Fix,,aploese,aploese,14/Jul/11 08:58,04/Mar/13 18:57,07/Apr/19 20:38,02/Oct/11 21:16,3.0,,,,,,,,,,0,,,,,,,,"Take the following testcase 
{code}
Assert.assertEquals(neginf, inf* neginf, Double.MIN_VALUE);  // <--Passes ordinary double
Assert.assertEquals(new Complex(neginf, 0), new Complex(inf, 0).multiply(new Complex(neginf, 0)));// <-- Fail only real parts no imaginary parts 
{code}

The outcome of multiply is Complex.INF if one part is infinity.
why not simply compute the multiplication and thats is?
",,,,,,,,,,,,,,,,,,,,,06/Sep/11 14:06;aploese;ComplexOctaveTest.java;https://issues.apache.org/jira/secure/attachment/12493151/ComplexOctaveTest.java,21/Sep/11 19:08;aploese;arne_tests.zip;https://issues.apache.org/jira/secure/attachment/12495425/arne_tests.zip,10/Sep/11 00:30;erans;arne_tests.zip;https://issues.apache.org/jira/secure/attachment/12493876/arne_tests.zip,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-08-10 06:42:14.014,,,false,,,,,,,,,,,,,,2207,,,Sun Oct 02 21:16:38 UTC 2011,,,,,,0|i0rtjb:,160429,,,,,,,,"14/Jul/11 09:08;aploese;This goes also wrong:
{code}
        Assert.assertEquals(new Complex(neginf, nan), new Complex(0,
inf).multiply(new Complex(0, inf)));// <-- Fail
{code}
the result is the output of octave - so I would expect the same.


Am Donnerstag, den 14.07.2011, 08:58 +0000 schrieb Arne Plöse (JIRA): 


","10/Aug/11 06:42;psteitz;I recommend WONT_FIX.  Looks to me like the behavior matches the javadoc and the current contracts are reasonable, IMO.  ","10/Aug/11 08:07;aploese;evaluate this testcase:
{code}
    @Test
    public void testMultiplyInf() {
        Complex z = new Complex(1, neginf);
        Complex w = z.multiply(z);
        Assert.assertEquals(w.getReal(), neginf, 0);
        Assert.assertEquals(w.getImaginary(), neginf, 0);
        Assert.assertEquals(w, z.divide(Complex.ONE.divide(z)));
    }
{code}
the result should be -inf -infi but actually is inf + infi ...
The division looks also broken...","02/Sep/11 22:24;erans;These
{quote}
Assert.assertEquals(w.getReal(), neginf, 0);
Assert.assertEquals(w.getImaginary(), neginf, 0);
{quote}
behave as intended: all infinities are mapped to a single ""point at infinity"" (chosen to be ""INF"" a.k.a. (+inf, +inf)), which IIRC is a way to deal with the infinite number of infinities along all the directions in the complex plane.

This one
{quote}
Assert.assertEquals(w, z.divide(Complex.ONE.divide(z)));
{quote}
looks wrong indeed.
",02/Sep/11 22:39;erans;I've created a new ticket for the above: MATH-657,"05/Sep/11 10:36;erans;It would be helpful that you construct a complete unit test that compares Commons Math with Octave.

Also, there is a discussion, on the ""dev"" ML (cf. thread with subject ""Complex division""), about how operations in ""Complex"" should behave. Mentioning your problems and requirements might contribute to deciding which way to go.
","05/Sep/11 14:21;erans;Another bug (IMO) in CM:
{code}
Complex infInf = new Complex(Double.POSITIVE_INFINITY, Double.POSITIVE_INFINITY);
Assert.assertTrue(infInf.multiply(infInf).isInfinite());
{code}
succeeds, but the formula would have produced ""NaN"" in the real part.
Octave computes:
{noformat}
NaN + Infi
{noformat}
Added unit test (with alternative expectation) in revision 1165296.
","06/Sep/11 14:06;aploese;Here is the comparision between commons math and octave.
I hope this what you want Gilles ;-).","09/Sep/11 23:27;erans;It's on the right track, but the test method stops as soon as it encounters a difference; thus, we cannot have a complete overview of all the differences with Octave.
","10/Sep/11 00:30;erans;See what I mean in the attached file. You could readily apply the same layout to new test classes for ""subtract"", ""multiply"" and ""divide"" (which were in ""test4"" in your file).
","21/Sep/11 19:08;aploese;added test cases for sud/mul/div
fixed sign detection (readable output)
replaced Complex.valueOf with new Double(r, i)","24/Sep/11 10:04;erans;Hi.

# I don't really understand the necessity of ""getSign"". Couldn't you use ""Math.signum"" for the same
purpose?
# It would better to merge the assertions on the signs within the main test because, having them separate forces the operation (add, multiply, ...) to be performed 3 times. Really it is the same test (two results must be equal, sign included).
# I don't understand the statement with ""Complex.valueOf"".

Did you notice the MATH-667 issue?
","26/Sep/11 08:35;aploese;1. Math.signum: octave makes a distinction between +0 and -0, from the javadocs Math.signum does not.
2. No real need (just more verbose if a case failed) you can collapse them in one test case.
3. valueOf return Complex.NAN id a part is NAN (same for INF) ","26/Sep/11 11:40;erans;bq. Math.signum: octave makes a distinction between +0 and -0, from the javadocs Math.signum does not.

This is actually from the {{Math.signum}} Javadoc:
{panel}
[...]
* If the argument is positive zero or negative zero, then the result is the same as the argument.
{panel}
",02/Oct/11 21:16;erans;See MATH-667 for an alternative solution.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ADJUSTED R SQUARED INCORRECT IN REGRESSION RESULTS,MATH-619,12514014,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,gsteri1,gsteri1,14/Jul/11 05:33,24/Mar/12 16:16,07/Apr/19 20:38,14/Jul/11 06:15,3.0,,,,,,,3.0,,,0,,,,,,,,"I forgot to cast to double when dividing two integers:

            this.globalFitInfo[ADJRSQ_IDX] = 1.0 - 
                    (1.0 - this.globalFitInfo[RSQ_IDX]) *
                    (  nobs / ( (nobs - rank)));
Should be
            this.globalFitInfo[ADJRSQ_IDX] = 1.0 - 
                    (1.0 - this.globalFitInfo[RSQ_IDX]) *
                    ( (double) nobs / ( (double) (nobs - rank)));

Patch attached.",Java,,,,,,,,,,,,,,,,,,,,14/Jul/11 05:33;gsteri1;regres;https://issues.apache.org/jira/secure/attachment/12486407/regres,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-14 06:15:50.147,,,false,,,,,,,,,,,,,,68104,,,Thu Jul 14 06:15:50 UTC 2011,,,,,,0|i0ap4v:,60345,,,,,,,,14/Jul/11 06:15;psteitz;Fixed in r1146575.  Lets keep the updates (including test cases :) for RegressionResults on (still open) MATH-607.  We can use separate issues for implementations; but we should keep the updates to RegressionResults attached to that issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same",MATH-618,12513980,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,13/Jul/11 20:23,24/Mar/12 16:16,07/Apr/19 20:38,14/Jul/11 06:08,1.2,2.0,2.1,2.2,,,,3.0,,,0,,,,,,,,"For both Complex add and subtract, the javadoc states that

{code}
     * If either this or <code>rhs</code> has a NaN value in either part,
     * {@link #NaN} is returned; otherwise Inifinite and NaN values are
     * returned in the parts of the result according to the rules for
     * {@link java.lang.Double} arithmetic
{code}

Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,68108,,,Thu Jul 14 06:08:54 UTC 2011,,,,,,0|i0ap53:,60346,,,,,,,,14/Jul/11 06:08;psteitz;Fixed in r1146573,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OLSMultipleRegression seems to fail on the Filippelli Data,MATH-615,12513813,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,,gsteri1,gsteri1,12/Jul/11 16:08,04/Mar/13 18:58,07/Apr/19 20:38,26/Nov/11 22:13,3.0,,,,,,,,,,0,data,Filippelli,NIST,OLSMutlipleRegression,QR,,,"Running the Filipelli data results in an exception being thrown by OLSMutlipleRegression. The exception states that the matrix is singular. 
http://www.itl.nist.gov/div898/strd/lls/data/Filip.shtml

I have added the data to the OLSMutlipleRegressionTest file. 

Unless I screwed something up in the passing of the data, it looks like the QR decomposition is failing.",Java,,,,,,,,,,,,,,,,,,,,20/Jul/11 02:46;gsteri1;filippelli2;https://issues.apache.org/jira/secure/attachment/12487100/filippelli2,12/Jul/11 16:11;gsteri1;tstdiff;https://issues.apache.org/jira/secure/attachment/12486197/tstdiff,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-08-10 06:12:12.375,,,false,,,,,,,,,,,,,,2208,,,Wed Aug 10 06:21:27 UTC 2011,,,,,,0|i0rtjj:,160430,,,,,,,,12/Jul/11 16:11;gsteri1;The OLSMutlipleRegressionTest changes with Filipelli included... ,"20/Jul/11 02:46;gsteri1;The situation is not as dire as I first thought. The original test I uploaded had a bug which resulted in a singular matrix, correctly. This current test still fails, but the failure occurs with a tolerance of 1.0e-5 for the parameters. ","10/Aug/11 06:12;psteitz;I am tempted to close this as not a problem.  What do you think, Greg?","10/Aug/11 06:17;gsteri1;Yes,  the major issue of singularity was one where I had a bug in the test.



","10/Aug/11 06:21;gsteri1;Do check in the Filipelli test though.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
optional dense output in ODE step handlers leads to weird results,MATH-604,12511565,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,26/Jun/11 15:05,24/Mar/12 16:16,07/Apr/19 20:38,26/Jun/11 16:27,2.2,,,,,,,3.0,,,0,,,,,,,,"The requiresDenseOutput method defined in the StepHandler interface was first created as an optimization feature to avoid calling computeDerivatives too many times for some integrators. In fact, only Dormand-Prince 8 (5,3) needs it because we can save 3 calls per step when the interpolator is not used.

This feature brings more problems than it solves:
* it forces users to implement this method despite its purpose is not clear,
* even if the method returns false, sometimes dense output will be generated (in fact when there are events detectors),
* it creates problems with at least Gragg-Bulirsch-Stoer since this integrator really needs interpolation,
* it will create the same problems for Adams integrators (they also need interpolation),
* this ""optimization"" is useful only for one integrator: Dormand-Prince 8 (5,3),
* in many cases, even for Dormand-Prince 8 (5,3) it does not optimize anything since people will often need interpolation

So I would like to completely remove this.

Removing the method is backward compatible for users.
",,,,,,,,,,,,,,,,,,,MATH-596,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-06-27 06:24:58.474,,,false,,,,,,,,,,,,,,69933,,,Mon Jun 27 06:24:58 UTC 2011,,,,,,0|i0ap7b:,60356,,,,,,,,26/Jun/11 16:27;luc;Fixed in subversion repository as of r1139831,"27/Jun/11 06:24;dhendriks;> Removing the method is backward compatible for users.

Since I used @Override on that method (Java 6), I now get an error, as the interface no longer has the method, so I'm no longer overriding anything. It is easy to remove the method, and the error is nice, as you'd want to remove the method anyway. It is therefore however not backward compatible...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SingularValueDecompositionImpl psuedoinverse is not consistent with Rank calculation,MATH-601,12511394,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,gsteri1,gsteri1,23/Jun/11 19:31,24/Mar/12 16:16,07/Apr/19 20:38,18/Sep/11 21:07,2.2,,,,,,,3.0,,,0,Pseudoinverse,,,,,,,"In the SingularValueDecompositionImpl's internal private class Solver, a pseudo inverse matrix is calculated:

In lines 2600-264 we have:

                if (singularValues[i] > 0) {
                 a = 1 / singularValues[i];
                } else {
                 a = 0;
                }

This is not consistent with the manner in which rank is determined (lines 225 to 233). That is to say a matrix could potentially be rank deficient, yet the psuedoinverse would still include the redundant columns... 

Also, there is the problem of very small singular values which could result in overflow.  ",All,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,22/Jul/11 11:02;joubert;SingularValueDecompositionImpl.patch;https://issues.apache.org/jira/secure/attachment/12487425/SingularValueDecompositionImpl.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-07-22 11:02:05.434,,,false,,,,,,,,,,,,,,60261,,,Sat Aug 13 07:26:46 UTC 2011,,,,,,0|i0ap7r:,60358,,,,,,,,"22/Jul/11 11:02;joubert;getRank() employs a local tolerance value to determine if a singular value should be counted.  

I have attached a patch that moves this tolerance value to a static variable in the main class to be accessed by the Solver in working out the pseudo-inverse.  This tolerance value will also guard against overflow.",22/Jul/11 17:23;gsteri1;Patch looks good to me... ,"23/Jul/11 10:29;axelclk;For reference values try wolfram alpha:
N[SingularValueDecomposition[{{1,2},{1,2}}]]

It's still different.
See: MATH-320","23/Jul/11 15:33;gsteri1;Axel,

You are correct, while the getRank()  emthods criterion was changed to:
double tol = FastMath.max(m, n) * singularValues[0] * EPS;
there is nothing happening at line 591. The moore-penrose will still be not
consistent to the ranks calculation.

Line 591 and onwards:

                if (singularValues[i] > 0) {
                    a = 1 / singularValues[i];
                } else {
                    a = 0;
                }

So the change of the zero criterion is good, there is one more spot to fix.

I would also put a lower bound on tol:

tol = FastMath.max(m, n) * singularValues[0] * EPS;

if( FastMath.abs(tol) < FastMath.sqrt( MathUtils.SAFE_MIN) ){

}


-Greg


","23/Jul/11 15:35;gsteri1;Sorry,

Sent the previous inadvertently,

f( FastMath.abs(tol) < FastMath.sqrt( MathUtils.SAFE_MIN) ){
 tol = FastMath.sqrt( MathUtils.SAFE_MIN);
}

That should guard against the case of a small matrix with small eigenvalues.

-Greg

On Sat, Jul 23, 2011 at 10:31 AM, Greg Sterijevski

","13/Aug/11 06:24;psteitz;I just committed (r1157336) a modified version of Chris' patch.  Please review, test and confirm that it fixes the issue.

Changes to the patch:
0) Incorporated Greg's suggestion to put a floor on tol
1) Made tol final, but not static and explicitly passed it to the Solver constructor.  The value depends on instance data so should not be static.","13/Aug/11 07:26;erans;Also changed ""max(m, n)"" to ""m"" in accordance with Greg's remark on the ""dev"" ML for other such occurrences (revision 1157342).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GraggBulirschStoerIntegrator output too low,MATH-596,12510666,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,dhendriks,dhendriks,17/Jun/11 14:44,24/Mar/12 16:16,07/Apr/19 20:38,26/Jun/11 16:28,3.0,,,,,,,,,,0,,,,,,,,"I have the following problem:
x(3.0) = 4.0
x' = 3.0
t0 = 3.0
tend = 10.0

ODE solving using the GraggBulirschStoerIntegrator(1e-10, 100.0, 1e-7, 1e-7) integrator, gives me:

t, x, x'
3.0, 4.0, 3.0
3.105840007284127, 4.0, 3.0
3.829973288493221, 4.31752002185238, 3.0
8.784328663271161, 6.489919865479664, 3.0
10.0, 21.35298598981348, 3.0

Clearly, the value of x at time 3.10... should be something like 4.30... and not 4.0. Also, the value of x at time 10.0 should be around 25.0 and not be 21.35...

If we switch to the DormandPrince853Integrator(1e-10, 100.0, 1e-7, 1e-7), it gives me:

3.0, 4.0, 3.0
3.079933916721644, 4.239801750164932, 3.0
3.8792730839380845, 6.637819251814253, 3.0
10.0, 24.999999999999996, 3.0

as expected.

This seems to me like the GraggBulirschStoerIntegrator has a bug...",,,,,,,,,,,,,,,,,,,,,17/Jun/11 14:46;dhendriks;MyTest.java;https://issues.apache.org/jira/secure/attachment/12482939/MyTest.java,23/Jun/11 09:08;dhendriks;MyTest2.java;https://issues.apache.org/jira/secure/attachment/12483564/MyTest2.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-22 14:12:57.981,,,false,,,,,,,,,,,,,,69988,,,Sun Jun 26 16:28:07 UTC 2011,,,,,,0|i0rtkn:,160435,,,,,,,,17/Jun/11 14:46;dhendriks;This is the file I used to test this. Running it gives the output as given in de issue description.,"22/Jun/11 14:12;luc;It seems to me the problem is related to the interpolator the integrator sends to the step handler.
If I change the implementation of requiresDenseOutput to return true instead of false in the step handle, the error becomes about 9e-16.

Could you check this change completely solves the problem for you.

This requiresDenseOutput was added long ago (even before this code was included in Apache Commons Math) as an optimization to avoid calling computeDerivatives too many times for some integrators. In fact, only Dormand-Prince 8 (5,3) needs it because if the interpolator is not used, we can save 3 calls per step.

Now I think this feature brings more problems than it solves:
 - it forces users to implement this method despite its purpose is not clear,
 - obviously it creates problems with at least Gragg-Bulirsch-Stoer since this
   integrator really needs interpolation
 - it will create the same problems for Adams integrators (they also need interpolation)
 - this ""optimization"" is useful only for one integrator
 - in many cases, even for this integrator it does not optimize anything since
   people will need interpolation

So I would like to completely remove this.
I'm switching to the developers mailing list to discuss about it. It is a better place for discussion than
this JIRA issue. Please join the discussion here, and we will post the conclusion to complete this report.","23/Jun/11 09:08;dhendriks;New test, with output for both dense and non-dense step handler.

Note how for the non-dense step handler, the derivatives are computed for 10.0, and the result is 24.999999999999996. It then continues to call the step handler for time 10.0 with value 21.35298598981348, which is the value calculated for time 8.784328663271161, from before the previous time the step handler was called. It seems the compute derivative method is called for the appropriate times, only the output is not correctly used to set the interpolator values.

If you think this is indeed *only* caused by the optionality of dense output, or it is no longer relevant if non-dense output is removed, then removing the non-dense option would indeed fix this issue.

It would probably be better to create a separate issue for the removal of non-dense output. I have no objections to the removal of non-dense output.
","26/Jun/11 14:57;luc;Yes, I think the optional dense output feature is the only problem here.
When the step handler doesn't requirs dense output and there are no events, a dummy step interpolator is used. This is due to the following statements in the integrator:
{code}
if (denseOutput) {
  interpolator = new GraggBulirschStoerStepInterpolator(y, yDot0,
                                                        y1, yDot1,
                                                        yMidDots, forward);
} else {
  interpolator = new DummyStepInterpolator(y, yDot1, forward);
}
{code}

So in your case, you get a DummyStepInterpolator which simply copies some intermediate states computed earlier.

I will open a separate issue for removing optional dense output, solve the new issue and solve this one afterwards.
","26/Jun/11 16:28;luc;Fixed in subversion repository as of r1139831.

Thanks for the report",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 org.apache.commons.math.stats generated an error for OLSMultipleLinearRegression,MATH-590,12510339,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,,dtran,dtran,14/Jun/11 18:47,15/Jun/11 16:31,07/Apr/19 20:38,15/Jun/11 16:31,2.2,,,,,,,,,,0,,,,,,,,"Hello, please be gentil since i am freshman student in statistics. I am trying to use the org.apache.commons.math.stats library for my multiple regression homework. The problem i am facing is that the error message telling me that OLSMultipleLinearRegression could not be resolved when i try to compile my java program; eventhough, i import org.apache.commons.math.stats in my program, 
and i have OLSMultipleLinearRegression reg = new OLSMultipleLinearRegression() in my code.

I would like to report this error and get your help.

Many thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-06-15 16:31:03.535,,,false,,,,,,,,,,,,,,71182,,,Wed Jun 15 16:31:03 UTC 2011,,,,,,0|i0rtl3:,160437,,,,,,,,15/Jun/11 16:31;psteitz;This is a question for the user list.  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaDoc error in OneWayAnovaImpl,MATH-589,12510244,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,logongas,logongas,13/Jun/11 23:00,24/Mar/12 16:16,07/Apr/19 20:38,15/Jun/11 16:43,2.2,,,,,,,3.0,,,0,javadoc,,,,,,,"JavaDoc por OneWayAnovaImpl say:
Implements one-way ANOVA statistics defined in the OneWayAnovaImpl interface. 
And the Javadoc must say:
Implements one-way ANOVA statistics defined in the OneWayAnova interface. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-06-15 16:43:21.137,,,false,,,,,,,,,,,,,,71180,,,Wed Jun 15 16:43:21 UTC 2011,,,,,,0|i0rtlb:,160438,,,,,,,,15/Jun/11 16:43;psteitz;Fixed in r1136112.  Thanks for reporting this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Weighted Mean evaluation may not have optimal numerics,MATH-588,12510061,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,12/Jun/11 18:19,24/Mar/12 16:16,07/Apr/19 20:38,05/Feb/12 19:54,2.1,2.2,,,,,,3.0,,,0,,,,,,,,"I recently got this in a test run
{code}
testWeightedConsistency(org.apache.commons.math.stat.descriptive.moment.MeanTest)  Time elapsed: 0 sec  <<< FAILURE!
java.lang.AssertionError: expected:<0.002282165958997601> but was:<0.002282165958997157>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:441)
	at org.apache.commons.math.TestUtils.assertRelativelyEquals(TestUtils.java:178)
	at org.apache.commons.math.TestUtils.assertRelativelyEquals(TestUtils.java:153)
	at org.apache.commons.math.stat.descriptive.UnivariateStatisticAbstractTest.testWeightedConsistency(UnivariateStatisticAbstractTest.java:170)
{code}

The correction formula used to compute the unweighted mean may not be appropriate or optimal in the presence of weights:

{code}
// Compute initial estimate using definitional formula
double sumw = sum.evaluate(weights,begin,length);
double xbarw = sum.evaluate(values, weights, begin, length) / sumw;

// Compute correction factor in second pass
double correction = 0;
for (int i = begin; i < begin + length; i++) {
  correction += weights[i] * (values[i] - xbarw);
}
return xbarw + (correction/sumw);
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-02-05 19:54:51.113,,,false,,,,,,,,,,,,,,2215,,,Sun Feb 05 21:17:31 UTC 2012,,,,,,0|i0rtlj:,160439,,,,,,,,"05/Feb/12 19:54;tn;Fixed it in r1240790.

There was a too strict equality test using an relative error of 10-14 which resulted in certain unforunate cases of an absolute error of 10-18.","05/Feb/12 21:17;tn;Corrected the equality test in r1240795 as it was leading to failure. In fact the test can range from very small to very large values which really requires a relative error estimate.

The test is problematic in general, as it may contain values from very different scales (due to its random nature), leading to unavoidable precision errors in the above formula.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"AbstractIntegerDistribution cumulativeProbablitity(int, int) implementation does not match API documentation",MATH-587,12510035,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,,psteitz,psteitz,12/Jun/11 06:55,12/Jun/11 19:42,07/Apr/19 20:38,12/Jun/11 19:42,,,,,,,,,,,0,,,,,,,,"The documentation for this method says it computes p(x0 < X < x1), but the parameter documentation inconsistently states that the bounds are ""inclusive"" and what it actually returns is
{code}
cumulativeProbability(x1) - cumulativeProbability(x0 - 1);
{code}
which is p(x0-1 <= X < x1).
In the code above, x0 - 1 should be replaced by x0 + 1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,71364,,,Sun Jun 12 19:42:58 UTC 2011,,,,,,0|i0rtlr:,160440,,,,,,,,"12/Jun/11 19:42;psteitz;Once the javadoc API descriptions were reverted to last release versions, the documentation matches the implementation.  Sorry I was confused by the javadoc.  I added a standalone test class confirming that the API contracts match the default impls.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KMeansPlusPlusClusterer incorrectly selects initial cluster centers and is unnecessarily slow,MATH-584,12508995,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,drrandys,drrandys,01/Jun/11 16:50,24/Mar/12 16:16,07/Apr/19 20:38,05/Jun/11 16:28,2.2,,,,,,,3.0,,,0,cluster,clustering,,,,,,"The chooseInitialClusters() method declares sum as an int, when it should be double.  It also is quite slow because it contains a lot of unnecessary computation.  I'll attached a patch which corrects the problems.

I found the problems while comparing an optimized implementation of KMeans++ I've been working on with the one in commons math. 

",All environments,,,,,,,,,,,,,,,,,,,,01/Jun/11 18:09;drrandys;kmeans_plus_plus.patch;https://issues.apache.org/jira/secure/attachment/12481120/kmeans_plus_plus.patch,01/Jun/11 16:53;drrandys;kmeans_plus_plus.patch;https://issues.apache.org/jira/secure/attachment/12481109/kmeans_plus_plus.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-06-05 16:28:48.97,,,false,,,,,,,,,,,,,,71902,,,Sun Jun 05 16:28:48 UTC 2011,,,,,,0|i0rtmf:,160443,,,,,,,,01/Jun/11 16:53;drrandys;Patch file for correcting KMeansPlusPlusClusterer,"01/Jun/11 18:09;drrandys;Noticed that one of the KMeanPlusPlusClusterer tests got caught in an infinite loop with my patched version, because of extremely small distances.  

Apply this patch instead -- it ensures an exit from the while loop.
","05/Jun/11 16:28;luc;Fixed in subversion repository as of r1132448.

Thanks for the report and for the patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Eigen value and SVD fail on a matrix with InvalidMatrixException while numpy has no problem on the same matrix,MATH-583,12508221,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,vjeet,vjeet,24/May/11 17:22,24/Mar/12 16:16,07/Apr/19 20:38,20/Jul/11 12:21,,,,,,,,3.0,,,0,,,,,,,,"Eigen value and SVD fail on a matrix with InvalidMatrixException while numpy has no problem on the same matrix

",,,,,,,,,,,,,,,,,,,,,24/May/11 17:24;vjeet;EigenAndSVDFailure.java;https://issues.apache.org/jira/secure/attachment/12480284/EigenAndSVDFailure.java,24/May/11 17:23;vjeet;matrix.csv;https://issues.apache.org/jira/secure/attachment/12480283/matrix.csv,24/May/11 17:24;vjeet;svd.py;https://issues.apache.org/jira/secure/attachment/12480285/svd.py,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2011-07-20 12:21:42.43,,,false,,,,,,,,,,,,,,67394,,,Wed Jul 20 12:21:42 UTC 2011,,,,,,0|i0rtmn:,160444,,,,,,,,24/May/11 17:23;vjeet;matrix data,"24/May/11 17:24;vjeet;1) java code to reproduce error

2) python code that works","20/Jul/11 12:21;luc;Fixed in subversion repository as of r1148714.

This issue was fixed by changing SVD implementation according to issue MATH-611.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Percentile does not work as described in API,MATH-582,12508170,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,moormaster,moormaster,24/May/11 11:33,24/Mar/12 16:16,07/Apr/19 20:38,11/Jun/11 22:30,2.2,,,,,,,,,,0,,,,,,,,"example call:

StatUtils.percentile(new double[]{0d, 1d}, 25)   returns 0.0

The API says that there is a position being computed:  p*(n+1)/100 -> we have p=25 and n=2
I would expect position 0.75 as result. Next step according to the API is: interpolation between both values at floor(0.25) and at ceil(0.25). Those values are 0d and 1d ... so lower + d * (upper - lower) should give 0d + 0.25*(1d - 0d) = 0.25

But the above call returns 0 as result. This does not make sense to me.


another example where I think the result is not correct:

StatUtils.percentile(new double[]{0d, 1d, 1d, 1d}, 25)   returns 0.25

we have pos = 25*5/100 = 1.25  ... so d = 0.25
values at position floor(1.25) and ceil(1.25) are 1d and 1d. How comes that the result is not between 1d?

",,,,,,,,,,,,,,,,,,,,,10/Jun/11 17:45;joubert;MATH-582.patch;https://issues.apache.org/jira/secure/attachment/12482081/MATH-582.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-06-08 23:07:43.441,,,false,,,,,,,,,,,,,,676,,,Sat Jun 11 22:30:17 UTC 2011,,,,,,0|i0rtmv:,160445,,,,,,,,"08/Jun/11 23:07;joubert;I believe the implementation of percentiles within the library is in accordance with the NIST definition of percentiles.  To address your examples separately:

1.  What is missing from the API in the description of the implementation is ""If pos < 1 then return the smallest element in the array"".  As such, the value of 0.0 returned in your first example is indeed correct for this implementation.

2.  In this definition of percentiles, the value of pos is a position in the array to be interpolated, but with array indices starting with 1. So with pos = 1.25, the value returned is correctly a quarter between the 1st and 2nd array values.

Percentiles do not meet intuition well when working with small datasets.  Other definitions, for example one with pos = 1+p*(n-1)/100 (like in MS Excel), may meet your requirement better in the above datasets, but not so well with medium ones.  With large datasets, the two definitions converge.

Hope this helps,

Chris N",10/Jun/11 17:45;joubert;Added a patch for org.apache.commons.math.stat.Percentile documentation in order to better describe the correct implementation of percentiles as recommended by NIST.,"11/Jun/11 22:30;psteitz;Patch applied (with test) in trunk in r1134802.

Thanks, Andre for the report and thanks, Christopher for the patch!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath Performance Test should use larger ranges where the functions support them,MATH-580,12507524,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,17/May/11 20:05,07/Apr/13 09:14,07/Apr/19 20:38,13/Mar/13 12:57,,,,,,,,3.2,,,0,,,,,,,,"FastMath calculations should be faster than Math, especially over larger ranges of operands.

The Performance test code currently uses a very small range of operands (probably so the same range can be used for all tests).

The test should be enhanced to use much larger ranges where this is supported by the function under test.",,,,,,,,,,,,,,,,,MATH-904,MATH-905,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-08-10 06:25:32.066,,,false,,,,,,,,,,,,,,2217,,,Sun Apr 07 09:14:18 UTC 2013,,,,,,0|i0293j:,11075,,,,,,,,10/Aug/11 06:25;psteitz;Pushing out to 3.1 awaiting patch,"16/Oct/11 20:43;ijuma;Not sure if this has been fixed in trunk, but the performance test code had some issues: http://blog.juma.me.uk/2011/02/23/performance-of-fastmath-from-commons-math/","17/Oct/11 00:08;erans;In revision 1184958, the test class ""FastMathTestPerformance"" contains an additional method that performs the same micro-benchmark using another utility.  You are very welcome to check the methodology (see also MATH-637).
","13/Mar/13 12:57;luc;Fixed in subversion repository as of r1455921.

The extension was done in the individual tests, not in the global catch-all one. The results show that for large ranges, there are some important slow down (for example for asin/acos/atan, probably due to range reduction).",07/Apr/13 09:14;luc;Closing issue as version 3.2 has been released on 2013-04-06.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath.pow much slower than Math.pow,MATH-579,12507487,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,aploese,aploese,17/May/11 13:14,24/Mar/12 16:16,07/Apr/19 20:38,18/May/11 09:50,3.0,,,,,,,3.0,,,0,,,,,,,,"calculating FastMath.pow(10, 0.1 / 20) is approximately 65 times slower as the Math.pow() function.
Ether this is a bug or a javadoc comment is missing.","java version ""1.6.0_22""
OpenJDK Runtime Environment (IcedTea6 1.10.1) (6b22-1.10.1-0ubuntu1)
OpenJDK 64-Bit Server VM (build 20.0-b11, mixed mode)


java version ""1.6.0_24""
Java(TM) SE Runtime Environment (build 1.6.0_24-b07)
Java HotSpot(TM) 64-Bit Server VM (build 19.1-b02, mixed mode)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-05-17 18:33:05.774,,,false,,,,,,,,,,,,,,150674,,,Thu May 19 06:05:19 UTC 2011,,,,,,0|i0rtnb:,160447,,,,,,,,"17/May/11 18:33;luc;This is strange. Could you run the FastMathTestPerformance junit test (in the utils package).
On my personal computer (which is also a 64 bits machine running Ubuntu), FastMath.pow is about 28% faster than either StrictMath.pow or Math.pow on Java 5 and 26% faster on Java 6.","17/May/11 18:36;aploese;This happens only on the first! run.
So take a separate project and run this test case:


    @Test
    public void testPowDuration() {
        int n = 10;
        long t1 = System.currentTimeMillis();
        for (int i = 0; i < n; i++) {
            double x1 = FastMath.pow(10, 0.1 / 20);
        }
        long t2 = System.currentTimeMillis();
        for (int i = 0; i < n; i++) {
            double x1 = Math.pow(10, 0.1 / 20);
        }
        long t3 = System.currentTimeMillis();
        double d1 = t2 - t1;
        double d2 = t3 - t2;
          if (d2 == 0) {
            d2 = 1;
        }
        if (d1 / d2 > 2.0)
            throw new RuntimeException(""pow(10, 0.1 / 20) ratio"" + (d1 / d2));
    }
Looks like some initialization takes quite long ...","17/May/11 19:34;luc;This is normal and is probably the same for all FastMath methods (I just checked for sin, abs and sqrt, knowing that FastMath.sqrt simply calls Math.sqrt). The initialization occurs at class loading as many tables are computed, so this overhead occurs only one per program run and does not change with the number of calls.

The number of calls is also important as the native code optimizing compilers kicks of only after the same part of code has been used many times. FastMath relies heavily on this and attempts to be fast for large scale computation. The side effect you see is that it is much slower for very short programs like the benchmark above.

Also note that 10 runs is far too low with regard to both the resolution of currentTimeMillis (one can use System.nanoTime() instead) and for results significance.

So I would like to close this as WONTFIX. Perhaps we should add in the javadoc that FastMath targets large scale computation.","17/May/11 20:01;sebb@apache.org;AIUI FastMath is also targetted at faster calculation over the full range of the operands.

The performance test does not (currently) use suitable ranges for all the functions.","17/May/11 20:30;aploese;If you add a hint in the javadocs of class FastMath -> usage for large scale (and initilaization time up to 100 ms) this issue is FIXED

So having small numbers of computation, stick with Math.* , except you need asinh, ... ;-)  ","17/May/11 22:40;erans;I don't think that a remark like ""targets large scale computation"" adds any more information here (i.e. for ""FastMath"") than it would on any other class (slow initialization and JIT compiler behaviour is the same).
Also, I guess that if one fires up a Java application that only does a few math functions calls, there wouldn't be any noticeable time differences; they will be dwarfed by the JVM startup time.

OK for ""won't fix"".
","18/May/11 06:04;aploese;How it I stumbled over it:

I did debug my app, and came across the call to FastMath.pow() (first call) it took some time (100 ms) to complete the call.
Then I searched the javadoc for pow - nothing the same for class FastMath - nothing. So I decided must be a bug.

As you wrote FastMath is no general replacement of Math as the name Fast* would suggest, but it is intended for large scale operation.
If you want prevent confusion by end users (developers), a hint would be fine. ","18/May/11 09:50;luc;So I have added a thorough explanation in the class javadoc without any code change as of r1124151.

Thanks for reporting the issue and discussing about it.
","18/May/11 09:53;erans;In the end, it is the _absolute_ running time that counts. If you call ""pow"" _once_ in an application, why would it matter that it takes 100 ms or 1 ms?

Could it be that the ""Math"" class is already loaded (as part of the JVM initialization), so that ""FastMath"" is at a disadvantage in your benchmark (because it still needs to be loaded at the first call to ""pow"")?

I maintain that such a disclaimer (""large scale usage"") provides more confusion that it clears: What is large-scale? People who are worried will profile their (complete) application and can decide which implementation (of any interface) to use, based on realistic timings, not on a micro-benchmark (which can provide contradictory results).

What we could do is add a link to the performance test class.
","18/May/11 15:05;psteitz;I think Luc did a great job providing the right information to users.  In answer to your question, Gilles, there are some applications where 100ms is a big deal and if they just make one or two calls, the associated latency will be surprising and could cause problems for them.  If we get a lot of feedback from users indicating that this latency is a material practical problem for them, or prevents them using the library, we may want to consider making the embedded use configurable.  I can think of only a few cases in my experience where this might be an issue; but I am thankful to Arne for having pointed it out and Luc for improving the documentation.","18/May/11 16:02;erans;{quote}
In answer to your question, Gilles, there are some applications where 100ms is a big deal and if they just make one or two calls [...]
{quote}

This does not answer my question, which could be restated as: How can 100 ms matter when the JVM can take several seconds to start up?
I'm curious of what real applications (that use CM) would run for less than a few seconds...

{quote}
If we get a lot of feedback from users indicating that this latency is a material practical problem for them, or prevents them using the library, we may want to consider making the embedded use configurable.
{quote}

I surely hope that micro-benchmarks are not going to be taken into consideration...

{quote}
I am thankful to Arne for having pointed it out [...]
{quote}

At the time ""FastMath"" was introduced, I had already pointed out the relative slowness of some functions, to which it had been answered that the ""fast"" in ""FastMath"" would kick in only when doing several millions calls (i.e. after the JIT compiler would compile the methods to native code).

{quote}
I am thankful to [...] Luc for improving the documentation.
{quote}

+1
","18/May/11 17:21;luc;{quote}
This does not answer my question, which could be restated as: How can 100 ms matter when the JVM can take several seconds to start up?
I'm curious of what real applications (that use CM) would run for less than a few seconds...
{quote}

I think mainly about hosted application, in environments like Eclipse, web servers, service oriented architectures, perhaps even Android devices. The JVM is already started but I'm not sure the class are reused between requests, I think a new fresh context is set up with a new classloader, which involves reloading the class.

For sure, micro-benchmark should be avoided. Despite it is quite old, the paper about flawed micro benchmark by Brian Goetz [http://www.ibm.com/developerworks/java/library/j-jtp02225/index.html] is really enlightening.

If we get further reports about this latency, we may look at a way to pre-compute the tables at compile time rather than at runtime to see if we can save some milliseconds.","18/May/11 20:33;erans;{quote}
hosted application, in environments like [...] web servers, [...]
{quote}

From what I've just been reading, servlets are only reloaded when their "".class"" file has changed. And they refer to this as a feature (to allow code to be modified without needing a server restart) but also as a hack (because the usual class loader of the JVM does not do that)...

I don't know how Eclipse or Android works but I don't see why a class would be reloaded inside a given application. For Android, the recompilation argument doesn't even apply.
","19/May/11 06:05;aploese;{quote}This does not answer my question, which could be restated as: How can 100 ms matter when the JVM can take several seconds to start up?
I'm curious of what real applications (that use CM) would run for less than a few seconds...{quote}

I could imagin of an GUI app where the users put some values (maybe a 3x3 matix) ant hit the calc button and wait. In a GUI 100 ms are a long time.

So the developer can load FastMath (if she really want to use FastMath) in a separate thread at startup ...

Thist hwole issue is more about usabillity and expected behavior and not the fact that it takes up to 100ms to initialize FastMath.

If I use a lib where I am not really satified with, I will try to replace it. If I know there is a startup penalty, I know it, and I can put the startup time in a place where it does not hurt - its fine.   ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decrease DescriptiveStatistics performance from 2.0 to 2.2,MATH-578,12507371,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,mikl,paolo.repele,paolo.repele,16/May/11 14:34,04/Mar/13 18:53,07/Apr/19 20:38,22/Jul/12 15:01,2.2,,,,,,,3.1,,,0,,,,,,,,"Switching between commons-math 2.0 to 2.2 we note how the
DescriptiveStatistics.addValue(double) has decrease the performance.

I tested with 2 million values.

DescriptiveStatistics ds = new DescriptiveStatistics();
for(int i = 0; i<1000*1000*2; i++) { //2 million values
    ds.addValue(v);
}

ds.getPercentile(50);


Seems that depending by the values inserted in the DescriptiveStatistics it takes different time:

* with a single value (0)
** 2.0 -> take ~500 ms
** 2.2 -> take more than 10 minutes
* with 50% fixed value (0) and 50% Math.random()
** 2.0 -> take ~500 ms
** 2.2 -> take ~250000 ms -> ~250 second
* with 100% Math.random()
** 2.0 -> take ~500 ms
** 2.2 -> take ~70 ms

",Linux,,,,,,,,,,,,,,,,,,,,16/May/11 15:45;paolo.repele;percentile.png;https://issues.apache.org/jira/secure/attachment/12479338/percentile.png,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-05-16 15:18:48.499,,,false,,,,,,,,,,,,,,2218,,,Sun Jul 22 15:17:52 UTC 2012,,,,,,0|i0rtnj:,160448,,,,,,,,16/May/11 15:18;mikl;Have you tried more detailed profiling? E.g. in Eclipse to see which methods are using the majority of time?,"16/May/11 15:33;psteitz;Thanks for reporting this.  I assume the timings include the percentile calculation, right? 

This could be related to the changes in the Percentile implementation in 2.2. If isolating the timing to just the percentile calculation shows that is where the latency difference is, we should reopen MATH-417.  The changes there were to improve Percentile performance, which in most cases they do.  The first two results above are disturbing, however.  If your data is largely constant and this creates a problem in your application, as a workaround, you can provide an alternative Percentile implementation to DescriptiveStatistics using setPercentileImpl.",16/May/11 15:45;paolo.repele;Image file to show the profile snapshot,"16/May/11 15:47;mikl;Sorry for my (too) short first answer. Thanks for your proper introduction, Phil.

I'll try a more detailed profiling to see what's causing the performance problems.

","16/May/11 15:56;paolo.repele;No Problem :)
* yep, the time was only for the getPercentile() method.
* I added an image where you can see the profile snapshot

Usually we use this library to analyze some grids. These grids can be very huge and can be generated using the same values for all the cells or a continue function around the grid or any combination of both.
Then we have really no idea how these grids can be generated.","16/May/11 16:44;mikl;Also, it seems like FastMath is new to 2.2. I'll try to investigate what causes this.","16/May/11 17:56;mikl;As far as I can see, Percentile contributes a lot to the longer execution time, so reopening MATH-417 for datasets of this type might be the right thing to do.","10/Aug/11 06:35;psteitz;Not sure this is in fact a bug, but rather a feature resulting from overall performance improvements in Percentile (poorer performance for a relatively small number of problem instances).  I do not see it as showstopper for 3.0, so moving to 3.1.","22/Jul/12 01:13;erans;Please rerun your test when MATH-805 has been solved, as it seems that this issue might be caused by the same bug.","22/Jul/12 15:01;tn;Fixed in r1364318.
See also MATH-805 with a description of the problem.",22/Jul/12 15:17;tn;I did the provided test myself and indeed it is the same problem and is fixed by the suggested changes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions in genetics package or not consistent with the rest of [math],MATH-575,12507228,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,tn,psteitz,psteitz,14/May/11 16:40,24/Mar/12 16:16,07/Apr/19 20:38,02/Feb/12 11:12,2.0,2.1,2.2,,,,,3.0,,,0,,,,,,,,"InvalidRepresentationException is checked and non-localized.  This exception should be placed in the [math] hierarchy.  The AbstractListChromosome constructor also throws a non-localised IAE, which should be replaced by an appropriate [math] exception.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2012-01-23 22:30:24.512,,,false,,,,,,,,,,,,,,2219,,,Thu Feb 02 11:12:52 UTC 2012,,,,,,0|i0rto7:,160451,,,,,,,,"23/Jan/12 22:30;tn;Phil started to work on this issue in r1135025.

In r1235038 additional cleanups have been performed:

 - add localized messages for all exceptions
 - add @throws to javadoc where appropriate
 - add final to method parameters

What is missing:

 - Phil mentioned that InvalidRepresentationException should be placed into [math], although I am not sure why, as it is not used outside the genetics package
 - add more custom exception classes specific to the genetics package (optional). By now mostly MathIllegalArgumentException or other appropriate ones have been used.","23/Jan/12 23:16;erans;Thanks for working on this, but before you do start to make modifications, please assign the issue to yourself!

For the changes themselves, I don't agree with the creation of those many localized messages: We have been trying to rationalize and reduce the number of those, by removing duplicates and combining several ones to convey the full explanation of the problem. See my reply to the commit message.
","24/Jan/12 10:18;tn;Fixed in r1235197.

Thanks for your suggestions!","02/Feb/12 10:34;erans;Thomas,
Could please check whether this issue is resolved? And if it is, mark it so? Thanks.
","02/Feb/12 10:47;tn;As from the original issue description, Phil intended to move the InvalidRepresentationException to the general o.a.c.m.exceptions package. I am not sure about this, that's why I kept it aside for the time being. If we agree on keeping it in the genetics package we can resolve this issue.","02/Feb/12 11:08;erans;Phil had always been opposed to having all exceptions grouped in their own package; so I doubt that he meant to move that one over there... ;-)
Here, the description just indicates that the exception should become _unchecked_ and that the ""detailed message"" should be an element from the ""LocalizedFormats"" enum (i.e. derive from one of the base CM exceptions).
","02/Feb/12 11:12;tn;Ah ok, that makes it clear. When reading hierarchy I was just thinking in terms of packages rather than class hierarchy.

Thus, I resolve this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
in ArrayFielVector i.e. subtract calls wrong constructor,MATH-573,12506690,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,aploese,aploese,10/May/11 08:23,24/Mar/12 16:16,07/Apr/19 20:38,05/Jun/11 15:32,3.0,,,,,,,3.0,,,0,,,,,,,,"I.E. subtract calls

""return new ArrayFieldVector<T>(out)"" this constructor clones the array...
""return new ArrayFieldVector<T>(field, out, false)"" would be better (preserving field as well)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-05-10 18:20:24.611,,,false,,,,,,,,,,,,,,71909,,,Sun Jun 05 15:32:33 UTC 2011,,,,,,0|i0rton:,160453,,,,,,,,"10/May/11 18:20;luc;Fixed in subversion repository as of r1101579.

The same problem was also present in the add methods.

Thanks for reporting the issue and providing the fix.","10/May/11 20:21;aploese;Sorry, but look at mapSubtract and further down in the source you will find plenty.

The same goes for AbstractArrayMatrix... 

Sorry ;-)

","11/May/11 06:59;luc;Thanks to have checked this and sorry for forgetting these occurrences. Could you check again and reopen if I still forgot a few ones ?

Fixed in subversion repository as of r1101763","11/May/11 17:19;aploese;At least copy() and public constructor Array2DRowFieldMatrix<T> add(final Array2DRowFieldMatrix<T> m) in Array2DRowFieldMatrix.java ignores field | there is no contract what should happen with field (whetzer it is to pe basses or not, and if yes after what pattern) - so not really a bug more a hint.

Maybe an other issue: should the ArrayFieldVector v from which data is used, not given the chance to review its internal state by asking v.getDataRef() instead of simply taking v.data (currently just a philosophical question :-)) ?","11/May/11 20:03;luc;Nice catch. I have added a few constructors with the field parameter as of r1102057.

Concerning v.data versus v.getDataRef(), there is no fixed rule. As far as I am concerned, I use directly the attribute as it is shorter and easier to read. Some other [math] developers may do otherwise. I don't think derived class completely discard an attribute from their base class to replace it. However, in these rare cases, using a getter would of course be the right way to go.","12/May/11 06:31;aploese;No, I think more of a derived class which do some fancy cashing and not updating data immediately - as I said just a thought.",12/May/11 09:53;aploese;Array2DRowFieldMatrix.java: 263 | 295,"13/May/11 06:47;aploese;I suggest to remove all constructores that not specify whether the data should be copied or not. If you do so you will find many ""wrong"" usages ...

An other point why there is the interface FieldVector when the implementing class (ArrayFieldVector) of FieldVector is leaking out (and in) of ArrayFieldVector on every place - IMHO this is bad coding style.","13/May/11 06:47;aploese;sorry, see my last comment","13/May/11 07:19;aploese; public T[] preMultiply(final T[] v) {

is implemented twice 1st in AbstractFieldMatrix.java and 2nd in Array2DRowFieldMatrix.java","14/May/11 10:05;luc;I have tried to specify field wherever it was possible. Thanks for the tip about removing the constructor and fixing the errors. I have left many calls of the no-field constructors in the tests, for testing purposes, there are no such calls left in the library itself.

We know ArrayFieldVector leaks out in the interface. This was on purpose when people already know they want to use this implementation. They are however free to have different implementations for storage, so the interface is also useful (but not to the same users). In fact, when we have a method that returns a FieldVector in the interface, we should say it returns an ArrayFieldVector in the implementation if we know this is what always happen. Implementing or overriding a method with a narrowed return type is allowed by the Java language. We did not do it on all methods, so I agree we are inconsistent here.

The two implementations of premultiply are different. One uses the generic getEntry methods and the other uses direct array access. At that time, it was done for efficiency. We did not benchmark it recently with new JVMs, but it may not be needed anymore. The usefulness of this overriding is really JVM dependent.","14/May/11 15:05;aploese;OK so fare.

I hope that you don't flame me for nagging you again :-)

Looking at RealArrayVecor and FieldArrayVector I see different implementations of mapSOMETHING ... Maybe check performance, define some best condiing practice and refactor | clean up?

Furthermore I saw the checkDimension() is sometimes called sometimes not? - Just a hint... ","15/May/11 13:22;luc;Don't worry Arne, I'm happy you point out mistakes in our code.

I seem to have forgotten committing the changes I made yesterday about specifying field wherever possible. It's in the repository now.

Doing some benchmarks to check if we should remove or keep different implementations would be a good thing. Would you try to give it a try and report your results, preferably opening a new Jira issue ?

I found checkDimension only in MultivariateSummaryStatistics and checkdimension (small case ""d"") only in tests. Where did you see other use ?","15/May/11 17:46;aploese;In AbstractRealvector

public RealVector add(double[] v) {
there is no check of any dimension (goes for (at least some) other operators as well.

What kind of benchmark do you want (RealVector current implementations v.s. true array|sparse ?) or the operators in map*?

Maybe I could think of an implementations where there is a private interface Storage {
 T getEntrx(int i);
 void setEntry(int i; T e);
 int getDimension();
 int setDimension(Field f, int size);
 int setDimension(T[] v, boolean makeCopy); << I dont like the name copyArray its implemetation specifict but dataIsInmutabe sounds also bad ....
}

this is instatiated with ether An array implentation or a sparse list one could define if the fillstate is more than 50 % switch to array ore something else ... just an idea ....","05/Jun/11 15:32;luc;It now seems everything has finally been sorted out for this issue, after several failed attempts, so marking it as resolved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Constructor parameter not used,MATH-572,12506688,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,aploese,aploese,10/May/11 08:07,24/Mar/12 16:16,07/Apr/19 20:38,10/May/11 18:09,3.0,,,,,,,3.0,,,0,,,,,,,,"the constructor public ArrayFieldVector(Field<T> field, T[] v1, T[] v2)
sets this
""this.field = data[0].getField();""
in the fast line...

""this.field = field;""

would be right - field was explicitly provided.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-05-10 18:09:45.165,,,false,,,,,,,,,,,,,,150671,,,Tue May 10 18:09:45 UTC 2011,,,,,,0|i0rtov:,160454,,,,,,,,"10/May/11 18:09;luc;Fixed in subversion repository as of r1101575.

Thanks for reporting the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"class Dfp toDouble method return -inf whan Dfp value is 0 ""zero""",MATH-567,12506317,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,michubuntu,michubuntu,05/May/11 17:49,24/Mar/12 16:16,07/Apr/19 20:38,05/May/11 19:44,2.2,,,,,,,3.0,,,0,,,,,,,,"I found a bug in the toDouble() method of the Dfp class.
If the Dfp's value is 0 ""zero"", the toDouble() method returns a  negative infini.

This is because the double value returned has an exposant equal to 0xFFF 
and a significand is equal to 0.
In the IEEE754 this is a -inf.

To be equal to zero, the exposant and the significand must be equal to zero.

A simple test case is :
----------------------------------------------
import org.apache.commons.math.dfp.DfpField;


public class test {

	/**
	 * @param args
	 */
	public static void main(String[] args) {
		DfpField field = new DfpField(100);
		System.out.println(""toDouble value of getZero() =""+field.getZero().toDouble()+
				""\ntoDouble value of newDfp(0.0) =""+
				field.newDfp(0.0).toDouble());
	}
}

May be the simplest way to fix it is to test the zero equality at the begin of the toDouble() method, to be able to return the correctly signed zero ?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-05-05 19:44:09.43,,,false,,,,,,,,,,,,,,150670,,,Thu May 05 19:44:09 UTC 2011,,,,,,0|i0rtpr:,160458,,,,,,,,"05/May/11 19:44;luc;Fixed in subversion repository as of r1099938.

There were conversions problems in both directions! As you noticed, converting from Dfp to double generated infinities, but creating new Dfp(-0.0) did not preserve the sign.

Thanks for the report and the hint for fixing the bug.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect @Deprecated tags added to no-args Solver constructors in org.apache.commons.math.analysis.solvers,MATH-560,12504806,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,nfortescue,nfortescue,20/Apr/11 16:48,23/Jul/12 23:18,07/Apr/19 20:38,13/Aug/11 17:34,2.2,,,,,,,3.0,,,0,,,,,,,,"During refactoring of the solvers, an @Deprecated javadoc comment was added to the no argument constructor for a number of Solvers, saying they would be removed for 3.0. 

This is incorrect - there is no plan to remove those constructors. See the discussion on the user list on 2011-04-20. The @deprecated tag should be removed.

This causes deprecation warnings in previously compliant code, and should not.

This affects at least the following Solvers: MullerSolver, SecantSolver, NewtonSolver. ",,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-08-13 17:34:12.871,,,false,,,,,,,,,,,,,,65080,,,Sat Aug 13 17:34:12 UTC 2011,,,,,,0|i0rtrb:,160465,,,,,,,,"13/Aug/11 17:34;erans;Removed deprecations in revision 1157395.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove ""assert"" from ""MathUtils.equals""",MATH-559,12504038,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,erans,erans,erans,12/Apr/11 11:37,24/Mar/12 16:16,07/Apr/19 20:38,12/Apr/11 11:46,,,,,,,,3.0,,,0,cleanup,,,,,,,"The ""assert"" in methods ""equals(double,double,int)"" and ""equals(float,float,int)"" is not necessary.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150665,,,Tue Apr 12 11:46:31 UTC 2011,,,,,,0|i0rtrj:,160466,,,,,,,,12/Apr/11 11:46;erans;Revision 1091378.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CMAESOptimizer constructor should copy rather than reference input arrays,MATH-556,12503332,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,psteitz,psteitz,04/Apr/11 20:03,23/Jul/12 23:24,07/Apr/19 20:38,05/Apr/11 04:55,,,,,,,,3.0,,,0,,,,,,,,"The CMAESOptimizer constructors take array arguments but do not copy the input arrays.  The current implementation does not write to the input arrays, but client code could, making the API contract ambiguous.  This practice should in general be avoided unless the arrays are meant to be in/out parameters or they are expected to be very large.  Neither of these conditions appear to hold in CMAESOptimizer, so the constructors should be modified to copy the input arrays.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150662,,,Tue Apr 05 04:55:13 UTC 2011,,,,,,0|i0rts7:,160469,,,,,,,,05/Apr/11 04:55;psteitz;Fixed in r1088895.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MathUtils round method should propagate rather than wrap Runitme exceptions,MATH-555,12503246,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,psteitz,psteitz,04/Apr/11 04:13,24/Mar/12 16:16,07/Apr/19 20:38,04/Apr/11 04:53,2.0,2.1,2.2,,,,,3.0,,,0,,,,,,,,"MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150661,,,Mon Apr 04 04:53:13 UTC 2011,,,,,,0|i0rtsf:,160470,,,,,,,,04/Apr/11 04:53;psteitz;Fixed in trunk in r1088473,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vector3D.crossProduct is sensitive to numerical cancellation,MATH-554,12503224,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,03/Apr/11 14:14,24/Mar/12 16:16,07/Apr/19 20:38,03/Apr/11 14:33,2.2,,,,,,,3.0,,,0,,,,,,,,"Cross product implementation uses the naive formulas (y1 z2 - y2 z1, ...). These formulas fail when vectors are almost colinear, like in the following example:
{code}
Vector3D v1 = new Vector3D(9070467121.0, 4535233560.0, 1);
Vector3D v2 = new Vector3D(9070467123.0, 4535233561.0, 1);
System.out.println(Vector3D.crossProduct(v1, v2));
{code}

The previous code displays { -1, 2, 0 } instead of the correct answer { -1, 2, 1 }","Linux, Sun JDK 1.5.0.22",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,34151,,,Sun Apr 03 14:33:56 UTC 2011,,,,,,0|i0rtsn:,160471,,,,,,,,03/Apr/11 14:33;luc;fixed in subversion repository as of r1088316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in class# org.apache.commons.math.dfp.Dfp / org.apache.commons.math.linear.RealVectorwith reproducible JUnit test,MATH-553,12503221,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,,saizhang,saizhang,03/Apr/11 08:10,04/Mar/13 18:58,07/Apr/19 20:38,03/Apr/11 19:16,2.2,,,,,,,,,,0,,,,,,,,"Hi all:


I am writing an automated bug finding tool, and using
Apache Commons Math as an experimental subject
for evaluation.

The tool creates executable JUnit tests as well as
explanatory code comments. I attached one bug-revealing
test as follows. Could you please kindly check it, to
see if it is a real bug or not?

Also, it would be tremendous helpful if you could give
some feedback and suggestion on the quality of generated code comments?
From the perspective of developers who are familiar with the code,
is the automatically-inferred comment useful in understanding
the generated test? is the comment helpful in bug fixing from the
perspective of developers?
Particularly when the automatically-generated tests
are often long.

Your suggestion will help us improve the tool.

Please see attachment for the failed test.

The comment appears in the form of:
//Tests pass if .... (it gives some small change to the test which can make the failed test pass)

For example:

//Test passes if var10 is: (double)<0
java.lang.Double var10 = new java.lang.Double(0.0d);

means if you change var10 to a double value which is < 0 (e.g., -1d), the failed test will pass",jdk 1.6,,,,,,,,,,,,,,,,,,,,03/Apr/11 08:10;saizhang;ApacheMath_Documented_Test.java;https://issues.apache.org/jira/secure/attachment/12475304/ApacheMath_Documented_Test.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-03 19:16:57.743,,,false,,,,,,,,,,,,,,64753,,,Thu Aug 18 01:05:26 UTC 2011,,,,,,0|i0rtsv:,160472,,,,,,,,"03/Apr/11 19:16;luc;Apart from Sebb comments on COLLECTIONS-374, here are a few additional comments for math.

The test is targeted at 2.2 which will probably be the last version in the 2.X series. In this series, we used a mix of Junit 3 and Junit 4 tests. The current code in the trunk for 3.0 has completely switched to Junit 4. It would be nice to have Junit 4 tests rather than Junit 3 tests.

In math, we use a lot of double primitive variables. What about replacing the java.lang.double objects to primitive doubles (and similarly for other primitives types), this would help read the tests.

The failing test for var87.equals(var87) is a false positive. This is a very specific feature of real numbers, some special numbers called NaN (which stands for Not A Number) are equals to nothing, including themselves. So var87.equals(var87) must return false for a NaN. In fact, all boolean predicates on NaN must fail, so if you have two NaNs a and b, the predicates a == b, a != b, a < b, a <= b, a > b, a >= b are ALL false!

This is very specific and I don't expect an automatic tool to be aware of this. Also having a way to configure the tool for such a rare case would really be overkill.

So this bug is in fact not a bug.","03/Apr/11 19:48;saizhang;Thanks Luc.

Seems there is a contradiction between mathematical equality and Java object
equality. OK, I understand
that is very special case.

Could you please check the other test ""test1()"", to see if it is a real bug?

-Sai





-- 

          Thanks

                    Sai
","03/Apr/11 20:27;luc;Test1 one is another strange feature of IEEE754 arithmetic. There are two different encodings for 0: +0 and -0 (i.e. in IEEE754 standard, the sign of 0 is meaningful). Most of the time, -0 and +0 cannot be distinguished, and they evaluate as equals (this is the reason why var19.equals(var44) returns true). However, they do not correspond to the same encoding. -0.0 is encoded as 0x8000000000000000 whereas +0.0 is encoded as 0x0000000000000000 (the most significant bit is the sign bit). This implies that these values are equal but do not have the same hash code.

In your case, var19 is an array containing { 1, 0, 1 }, it was obtained by raising the same numbers to the power 100, which did not affect their sign. On the other hand, var44 is an array containing { 1, -0, 1 }, it was obtained by dividing { -1, 0, -1 } by -1, and this changed all signs, including the sign of the central 0. Hence both arrays contain equals elements, but their hash code are different due to the central element.

This behavior is also produced by the following code:
{code}
double a = Math.pow(0.0, 100.0);
double b = 0.0 / -1.0;
System.out.println(a == b);
System.out.println(""0X"" + Long.toHexString(Double.doubleToLongBits(a)));
System.out.println(""0X"" + Long.toHexString(Double.doubleToLongBits(b)));
{code}

which displays:
{code}
true
0X0
0X8000000000000000
{code}

So test1 is not a bug either.

These two cases are not the only ones to be strange in double arithmetic. There are also encodings for infinity (plus and minus infinity) and there are subnormal numbers (which are almost like standard numbers, only with decreasing accuracy as the numbers vanish towards 0).
","03/Apr/11 20:44;saizhang;Thanks Luc for your detailed explanation. It makes me much clearer on these
cases.

We will carefully consider your suggestion into the design of our tool.

As I mentioned in the post, another feature of this tool is trying to infer
descriptive code
comments to explain failure,  could you please kindly give us some comments
on
such generated comments? like:

//Test passes if var10 is: (double)<0
java.lang.Double var10 = new java.lang.Double(0.0d);

From the viewpoint of developers, do you think such comment is useful for
debugging?
Particularly, if the automatically-generated test itself is long and does
not have good readability. Can such
comment be used as a good start point of diagnose?  We would like to hear
developers'
voice and feedback on such code enhancement, to decide whether to improve
along
that line. (of course,  you are already very familiar with the code base,
and may identify
the failure cause by simply look at the execution trace and assertions.But
intuitively,
we think such comments might be helpful for junior developers)

Thanks a lot. Really appreciate your reply!



","03/Apr/11 20:53;luc;Yes, I think these comments are helpful.
They give an hint about what to look at. In both cases, I add to run the test in a debugger to see exactly what happened but the comment showed me exactly the variable to look at. I don't know how you generate these comments, would it be possible to have both the information about the values that make the test pass and the value that did make the test fail ? This would allow the reader to simply compare these values without having to run a debugger by himself. If this is not possible, the current comments already provide valuable information.","03/Apr/11 21:06;saizhang;Thanks Luc for your comments.

I used a statistical algorithm to summarize the properties of a set of
""failure-correcting"" objects. A technical report
describing the technique is on its way.

Yes, it is possible to have both an example value which make the test pass
(we have already implemented it
in our tool). For the reported test, the generated comment is: (after
appending an example value):

//Test passes if var10 is: (double)<0, for example, -1.0d
 java.lang.Double var10 = new java.lang.Double(0.0d);

The value making the test fail is already shown in the test (var10 = 0.0d in
the above test).








-- 

          Thanks

                    Sai
","18/Aug/11 01:05;saizhang;Hi Luc:

(sorry for the late reply. I thought my previous email was sent out, but
found it
still lies in my draft box)

I agree that  it does not make sense to compare 2 NaNs. However,  I think
two Java objects can still be compared, and the code maybe improved to
avoid violation of  the Java specification.

For the problem revealed in the test case, I suggest to fix this bug by
adding a comparison to
*this* object itself. Doing so will avoid   ""var.equals(var)"" returns
false. and prevent Java containers becoming unstable, when they
are containing a Dfp object.

A suggested fix is:

{code}
Class: org.apache.commons.math.dfp.Dfp

@Override
    public boolean equals(final Object other) {
          if(this == other) {
               return true;
          }
          ....// all existing code goes here
   }
{code}


For the other case, when comparing RealVector { 1, 0, 1 }  and {1, -0, 1},
 it seems that
the equals method uses ""numeric comparison"" to conclude that  0 == -0.
However,
when computing their hashcodes, the code implicitly treats ""0"" as an object,
and use
Arrays.hashcode to get the results (0, -0 are 2 different objects).

Therefore, I suggest to use a *consistent* way for comparison and computing
hashcodes. It is
completely feasible to fix the above problem. Here are 2 possible fixes:

1.  change the ArrayRealVector#equals, lines 1207 - 1211

{code}
for (int i = 0; i < data.length; ++i) {
        if (data[i] != rhs.getEntry(i)) {      //do not use numeric
comparison, convert both sides
          return false;                               //into Double type,
and use Double.equals to compare
        }
      }
{code}

2.  change the ways to compute hashcode in: ArrayRealVector#hashCode

Do not use {code}Arrays.hashCode(value);{code} to get hashcode, since it
will implicitly
boxes each primitive values. Instead, use a method like below to compute
hashcode numerically:

{code}
public static int hash(double[] value) {
        int retHashCode = 0;
        for(double v : value) {
             retHashCode +=  retHashCode*3 + 17*v;
        }
       return retHashCode;
    }
{code}

I think either way above will fix this problem (though it may be not that
important), and keep
the code behavior consistent!

Thanks

-Sai





-- 

Sai Zhang

http://www.cs.washington.edu/homes/szhang/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultidimensionalCounter.getCounts(int) returns wrong array of indices,MATH-552,12503079,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,jbence@pobox.com,jbence@pobox.com,31/Mar/11 22:36,24/Mar/12 16:16,07/Apr/19 20:38,01/Apr/11 10:12,2.2,,,,,,,3.0,,,0,,,,,,,,"MultidimensionalCounter counter = new MultidimensionalCounter(2, 4);
for (Integer i : counter) {
    int[] x = counter.getCounts(i);
    System.out.println(i + "" "" + Arrays.toString(x));
}

Output is:
0 [0, 0]
1 [0, 1]
2 [0, 2]
3 [0, 2]   <=== should be [0, 3]
4 [1, 0]
5 [1, 1]
6 [1, 2]
7 [1, 2]   <=== should be [1, 3]",Java 1.6,,,,,,,,,,,,,,,,,,,,31/Mar/11 23:16;jbence@pobox.com;MultidimensionalCounter.patch;https://issues.apache.org/jira/secure/attachment/12475168/MultidimensionalCounter.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-04-01 10:12:29.94,,,false,,,,,,,,,,,,,,34213,,,Fri Apr 01 10:12:29 UTC 2011,,,,,,0|i0rtt3:,160473,,,,,,,,"31/Mar/11 22:39;jbence@pobox.com;To resolve this, replace lines 198 through 204:

int idx = 1;
while (count < index) {
   count += idx;
   idx += 1;
}
--idx;
indices[last] = idx;

with the following:


     indices[last] = index - count;

",31/Mar/11 22:42;jbence@pobox.com;I am working on a patch and some tests.,"31/Mar/11 23:16;jbence@pobox.com;This patch file patches the corresponding test to reveal the problem, and then patches the MultidimensionalCounter class to fix the problem.","01/Apr/11 10:12;erans;Oops, I wrote this code :(. Many thanks for finding out and fixing that bug.
Patch applied in revision 1087637.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove ""SerializablePair""",MATH-551,12502524,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,erans,erans,erans,27/Mar/11 21:40,24/Mar/12 16:16,07/Apr/19 20:38,27/Mar/11 21:51,,,,,,,,3.0,,,0,cleanup,,,,,,,"Class ""SerializablePair"" is not serializable. And rendering it so is outside the scope of CM. It was created as an implementation detail for enabling the serialization of ""MathRuntimeException"" objects. This class will thus be modified so that it will not need ""SerializablePair"" objects.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150660,,,Sun Mar 27 21:51:45 UTC 2011,,,,,,0|i0rttb:,160474,,,,,,,,"27/Mar/11 21:51;erans;Revision 1086045.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Truncation issue in KMeansPlusPlusClusterer,MATH-546,12501227,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,npaymer,npaymer,12/Mar/11 04:52,24/Mar/12 16:16,07/Apr/19 20:38,16/Mar/11 12:58,3.0,,,,,,,3.0,,,0,cluster,,,,,,,"The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable
  int sum = 0;
This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1.

As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.",,,,,,,,,,,,,,,,,,,,,13/Mar/11 13:13;npaymer;MATH-546.txt;https://issues.apache.org/jira/secure/attachment/12473504/MATH-546.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-15 12:22:16.071,,,false,,,,,,,,,,,,,,150657,,,Tue Mar 15 12:31:46 UTC 2011,,,,,,0|i0rtuf:,160479,,,,,,,,"13/Mar/11 13:13;npaymer;I've a patch to fix this bug.

This is my first contribution to this project, so apologies if I've screwed something up :)","15/Mar/11 12:22;erans;Fixed in revision 1081744.
Thanks for the report and the patch.

Leaving open until an answer can be provided concerning the ""EmptyClusterStrategy"" question.
","15/Mar/11 12:31;luc;The empty cluster strategy is needed regardless of this bug. It may appear with different conditions and is a feature commonly found in clustering implementations.
This issue can be marked as resolved if the patch has been applied and works.

Thanks to Nate for reporting and fixing the issue, thanks to Gilles for reviewing and applying the patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractIntegerDistribution.inverseCumulativeProbability(...) Bug,MATH-540,12500531,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ole,ole,06/Mar/11 00:43,24/Mar/12 16:16,07/Apr/19 20:38,12/Jun/11 05:58,2.1,,,,,,,3.0,,,0,,,,,,,,The AbstractIntegerDistribution.inverseCumulativeProbability(...) function attempts to decrement the lower bound of discrete distributions to values that go below the lower bound.,,,,,,,,,,,,,,,,,,,,,06/Mar/11 00:45;ole;DummyDiscreteDistribution.java;https://issues.apache.org/jira/secure/attachment/12472763/DummyDiscreteDistribution.java,06/Mar/11 00:45;ole;DummyDiscreteDistributionTest.java;https://issues.apache.org/jira/secure/attachment/12472764/DummyDiscreteDistributionTest.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-06 05:32:11.815,,,false,,,,,,,,,,,,,,71391,,,Sun Jun 12 05:58:50 UTC 2011,,,,,,0|i0rtvr:,160485,,,,,,,,"06/Mar/11 05:32;psteitz;I don't think this is a bug.  Per the javadoc, the contract for inverse cum is
{code}
/**
 * For a random variable {@code X} whose values are distributed according
 * to this distribution, this method returns the largest {@code x}, such
 * that {@code P(X < x) < p}.
{code}

This implies that if the first non-zero mass point has probability greater than p, the right value to return is one less than that value, which is whet the method will do.  Your example distribution throws NPE when trying to compute probabilities outside of its domain of support. 
","06/Mar/11 05:58;ole;I'm looking at it like this.  I have very simple distribution like the one provided (Four sided dice).  I'm trying to write a simulation that draws values of x for a a set of uniform 0-1 probabilities.  So I'm expecting:

0 When p is less than or equal to 0.25
1 When p is greater than 0.25 but less than or equal to 0.50
2 When p is greater than 0.50 but less than or equal to 0.75
3 When p is greater than 0.75 but less than or equal to 1.0

So for the line 

int neverSucceeds = d.inverseCumulativeProbability(0.0001);

I'm really expecting 0 to be returned.

Make sense?","06/Mar/11 06:39;psteitz;I see now that there actually does appear to be an error in the javadoc.  The implementation really returns the largest x such that p(X <= x) <= p.  In the discrete case, <= matters and I think both inequalities in the javadoc should be changed.

In your example, if the probability distribution vanishes outside 0, 1, 2, 3 and puts .25 mass on each of these values, the inverse cumulative probability function evaluated at .0001 should be -1, as this is the largest value such that 
p(X <= x) <= .0001.

If you fix your distribution so that both probability and cumulativeProbability return correct values (rather than throwing NPEs) outside of the mass values, you should get -1 returned.","06/Mar/11 15:16;psteitz;Reading your last comment a little more carefully, it looks like what you are trying to do is implement sampling.  IIUC, something like what you are suggesting should work - you just have an off-by-one problem vis-s-vis the contract of inverse cumulative probabilities as we define them.  I would be +1 for adding direct support for sampling from discrete distributions, but we should open a separate ticket for that.",06/Mar/11 17:36;ole;OK - I'll close this one and open a separate ticket.,06/Mar/11 22:05;psteitz;There is a javadoc bug that needs to be fixed here,"07/Mar/11 03:41;ole;Ooops - Thanks.  

{quote}
...inverse cumulative probability function evaluated at .0001 should be -1, as this is the largest value such that 
p(X <= x) <= .0001.
{quote}

It seems to me that users would be better served if it returned 0 and that it is also correct to do so.

In the definition we say ""For a random variables X whose values are distributed according to this distribution..."".

Suppose the distribution was for a six sided dice.  One could assert that the distribution is only defined for the values 1,2,3,4,5,6.  In this case the inverseCumulativeDistribution returns 0, but that does not have any meaning.  So now developers are forced to define the meaning of 0 for a six sided dice implementation.  

In Grad school we were taught the the inverse cumulative distribution is for sampling.  So for a six sided dice uniform probabilities less than 1/6 would return 1, less than 2/6 would return 2, etc.

With the current implementation for values less than 1/6 we get 0 which is meaningless, and the only time we get 6 is when the uniform probability argument is 1.

So if someone mistakenly tries to use the inverseCumulativeProbability function for sampling the results are going to be wacked.  What is the use case for the inverseCumulativeProbability the way it is right now?",07/Mar/11 14:39;psteitz;You have a choice in defining the inverse cum whether to define it the way we have or to use and inf rather than a sup.  We can implement sampling using the current impl.  We just need to take into account the way the inverse cum is defined in AbstractIntegerDistribution.  ,"07/Mar/11 16:10;ole;OK - I think it's starting to make more sense to me now.  So when implementing sampling we just add one to the value returned by inverseCumulativeDistribution, unless the uniform probability argument is 1?",07/Mar/11 17:53;psteitz;I am sorry.  I forgot that we had in fact already implemented this in version 2.2. See AbstractIntegerDistribution#sample.  The base class implementation delegates to RandomDataImpl#nextInversionDeviate (adding one per the last comment).,07/Mar/11 20:38;psteitz;Sorry for the noise. I closed the wrong ticket.  Still need to fix the javadoc to match behavior and user guide.,12/Jun/11 05:58;psteitz;Javadoc fixed in trunk r1134866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removing some packages that are not needed in some classes,MATH-539,12500525,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Not A Problem,,miccagiann,miccagiann,05/Mar/11 21:21,23/Mar/11 20:44,07/Apr/19 20:38,06/Mar/11 01:52,3.0,,,,,,,,,,0,cleanup,patch,,,,,,"Well,

Looking at the junit test files of CM project i noticed that in some of them are imported some classes that are never used so i decided to remove them... Here are the patches!",Ubuntu Linux 10.04 (lucid),,,,,,,,,,,,,,,,,,,,05/Mar/11 21:22;miccagiann;CorrelatedRandomVectorGeneratorTest_Patch.txt;https://issues.apache.org/jira/secure/attachment/12472759/CorrelatedRandomVectorGeneratorTest_Patch.txt,05/Mar/11 21:23;miccagiann;UncorrelatedRandomVectorGeneratorTest_Patch.txt;https://issues.apache.org/jira/secure/attachment/12472760/UncorrelatedRandomVectorGeneratorTest_Patch.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-06 01:52:29.55,,,false,,,,,,,,,,,,,,150651,,,Wed Mar 23 20:44:40 UTC 2011,,,,,,0|i0rtvz:,160486,,,,,,,,"06/Mar/11 01:52;sebb@apache.org;These imports are currently needed.

By the way, please don't use Eclipse workspace-relative patches. They only work if everyone uses exactly the same name for projects.

Project-relative patches can be used by anyone.","06/Mar/11 03:04;erans;I don't see those imports in ""trunk"" as of revision 1078400.
",23/Mar/11 20:44;luc;Closing an issue that was not a problem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] org.apache.commons.math.ode.DerivativeException constuctor should be deprecated,MATH-538,12500015,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,01/Mar/11 07:01,24/Mar/12 16:16,07/Apr/19 20:38,05/Jun/11 11:19,2.2,,,,,,,3.0,,,0,cleanup,"patch,",,,,,,"In class 'org.apache.commons.math.ode.DerivativeException' the public method (constructor) 'public DerivativeException(final String specifier, final Object ... parts)' should be deprecated since 'public DerivativeException(final Localizable specifier, final Object ... parts)' constructor exists (since version 2.2...)...",Ubuntu Linux (lucid) 10.04,,,,,,,,,,,,,,,,,,,,01/Mar/11 07:03;miccagiann;DerivativeException.java;https://issues.apache.org/jira/secure/attachment/12472288/DerivativeException.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-01 09:42:58.226,,,false,,,,,,,,,,,,,,71920,,,Sun Jun 05 11:19:49 UTC 2011,,,,,,0|i0rtw7:,160487,,,,,,,,01/Mar/11 07:03;miccagiann;Here is the patch...,"01/Mar/11 09:42;luc;Thanks Michael.
I'm not sure this one will be fixed as a 6th release candidate for 2.2 as already been frozen and the vote is ongoing.
If the 6th release candidate fails, we will certainly include the fix for the next release candidate.
If the 6th release candidate succeds, the issue will be closed as ""won't fix"" since the DerivativeException has been removed in 3.0 and is unlikely to reappear.",05/Jun/11 11:19;luc;The DerivativeException class has been removed in 3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] org.apache.commnons.math.linear.MatrixVisitorException constructor should be deprecated,MATH-537,12500014,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,01/Mar/11 06:48,24/Mar/12 16:16,07/Apr/19 20:38,05/Jun/11 11:17,2.2,,,,,,,3.0,,,0,cleanup,patch,,,,,,"In the class org.apache.commons.math.linear.MatrixVisitorException the method ""public MatrixVisitorException(final String pattern, final Object[] arguments)"" should be deprecated, since method ""public MatrixVisitorException(final Localizable pattern, final Object[] arguments)"" already exists from version 2.2...",Linux Ubuntu 10.04 (lucid),,,,,,,,,,,,,,,,,,,,01/Mar/11 06:50;miccagiann;MatrixVisitorException.java;https://issues.apache.org/jira/secure/attachment/12472287/MatrixVisitorException.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-01 09:42:03.001,,,false,,,,,,,,,,,,,,71921,,,Sun Jun 05 11:17:56 UTC 2011,,,,,,0|i0rtwf:,160488,,,,,,,,01/Mar/11 06:50;miccagiann;Here is the patch...,"01/Mar/11 09:42;luc;Thanks Michael.
I'm not sure this one will be fixed as a 6th release candidate for 2.2 as already been frozen and the vote is ongoing.
If the 6th release candidate fails, we will certainly include the fix for the next release candidate.
If the 6th release candidate succeds, the issue will be closed as ""won't fix"" since the MatrixVisitorException has been removed in 3.0 and is unlikely to reappear.
By the way, instead of providing a full class for short fixes, you can provide just a patch (you can create them with the unix diff command, or with svn diff, or with Eclipse team->create path menu entry).",05/Jun/11 11:17;luc;The MatrixVisitorException class has been removed in 3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] Clirr report and 'org.apache.commons.math.util.ResizableDoubleArray',MATH-533,12499379,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,22/Feb/11 18:08,23/Mar/11 20:43,07/Apr/19 20:38,22/Feb/11 19:53,2.2,,,,,,,2.2,,,0,documentation,,,,,,,"Clirr reports mention that method 'public void addElements(double[])' has been added, however there is no ""@since 2.2"" indication in the javadoc of this function...",Linux Ubuntu 10.04 (lucid),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-22 19:53:39.094,,,false,,,,,,,,,,,,,,150647,,,Wed Mar 23 20:43:40 UTC 2011,,,,,,0|i0rtxb:,160492,,,,,,,,22/Feb/11 19:53;luc;fixed in subversion repository as of r1073474 for branch 2.X and r1073475 for trunk,"23/Mar/11 20:43;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] Clirr report and 'org.apache.commons.math.util.MathUtils',MATH-532,12499377,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,22/Feb/11 17:57,23/Mar/11 20:43,07/Apr/19 20:38,22/Feb/11 19:50,2.2,,,,,,,2.2,,,0,documentation,,,,,,,"Clirr report mentions that method 'public void checkOrder(double[], org.apache.commons.math.util.MathUtils$OrderDirection, boolean)' has been added, however this doesn't comply with the javadoc of the very same function (because not ""@since 2.2"" tag was found). The same thing happens with the methods listed bellow:
'public void checkOrder(double[])'
'public boolean equals(float, float, float)'
'public boolean equals(float, float, int)'
'public boolean equalsIncludingNaN(float, float)'
'public boolean equalsIncludingNaN(float, float, float)'
'public boolean equalsIncludingNaN(float, float, int)'
'public boolean equalsIncludingNaN(float[], float[])'
'public boolean equalsIncludingNaN(double, double)'
'public boolean equalsIncludingNaN(double, double, double)'
'public boolean equalsIncludingNaN(double, double, int)'
'public boolean equalsIncludingNaN(double[], double[])'
'public double safeNorm(double[])'

In addition to this some functions have been deprecated but neither is it mentioned in the javadoc the version as of which they have been deprecated nor the clirr report refers to these methods. These are:
Deprecated Methods:
'public boolean equals(float, float)'
'public boolean equals(float[], float[])'",Linux Ubuntu 10.04 (lucid),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-22 19:50:19.481,,,false,,,,,,,,,,,,,,150646,,,Wed Mar 23 20:43:23 UTC 2011,,,,,,0|i0rtxj:,160493,,,,,,,,"22/Feb/11 18:01;miccagiann;What is more, the enumeration 'public static enum OrderDirection' that is included in the same class doesn't mentions in its javadoc a ""@since 2.2"" tag, although clirr reports state that the enum is added in the 2.2 version of ASF Commons Math project...",22/Feb/11 19:50;luc;fixed in subversion repository as of r1073472 for branch 2.X and r1073473 for trunk,"23/Mar/11 20:43;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] Clirr report and 'org.apache.commons.math.stat.regression.OLSMultipleLinearRegression',MATH-531,12499371,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,22/Feb/11 17:19,23/Mar/11 20:43,07/Apr/19 20:38,22/Feb/11 19:36,2.2,,,,,,,2.2,,,0,documentation,,,,,,,"All the methods that are marked as added in the above class in the clirr report haven't ""@since 2.2"" tag on their javadocs respectively... ",Linux Ubuntu 10.04 (lucid),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-22 19:36:15.568,,,false,,,,,,,,,,,,,,150645,,,Wed Mar 23 20:43:04 UTC 2011,,,,,,0|i0rtxr:,160494,,,,,,,,22/Feb/11 19:36;luc;fixed in subversion repository as of r1073464 for branch 2.X and r1073466 for trunk,"23/Mar/11 20:43;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] Clirr report and 'org.apache.commons.math.stat.regression.GLSMultipleLinearRegression,MATH-530,12499365,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,22/Feb/11 16:58,23/Mar/11 20:42,07/Apr/19 20:38,22/Feb/11 19:30,2.2,,,,,,,2.2,,,0,documentation,,,,,,,"Clirr report mentions that method 'protected double calculateErrorVariance()' has been added... However this addition isn't reflected to the javadoc with the ""@since 2.2"" clause.",Linux Ubuntu 10.04 (lucid),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-22 19:30:50.747,,,false,,,,,,,,,,,,,,150644,,,Wed Mar 23 20:42:46 UTC 2011,,,,,,0|i0rtxz:,160495,,,,,,,,22/Feb/11 19:30;luc;fixed in subversion repository as of r1073450 for branch 2.X and r1073463 for trunk,"23/Mar/11 20:42;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] Clirr report and 'org.apache.commons.math.stat.regression.AbstractMultipleLinearRegression',MATH-529,12499355,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,22/Feb/11 15:37,23/Mar/11 20:42,07/Apr/19 20:38,22/Feb/11 19:19,2.2,,,,,,,2.2,,,0,documentation,,,,,,,"Clirr report states that method 'protected double calculateErrorVariance()' has been added, but in the javadoc no ""@since 2.2"" tag found...",Linux Ubuntu 10.04 (lucid),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-22 19:19:51.487,,,false,,,,,,,,,,,,,,150643,,,Wed Mar 23 20:42:28 UTC 2011,,,,,,0|i0rty7:,160496,,,,,,,,"22/Feb/11 15:43;miccagiann;Also check out the following methods: 'public double estimateErrorVariance()', 'public double estimateRegressionStandardError()', 'public boolean isNoIntercept()'and 'public void setNoIntercept(boolean)'... Thanks in advance! ",22/Feb/11 19:19;luc;fixed in subversion repository as of r1073459 for branch 2.X and r1073458 for trunk,"23/Mar/11 20:42;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] Clirr report and 'org.apache.commons.math.stat.StatUtils',MATH-527,12499311,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,22/Feb/11 07:51,23/Mar/11 20:42,07/Apr/19 20:38,22/Feb/11 09:35,2.2,,,,,,,2.2,,,0,documentation,,,,,,,"While clirr report mentions that method 'public double[] normalize(double[])' has been added in the 'org.apache.commons.math.stat.StatUtils' class, javadoc doesn't comply with this change (i think should mention ""@since 2.2"")... Thanks for your time!",Linux Ubuntu 10.04 (lucid),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-22 09:35:50.134,,,false,,,,,,,,,,,,,,150642,,,Wed Mar 23 20:42:16 UTC 2011,,,,,,0|i0rtyn:,160498,,,,,,,,22/Feb/11 09:35;luc;fixed in subversion repository as of r1073276,"23/Mar/11 20:42;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] Clirr report and 'org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizer',MATH-526,12499310,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,22/Feb/11 07:37,23/Mar/11 20:42,07/Apr/19 20:38,22/Feb/11 09:29,2.2,,,,,,,2.2,,,0,documentation,,,,,,,"Clirr report mentions that method 'public void setQRRankingThreshold(double)' has been added in this specific class but the javadoc hasn't been updated so as to mention ""@since 2.2""... Have a look at this one please and thanks for your time!",Linux Ubuntu 10.04 (lucid),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-22 09:29:56.675,,,false,,,,,,,,,,,,,,150641,,,Wed Mar 23 20:42:00 UTC 2011,,,,,,0|i0rtyv:,160499,,,,,,,,22/Feb/11 09:29;luc;fixed in subversion repository as of r1073272,"23/Mar/11 20:42;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] Clirr report and 'org.apache.commons.math.optimization.fitting.PolynomialFitter',MATH-525,12499309,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,22/Feb/11 07:34,23/Mar/11 20:41,07/Apr/19 20:38,22/Feb/11 09:20,2.2,,,,,,,2.2,,,0,documentation,,,,,,,"In this one clirr report mentions that method 'public void clearObservations()' has been added, however the javadoc of this function in this class doesn't have an ""@since 2.2"" tag... Please have a look at this! Thanks a lot!",Linux Ubuntu 10.04 (lucid),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-22 09:20:41.297,,,false,,,,,,,,,,,,,,150640,,,Wed Mar 23 20:41:31 UTC 2011,,,,,,0|i0rtz3:,160500,,,,,,,,22/Feb/11 09:20;luc;fixed in subversion repository as of r1073270,"23/Mar/11 20:41;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] Clirr report and 'org.apache.commons.math.ode.AbstractIntegrator',MATH-524,12499308,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,22/Feb/11 07:31,23/Mar/11 20:40,07/Apr/19 20:38,22/Feb/11 09:17,2.2,,,,,,,2.2,,,0,documentation,,,,,,,"I have noticed that clirr report mentions that method 'protected double acceptStep(org.apache.commons.math.ode.sampling.AbstractStepInterpolator, double[], double[], double)' and 'protected void setStateInitialized(boolean)' have been added... However the javadoc for those 2 functions doesn't mention anything about ""@since 2.2""... Please check this out!!! Thanks a lot for your attention and your time!",Linux Ubuntu 10.04 LTS (lucid),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-22 09:17:45.155,,,false,,,,,,,,,,,,,,150639,,,Wed Mar 23 20:40:51 UTC 2011,,,,,,0|i0rtzb:,160501,,,,,,,,22/Feb/11 09:17;luc;fixed in subversion repository as of r1073266 ,"23/Mar/11 20:40;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] Clirr report and 'org.apache.commons.math.linear.OpenMapRealVector',MATH-523,12499306,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,22/Feb/11 07:28,23/Mar/11 20:40,07/Apr/19 20:38,22/Feb/11 09:16,2.2,,,,,,,2.2,,,0,documentation,,,,,,,"I have noticed that the clirr report mentions that method 'public double getSparcity()' has been deprecated whereas method 'public double getSparsity()' has been added... However in the class 'org.apache.commons.math.linear.OpenMapRealVector' the javadoc of the first function doesn't mention in which version has been deprecated, while the javadoc of the second function doesn't mention when the second one has been added... I believe that this is a bug... Please have a look at this one... Thanks a lot!",Linux Ubuntu 10.04 LTS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-22 09:16:37.074,,,false,,,,,,,,,,,,,,150638,,,Wed Mar 23 20:40:33 UTC 2011,,,,,,0|i0rtzj:,160502,,,,,,,,22/Feb/11 09:16;luc;fixed in subversion repository as of r1073263 ,"23/Mar/11 20:40;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] Clirr report and MatrixIndexException,MATH-522,12499305,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,miccagiann,miccagiann,22/Feb/11 07:25,23/Mar/11 20:40,07/Apr/19 20:38,22/Feb/11 09:15,2.2,,,,,,,2.2,,,0,documentation,,,,,,,"In the Clirr report is mentioned that the function 'MatrixIndexException(org.apache.commons.math.exception.util.Localizable, java.lang.Object[])' has been added to the class 'org.apache.commons.math.linear.MatrixIndexException'... However the javadoc doesn't comply with this change as it is stated: ""@since 2.0""... Please check out this issue so as to be clear... Thanks for your time!",Linux Ubuntu 10.04 LTS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-22 09:15:33.548,,,false,,,,,,,,,,,,,,150637,,,Wed Mar 23 20:40:15 UTC 2011,,,,,,0|i0rtzr:,160503,,,,,,,,22/Feb/11 09:15;luc;fixed in subversion repository as of r1073255 ,"23/Mar/11 20:40;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GaussianFitter Unexpectedly Throws NotStrictlyPositiveException,MATH-519,12499105,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ole,ole,19/Feb/11 04:37,24/Mar/12 16:16,07/Apr/19 20:38,22/Feb/11 23:51,3.0,,,,,,,3.0,,,0,,,,,,,,"Running the following:

    	double[] observations = 
    	{ 
    			1.1143831578403364E-29, 
    			 4.95281403484594E-28, 
    			 1.1171347211930288E-26, 
    			 1.7044813962636277E-25, 
    			 1.9784716574832164E-24, 
    			 1.8630236407866774E-23, 
    			 1.4820532905097742E-22, 
    			 1.0241963854632831E-21, 
    			 6.275077366673128E-21, 
    			 3.461808994532493E-20, 
    			 1.7407124684715706E-19, 
    			 8.056687953553974E-19, 
    			 3.460193945992071E-18, 
    			 1.3883326374011525E-17, 
    			 5.233894983671116E-17, 
    			 1.8630791465263745E-16, 
    			 6.288759227922111E-16, 
    			 2.0204433920597856E-15, 
    			 6.198768938576155E-15, 
    			 1.821419346860626E-14, 
    			 5.139176445538471E-14, 
    			 1.3956427429045787E-13, 
    			 3.655705706448139E-13, 
    			 9.253753324779779E-13, 
    			 2.267636001476696E-12, 
    			 5.3880460095836855E-12, 
    			 1.2431632654852931E-11 
    	};
  
    	GaussianFitter g = 
    		new GaussianFitter(new LevenbergMarquardtOptimizer());
    	
    	for (int index = 0; index < 27; index++)
    	{
    		g.addObservedPoint(index, observations[index]);
    	}
       	g.fit();

Results in:

org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0)
	at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184)
	at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129)


I'm guessing the initial guess for sigma is off.  ",,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,19/Feb/11 22:12;erans;GaussianFitter.java;https://issues.apache.org/jira/secure/attachment/12471493/GaussianFitter.java,19/Feb/11 18:03;ole;GaussianFitter2Test.java;https://issues.apache.org/jira/secure/attachment/12471478/GaussianFitter2Test.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-02-19 12:28:41.507,,,false,,,,,,,,,,,,,,150635,,,Tue Feb 22 23:51:58 UTC 2011,,,,,,0|i0ru0f:,160506,,,,,,,,"19/Feb/11 12:28;erans;Will you investigate?
A good starting point would be to prepare a unit test.

If it's indeed the guesser, we should add a check that the guessed sigma is positive (and, if not, return the opposite value).
",19/Feb/11 13:48;ole;Yes I'm going to start looking at it this weekend.,"19/Feb/11 17:38;ole;Gilles - BTW - Did you ever figure out what was happening with this test in the ParametricGaussianFunction test:

    /**
     * The parameters d is 0.
     *
     * @throws MathUserException in the event of a test case error
     */
    @Test(expected=ZeroException.class)
    public void testValue03() throws MathUserException {
        Gaussian.Parametric f = new Gaussian.Parametric();
        f.value(0.0, new double[] {1.0, 1.0, 0.0});
    }

",19/Feb/11 18:13;ole;It seems the optimizer is producing a negative value for sigma...,"19/Feb/11 18:38;ole;I tried commenting out the validateParameters(param) line in Gaussian.value() and now it converges:

Mean: 53.15727927329495
Sigma: 5.752146229144571","19/Feb/11 22:12;erans;I guess that the clean solution would be to be able to specify constraints such that the optimizer won't try invalid parameters.

An ugly workaround would be that the ""CurveFitter"" internally catches all ""RuntimeException""s generated by the function, and consider the residuals to be infinite. But this would violate the stated policy that CM does not catch ""MathUserException""s (and could have nasty side-effects when the exception is really unexpected).

I've tried another workaround in ""GaussianFitter"" (see attached file) whereby an invalid parameter is turned into the function returning ""NaN"" (""POSITIVE_INFINITY"" also works).
Let me know if you see any issues with this solution or if would be an adequate solution for this problem.
","19/Feb/11 23:52;ole;I like the last option the best.  I'm wondering though whether we are placing constraints on variables that are not supposed to be constrained?

Perhaps the optimizer should be allowed to call a method on Gaussian that does not effectively constrain the valid values of sigma?

","20/Feb/11 18:42;erans;In the case of the Gaussian, that parameter (""sigma"") is used squared, so negative values don't really matter; however, the semantics of ""sigma"" calls for it to be non-negative.
But it certainly cannot be zero, so that's a constraint that cannot be removed.
","21/Feb/11 01:53;ole;I think that while the optimizer is searching for a solution, it should be able to pass parameters that are not necessarily valid, but enable it to proceed.  If it finds a solution that is not valid, then an exception should be thrown. 

","21/Feb/11 02:25;erans;{quote}
[...] pass parameters that are not necessarily valid [...]
{quote}

What would that mean?
To direct its search, the optimizer needs feedback from the objective function; in case of invalid parameters, the objective function cannot provide feedback (i.e. compute the theoretical curve)!

The only issue is how to ""tell"" the optimizer that it should discard invalid parameters (i.e. make it consider that they are far away from a solution).
","21/Feb/11 04:01;ole;I'm assuming that the value that results from passing in a negative sigma is closer to the optimal than POSITIVE infinity or NaN, and that this will result in faster convergence.  How about only returning Nan or POSITIVE_INFINITY if the optimizer passes in zero for sigma, but letting it proceed otherwise?


","21/Feb/11 08:27;luc;Maybe this discussion should be held on the dev list since it becomes long.
Anyway, returning NaN or POSITIVE_INFINITY would work only with some optimizers.
I guess a proper solution would be to have constrained optimization available (see MATH-196).
For simple bounds on estimated parameters, this can be done using intermediate variables and mapping functions, but for general non-linear constraints, we need Lagrangian multipliers and all this stuff.","21/Feb/11 12:18;erans;{quote}
I'm assuming that the value that results from passing in a negative sigma is closer to the optimal than POSITIVE infinity or NaN [...]
{quote}

POSITIVE_INFINITY or NaN are the returned values of the objective function and its gradient, specially chosen because they are quite likely to be different from the actual values of the function and its gradient; it's not ""sigma"" that is assumed to infinity or NaN.

If you think is that we can accept a negative sigma as the result of the fitting, I don't agree. In the case of the Gaussian, it's by ""chance"" that a semantically invalid parameter (negative sigma) would still be usable (as it is being squared before use).
In most case you cannot expect such a forgiving situation. For example, if you want to fit ""a"" in the following function:
{noformat}
  log(a * x)
{noformat}
no invalid values for ""a"" are usable.
The ""Gaussian"" class should not be unsafe (no validation of sigma) just because of its particular use here.
[Moreover the workaround is useful in showing users how to setup a fitting of a function that can raise an exception.]

{quote}
[...] and that this will result in faster convergence.
{quote}

Did you try?
","21/Feb/11 20:21;ole;Yes!  I figured out how to quote!

First of all I hope we are talking about this function:
http://en.wikipedia.org/wiki/Gaussian_function

As the objective function right?  If I got that wrong then ignore the below.

{quote}
Anyway, returning NaN or POSITIVE_INFINITY would work only with some optimizers.
{quote}

Seems to me that if the optimizer does not understand POSITIVE_INFINITY then that's a bug.  

{quote}
If you think is that we can accept a negative sigma as the result of the fitting...
{quote}

No no no - Not at all.  I'm saying that we should let the optimizer try negative values for sigma if it wants to while it's in the middle of trying to find the optimal sigma.  If it returns a negative sigma as a result, then we need to throw a NotStrictlyPositiveException.

{quote}
Did you try?
{quote}

I could give it a whirl, but it does not necessarily prove anything.  Even it it converges quicker, does that mean it will do so in all cases?  It just seems to me like POSITIVE_INFINITE is as far from the optimal as you can get, and therefore it will take longer to get to the optimal.

Also, I changed my mind about an earlier comment.  If sigma is zero then the gaussian function is zero, so we should probably just return zero.
","21/Feb/11 22:14;erans;{quote}
I'm saying that we should let the optimizer try negative values for sigma if it wants to while it's in the middle of trying to find the optimal sigma.
{quote}

The point is that we cannot allow invalid parameters because, for those values (of the parameters), the objective function is, by definition of ""invalid"", undefined.

{quote}
[...] Even it it converges quicker [...]
{quote}

Well, actually it doesn't (cf. my mail on the ""dev"" ML).

{quote}
It just seems to me like POSITIVE_INFINITE is as far from the optimal as you can get, [...]
{quote}

Indeed, that's exactly the intention: it tells the optimizer to step back from this wrong value for sigma.
[Note that POSITIVE_INFINITY is not a value of sigma, it is the value of the objective function for any negative sigma.]

{quote}
If sigma is zero then the gaussian function is zero, [...]
{quote}

No, when sigma is strictly zero, there is no Gaussian anymore: the value at ""b"" (mean) is undefined.
Also, cf. http://en.wikipedia.org/wiki/Dirac_delta

",21/Feb/11 23:04;ole;OK - I see what you mean - I also get 17009 iterations with just the validate line commented out.  I'm on board with POSITIVE_INFINITY.    ,"22/Feb/11 08:56;luc;If this works, go for it.",22/Feb/11 23:51;erans;Workaround in revision 1073554.,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The static field ChiSquareTestImpl.distribution serves no purpose,MATH-506,12497371,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,psteitz,sebb@apache.org,sebb@apache.org,01/Feb/11 18:38,24/Mar/12 16:16,07/Apr/19 20:38,20/Aug/11 21:14,1.2,2.0,2.1,2.2,,,,3.0,,,0,,,,,,,,"The static field ChiSquareTestImpl.distribution serves no purpose.

There is a setter for it, but in every case where the field is used, it is first overwritten with a new value.

The field and the setter should be removed, and the methods that create a new instance should create a local variable instead.

For Math 2.1, the field can be removed and the setter deprecated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-05 02:56:06.34,,,false,,,,,,,,,,,,,,64321,,,Sat Aug 20 21:14:57 UTC 2011,,,,,,0|i0ru3b:,160519,,,,,,,,05/Feb/11 02:56;psteitz;Agreed.  Since the fix for MATH-349 this instance field is unnecessary.,"05/Feb/11 03:16;psteitz;See the discussion in MATH-349 where it was decided to remove the distribution pluggability in 3.0.  In 2.x, the distribution is pluggable and the instance field is useful.  The 3.0 code in trunk removes the pluggability and makes the field useless.","05/Feb/11 12:42;sebb@apache.org;Sorry - I thought I had checked the 2.x implementation as well, but obviously not, as it does use the field.

However, we should still deprecate the setter in 2.2, as it is removed in 3.0 - OK?","05/Feb/11 13:23;sebb@apache.org;Just tried removing the field and setter in 3.0, and found that the constructors rely on the setter (which is a separate bug, as the setter is not final - but easily fixable if required).

The fix for MATH-349 merely removed deprecated code.

It replaced ""distribution.setDegreesOfFreedom(dof)"" with ""distribution = new ChiSquaredDistributionImpl(dof)"" which is how the field became useless.

There are two constructors which still create values for the distribution field.

I don't know enough about the Math to know whether there would be any use cases for having additional methods that used a distribution provided by the class instance, rather than calculated by the individual methods (as at present).

If there is no need for external provision of the distribution degree of freedom, then the constructor with parameter can be dropped.

Otherwise, we need to add some methods that can use the provided distribution (which should be a final instance field).

In any case, I think the setter needs to be dropped from 3.x","05/Feb/11 14:52;psteitz;The instance field was there originally so that different ChiSquareDistribution implementations could be provided at construction time or via a setter (making the underlying ChiSquareDistribution pluggable).  MATH-349 pointed to a different problem related to mutability of implementation instances.  The simplest solution to both problems is to eliminate the pluggability, which the change in MATH-349 does for this class.  The degrees of freedom are always computed from the data, so there is no need for the constructor that takes a distribution instance as argument.  Both the constructor and setter can be deprecated in 2.2 and removed in 3.0 unless we want to keep pluggability, which would require

1) making the distribution field final (so removing the setter)
2) copying, rather than referencing the actual parameter provided to the constructor

I am on the fence on this.  Maybe others can chime in (next week :)","05/Feb/11 15:30;sebb@apache.org;OK, I see now, thanks!","20/Aug/11 21:14;psteitz;I removed the field (hence eliminating pluggability) in r1159916.  As of 3.0, the distribution classes are immutable, so to support pluggability a factory or class name rather than a distribution instance would have to be provided.  There is only one implementation provided by [math], so I do not see this as worth the effort and complexity to retain.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestUtils is thread-hostile,MATH-505,12497270,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,sebb@apache.org,sebb@apache.org,sebb@apache.org,01/Feb/11 00:28,24/Mar/12 16:16,07/Apr/19 20:38,01/Feb/11 18:58,1.2,2.0,2.1,,,,,3.0,,,0,,,,,,,,"TestUtils has several mutable static fields which are not synchronised, or volatile.

If one of the fields is updated by thread A, there is no guarantee that thread B will see the full update - it may see a partially updated object.

Furthermore, at least some of the static fields reference a mutable object, which can be changed whilst another thread is using it.

As far as I can tell, this class must only ever be used by a single thread otherwise the results will be unpredictable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-02-01 00:49:14.878,,,false,,,,,,,,,,,,,,150623,,,Tue Feb 01 15:02:19 UTC 2011,,,,,,0|i0ru3j:,160520,,,,,,,,"01/Feb/11 00:49;psteitz;What fields, exactly?","01/Feb/11 01:01;sebb@apache.org;{code}
/** Singleton TTest instance using default implementation. */
private static TTest tTest = new TTestImpl();

/** Singleton ChiSquareTest instance using default implementation. */
private static ChiSquareTest chiSquareTest =
        new ChiSquareTestImpl();

/** Singleton ChiSquareTest instance using default implementation. */
private static UnknownDistributionChiSquareTest unknownDistributionChiSquareTest =
        new ChiSquareTestImpl();

/** Singleton OneWayAnova instance using default implementation. */
private static OneWayAnova oneWayAnova =
        new OneWayAnovaImpl();
{code}

All of the above may be changed by set methods. There is no synch.","01/Feb/11 01:12;psteitz;OK, I was looking at the wrong TestUtils :)

The reason for this strange-looking setup is to allow the implementations to be pluggable at runtime.  ""Hostile"" is a harsh word, but this class is certainly *not* threadsafe.  Ideas / patches to achieve the design goal with less ""hostility"" would be appreciated.

I would have to double-check, but I don't think that there is any test instance state used by the methods in this class. ","01/Feb/11 02:33;sebb@apache.org;By thread-hostile, I mean that it is not possible in general for two different threads to use the class safely.
If one thread changes any of the static fields, there is no way of knowing how the methods called by the other thread will behave. This is partly because the values are not safely published currently, but even if they were, the threads don't know what settings will be used as they can be changed at any time by another thread.

In general, any class which relies on mutable static state for its behaviour is thread-hostile.
The shared state cannot simultaneously satisfy two threads needing different behaviour.

I think the only safe way for two threads to use the class as it stands is if they both synchronize on the class.
This will ensure safe publication of any field changes, and enforce serial usage which can guarantee the setting that will be used (but the lock will have to be held for the set call as well).

ChiSquareTestImpl has a non-final instance field which means its value won't necessarily be safely published.
The field also has a setter which could be invoked by one thread while another was using it.

TTestImpl is immutable (has no fields), and OneWayAnovaImpl can be made immutable, but other implementations of the interfaces might exist which are not immutable.

The simplest way to make the class thread-safe would be to convert all the methods and fields from static to instance, but I don't know if that is acceptable.","01/Feb/11 05:14;psteitz;Making the methods instance sort of defeats the purpose of the class.  None of the instance data in any of the static singletons is actually used or depended on by the methods of this class.  You are correct though that if one thread changes the impl for one of the singletons while another is using the class, the other could see a different than expected impl.  I think the practical likelihood of this is pretty much nil, as it is hard to imagine an application supplying two different implementations for the tests and wanting different threads to use different impls.  Personally, I would be happy just documenting the fact that the class is not threadsafe and if concurrent threads want to plug in different implementations, they need to synchronize on the class.  If this is not acceptable, my next preference would be to remove the pluggability - i.e., make the singletons final or get rid of them altogether, creating instances as needed for static method calls.  There is no initialization overhead creating the test classes.",01/Feb/11 07:15;joehni;@Phil: Please also keep in mind that M3 supports now (currently optional) parallel execution and it might be no longer a proper assumption that all tests are executed serially.,"01/Feb/11 12:53;sebb@apache.org;There is another possible option, which would be to fix the default implementations, and create new static methods that took an extra parameter for the implementation to be used.

At present, changes to the static fields are not guaranteed to be published correctly. Making them volatile would fix this, but would not help with concurrent access.","01/Feb/11 15:02;psteitz;Thanks, Joerg.  There should be no problems with the unit tests unless and until we introduce different tests that actually test the pluggability.  

I thought about the additional parameter option, Sebb; but that again defeats the purpose of this ""convenience class"" - you might as well just instantiate the implementation and use it.

I think the best solution is to just make the fields final and drop the getters and setters.  This is consistent with StatUtils.  So we should document the ""hostility"" issues in 2.2 and deprecate there and drop in 3.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOESS interpolation & tricube fonction,MATH-504,12497210,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,colsalt,colsalt,31/Jan/11 15:02,24/Mar/12 16:16,07/Apr/19 20:38,05/Jun/11 16:00,2.1,,,,,,,3.0,,,0,loess,,,,,,,"the doc for tricube fonction used in LOESS smooth() says :

* @return <code>(1 - |x|<sup>3</sup>)<sup>3</sup></code>.
But I'dont see the absolute value for x.

Also, the ""classical"" tricube fonction (see http://www.itl.nist.gov/div898/handbook/pmd/section1/pmd144.htm)
seems to return 0 if abs( x )>1. 
In the code of the tricube method, It is apparently not the case...


    private static double tricube(final double x) {
        final double tmp = 1 - x * x * x;
        return tmp * tmp * tmp;
    }",Win XP,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-06-05 16:00:09.15,,,false,,,,,,,,,,,,,,71906,,,Sun Jun 05 16:00:09 UTC 2011,,,,,,0|i0ru3r:,160521,,,,,,,,"05/Jun/11 16:00;luc;Fixed in subversion repository as of r1132439.

Thanks for the report",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath scalb() does not handle large magnitude exponents correctly,MATH-502,12496590,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,24/Jan/11 19:15,23/Mar/11 20:39,07/Apr/19 20:38,24/Jan/11 19:24,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,,,"scalb does not handle MAX_VALUE exponents properly:

double scalb(-1.7976931348623157E308, 2147483647) expected -Infinity actual -8.988465674311579E307 entries [6, 5]
double scalb(1.7976931348623157E308, 2147483647) expected Infinity actual 8.988465674311579E307 entries [7, 5]
double scalb(-1.1102230246251565E-16, 2147483647) expected -Infinity actual -5.551115123125783E-17 entries [8, 5]
double scalb(1.1102230246251565E-16, 2147483647) expected Infinity actual 5.551115123125783E-17 entries [9, 5]
double scalb(-2.2250738585072014E-308, 2147483647) expected -Infinity actual -0.0 entries [10, 5]
double scalb(2.2250738585072014E-308, 2147483647) expected Infinity actual 0.0 entries [11, 5]

float scalb(3.4028235E38, 2147483647) expected Infinity actual 1.7014117E38 entries [7, 5]
float scalb(-3.4028235E38, 2147483647) expected -Infinity actual -1.7014117E38 entries [9, 5]

It looks as though the problem is with the calculation of the scaledExponent - for large values, this can wrap round, so some of the checks against its value may give misleading results.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-24 19:24:13.447,,,false,,,,,,,,,,,,,,150621,,,Wed Mar 23 20:39:04 UTC 2011,,,,,,0|i0ru47:,160523,,,,,,,,24/Jan/11 19:24;luc;fixed in subversion repository as of r1062928 for trunk and r1062929 for branch 2.X,"23/Mar/11 20:39;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FastMath nextAfter(double,double) bugs with special doubles",MATH-499,12496428,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,sebb@apache.org,sebb@apache.org,sebb@apache.org,23/Jan/11 11:02,23/Mar/11 20:38,07/Apr/19 20:38,23/Jan/11 12:24,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,,,"nextAfter(double, double) is added in StrictMath 1.6, so one needs to test with Java 1.6 to see thi.

There are various boundary errors with nextAfter(double, double) - see below.

I think these are partially due to missing checks for special cases (e.g. the NaNs), and partially due to the following code:

{code}
if (d * (direction - d) >= 0) {
// we should increase the mantissa
{code}

This appears to be a shorthand for something like:

{code}
if (((d >=0) && (direction >= d)) || ((d<0) && (direction <0))) {
// we should increase the mantissa
{code}

however the expression (direction - d) overlows for some double values, thus causing the wrong branch to be taken.

double nextAfter(-0.0, -0.0) expected -0.0 actual 4.9E-324 entries [1, 1]
double nextAfter(-0.0, 0.0) expected 0.0 actual 4.9E-324 entries [1, 2]
double nextAfter(-0.0, NaN) expected NaN actual 4.9E-324 entries [1, 3]
double nextAfter(0.0, -0.0) expected -0.0 actual 4.9E-324 entries [2, 1]
double nextAfter(0.0, 0.0) expected 0.0 actual 4.9E-324 entries [2, 2]
double nextAfter(0.0, NaN) expected NaN actual 4.9E-324 entries [2, 3]
double nextAfter(-Infinity, NaN) expected NaN actual -Infinity entries [4, 3]
double nextAfter(Infinity, NaN) expected NaN actual Infinity entries [5, 3]
double nextAfter(-1.7976931348623157E308, NaN) expected NaN actual -1.7976931348623155E308 entries [6, 3]
double nextAfter(1.7976931348623157E308, NaN) expected NaN actual 1.7976931348623155E308 entries [7, 3]
double nextAfter(-1.1102230246251565E-16, NaN) expected NaN actual -1.1102230246251564E-16 entries [8, 3]
double nextAfter(1.1102230246251565E-16, NaN) expected NaN actual 1.1102230246251564E-16 entries [9, 3]
double nextAfter(-2.2250738585072014E-308, -0.0) expected -2.225073858507201E-308 actual -2.225073858507202E-308 entries [10, 1]
double nextAfter(-2.2250738585072014E-308, 0.0) expected -2.225073858507201E-308 actual -2.225073858507202E-308 entries [10, 2]
double nextAfter(-2.2250738585072014E-308, NaN) expected NaN actual -2.225073858507201E-308 entries [10, 3]
double nextAfter(-2.2250738585072014E-308, 1.1102230246251565E-16) expected -2.225073858507201E-308 actual -2.225073858507202E-308 entries [10, 9]
double nextAfter(-2.2250738585072014E-308, 2.2250738585072014E-308) expected -2.225073858507201E-308 actual -2.225073858507202E-308 entries [10, 11]
double nextAfter(-2.2250738585072014E-308, -4.9E-324) expected -2.225073858507201E-308 actual -2.225073858507202E-308 entries [10, 12]
double nextAfter(-2.2250738585072014E-308, 4.9E-324) expected -2.225073858507201E-308 actual -2.225073858507202E-308 entries [10, 13]
double nextAfter(2.2250738585072014E-308, -0.0) expected 2.225073858507201E-308 actual 2.225073858507202E-308 entries [11, 1]
double nextAfter(2.2250738585072014E-308, 0.0) expected 2.225073858507201E-308 actual 2.225073858507202E-308 entries [11, 2]
double nextAfter(2.2250738585072014E-308, NaN) expected NaN actual 2.225073858507201E-308 entries [11, 3]
double nextAfter(2.2250738585072014E-308, -1.1102230246251565E-16) expected 2.225073858507201E-308 actual 2.225073858507202E-308 entries [11, 8]
double nextAfter(2.2250738585072014E-308, -2.2250738585072014E-308) expected 2.225073858507201E-308 actual 2.225073858507202E-308 entries [11, 10]
double nextAfter(2.2250738585072014E-308, -4.9E-324) expected 2.225073858507201E-308 actual 2.225073858507202E-308 entries [11, 12]
double nextAfter(2.2250738585072014E-308, 4.9E-324) expected 2.225073858507201E-308 actual 2.225073858507202E-308 entries [11, 13]
double nextAfter(-4.9E-324, -0.0) expected -0.0 actual -1.0E-323 entries [12, 1]
double nextAfter(-4.9E-324, 0.0) expected -0.0 actual -1.0E-323 entries [12, 2]
double nextAfter(-4.9E-324, NaN) expected NaN actual -0.0 entries [12, 3]
double nextAfter(-4.9E-324, 1.1102230246251565E-16) expected -0.0 actual -1.0E-323 entries [12, 9]
double nextAfter(-4.9E-324, 2.2250738585072014E-308) expected -0.0 actual -1.0E-323 entries [12, 11]
double nextAfter(-4.9E-324, 4.9E-324) expected -0.0 actual -1.0E-323 entries [12, 13]
double nextAfter(4.9E-324, -0.0) expected 0.0 actual 1.0E-323 entries [13, 1]
double nextAfter(4.9E-324, 0.0) expected 0.0 actual 1.0E-323 entries [13, 2]
double nextAfter(4.9E-324, NaN) expected NaN actual 0.0 entries [13, 3]
double nextAfter(4.9E-324, -1.1102230246251565E-16) expected 0.0 actual 1.0E-323 entries [13, 8]
double nextAfter(4.9E-324, -2.2250738585072014E-308) expected 0.0 actual 1.0E-323 entries [13, 10]
double nextAfter(4.9E-324, -4.9E-324) expected 0.0 actual 1.0E-323 entries [13, 12]",Java 1.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-23 12:14:55.293,,,false,,,,,,,,,,,,,,150619,,,Wed Mar 23 20:38:42 UTC 2011,,,,,,0|i0ru4v:,160526,,,,,,,,"23/Jan/11 12:14;luc;This should have been fixed in repository this morning.
Do you still see these problems ?","23/Jan/11 12:24;sebb@apache.org;Fixed in:

URL: http://svn.apache.org/viewvc?rev=1062387&view=rev
Log:
fixed nextAfter implementations for handling of some special values
fixed the signature of the float version, as the spec is to have a double second argument
moved the existing tests that were used in the former implementation in MathUtils,
fixing them also as two of them were not compliant with the spec for equal numbers
Jira: MATH-478

and

URL: http://svn.apache.org/viewvc?rev=1062385&view=rev
Log:
fixed nextAfter implementations for handling of some special values
fixed the signature of the float version, as the spec is to have a double second argument
moved the existing tests that were used in the former implementation in MathUtils,
fixing them also as two of them were not compliant with the spec for equal numbers
Jira: MATH-478","23/Mar/11 20:38;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath atan2 does not agree with StrictMath for special cases,MATH-494,12496341,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,21/Jan/11 21:56,23/Mar/11 20:38,07/Apr/19 20:38,22/Jan/11 20:21,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,,,"FastMath atan2 does not agree with StrictMath for special cases.

There are two sign problems:
atan2(double -0.0, double -Infinity) expected -3.141592653589793 actual 3.141592653589793 entries [1, 4]
atan2(double -0.0, double Infinity) expected -0.0 actual 0.0 entries [1, 5]

A lot of NaNs where there should be a valid return:
atan2(double -1.7976931348623157E308, double -1.7976931348623157E308) expected -2.356194490192345 actual NaN entries [6, 6]
atan2(double -1.7976931348623157E308, double 1.7976931348623157E308) expected -0.7853981633974483 actual NaN entries [6, 7]
atan2(double -1.7976931348623157E308, double -1.1102230246251565E-16) expected -1.5707963267948968 actual NaN entries [6, 8]
atan2(double -1.7976931348623157E308, double 1.1102230246251565E-16) expected -1.5707963267948966 actual NaN entries [6, 9]
atan2(double -1.7976931348623157E308, double -2.2250738585072014E-308) expected -1.5707963267948968 actual NaN entries [6, 10]
atan2(double -1.7976931348623157E308, double 2.2250738585072014E-308) expected -1.5707963267948966 actual NaN entries [6, 11]
atan2(double -1.7976931348623157E308, double -4.9E-324) expected -1.5707963267948968 actual NaN entries [6, 12]
atan2(double -1.7976931348623157E308, double 4.9E-324) expected -1.5707963267948966 actual NaN entries [6, 13]
atan2(double 1.7976931348623157E308, double -1.7976931348623157E308) expected 2.356194490192345 actual NaN entries [7, 6]
atan2(double 1.7976931348623157E308, double 1.7976931348623157E308) expected 0.7853981633974483 actual NaN entries [7, 7]
atan2(double 1.7976931348623157E308, double -1.1102230246251565E-16) expected 1.5707963267948968 actual NaN entries [7, 8]
atan2(double 1.7976931348623157E308, double 1.1102230246251565E-16) expected 1.5707963267948966 actual NaN entries [7, 9]
atan2(double 1.7976931348623157E308, double -2.2250738585072014E-308) expected 1.5707963267948968 actual NaN entries [7, 10]
atan2(double 1.7976931348623157E308, double 2.2250738585072014E-308) expected 1.5707963267948966 actual NaN entries [7, 11]
atan2(double 1.7976931348623157E308, double -4.9E-324) expected 1.5707963267948968 actual NaN entries [7, 12]
atan2(double 1.7976931348623157E308, double 4.9E-324) expected 1.5707963267948966 actual NaN entries [7, 13]
atan2(double -1.1102230246251565E-16, double -1.7976931348623157E308) expected -3.141592653589793 actual NaN entries [8, 6]
atan2(double -1.1102230246251565E-16, double 1.7976931348623157E308) expected -0.0 actual NaN entries [8, 7]
atan2(double -1.1102230246251565E-16, double -4.9E-324) expected -1.5707963267948968 actual NaN entries [8, 12]
atan2(double -1.1102230246251565E-16, double 4.9E-324) expected -1.5707963267948966 actual NaN entries [8, 13]
atan2(double 1.1102230246251565E-16, double -1.7976931348623157E308) expected 3.141592653589793 actual NaN entries [9, 6]
atan2(double 1.1102230246251565E-16, double 1.7976931348623157E308) expected 0.0 actual NaN entries [9, 7]
atan2(double 1.1102230246251565E-16, double -4.9E-324) expected 1.5707963267948968 actual NaN entries [9, 12]
atan2(double 1.1102230246251565E-16, double 4.9E-324) expected 1.5707963267948966 actual NaN entries [9, 13]
atan2(double -2.2250738585072014E-308, double -1.7976931348623157E308) expected -3.141592653589793 actual NaN entries [10, 6]
atan2(double -2.2250738585072014E-308, double 1.7976931348623157E308) expected -0.0 actual NaN entries [10, 7]
atan2(double 2.2250738585072014E-308, double -1.7976931348623157E308) expected 3.141592653589793 actual NaN entries [11, 6]
atan2(double 2.2250738585072014E-308, double 1.7976931348623157E308) expected 0.0 actual NaN entries [11, 7]
atan2(double -4.9E-324, double -1.7976931348623157E308) expected -3.141592653589793 actual NaN entries [12, 6]
atan2(double -4.9E-324, double 1.7976931348623157E308) expected -0.0 actual NaN entries [12, 7]
atan2(double 4.9E-324, double -1.7976931348623157E308) expected 3.141592653589793 actual NaN entries [13, 6]
atan2(double 4.9E-324, double 1.7976931348623157E308) expected 0.0 actual NaN entries [13, 7]

There are also some spurious errors, which are due to a bug in the test case - expecting the values to be exactly the same as StrictMath
atan2(double 2.2250738585072014E-308, double -4.9E-324) expected 1.570796326794897 actual 1.5707963267948968 entries [11, 12]
atan2(double -2.2250738585072014E-308, double -4.9E-324) expected -1.570796326794897 actual -1.5707963267948968 entries [10, 12]
atan2(double 1.1102230246251565E-16, double -2.2250738585072014E-308) expected 1.5707963267948968 actual 1.5707963267948966 entries [9, 10]
atan2(double -1.1102230246251565E-16, double -2.2250738585072014E-308) expected -1.5707963267948968 actual -1.5707963267948966 entries [8, 10]
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:38:08.963,,,false,,,,,,,,,,,,,,150614,,,Wed Mar 23 20:38:08 UTC 2011,,,,,,0|i0ru5z:,160531,,,,,,,,"22/Jan/11 01:22;sebb@apache.org;Note: using MathUtils.equals(double, double, 1) instead of Double.equals() allows the last few comparisons to succeed; however this also ignores the differences between +/- 0.0, so a more sensitive test is needed.","23/Mar/11 20:38;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FastMath min and max fail with (Infinity,-Infinity) and (0,0, -0.0)",MATH-493,12496333,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,21/Jan/11 21:12,23/Mar/11 20:36,07/Apr/19 20:38,24/Jan/11 12:49,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,,,"FastMath min and max fail with (Infinity,-Infinity) and (0,0, -0.0):

min(float 0.0, float -0.0) expected -0.0 actual 0.0
min(float Infinity, float -Infinity) expected -Infinity actual NaN
max(float 0.0, float -0.0) expected 0.0 actual -0.0
max(float Infinity, float -Infinity) expected Infinity actual NaN

Similarly for the double versions.

The Infinity failures are because the code uses Float.isNaN(a + b) which gives NaN when +/1- Infinity are added together.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:36:27.272,,,false,,,,,,,,,,,,,,150613,,,Wed Mar 23 20:36:27 UTC 2011,,,,,,0|i0ru67:,160532,,,,,,,,"24/Jan/11 12:49;sebb@apache.org;Fixed by using the Harmony code.

Note: this appears to be at least as quick as StrictMath on Sun Java 1.6 in a crude test","23/Mar/11 20:36;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath acos fails when input abs value is less than about 5.7851920321187236E-300 - returns NaN,MATH-489,12496028,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,19/Jan/11 17:11,23/Mar/11 20:35,07/Apr/19 20:38,21/Jan/11 03:33,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,,,"FastMath acos fails when input absolute value is less than about 5.7851920321187236E-300

It returns NaN instead of an expected value close to PI/2.0

This appears to be due to the following code:

{code}
// Compute ratio r = y/x
double r = y/x;
temp = r * 1073741824.0;
{code}

r and temp can become infinite or Nan.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:35:36.96,,,false,,,,,,,,,,,,,,150610,,,Wed Mar 23 20:35:36 UTC 2011,,,,,,0|i0ru73:,160536,,,,,,,,"21/Jan/11 03:33;sebb@apache.org;URL: http://svn.apache.org/viewvc?rev=1061608&view=rev
Log:
Fix overflows in acos calculation

URL: http://svn.apache.org/viewvc?rev=1061609&view=rev
Log:
Fix overflows in acos calculation","23/Mar/11 20:35;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath toRadian and toDegree don't handle large double numbers well,MATH-486,12495960,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,19/Jan/11 04:46,23/Mar/11 20:35,07/Apr/19 20:38,19/Jan/11 20:28,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,,,"FastMath toRadian and toDegree don't handle very large double numbers well.

For example, toDegrees(Double.MAX_VALUE) => NaN, but it should be INFINITY
and toRadian(Double.MAX_VALUE) => NaN instead of the proper value

This is because of the lines:

{code}
double temp = x * 1073741824.0; // == 0x40 00 00 00
double xa = x + temp - temp; // => NaN for x large enough
{code}

This seems to be an attempt to split x into a large and a small part, but fails when x >= MAX_VALUE / 1073741824.0

Not sure how to fix this",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-19 07:32:32.379,,,false,,,,,,,,,,,,,,150609,,,Wed Mar 23 20:35:20 UTC 2011,,,,,,0|i0ru7r:,160539,,,,,,,,"19/Jan/11 07:32;dimpbx;What about the reciprocity of these two methods?  toDegrees(toRadian(x))==x and toRadian(toDegrees(x))==x
If you use INFINITY, you run into troubles.  If you return Double.MAX_VALUE instead, you look safer.   That does not prevent you returning INFINITY if you start with INFINITY and NaN if it is the argument of the method.

Furthermore, some consistency would help.  Why plural for degrees and singular for radian?","19/Jan/11 12:34;sebb@apache.org;I wrote that toDegrees(Double.MAX_VALUE) should be INFINITY because that is how the Sun/Oracle Math.toDegrees() method behaves.

[Sorry, my bad - the methods are both plural in the code - only the JIRA is inconsistent]
",19/Jan/11 20:28;sebb@apache.org;Fixed by setting temp =0 for very large input.,"23/Mar/11 20:35;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
events detection in ODE solvers is too complex and not robust,MATH-484,12495935,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,18/Jan/11 20:49,23/Mar/11 20:35,07/Apr/19 20:38,14/Feb/11 14:20,2.1,,,,,,,2.2,,,0,,,,,,,,"All ODE solvers support multiple events detection since a long time. Events are specified by users by implementing the EventHandler interface. Events occur when the g(t, y) function evaluates to 0. When an event occurs, the solver step is shortened to make sure the event is located at the end of the step, and the event is triggered by calling the eventOccurred method in the user defined implementation class. Depending on the return value of this method, integration can continue, it can be stopped, or the state vector can be reset.

Some ODE solvers are adaptive step size solvers. They can modify step size to match an integration error setting, increasing step size when error is low (thus reducing computing costs) or reducing step size when error is high (thus fulfilling accuracy requirements).

The step adaptations due to events on one side and due to adaptive step size solvers are quite intricate by now, due to numerous fixes (MATH-161, MATH-213, MATH-322, MATH-358, MATH-421 and also during standard maintenance - see for example r781157). The code is very difficult to maintain. It seems each bug fix introduces new bugs (r781157/MATH-322) or tighten the link between adaptive step size and event detection (MATH-388/r927202).

A new bug discovered recently on an external library using a slightly modified version of this code could not be retroffitted into commons-math, despite the same problem is present. At the beginning of EventState.evaluateStep, the initial step may be exactly 0 thus preventing root solving, but preventing this size to drop to 0 would reopen MATH-388. I could not fix both bugs at the same time.

So it is now time to untangle events detection and adaptive step size, simplify code, and remove some inefficiency (event root solving is always done twice, once before step truncation and another time after truncation, of course with slightly different results, events shortened steps induce high computation load until the integrator recovers its optimal pace again, steps are rejected even when the event does not requires it ...).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,34184,,,Wed Mar 23 20:35:01 UTC 2011,,,,,,0|i0ru87:,160541,,,,,,,,20/Jan/11 20:57;luc;fixed in subversion repository as of r1061507 for branch 2.X and as of r1061508 for trunk,"14/Feb/11 13:39;luc;The fix introduced in r1061507 fails in several cases. If several events of the same type occur within a single long step, only the first one is triggered. If several events of different types occur during a backward integration, they are triggered in the wrong order (i.e. they are triggered in forward occurrence time order instead of backward).",14/Feb/11 14:20;luc;fixed in subversion repository as of r1070498 for branch 2.X and r1070499 for trunk,"23/Mar/11 20:35;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath does not handle all special cases correctly,MATH-483,12495841,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,18/Jan/11 00:24,23/Mar/11 20:34,07/Apr/19 20:38,19/Jan/11 19:51,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,,,"FastMath has some issues with special cases such as +0.0 and -0.0.

Here are the double cases so far found:

abs(-0.0) expected:<0.0> but was:<-0.0>
signum(-0.0) expected:<-0.0> but was:<0.0>
asin(-0.0) expected:<-0.0> but was:<0.0>
atan(-0.0) expected:<-0.0> but was:<0.0>
log10(-0.0) expected:<-Infinity> but was:<NaN>
toDegrees(-0.0) expected:<-0.0> but was:<0.0>
toRadians(-0.0) expected:<-0.0> but was:<0.0>
ulp(-Infinity) expected:<Infinity> but was:<NaN>

And float cases:
abs(-0.0) expected:<0.0> but was:<-0.0>
",,,,,,,,,,,,,,,,,,,,,19/Jan/11 12:36;sebb@apache.org;MATH-483-generic.patch;https://issues.apache.org/jira/secure/attachment/12468750/MATH-483-generic.patch,18/Jan/11 00:25;sebb@apache.org;MATH-483.patch;https://issues.apache.org/jira/secure/attachment/12468612/MATH-483.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:34:46.943,,,false,,,,,,,,,,,,,,150608,,,Wed Mar 23 20:34:46 UTC 2011,,,,,,0|i0ru8f:,160542,,,,,,,,18/Jan/11 00:25;sebb@apache.org;Fix + test cases,"18/Jan/11 01:27;sebb@apache.org;Generic test methods using reflection to compare Math methods with FastMath methods for float and double special cases.

Replaces testHyperbolicSpecialCases() and testTrigSpecialCases() in the previous patch",19/Jan/11 12:36;sebb@apache.org;Fixed bug - duplicate MAX_VALUE should have MIN_VALUE,19/Jan/11 19:51;sebb@apache.org;There are still some issues with toDegrees() and toRadians() but these are covered by MATH-486,"23/Mar/11 20:34;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f",MATH-482,12495818,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,sebb@apache.org,sebb@apache.org,sebb@apache.org,17/Jan/11 19:52,23/Mar/11 20:34,07/Apr/19 20:38,17/Jan/11 20:01,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,,,"FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f.

This is because the wrong variable is returned.

The bug was not detected by the test case ""testMinMaxFloat()"" because that has a bug too - it tests doubles, not floats.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:34:29.616,,,false,,,,,,,,,,,,,,150607,,,Wed Mar 23 20:34:29 UTC 2011,,,,,,0|i0ru8n:,160543,,,,,,,,"23/Mar/11 20:34;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MathUtils.equals(double x, double y) disagrees with Javadoc",MATH-481,12495806,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,17/Jan/11 17:15,23/Mar/11 20:33,07/Apr/19 20:38,17/Jan/11 22:39,2.1,,,,,,,2.2,,,0,,,,,,,,"MathUtils.equals(double x, double y) disagrees with Javadoc.

The Javadoc says:

Returns true iff they are equal as defined by  {@link #equals(double,double,int)}

However, the code actually uses == and checks for NaN:

{code}
public static boolean equals(double x, double y) {
    return (Double.isNaN(x) && Double.isNaN(y)) || x == y;
}
{code}

The method is deprecated, but it should probably still be consistent with its documentation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-17 22:39:52.004,,,false,,,,,,,,,,,,,,150606,,,Wed Mar 23 20:33:40 UTC 2011,,,,,,0|i0ru8v:,160544,,,,,,,,"17/Jan/11 22:39;erans;Corrected Javadoc in revision 1060117.
","23/Mar/11 20:33;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""ulp"" in ""FastMath""",MATH-480,12495639,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,erans,erans,14/Jan/11 23:51,23/Mar/11 20:33,07/Apr/19 20:38,19/Jan/11 19:18,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,,,"When the argument is infinite, method ""ulp"" in ""FastMath"" produces ""NaN"" (whereas ""Math"" gives ""Infinity"").
",,,,,,,,,,,,,,,,,,,,,17/Jan/11 19:38;sebb@apache.org;MATH-480.patch;https://issues.apache.org/jira/secure/attachment/12468591/MATH-480.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-01-17 19:38:10.902,,,false,,,,,,,,,,,,,,150605,,,Wed Mar 23 20:33:23 UTC 2011,,,,,,0|i0ru93:,160545,,,,,,,,"17/Jan/11 19:38;sebb@apache.org;Fix for ulp(double) bug.
Also adds ulp(float) method and test cases for both","23/Mar/11 20:33;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath.signum(-0.0) does not agree with Math.signum(-0.0) ; no tests for signum,MATH-479,12495631,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,14/Jan/11 22:46,23/Mar/11 20:33,07/Apr/19 20:38,19/Jan/11 19:06,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,,,"There are no unit tests for FastMath.signum(double) as yet.

Here is one that should work, but fails:

{code}
@Test
public void testSignum() {
    Assert.assertTrue(Double.valueOf(FastMath.signum(+0.0)).equals(Double.valueOf(Math.signum(+0.0)))); // OK
    Assert.assertTrue(Double.valueOf(FastMath.signum(-0.0)).equals(Double.valueOf(Math.signum(-0.0)))); // FAILS
}
{code}

",,,,,,,,,,,,,,,,,,,,,15/Jan/11 01:32;sebb@apache.org;Math479.patch;https://issues.apache.org/jira/secure/attachment/12468435/Math479.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:33:07.718,,,false,,,,,,,,,,,,,,150604,,,Wed Mar 23 20:33:07 UTC 2011,,,,,,0|i0ru9b:,160546,,,,,,,,15/Jan/11 01:32;sebb@apache.org;Patch to add test case against Math.signum(double) and fix for bug (also simplifies the code),"23/Mar/11 20:33;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MathUtils.equals(double, double, double) does not agree with Javadoc",MATH-475,12495619,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Not A Problem,,sebb@apache.org,sebb@apache.org,14/Jan/11 19:51,23/Mar/11 20:32,07/Apr/19 20:38,17/Jan/11 13:08,,,,,,,,3.0,,,0,,,,,,,,"MathUtils.equals(double, double, double) does not agree with its Javadoc.

The Javadoc says:

""Returns true if both arguments are equal or within the range of allowed error (inclusive).""

However the following test fails:

{code}
double top=1.7976931348623184E16;
double pen=1.7976931348623182E16;
double diff=Math.abs(top-pen);
assertTrue(MathUtils.equals(top, pen, 1.0)); // OK - implies the difference is <= 1.0
assertTrue(""expected < 1.0, but was: ""+diff,diff <= 1.0); // reports: expected < 1.0, but was: 2.0
{code}

This discrepancy is because the equals(double, double, double) method also checks to see if the numbers are within one ULP of each other.

Either the Javadoc needs to be corrected, or the code needs to be corrected to drop the ULP comparison.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-15 01:53:08.981,,,false,,,,,,,,,,,,,,150601,,,Wed Mar 23 20:32:08 UTC 2011,,,,,,0|i0ru9z:,160549,,,,,,,,"15/Jan/11 01:53;erans;{quote}
assertTrue(MathUtils.equals(top, pen, 1.0)); // OK - implies the difference is <= 1.0
{quote}

The comment seems reasonable but I'm not sure that it must be correct when dealing with floating point numbers.

If we change the definition of {{equals(a, b, eps)}} to
{noformat}
  min(a, b) + abs(eps) >= max(a, b)
{noformat}
then
{noformat}
  equals(top, pen, 1.0)
{noformat}
will return {{true}}, because
{noformat}
 pen + 1.0 = top
{noformat}
although
{noformat}
 pen - top == 2.0
{noformat}

However,
{noformat}
  equals(top, pen, 0.98)
{noformat}
will return {{false}} because
{noformat}
  pen + 0.98 == pen
{noformat}

IIUC, the issue is: What is the meaning of {{equals(a, b, eps)}} when {{eps < ulp(max(a, b))}} ?
Because in that case, I think that the call is equivalent to the strict equality test: {{a == b}}, I'd say that the call to ""equals"" is also meaningless.

The current implementation short-circuits the problem by assuming that two floating point numbers {{a}} and {{b}} separated by 1 ulp are equally likely representations of any real number within the interval {{[a, b]}}.
","15/Jan/11 16:13;psteitz;Very interesting example.  How do you come up with this stuff, Sebb?  :)

I tend to agree with Gilles' view that if if a and b are not distinguishable as doubles, we don't really have any confidence in the value of abs(a - b), so the short-circuit in the current code is better.  I would say just update the javadoc to make say something like

""Returns true if the values are indistinguishable as doubles or the difference between them is within the range of allowed error (inclusive).""","17/Jan/11 12:14;sebb@apache.org;The consensus seems to be to update the Javadoc as follows:

""Returns true if there is no double value strictly between the arguments or the difference between them is within the range of allowed error (inclusive).""

I am happy with that.
","17/Jan/11 13:08;erans;Javadoc clarified in revision 1059909.
","23/Mar/11 20:32;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastMath needs ulp(float),MATH-472,12495439,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,13/Jan/11 00:03,23/Mar/11 20:31,07/Apr/19 20:38,19/Jan/11 19:20,2.2,3.0,,,,,,2.2,3.0,,0,,,,,,,,"FastMath needs ulp(float), because ulp((double)float) does not generate the correct results.

Test case and patch to follow.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:31:44.374,,,false,,,,,,,,,,,,,,150600,,,Wed Mar 23 20:31:44 UTC 2011,,,,,,0|i0rua7:,160550,,,,,,,,"13/Jan/11 00:05;sebb@apache.org;Similarly, there needs to be a nextAfter(float, float)",17/Jan/11 19:39;sebb@apache.org;See MATH-480 for patch to add ulp(float),19/Jan/11 19:20;sebb@apache.org;Fixed as part of MATH-478,"23/Mar/11 20:31;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MathUtils.equals(double, double) does not work properly for floats",MATH-471,12495437,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,12/Jan/11 23:30,23/Mar/11 20:31,07/Apr/19 20:38,17/Jan/11 20:33,,,,,,,,2.2,3.0,,0,,,,,,,,"MathUtils.equals(double, double) does not work properly for floats.

There is no equals(float,float) so float parameters are automatically promoted to double. However, that is not necessarily appropriate, given that the ULP for a double is much smaller than the ULP for a float.

So for example:

{code}
double oneDouble = 1.0d;
assertTrue(MathUtils.equals(oneDouble, Double.longBitsToDouble(1 + Double.doubleToLongBits(oneDouble)))); // OK
float oneFloat = 1.0f;
assertTrue(MathUtils.equals(oneFloat, Float.intBitsToFloat(1 + Float.floatToIntBits(oneFloat)))); // FAILS
float  f1 = 333.33334f;
double d1 = 333.33334d;
assertTrue(MathUtils.equals(d1, f1)); // FAILS
{code}

I think the equals() methods need to be duplicated with the appropriate changes for floats to avoid any problems with the promotion of floats.
",,,,,,,,,,,,,,,,,,,,,12/Jan/11 23:51;sebb@apache.org;Math471.patch;https://issues.apache.org/jira/secure/attachment/12468189/Math471.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-01-13 00:49:05.269,,,false,,,,,,,,,,,,,,34185,,,Wed Mar 23 20:31:27 UTC 2011,,,,,,0|i0ruaf:,160551,,,,,,,,12/Jan/11 23:51;sebb@apache.org;Implementation of equals(float...) methods,"12/Jan/11 23:54;sebb@apache.org;I now realise that the 3rd example above may not be easy - or even possible - to fix, but it does offer a good example of the difficulties of comparing floating point numbers!","13/Jan/11 00:49;psteitz;Ouch!

Thanks for reporting and patching this. Sebb.  

+1 for applying the patch to both 2_X and trunk, with unit tests.","13/Jan/11 01:34;psteitz;It is not obvious to me that the third example should succeed.  An argument could be made that the existing code does the right thing when passed a double and a float, which is to convert the float according to the value set conversion rules and then compare the result to the double.  ","13/Jan/11 02:27;sebb@apache.org;Yes, I think the 3rd example is not relevant to the immediate problem (although indirectly it was what alerted me to the problem).

Though it may perhaps be worthwhile noting the conversion issue somewhere in the Javadoc - perhaps at class level?","13/Jan/11 03:04;sebb@apache.org;Just had a thought:

I don't think the exactly same code can be added to 2.x, because the semantics of some of the double methods has changed.
It would be confusing if floats behaved differently from doubles!

So I propose to rework the fix for 2.x to preserve the deprecated behaviour and deprecated tags.

OK?","13/Jan/11 03:10;psteitz;Yes, you are right.  Needs to be reworked slightly for 2.x.","13/Jan/11 11:05;erans;{quote}
MathUtils.equals(double, double) does not work properly for floats.

There is no equals(float,float) so float parameters are automatically promoted to double. However, that is not necessarily appropriate, given that the ULP for a double is much smaller than the ULP for a float.
{quote}

This is not a bug, but expected behaviour. But I certainly agree that it does not hurt to mention the conversion issue for the unwary.

However, shouldn't there be emphasis, in the user guide, that CM is a ""double"" precision library? A quick poll of the code gives the following numbers:
Occurrences of the string ""float "": *41* (roughly half of which were introduced with this patch)
Occurrences of the string ""double "": *4061*

Also, I'm curious as to what use case you were having that requires comparing {{float}} numbers.

Finally if we want to help users avoid such pitfalls, I think that we should consider refactoring {{MathUtils}} so that similar methods that should behave differently for different types are in _different_ classes (exactly as with classes {{Float}} and {{Double}}). Thus, we should create {{MathUtilsDouble}} (or assume that the current {{MathUtils}} is for {{double}} utilities) and {{MathUtilsFloat}}.

","14/Jan/11 16:09;sebb@apache.org;Many of the double-only methods work OK with widened float parameters.

Where this is not the case, we either need to fix the discrepancy (as per this JIRA) or document the restriction.

I don't see the point of separating the utilities into two different classes, but if the consensus is that we should, then we need to make sure that the same methods are present in both, even if the double method works perfectly well, otherwise it would be even more confusing for users (where do I find the method?).

But I think the main issue is that having a separate class would force users to *edit and recompile* to take advantage of any fixes such as this one.","15/Jan/11 17:11;psteitz;I agree with Sebb on this one.  I think it is a bug (the second example).  The promotion separates the values that are indistinguishable as floats.  Consistently with comments on MATH-474, equals(-,-) should identify indiscernibles.  I also agree that it is better to just add the methods to MathUtils.
",17/Jan/11 20:33;sebb@apache.org;Patches now applied.,"23/Mar/11 20:31;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MathUtilsTest.testArrayEquals() is flagged as deprecated, but should not be?",MATH-470,12495352,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,12/Jan/11 11:14,24/Mar/12 16:16,07/Apr/19 20:38,12/Jan/11 12:38,,,,,,,,3.0,,,0,,,,,,,,"MathUtilsTest.testArrayEquals() is flagged as deprecated, but probably should not be.

The method that it tests - MathUtils.equals(double[] x, double[] y) - is not deprecated, so surely its tests should not be deprecated?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-12 12:38:56.072,,,false,,,,,,,,,,,,,,150599,,,Wed Jan 12 12:38:56 UTC 2011,,,,,,0|i0ruan:,160552,,,,,,,,"12/Jan/11 12:38;erans;Originally it was going to be removed because IIRC the code in CM only used the method where NaNs are considered equal. However someone might indeed find the new semantics useful too...
Removed deprecation in revision 1058110.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HarmonicCoefficientsGuesser.sortObservations() potentlal NPE warning,MATH-467,12494849,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,sebb@apache.org,sebb@apache.org,06/Jan/11 14:49,23/Mar/11 20:30,07/Apr/19 20:38,06/Jan/11 19:43,2.2,3.0,,,,,,,,,0,,,,,,,,"HarmonicCoefficientsGuesser.sortObservations()

generates an NPE warning from Eclipse which thinks that mI can be null in the while condition.

The code looks like:
{code}
WeightedObservedPoint mI = observations[i];
while ((i >= 0) && (curr.getX() < mI.getX())) {
    observations[i + 1] = mI;
    if (i-- != 0) {
        mI = observations[i];
    } else {
        mI = null;
    }
}
// mI is not used further
{code}

It looks to me as though the ""mI = null"" statement is either redundant or wrong - why would one want to replace one of the observations with null during a sort?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-06 19:43:47.327,,,false,,,,,,,,,,,,,,150596,,,Wed Mar 23 20:30:30 UTC 2011,,,,,,0|i0rubb:,160555,,,,,,,,"06/Jan/11 19:43;luc;You are right. The else statement is not needed.
Fixed in subversion repository as of r1056034 for branch 2.X and as of r1056035 for trunk","23/Mar/11 20:30;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BaseMultiStartMultivariateRealOptimizer.optimize() can generate NPE if starts < 1,MATH-466,12494847,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,sebb@apache.org,sebb@apache.org,06/Jan/11 14:33,24/Mar/12 16:16,07/Apr/19 20:38,07/Jan/11 17:01,3.0,,,,,,,3.0,,,0,,,,,,,,"The Javadoc for BaseMultiStartMultivariateRealOptimizer says that starts can be <= 1; however if it is set to 0, then the optimize() method will try to throw a null exception.

Perhaps starts should be constrained to be at least 1?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-06 19:50:20.572,,,false,,,,,,,,,,,,,,150595,,,Fri Jan 07 17:01:48 UTC 2011,,,,,,0|i0rubj:,160556,,,,,,,,06/Jan/11 14:34;sebb@apache.org;Same issue applies to BaseMultiStartMultivariateVectorialOptimizer and MultiStartUnivariateRealOptimizer,"06/Jan/11 19:50;luc;Yes, the javadoc should be changed and the number of starts should be checked with an error triggered if it is not at least 1.",07/Jan/11 17:01;erans;Added preconditions checks in revision 1056391.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect matrix rank via SVD,MATH-465,12494744,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,marisa,marisa,05/Jan/11 17:34,24/Mar/12 16:17,07/Apr/19 20:38,20/Jul/11 12:20,2.1,,,,,,,3.0,,,0,,,,,,,,"The getRank() function of SingularValueDecompositionImpl does not work properly. This problem is probably related to the numerical stability problems mentioned in [MATH-327|https://issues.apache.org/jira/browse/MATH-327] and [MATH-320|https://issues.apache.org/jira/browse/MATH-320].

Example call with the standard matrix from R (rank 2):

{code:title=TestSVDRank.java}
import org.apache.commons.math.linear.Array2DRowRealMatrix;
import org.apache.commons.math.linear.RealMatrix;
import org.apache.commons.math.linear.SingularValueDecomposition;
import org.apache.commons.math.linear.SingularValueDecompositionImpl;

public class TestSVDRank {
	public static void main(String[] args) {
		double[][] d = { { 1, 1, 1 }, { 0, 0, 0 }, { 1, 2, 3 } };
		RealMatrix m = new Array2DRowRealMatrix(d);
		SingularValueDecomposition svd = new SingularValueDecompositionImpl(m);
		int r = svd.getRank();
		System.out.println(""Rank: ""+r);
	}
}
{code} 

The rank is computed as 3. This problem also occurs for larger matrices. I discovered the problem when trying to replace the corresponding JAMA method.",Windows XP Prof. Vs. 2002,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-15 17:21:26.332,,,false,,,,,,,,,,,,,,67397,,,Wed Jul 20 12:20:51 UTC 2011,,,,,,0|i0rubr:,160557,,,,,,,,15/Jan/11 17:21;psteitz;Thanks for reporting this.  Looks like it could as you suggest be related to MATH-327.  ,"15/Jan/11 17:22;psteitz;For now, pushing to 3.0.  If we get a fix for this and MATH-327 before 3.0 is ready, I may propose a 2.2.1 to include it.","23/Jun/11 19:20;gsteri1;My apologies if I am missing something, but here is what I noticed about the SVD. 

On lines 124-127 of SingularValueDecompositionImpl we have:

        for (int i = 0; i < p; i++) {
            singularValues[i] = FastMath.sqrt(FastMath.abs(singularValues[i]));
        }

This is potentially the offending line. First is the problem of negative eigenvalues. Negative variance in the principal components should probably be dealt with explicitly? Perhaps by throwing a MathException? Second, and the issue which this bug report deals with, is taking a square root of a very small number (<1) will return a larger number. If you apply the threshold test in getRank() (final double threshold = FastMath.max(m, n) * FastMath.ulp(singularValues[0]) )  prior to taking the square root, I believe this problem would be resolved. More importantly, philosophically, you test for zero variance. This is the appropriate test.

Also, rank could be precalculated in the above loop. ","20/Jul/11 12:20;luc;Fixed in subversion repository as of r1148714.

This issue was fixed by changing SVD implementation according to issue MATH-611.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LegendreGaussIntegrator ignores defaultMaximalIterationCount and does 38 million iterations,MATH-464,12494345,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,murkle,murkle,31/Dec/10 07:00,24/Mar/12 16:16,07/Apr/19 20:38,23/Aug/11 22:37,2.1,,,,,,,3.0,,,0,,,,,,,,"The following code results in count = 37801710 which is effectively an infinite loop for typical functions we are using
(in GeoGebra)

The argument defaultMaximalIterationCount = 100 is being ignored

This is the version we are using:
http://www.geogebra.org/trac/browser/trunk/geogebra/org/apache/commons/math/analysis/integration/LegendreGaussIntegrator.java

    	LegendreGaussIntegrator gauss = new LegendreGaussIntegrator(5, 100);
    
	try {
		double result = gauss.integrate(new testFun(), -10, 0.32462367623786328);
	} catch (Exception ee) {
		ee.printStackTrace();
	}



class testFun implements UnivariateRealFunction {

    public double value(double x) throws FunctionEvaluationException {
    	count ++;
        if (x>=0 && x<=5) return 0.2; else return 0;
    }

}
",,10800,10800,,0%,10800,10800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-13 21:37:15.669,,,false,,,,,,,,,,,,,,63983,,,Tue Aug 23 22:37:41 UTC 2011,,,,,,0|i0rubz:,160558,,,,,,,,"13/Jan/11 21:37;psteitz;Thanks for reporting this.

The problem here is not with the iteration count.  In the example above, only 26 iterations are executed and the method returns the correct value.  What is causing the number of function evaluations to be so large is that each iteration involves multiple function evaluations.   I need to dig more deeply into the algorithm to determine what (if anything) the problem is, but what is causing the high number of function evaluations is the following
{code}
// prepare next iteration
double ratio = FastMath.min(4, FastMath.pow(delta / limit, 0.5 / abscissas.length));
n = FastMath.max((int) (ratio * n), n + 1);
{code}

In the example, delta / limit becomes large, causing n to increase rapidly.  As n increases, the number of function evaluations increases.","14/Jan/11 12:06;psteitz;I am now thinking that this is not a bug, but a consequence of the fact that the integrand in the example is not at all well-approximated by a polynomial.  With a small-enough stepsize, the algorithm does converge, but requiring the large number of function evaluations above.  Here are some stepsize values for the example and the associated absolute error:

n 8 error 0.05738431110184819
n 28 error 0.027423287634332688
n 100 error 8.62162720248888E-5
n 249 error 5.308122631570711E-4
n 650 error 4.3582615516528367E-4
n 1641 error 2.519984967931377E-4
n 3829 error 5.838605030586419E-5
...
 n 1102593 error 6.71416523906343E-8

The last entry is from the last (26th) iteration.  I haven't verified the rationale for the updating formula for n above, but it does appear warranted in this case to increase n quickly as large n (= small stepsize) is required to get a decent estimate of the integral using Gaussian quadrature.
","14/Jan/11 13:40;luc;Perhaps we should also provide higher order formulas, using either a fixed set of precomputed constants or a way to compute the coefficients for any order.","15/Jan/11 16:56;psteitz;Moving to 3.0.  I don't think this is a bug, but points to a couple of possible enhancements:

1) higher order formulas (+0 on this suggestion from Luc - IMO the example and others like it are not suitable for Legendre-Gauss)
2) bound on the number of function evaluations (I vaguely recall us talking about this elsewhere, but can't find the reference.  If anyone else can, pls add.)","09/Apr/11 19:48;luc;We restarted a thread about this a few days after the previous comment on this issue.
The thread can be read here: http://markmail.org/thread/rnazrggnnuehz4qv

I think adding maxEvaluations while still preserving the existing maxIterations would be fine.","20/Jul/11 13:04;luc;Coming back to this issue.

I would propose to follow the same pattern we used for root solvers: adding a maxEval parameter in the top level integrate interface declaration. So we would have the same kind of configuration, with tolerances set at integrator/solver level and maxEval and function pointer passed at integrate/solve method call.

Since we are just in the phase we change interfaces, this would be a good time to add this parameter.

Does this seems reasonable ?","11/Aug/11 20:36;psteitz;+1 for your suggestion, Luc.  Lets try to get this into 3.0.","23/Aug/11 22:37;luc;Fixed in subversion repository as of r1160914.

The API of the integrators has been changed for consistency with solvers API. Now the main convergence parameters are set in the constructor and remain fixed, but a maximal number of function evaluation must be provided at each call to the integration method.

Thanks for the report",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Interpolators: Remove ""MathException"" from the signature of the ""interpolate"" method",MATH-458,12492863,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,erans,erans,erans,09/Dec/10 18:37,24/Mar/12 16:16,07/Apr/19 20:38,06/Jan/11 16:21,,,,,,,,3.0,,,0,,,,,,,,"The interfaces for the interpolators contain a ""throws"" clause that should be removed.
E.g.
{code}
public interface UnivariateRealInterpolator {

    /**
     * Computes an interpolating function for the data set.
     * @param xval the arguments for the interpolation points
     * @param yval the values for the interpolation points
     * @return a function which interpolates the data set
     * @throws MathException if arguments violate assumptions made by the
     *         interpolation algorithm
     */
    UnivariateRealFunction interpolate(double xval[], double yval[])
        throws MathException;
}
{code}

Assumptions violation should be dealt with by throwing appropriate unchecked exceptions.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-04 14:47:20.264,,,false,,,,,,,,,,,,,,150593,,,Fri Jan 07 00:11:47 UTC 2011,,,,,,0|i0rud3:,160563,,,,,,,,"04/Jan/11 14:47;miccagiann;Excuse me, but why interfaces for the interpolators shoudn't contain a ""throws"" clause? The appropriate unchecked exceptions are shown by the stack trace... Sorry for not understanding what would you like to be done... ","04/Jan/11 15:17;erans;One of the changes we set to implement in 3.0 is that all exceptions in Commons-Math will be unchecked. And in fact they will all inherit from a single {{MathRuntimeException}} base class (defined in the package {{exception}}).
The checked {{MathException}} will be removed.
","05/Jan/11 16:50;miccagiann;So if we just changed the ""throws MathException"" statement with ""throws MathRuntimeException"" statement then the new implementation would be compatible with the new standards? Thanks in advance Gilles!","06/Jan/11 11:22;erans;Yes but it's unnecessary. Some would argue that having runtime exceptions declared in the ""throws"" clause is confusing because
it is not guaranteed that only those exceptions listed there will be thrown, and not even that these exceptions will be thrown at all.
",06/Jan/11 16:21;erans;Revision 1055931.,"06/Jan/11 18:44;miccagiann;Just if you are interested i have modified the classes that were affected by this exception and i have changed the exception to ""MathRuntimeException""... If you need them i would gladly attached these files...","07/Jan/11 00:11;erans;I'm not sure of what you mean and what need to be changed. Please attach the ""diff"" files.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Erf.erf - handle infinities and large values,MATH-456,12492820,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,josh.milthorpe,josh.milthorpe,09/Dec/10 11:46,23/Mar/11 20:29,07/Apr/19 20:38,01/Jan/11 01:24,2.2,,,,,,,2.2,,,0,,,,,,,,"{{Erf.erf(double)}} crashes when presented with infinities or large values, as follows:
{noformat}
org.apache.commons.math.ConvergenceException: Continued fraction diverged to NaN for value ∞
	at org.apache.commons.math.util.ContinuedFraction.evaluate(ContinuedFraction.java:186)
	at org.apache.commons.math.special.Gamma.regularizedGammaQ(Gamma.java:266)
	at org.apache.commons.math.special.Gamma.regularizedGammaP(Gamma.java:173)
	at org.apache.commons.math.special.Erf.erf(Erf.java:56)
	at TestInfErf.main(TestInfErf.java:9)
{noformat}

The following code demonstrates this crash:
{code}
import org.apache.commons.math.MathException;
import org.apache.commons.math.special.Erf;

public class TestInfErf {
    public static void main(String[] args) {
        try {
            System.out.println(""erf(Inf) = "" + Erf.erf(Double.POSITIVE_INFINITY));
            System.out.println(""erf(-Inf) = "" + Erf.erf(Double.NEGATIVE_INFINITY));
            System.out.println(""erf(Huge) = "" + Erf.erf(1e300));
        } catch (MathException e) { 
            e.printStackTrace(); 
        }
    }
}
{code}

At double precision, erf\(x\) = 1.0 for x > 6.0 and erf\(x\) = -1.0 for x < -6.0.  Therefore Erf.java could be patched as follows:
{noformat}
Index: src/main/java/org/apache/commons/math/special/Erf.java
===================================================================
--- src/main/java/org/apache/commons/math/special/Erf.java	(revision 1043888)
+++ src/main/java/org/apache/commons/math/special/Erf.java	(working copy)
@@ -48,6 +48,12 @@
      * @throws MathException if the algorithm fails to converge.
      */
     public static double erf(double x) throws MathException {
+        // at double precision, erf(x) = (+/-)1.0 for |x| > 6.0
+        if (x > 6.0) {
+            return 1.0;
+        } else if (x <-6.0) {
+            return -1.0;
+        }
         double ret = Gamma.regularizedGammaP(0.5, x * x, 1.0e-15, 10000);
         if (x < 0) {
             ret = -ret;
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-01-01 01:24:04.217,,,false,,,,,,,,,,,,,,150592,,,Wed Mar 23 20:29:36 UTC 2011,,,,,,0|i0rudj:,160565,,,,,,,,01/Jan/11 01:24;psteitz;Fixed in r1054184,"23/Mar/11 20:29;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RealVector Javadoc refers to non-existent package org.apache.commons.math.analysis.function,MATH-453,12492117,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,sebb@apache.org,sebb@apache.org,06/Dec/10 02:01,24/Mar/12 16:16,07/Apr/19 20:38,06/Dec/10 12:53,2.1,,,,,,,3.0,,,0,,,,,,,,"RealVector Javadoc refers to non-existent package org.apache.commons.math.analysis.function.

As this explains how to recode deprecated method calls, it ought to be corrected before release.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-12-06 12:53:56.338,,,false,,,,,,,,,,,,,,150589,,,Mon Dec 06 12:53:56 UTC 2010,,,,,,0|i0rue7:,160568,,,,,,,,"06/Dec/10 12:53;erans;Removed references to the {{analysis.function}} package (revision 1042610).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Subclasses of BaseAbstractUnivariateRealSolver redefine the public static final field DEFAULT_ABSOLUTE_ACCURACY,MATH-452,12492116,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,06/Dec/10 01:58,24/Mar/12 16:16,07/Apr/19 20:38,06/Dec/10 11:57,,,,,,,,3.0,,,0,,,,,,,,"There are 8 subclasses of BaseAbstractUnivariateRealSolver which have the public static final field DEFAULT_ABSOLUTE_ACCURACY; this is ths same name as the field in the super-class (and the same value).

This field hiding is confusing, and should be avoided.

In this case, the field hiding is completely unnecessary as well (as far as I can tell).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-12-06 11:57:00.017,,,false,,,,,,,,,,,,,,150588,,,Mon Dec 06 11:57:00 UTC 2010,,,,,,0|i0ruef:,160569,,,,,,,,"06/Dec/10 11:57;erans;Changed access to {{private}} in revision 1042596.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect exception specification in AbstractLeastSquaresOptimizer.optimize(),MATH-443,12480321,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Duplicate,,roman.werpachowski,roman.werpachowski,18/Nov/10 10:59,24/Mar/12 16:16,07/Apr/19 20:38,20/Nov/10 21:18,2.0,2.1,,,,,,,,,0,,,,,,,,"AbstractLeastSquaresOptimizer.optimize() declares the following exceptions: FunctionEvaluationException, OptimizationException, IllegalArgumentException. However, it also throws MaxIterationsExceededException. I think that instead of OptimizationException, ConvergenceException should be declared.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-11-18 19:34:20.356,,,false,,,,,,,,,,,,,,150582,,,Sat Nov 20 21:18:45 UTC 2010,,,,,,0|i0rugf:,160578,,,,,,,,"18/Nov/10 19:34;luc;The exception hierarchy is currently completely overhauled, many exceptions have disappeared and a bunch of new exceptions have been created, many of them being unchecked exceptions.
The method you cite now throws only unchecked exceptions.
Could you have a look at the subversion repository and see if either the 2.X branch or the trunk (which is used for work on 3.0 version) do fit your needs ?
Any comments on this ongoing work before we release it is welcome.","18/Nov/10 19:39;roman.werpachowski;My personal preference is for checked exceptions, but I can live with unchecked, as long as they are documented and I know what to catch in my code.","18/Nov/10 19:46;luc;I understand.
It has been a looooong debate ;-)","20/Nov/10 21:18;luc;this is currently handled in MATH-195, which is nearly done now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver returns unfeasible solution,MATH-434,12479313,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,wmwitzel,wmwitzel,07/Nov/10 03:55,24/Mar/12 16:16,07/Apr/19 20:38,09/Apr/11 19:21,2.1,,,,,,,3.0,,,0,,,,,,,,"The SimplexSolver is returning an unfeasible solution:

import java.util.ArrayList;
import java.text.DecimalFormat;
import org.apache.commons.math.linear.ArrayRealVector;
import org.apache.commons.math.optimization.GoalType;
import org.apache.commons.math.optimization.OptimizationException;
import org.apache.commons.math.optimization.linear.*;

public class SimplexSolverBug {
    
    public static void main(String[] args) throws OptimizationException {
        
        LinearObjectiveFunction c = new LinearObjectiveFunction(new double[]{0.0d, 1.0d, 1.0d, 0.0d, 0.0d, 0.0d, 0.0d}, 0.0d);
        
        ArrayList<LinearConstraint> cnsts = new ArrayList<LinearConstraint>(5);
        LinearConstraint cnst;
        cnst = new LinearConstraint(new double[] {1.0d, -0.1d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d}, Relationship.EQ, -0.1d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {1.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, -1e-18d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.0d, 1.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 0.0d, 1.0d, 0.0d, -0.0128588d, 1e-5d}, Relationship.EQ, 0.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 0.0d, 0.0d, 1.0d, 1e-5d, -0.0128586d}, Relationship.EQ, 1e-10d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, -1.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, 1.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, 0.0d, -1.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, 0.0d, 1.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);
        cnsts.add(cnst);
                
        DecimalFormat df = new java.text.DecimalFormat(""0.#####E0"");
        
        System.out.println(""Constraints:"");
        for(LinearConstraint con : cnsts) {
            for (int i = 0; i < con.getCoefficients().getDimension(); ++i)
                System.out.print(df.format(con.getCoefficients().getData()[i]) + "" "");
            System.out.println(con.getRelationship() + "" "" + con.getValue());
        }
        
        SimplexSolver simplex = new SimplexSolver(1e-7);
        double[] sol = simplex.optimize(c, cnsts, GoalType.MINIMIZE, false).getPointRef();
        System.out.println(""Solution:\n"" + new ArrayRealVector(sol));
        System.out.println(""Second constraint is violated!"");
    }
}


It's an odd problem, but something I ran across.  I tracked the problem to the getPivotRow routine in SimplexSolver.  It was choosing a pivot that resulted in a negative right-hand-side.  I recommend a fix by replacing
                ...
                if (MathUtils.equals(ratio, minRatio, epsilon)) {
                ...
with
                ...
                if (MathUtils.equals(ratio, minRatio, Math.abs(epsilon/entry))) {
                ...

I believe this would be more appropriate (and at least resolves this particular problem).

Also, you may want to consider making a change in getPivotColumn to replace
            ...
            if (MathUtils.compareTo(tableau.getEntry(0, i), minValue, epsilon) < 0) {
            ...
with
            ...
            if (tableau.getEntry(0, i) < minValue) 
            ...
because I don't see the point of biasing earlier columns when multiple entries are within epsilon of each other.  Why not pick the absolute smallest.  I don't know that any problem can result from doing it the other way, but the latter may be a safer bet.

VERY IMPORTANT: I discovered another bug that occurs when not restricting to non-negatives.  In SimplexTableu::getSolution(), 
          ...          
          if (basicRows.contains(basicRow)) 
              // if multiple variables can take a given value
              // then we choose the first and set the rest equal to 0
              coefficients[i] = 0;
          ...
should be
          ...          
          if (basicRows.contains(basicRow)) {
              // if multiple variables can take a given value
              // then we choose the first and set the rest equal to 0
              coefficients[i] = (restrictToNonNegative ? 0 : -mostNegative);
          ...
If necessary, I can give an example of where this bug causes a problem, but it should be fairly obvious why this was wrong.
",,,,,,,,,,,,,,,,,,,,,11/Apr/11 16:21;tn;MATH-434-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12476012/MATH-434-cleanup.patch,04/Apr/11 21:25;tn;MATH-434-version2.patch;https://issues.apache.org/jira/secure/attachment/12475430/MATH-434-version2.patch,03/Apr/11 22:24;tn;MATH-434.patch;https://issues.apache.org/jira/secure/attachment/12475332/MATH-434.patch,14/Nov/10 18:03;wmwitzel;SimplexSolverIssues.java;https://issues.apache.org/jira/secure/attachment/12459563/SimplexSolverIssues.java,14/Nov/10 18:04;wmwitzel;SimplexSolverIssuesOutput.txt;https://issues.apache.org/jira/secure/attachment/12459565/SimplexSolverIssuesOutput.txt,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,2010-11-08 08:52:39.721,,,false,,,,,,,,,,,,,,34167,,,Mon Apr 11 17:59:29 UTC 2011,,,,,,0|i0rui7:,160586,,,,,,,,"08/Nov/10 03:30;wmwitzel;My original suggested fix had a potential for overflow errors (since minRatio is initialized to Double.MAX_VALUE).  Also, I added another suggestion and pointed out another bug which leads to invalid solutions.
","08/Nov/10 08:52;erans;Could you attach unit tests that demonstrate each problem?  Thank you.
",13/Nov/10 15:45;wmwitzel;I'll try to send some examples soon.  I'm noticing more problems with the right-hand-side going negative and want to cover all bases (as much as possible).,"14/Nov/10 18:03;wmwitzel;Code, and resulting output, that illustrates issues with the SimplexSolver.
",26/Dec/10 20:02;psteitz;Pushing out to 3.0.,"12/Jan/11 05:49;bmccann;Hey, sorry I took so long to look at this.  I've had very little time and am not working on this stuff anymore.  I'm honestly not going to be able to look at this stuff much moving forward, so hopefully there's a Commons Math contributor that can act as a reviewer.

When you say it's choosing a pivot with a negative RHS, I'm assuming that means it's not within the epsilon?
Why would it be more appropriate to divide by the entry?  I'm not sure I see why you'd want to use a bigger epsilon when the entry is 0.1 and a smaller epsilon when the entry is 10.  Maybe we should just make the default epsilon smaller?  I'm no expert with floating point math so I'm not real sure how to set the epsilon and just made up a value.
...
if (MathUtils.equals(ratio, minRatio, epsilon)) {
...
with
...
if (MathUtils.equals(ratio, minRatio, Math.abs(epsilon/entry))) {
","03/Apr/11 22:24;tn;Attached a patch for the reported problems.
The problems can be split into two groups:

 - wrong solution calculation with negative 
   variables
 - failing to select an appropriate pivot 
   row when values are below a given 
   epsilon

The patch addresses both problems:

 1. fix in SimplexTableau.getSolution()
 2. use BigReal for arbitrary precision  
    support when selecting the pivot row

Additionally, 4 test cases are included, as well as a minor typo fix for a method name.

The fixed epsilon is also used in some other places of the code, this may also create problems under certain circumstances. So if this patch is accepted, the other places could also be adapted.","04/Apr/11 19:00;luc;Thanks Thomas.

I had a look at the patch. I'm not a big fan of using BigReal, mainly when we don't specify a scale and we don't link it to the choice for epsilon. Also reading back Ben comments, I wonder if we should not replace epsilon by an integer number of ulps with a default set to a very small value (say something like 10 ulps).

What problem did you see in the accuracy of the variables to use BigReal ?
","04/Apr/11 21:24;tn;Hi Luc,

my initial idea was to use an epsilon that is adjusted to the magnitude of the respective value used for comparison. To be honest, I was not aware of [Math,FastMath].ulp, therefore I went with BigReal/BigDecimal to circumvent the problem in another way (by using an arbitrary precision datatype). After reading your comment, I investigated more into the problem, e.g. using http://www.cygnus-software.com/papers/comparingfloats/Comparing%20floating%20point%20numbers.htm, and addressed it (hopefully correct) in the way you proposed.

Though, I had to split up the epsilon test into two categories:

  - general comparison of floating-point values: using ulp, as values can be arbitrarily small
  - algorithm convergence check: using a standard epsilon, as the algorithm may not converge due to limited precision of 
    the double datatype otherwise

Please find attached my updated patch, any comments are welcome (e.g. I was unsure whether to expose the maxUlps parameter in the constructor, or to use a generic comparison epsilon, e.g. using FastMath.ulp(1d) instead of one adjusted to the current value in question).","04/Apr/11 21:25;tn;updated patch, incorporating comments from luc","05/Apr/11 10:56;erans;[Pardon the possibly naïve questions.]

In ""SimplexTableau"":
* Why not use directly ""equals(double, double, int)"" from ""MathUtils"" instead of computing an epsilon with ""getEpsilon""?
* Why is the ""isOptimal"" method not using the adjusted ""epsilon"" (at line 385)?
","05/Apr/11 11:32;tn;hmm, I feel a bit stupid now, as I have re-implemented MathUtils.equals(double, double, int) but in a mediocre way. So all calls to getEpsilon should be replaced with the equivalent MathUtils.equals.

for the isOptimal:

the idea was to have a user-defined threshold for the convergence criteria, which defaults to the original value of 1e-6. Using the same adjusted epsilon would possibly lead to more iterations as before. As the feasibility check in SimplexSolver.solvePhase1 has to use a static epsilon for convergence reasons, I thought to use the same epsilon in isOptimal makes sense for symmetry reasons (use the same epsilon to check for convergence /feasibility).

But it's good that you raise these points, because I was hesitating myself what is the best way to go forward, as I am also not considering myself a floating-point expert. I am mainly interested in the simplex algorithm, that's why I have chosen to provide a patch for this (very nice) implementation of it.","09/Apr/11 19:21;luc;Fixed in subversion repository as of r1090656.
Path applied with a very small change: adding the maxUlps parameter to the detailed constructor.

Thanks for the report and thanks for the patch.
","11/Apr/11 16:21;tn;Thanks for accepting the patch. The comparison using maxUlps has already been adapted according to MATH-557, but it was missing for SimplexTableau. The cleanup patch fixes this and also renames the test names for similarity.","11/Apr/11 17:59;luc;Cleanup patch applied.

thanks again",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Function objects,MATH-430,12478471,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,erans,erans,erans,27/Oct/10 15:46,24/Mar/12 16:16,07/Apr/19 20:38,26/Nov/10 22:00,,,,,,,,3.0,,,0,,,,,,,,"Create a package {{function}} to contain classes implementing the {{UnivariateRealFunction}} interface for all the usual functions.

As a consequence, all {{mapXxx}} methods and {{mapXxxToSelf}} methods in {{RealVector}} can be respectively replaced with
{code}
public void map(UnivariateRealFunction f);
{code}
and
{code}
public void mapToSelf(UnivariateRealFunction f);
{code}",,,,,,,,,,,,,,,,,,,,,05/Nov/10 15:57;erans;func.tar.gz;https://issues.apache.org/jira/secure/attachment/12458920/func.tar.gz,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-11-21 17:53:56.786,,,false,,,,,,,,,,,,,,150573,,,Fri Nov 26 22:00:33 UTC 2010,,,,,,0|i0ruiv:,160589,,,,,,,,"05/Nov/10 15:57;erans;As part of this issue, I'm proposing an alternative to the current ""ComposableFunction"" class.

The attached file contains part of the functionality (e.g. function composition, addition and multiplication). There are tests showing the intended usage.

The main difference with ""ComposableFunction"" is that utilities (e.g. operators) and function objects are separate: Having a ""FunctionUtils"" keeps the overall design more uniform (similar, in spirit,  to the other ""...Utils"" classes in CM). The purpose is also to stay minimalist (e.g. no need to have a ""divide"" method since you can create it with ""multiply"" and an ""Inverse"" function object).
Also, function objects are first-class citizens, not needing the ""import static"" construct to be used with relative ease.

Please, let me know whether this is going in the right direction.
",21/Nov/10 17:44;erans;No one objecting on this change?,"21/Nov/10 17:53;luc;No problem for me. I'm just wondering if you want to move only the implementations are the interfaces too (univariate, bivariate, trivariate, multivariate and real/vectorial).","21/Nov/10 18:17;erans;I thought of an {{analysis.function}} package for grouping the function classes so as to avoid overcrowding the {{analysis}} package; the interfaces would stay in {{analysis}}.
","21/Nov/10 19:45;luc;Fine, thanks.","22/Nov/10 21:34;erans;Functions and utilities added in revision 1037896. Replaced classes are deprecated.
In a second, step I'll remove the redundant {{mapXxx}} and {{mapXxxToSelf}} methods from the {{RealVector}} interface.
",26/Nov/10 17:06;erans;Removed {{ComposableFunction}} and {{BinaryFunction}} in revision 1039465.,26/Nov/10 22:00;erans;{{ComposableFunction}} and {{BinaryFunction}} classes deprecated in branch 2.x (revision 1039575).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KMeansPlusPlusClusterer breaks by division by zero,MATH-429,12478052,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Blocker,Fixed,,erikvaningen,erikvaningen,22/Oct/10 08:01,23/Mar/11 20:25,07/Apr/19 20:38,23/Oct/10 19:35,2.1,,,,,,,,,,0,,,,,,,,"For a certain space, KMeansPlusPlusClusterer  breaks. This is a blocker because this space occurs in our domain. ","Java, Windows",10800,10800,,0%,10800,10800,,,,,,,,,,,,,,22/Oct/10 08:04;erikvaningen;KMeansPlusPlusClustererTest.java;https://issues.apache.org/jira/secure/attachment/12457825/KMeansPlusPlusClustererTest.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-10-22 16:48:17.559,,,false,,,,,,,,,,,,,,150572,,,Wed Mar 23 20:25:46 UTC 2011,,,,,,0|i0ruj3:,160590,,,,,,,,22/Oct/10 08:04;erikvaningen;The testcase which breaks KMeansPlusPlusClusterer,"22/Oct/10 16:48;luc;You have encountered one classical problem with k-means: at some stage (here at the first iteration), one of the clusters becomes empty.
This case is currently no handled by commons-math (which is a bug, so we have to fix it).
When a cluster is empty, a new centroid must be defined from the other clusters. There are different strategies:

# take the point farthest from any cluster
# select a random point from the cluster with the largest distance variance
# select a random point from the cluster with the largest number of points

My prefered choice would be 2, what do other people think ?
","22/Oct/10 22:32;jwcarman;How about make it configurable?  Take a look at how the Mallet project did it:

http://mallet.cs.umass.edu/api/cc/mallet/cluster/KMeans.html

By the way, I have suggested that they try to enter the Incubator here at the ASF and they seem somewhat receptive to the idea!","23/Oct/10 19:35;luc;Fixed in subversion repository as of r1026666 for branche 2.X and as of r1026667 for trunk.
Users can now choose among four different strategies to deal with empty clusters:

* split the cluster with largest distance variance,
* split the cluster with largest number of points,
* create a cluster around the point farthest from its centroid,
* generate an error

The default is to split according to largest variance.

Thanks for reporting the issue.","23/Mar/11 20:25;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultiDimensionMismatchException.getWrongDimensions() fails to clone the array,MATH-424,12475638,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,sebb@apache.org,sebb@apache.org,01/Oct/10 16:36,23/Mar/11 20:24,07/Apr/19 20:38,01/Oct/10 18:11,,,,,,,,,,,0,,,,,,,,"MultiDimensionMismatchException.getWrongDimensions() returns a pointer to the array, which allows callers to modify it.

The constructor takes care to clone the input array, so presumably the getter should do so too?

Same applies to getExpectedDimensions()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-10-01 16:57:56.266,,,false,,,,,,,,,,,,,,150567,,,Wed Mar 23 20:24:11 UTC 2011,,,,,,0|i0ruk7:,160595,,,,,,,,"01/Oct/10 16:57;erans;I knew about this one. I didn't ""clone"" because I figured out that one should be pretty pervert to fiddle with data coming from an exception. At that point, the only sane thing would be to examine/print the content...
So this is an optimization to avoid copying the data several times (see the accessor methods in the subclass {{MatrixDimensionMismatchException}}).
","01/Oct/10 17:41;sebb@apache.org;Another way to avoid copying yet still provide read-only access to the array would be to use an indexed getter.

That would be safe even for ""insane"" code.","01/Oct/10 18:11;erans;Resolved as suggested (revision 1003606).
","23/Mar/11 20:24;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Constructor calls overrideable methods,MATH-422,12475562,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,01/Oct/10 00:49,24/Mar/12 16:16,07/Apr/19 20:38,07/Jan/11 20:33,,,,,,,,3.0,,,0,,,,,,,,"The ctor MicrosphereInterpolator(int microsphereElements, int brightnessExponent) calls 2 public setters that are not final.
This is unsafe if the class is ever extended.

Likewise for ComplexFormat(String imaginaryCharacter, NumberFormat realFormat,  NumberFormat imaginaryFormat)


Are the setters even needed?

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-10-01 12:58:23.098,,,false,,,,,,,,,,,,,,150566,,,Fri Jan 07 20:33:48 UTC 2011,,,,,,0|i0rukn:,160597,,,,,,,,"01/Oct/10 12:58;erans;I made {{MicrosphereInterpolator}} immutable in revision 1003521.
[Leaving this issue open for the other class.]
",05/Jan/11 21:20;miccagiann;As far as the ComplexFormat string is concerned i believe that setters and getters are useful and it's safer to be called by the constructor that Sebb mentions... I'm on my way to try to understand what MicrosphereInterpolator class does...,06/Jan/11 12:01;erans;I think that we should remove the setters.,"06/Jan/11 14:01;sebb@apache.org;The setters (in ComplexFormat) don't add any functionality to the class, because the constructor allows all the values to be set.
Rather than try to re-use the class with a different setting, just create a new instance of the class.
It has very little data, so won't require much storage.

The getters are potentially useful, as they allow access to the defaults.

I agree - the setters should be removed.",06/Jan/11 18:48;miccagiann;So guys we are on the way removing the setters??? I would like to handle this task if you don't mind... Thanks for all the opinions mentioned above!,"07/Jan/11 18:49;erans;Actually, If you'd like to help, I think that it would be more useful to start with MATH-459; there are still 40 files containing calls to methods in the deprecated {{MathRuntimeException}} class; one patch per package would be nice.",07/Jan/11 20:33;erans;Revision 1056493.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
restarting an ODE solver that has been stopped by an event doesn't work,MATH-421,12475435,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,luc,luc,29/Sep/10 18:24,23/Mar/11 20:23,07/Apr/19 20:38,29/Sep/10 19:51,2.1,,,,,,,2.2,,,0,,,,,,,,"If an ODE solver is setup with an EventHandler that return STOP when the even is triggered, the integrators stops (which is exactly the expected behavior).
If however the user want to restart the solver from the final state reached at the event with the same configuration (expecting the event to be triggered again at a later time), then the integrator may fail to start. It can get stuck at the previous event.

The occurrence of the bug depends on the residual sign of the g function which is not exactly 0, it depends on the convergence of the first event.

As this use case is fairly general, event occurring less than epsilon after the solver start in the first step should be ignored, where epsilon is the convergence threshold of the event. The sign of the g function should be evaluated after this initial ignore zone, not exactly at beginning (if there are no event at the very beginning g(t0) and g(t0+epsilon) have the same sign, so this does not hurt ; if there is an event at the very beginning, g(t0) and g(t0+epsilon) have opposite signs and we want to start with the second one. Of course, the sign of epsilon depend on the integration direction (forward or backward).
",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150565,,,Wed Mar 23 20:23:12 UTC 2011,,,,,,0|i0rukv:,160598,,,,,,,,"29/Sep/10 19:51;luc;Fixed in subversion repository, as of r1002827 for branch 2.X and 1002829 for trunk (3.0)","23/Mar/11 20:23;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MathRuntimeException#createInternalError() method loosing the exception ""cause""",MATH-415,12473912,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,axelclk,axelclk,11/Sep/10 17:05,23/Mar/11 20:21,07/Apr/19 20:38,11/Sep/10 17:21,3.0,,,,,,,2.2,,,0,,,,,,,,"The MathRuntimeException#createInternalError(Throwable cause) method doesn't store the exception ""cause"".
Is this intentionally?

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-09-11 17:21:33.028,,,false,,,,,,,,,,,,,,150560,,,Wed Mar 23 20:21:06 UTC 2011,,,,,,0|i0rulz:,160603,,,,,,,,"11/Sep/10 17:21;luc;No, it was an error.
It should now be fixed in subversion repository, as of r996178 fro branch 2.x and as of r996179 for trunk.
Thanks for reporting this","23/Mar/11 20:21;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConvergenceException in NormalDistributionImpl.cumulativeProbability(),MATH-414,12472930,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,gustav.ryd,gustav.ryd,31/Aug/10 11:01,23/Mar/11 20:20,07/Apr/19 20:38,30/Nov/10 11:57,2.1,,,,,,,2.2,,,0,,,,,,,,"I get a ConvergenceException in  NormalDistributionImpl.cumulativeProbability() for very large/small parameters including Infinity, -Infinity.
For instance in the following code:

	@Test
	public void testCumulative() {
		final NormalDistribution nd = new NormalDistributionImpl();
		for (int i = 0; i < 500; i++) {
			final double val = Math.exp(i);
			try {
				System.out.println(""val = "" + val + "" cumulative = "" + nd.cumulativeProbability(val));
			} catch (MathException e) {
				e.printStackTrace();
				fail();
			}
		}
	}

In version 2.0, I get no exception. 

My suggestion is to change in the implementation of cumulativeProbability(double) to catch all ConvergenceException (and return for very large and very small values), not just MaxIterationsExceededException.
",Jdk 1.6.,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-11-25 06:22:18.545,,,false,,,,,,,,,,,,,,34141,,,Wed Mar 23 20:20:43 UTC 2011,,,,,,0|i0rum7:,160604,,,,,,,,"25/Nov/10 06:22;psteitz;The difference between 2.0 and 2.1 is due to the changes in ContinuedFraction included in the fix for MATH-282.  For very large values, continued fractions are diverging to NaN. The suggested fix will work, but at this point, I wonder if we should just move the top-coding out of the catch - i.e., test the arguments and return 0 or 1 for extreme values without attempting the approximation.","26/Nov/10 21:17;psteitz;I am leaning toward adding top-coding outside of the catch.  Based on the inequality p(Z > t) < exp(-t^2/2) derived in [1] and Double.MIN_VALUE  = 2^-1074, I get that tail probabilities are not distinguishable from 0 for |t| > 39, so I propose that we top-code at 40 outside the catch.  Appreciate others checking my arithmetic.
 
[1] http://www.johndcook.com/normalbounds.pdf","28/Nov/10 11:13;luc;Your suggestion seems good to me.
I've check exp(-t^2/2) becomes lower than Double.MIN_VALUE/2 (i.e. rounds to 0) when |t|> 38.604",30/Nov/10 11:57;psteitz;Fixed in r1040471,"23/Mar/11 20:20;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Miscellaneous issues concerning the ""optimization"" package",MATH-413,12472850,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,erans,erans,30/Aug/10 13:19,24/Mar/12 16:16,07/Apr/19 20:38,13/Feb/12 10:46,,,,,,,,3.0,,,0,,,,,,,,"Revision 990792 contains changes triggered the following issues:
* [MATH-394|https://issues.apache.org/jira/browse/MATH-394]
* [MATH-397|https://issues.apache.org/jira/browse/MATH-397]
* [MATH-404|https://issues.apache.org/jira/browse/MATH-404]

This issue collects the currently still unsatisfactory code (not necessarily sorted in order of annoyance):
# ""BrentOptimizer"": a specific convergence checker must be used. ""LevenbergMarquardtOptimizer"" also has specific convergence checks.
# Trying to make convergence checking independent of the optimization algorithm creates problems (conceptual and practical):
 ** See ""BrentOptimizer"" and ""LevenbergMarquardtOptimizer"", the algorithm passes ""points"" to the convergence checker, but the actual meaning of the points can very well be different in the caller (optimization algorithm) and the callee (convergence checker).
 ** In ""PowellOptimizer"" the line search (""BrentOptimizer"") tolerances depend on the tolerances within the main algorithm. Since tolerances come with ""ConvergenceChecker"" and so can be changed at any time, it is awkward to adapt the values within the line search optimizer without exposing its internals (""BrentOptimizer"" field) to the enclosing class (""PowellOptimizer"").
# Given the numerous changes, some Javadoc comments might be out-of-sync, although I did try to update them all.
# Class ""DirectSearchOptimizer"" (in package ""optimization.direct"") inherits from class ""AbstractScalarOptimizer"" (in package ""optimization.general"").
# Some interfaces are defined in package ""optimization"" but their base implementations (abstract class that contain the boiler-plate code) are in package ""optimization.general"" (e.g. ""DifferentiableMultivariateVectorialOptimizer"" and ""BaseAbstractVectorialOptimizer"").
# No check is performed to ensure the the convergence checker has been set (see e.g. ""BrentOptimizer"" and ""PowellOptimizer""); if it hasn't there will be a NPE. The alternative is to initialize a default checker that will never be used in case the user had intended to explicitly sets the checker.
# ""NonLinearConjugateGradientOptimizer"": Ugly workaround for the checked ""ConvergenceException"".
# Everywhere, we trail the checked ""FunctionEvaluationException"" although it is never used.
# There remains some duplicate code (such as the ""multi-start loop"" in the various ""MultiStart..."" implementations).
# The ""ConvergenceChecker"" interface is very general (the ""converged"" method can take any number of ""...PointValuePair""). However there remains a ""semantic"" problem: One cannot be sure that the list of points means the same thing for the caller of ""converged"" and within the implementation of the ""ConvergenceChecker"" that was independently set.
# It is not clear whether it is wise to aggregate the counter of gradient evaluations to the function evaluation counter. In ""LevenbergMarquartdOptimizer"" for example, it would be unfair to do so. Currently I had to remove all tests referring to gradient and Jacobian evaluations.
# In ""AbstractLeastSquaresOptimizer"" and ""LevenbergMarquardtOptimizer"", occurences of ""OptimizationException"" were replaced by the unchecked ""ConvergenceException"" but in some cases it might not be the most appropriate one.
# ""MultiStartUnivariateRealOptimizer"": in the other classes (""MultiStartMultivariate..."") similar to this one, the randomization is on the firts-guess value while in this class, it is on the search interval. I think that here also we should randomly choose the start value (within the user-selected interval).
# The Javadoc utility raises warnings (see output of ""mvn site"") which I couldn't figure out how to correct.
# Some previously existing classes and interfaces have become no more than a specialisation of new ""generics"" classes; it might be interesting to remove them in order to reduce the number of classes and thus limit the potential for confusion.
",,,,,,,,,,,,,,MATH-404,MATH-397,MATH-394,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-09-01 07:49:58.332,,,false,,,,,,,,,,,,,,2242,,,Mon Feb 13 10:46:26 UTC 2012,,,,,,0|i0rumf:,160605,,,,,,,,"31/Aug/10 10:44;erans;IMO, the most urgent issue to be dealt with is the design decision to make the convergence checking criteria independent from the optimization algorithm. [This has a direct influence on the API and should be stabilized in v3.0.]

Points 1, 2, 6, 10 in the issue description indicate the problems with this approach.

IMO, while modularization is often a good thing, the attempt produces several drawbacks in this case:
* The user interface is more difficult than it could be: the user has to specify a ""complex"" {{ConvergenceChecker}} object, instead of just {{double}}s (relative and/or absolute threshold/tolerance/accuracy).
* At the CM level, there is no single implementation of {{ConvergenceChecker}} that is applicable for every algorithm (see {{BrentOptimizer}} and {{LevenbergMarquardtOptimizer}}).
* If we don't set a default, the user will get a NPE.
* If we set a default convergence checker, in an attempt to make usage easier, it will still not be very useful because the user will have to set a new checker as soon as he needs to change the algorithm accuracy.
* The user cannot rely on the description of convergence given by a {{ConvergenceChecker}}'s implementation because if he chooses a checker that is ""incompatible"" with the optimization algorithm, the behaviour is ""undefined"". I have overridden {{setConvergenceChecker}} to forbid the setting of anything other than a {{BrentConvergenceChecker}} but this completely voids the intended flexibility of having a separate convergence checking procedure.
* If we want to enforce the strict separation between optimization and convergence checking, we'll have to _modify_ standard algorithm so that they fit into the mold (i.e. we must ensure that ""convergence checking"" and ""optimization"" parts are indeed independent). This may not even be possible without removing features of some algorithms (e.g. specific tolerance definitions in ""Levenberg-Marquardt""). And if it is done, then the resulting algorithm will not be standard anymore!

In light of all these problems, we should seriously re-examine whether the {{ConvergenceChecker}} approach is the right way to go.

In the end, it is very well possible the best that can be done is to pass appropriate arguments to the constructor of each optimizer's implementation. [With the consequence that if one wants to change the convergence checking behaviour, one has to create a new instance. (Side note: this is consistent with going towards instances' immutability.)]","01/Sep/10 07:49;dimpbx;> In light of all these problems, we should seriously re-examine whether the
>  ConvergenceChecker approach is the right way to go.

Even without these problems, I am very sceptical about the usefulness of so much flexibility.  What users of numerical libraries have been used to is to pass some kind of precision (absolute or relative) to a solver, minimizer ... and to get the anser which matches the criterion.
Why would CM's users expect more?  I do see why some CM developers would like to give more, simply because it is fun to design such a general abstract checker but do the users care?  If we put the most flexible checker, the users will have to fill it  which might be a good reason to switch to a simpler (less flexible) library.

As long as the documentation describes how the precision (argument) is used by the class, the user knows what to expect. 
","01/Sep/10 08:40;luc;The ConvergenceChecker principle is inherited from the mantissa library that was merged with commons-math in 2006-2007.
I introduced it in mantissa because it leveraged very simply the two most important cases I encountered : sometimes one wants to check convergence on the free variable (i.e. the input) and sometimes one wants to check convergence on the cost (i.e. the output). I think even one time I add to check on both input and output using a custom checker but am not really sure. These two cases are real one and depend on the user problem.

I don't agree with the word ""complex"" about the interface, even if quoted. Furthermore, the user is not forced to implement it by himself, there are two implementations that fits the two cases above: the SimpleXxxChecker. So a user typically has to call:

  optimizer.setConvergenceChecker(new SimpleVectorialValueChecker(epsilon1, epsilon2));

This is a separate call from the optimize call, but it could be merged if needed. However, I still consider being able to check convergence either in x or in y is important and should be kept.","01/Sep/10 12:43;erans;The main problem is not whether it is deemed complicated or not to initialize the checker.
The fact is that the convergence checker procedure is _not_ (always) independent of the algorithm (as in the cases of {{BrentOptimizer}} and {{LevenbergMarquardtOptimizer}}).
Currently it is very dangerous to assume that convergence check is independent and will always work correctly. This severely impacts the robustness of CM; thus it is a big problem, and it seems that the current approach cannot solve it.
* As I have suggested, we could try to _modify_ the algorithms in such a way that an independent checking procedure is always valid. The drawback is that we'll have non-standard behaviour of otherwise well-known algorithms and some users won't like it (cf. request by the reporter of issue [MATH-362|https://issues.apache.org/jira/browse/MATH-362]).
* An alternative is to go back to a ""simple"", algorithm-specific convergence check (i.e. what most users are already used to), with tolerances parameters passed through the constructor.
* To enhance the checking (in effect, trying to recover the flexibility of {{ConvergenceChecker}} functionality), I'd propose to optionally enable a ""callback"" to be called after each iteration with two arguments (the previous and current best point/value pairs) and that should return ""SUCCESS"" (an {{enum}}) if the current best point is good enough and ""CONTINUE"" if it is not.
It is indeed very much like the existing {{converged}} method but the main idea is that the default convergence check (the one tied with the optimization algorithm) will always have a higher priority: if it passes, we exit from the iteration loop. The callback is a _second_ check that gives the opportunity to exit earlier if some additional, user-defined, criteria are met.

Combining the last two points, we
# always provide a self-consistent behaviour in the simple use-cases
# make it clear that the user can optionally define an ""out-of-band"" checking criterion: it is clearly independent in that any parameter (apart from the previous and current points) used to check convergence must be separately collected and passed to the constructor of the callback.

What do you think?
","01/Sep/10 16:08;luc;Your two-step feature seems fine to me.
The callback can even be exactly the existing convergence checker we already have as its sole method returns a boolean which is basically equivalent to a SUCCESS/CONTINUE alternative and it would be simpler for users to upgrade to the new design.
Documentation should state that first level double threshold must always be passed and only the checker is optional and used if not null.","06/Sep/10 09:13;erans;Revision 992976 resolves the problems indicated in point 1, 2 and 10  for {{BrentOptimizer}}, {{PowellOptimizer}} and {{LevenbergMarquardtOptimizer}}: there is a default (always applied) convergence check and a additional (optional) one defined with a sub-class of {{ConvergenceChecker}}.
The other optimizer classes must still be upgraded to this 2-level check.
","23/Sep/10 12:12;erans;Point 13 resolved in revision 1000422.
","24/Nov/10 15:08;erans;About point 6 (initialization of the {{ConvergenceChecker}}):

I think the safest would be to pass it as argument to the constructor(s). So, we would have, on the one hand, constructors that take the arguments for the ""standard"" convergence criterion of the algorithm and, on the other, constructors that take a {{ConvergenceChecker}} argument. That way, the same kind of information (how to stop) is passed at the same place (the constructors).
And I'd remove the {{setConvergenceChecker}} (another step towards immutability).

This procedure will also ""force"" implementations that use ""line search"" to take the necessary precaution of syncing the tolerances of the internal optimizer or solver. It is a problem for {{PowellOptimizer}} and, I just noticed, for {{NonLinearConjugateGradientOptimizer}}: A user unaware of the workings of the algorithm might use unnecessarily low tolerances thresholds for the line search, so that the line search computation will be take a needlessly high number iterations.
When given the ""main"" algorithm's tolerance levels, the constructor should adapt the tolerance of its internal optimizer.

The would also entail the removal of the {{setLineSearchSolver}} method in {{NonLinearConjugateGradientOptimizer}}.

What do you think?
","20/Jul/11 13:17;luc;Coming back to this old issue.

I'm fine with previous Gilles comment about passing the checker as argument to constructor.",18/Aug/11 14:12;erans;{{GaussNewtonOptimizer}} updated in revision 1159233.,"25/Aug/11 09:29;erans;{{LevenbergMarquardtOptimizer}} updated in revision 1161463.
",01/Sep/11 12:32;erans;{{NonLinearConjugateGradientOptimizer}} updated in revision 1164044.,"01/Sep/11 23:38;erans;{{SimplexOptimizer}} updated in revision 1164300.
",02/Sep/11 00:07;erans;{{PowellOptimizer}} updated in revision 1164303.,"13/Feb/12 10:46;erans;No comments for a long time; remaining issues, if any, probably have their own ticket. Thus, resolving.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Regression newSampleData methods inconsistently create / omit intercepts,MATH-411,12472796,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,28/Aug/10 22:14,23/Mar/11 20:20,07/Apr/19 20:38,13/Sep/10 02:04,2.0,2.1,,,,,,2.2,,,0,,,,,,,,"The newSampleData(double[], nrows, ncols) method used in the unit tests adds a unitary column to the design matrix, resulting in an intercept term being estimated among the regression parameters.  The other newSampleData methods do not do this, forcing users to add the column of ""1""s to estimate models with intercept.  Behavior should be consistent and users should not have to add the column.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:20:06.182,,,false,,,,,,,,,,,,,,150558,,,Wed Mar 23 20:20:06 UTC 2011,,,,,,0|i0rumv:,160607,,,,,,,,"08/Sep/10 01:30;psteitz;Fixed in r993574.  Modified multiple regression newSample methods to ensure that by default in all cases, regression models are estimated with intercept terms.  Prior to the fix for this issue,  newXSampleData(double[][]), newSampleData(double[], double[][]) and newSampleData(double[], double[][], double[][]) all required columns of ""1's  to be inserted into the x[][] arrays to create a model with an intercept term;while newSampleData(double[], int, int) created a model including an intercept term without requiring the unitary column.  All methods have  been changed to eliminate the need for users to add unitary columns to specify regression models.

Leaving open until MATH-409 is resolved. 
","23/Mar/11 20:20;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Wrong variable in ""FunctionEvaluationException""",MATH-410,12472576,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,erans,erans,erans,26/Aug/10 08:57,23/Mar/11 20:19,07/Apr/19 20:38,26/Aug/10 09:01,2.1,,,,,,,2.2,,,0,,,,,,,,"Some constructors use both {{argument}} and {{arguments}} as argument names and, in the body, {{argument}} is sometimes used in places where it should have been {{arguments}}.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:19:44.802,,,false,,,,,,,,,,,,,,150557,,,Wed Mar 23 20:19:44 UTC 2011,,,,,,0|i0run3:,160608,,,,,,,,"26/Aug/10 09:01;erans;Revision 989543.
","23/Mar/11 20:19;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Regression API should allow specification of whether or not to estimate intercept term,MATH-409,12472346,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,24/Aug/10 09:55,23/Mar/11 20:19,07/Apr/19 20:38,13/Sep/10 02:02,2.0,2.1,,,,,,2.2,,,0,,,,,,,,The OLS and GLS regression APIs should support estimating models including intercepts using design matrices including only variable data.,,,,,,,,,,MATH-411,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:19:13.668,,,false,,,,,,,,,,,,,,150556,,,Wed Mar 23 20:19:13 UTC 2011,,,,,,0|i0runb:,160609,,,,,,,,13/Sep/10 02:02;psteitz;Fixed in r996404 (both trunk and 2.x branch),"23/Mar/11 20:19;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GLSMultipleLinearRegression has no nontrivial validation tests,MATH-408,12472222,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,23/Aug/10 03:11,23/Mar/11 20:18,07/Apr/19 20:38,12/Dec/10 21:49,2.0,2.1,,,,,,2.2,,,0,,,,,,,,"There are no non-trivial tests verifying the computations for GLSMultipleLinearRegression.  Tests verifying computations against analytically determined models, R or some other reference package / datasets should be added to ensure that the statistics reported by this class are valid.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:18:48.796,,,false,,,,,,,,,,,,,,150555,,,Wed Mar 23 20:18:48 UTC 2011,,,,,,0|i0runj:,160610,,,,,,,,"12/Dec/10 21:49;psteitz;Added a non-trivial test in r1044935.  While still not really a full verification test, it does at least verify that the GLS impl provided does better than OLS for models with error structure conforming to its covariance matrix.","23/Mar/11 20:18;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation improvements for multiple regression classes,MATH-407,12472221,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,23/Aug/10 03:07,23/Mar/11 20:18,07/Apr/19 20:38,20/Sep/10 01:57,2.0,2.1,,,,,,2.2,,,0,,,,,,,,The user guide examples showing how to set up and estimate linear models using OLS and GLS multiple regression need to be updated to reflect changes in the API.  The javadoc for these classes and user guide descriptions also need to be improved to make it clear how to estimate a model with an intercept term.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:18:29.971,,,false,,,,,,,,,,,,,,150554,,,Wed Mar 23 20:18:29 UTC 2011,,,,,,0|i0runr:,160611,,,,,,,,20/Sep/10 01:57;psteitz;Fixed in r998761,"23/Mar/11 20:18;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong weight handling in Levenberg-Marquardt,MATH-406,12471625,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,dimpbx,dimpbx,dimpbx,14/Aug/10 21:57,23/Jul/12 23:24,07/Apr/19 20:38,14/Aug/10 22:02,2.1,,,,,,,,,15/Aug/10 00:00,0,,,,,,,,"A comparison with a Fortran version of Levenberg-Marquardt reveals that when observations have different weights, the 2.1 version reaches a value of the function which does not necessary correspond to the minimum",,,,,,,,,,,,,,,,,,,,,14/Aug/10 21:59;dimpbx;MATH-406.patch;https://issues.apache.org/jira/secure/attachment/12452114/MATH-406.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:18:04.523,,,false,,,,,,,,,,,,,,150553,,,Wed Mar 23 20:18:04 UTC 2011,,,,,,0|i0runz:,160612,,,,,,,,14/Aug/10 21:59;dimpbx;Correction patch.,"23/Mar/11 20:18;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent result from Levenberg-Marquardt,MATH-405,12471284,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,dimpbx,dimpbx,dimpbx,11/Aug/10 13:24,23/Jul/12 23:24,07/Apr/19 20:38,11/Aug/10 13:46,2.1,,,,,,,,,11/Aug/10 00:00,0,,,,,,,,"Levenberg-Marquardt (its method doOptimize) returns a VectorialPointValuePair.  However, the class holds the optimum point, the vector of the objective function, the cost and residuals.  The value returns by doOptimize does not always corresponds to the point which leads to the residuals and cost",,,,,,,,,,,,,,,,,,,,,11/Aug/10 13:27;dimpbx;MATH-405.patch;https://issues.apache.org/jira/secure/attachment/12451776/MATH-405.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:17:42.628,,,false,,,,,,,,,,,,,,34171,,,Wed Mar 23 20:17:42 UTC 2011,,,,,,0|i0ruo7:,160613,,,,,,,,11/Aug/10 13:27;dimpbx;Correction patch,"23/Mar/11 20:17;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Confusing interface for ""LevenbergMarquardtOptimizer""",MATH-404,12471107,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Later,erans,erans,erans,09/Aug/10 11:44,24/Mar/12 16:17,07/Apr/19 20:38,30/Aug/10 13:53,2.1,,,,,,,3.0,,,0,,,,,,,,"{{LevenbergMarquardtOptimizer}} inherits from {{AbstractLeastSquaresOptimizer}} which in turn implements {{DifferentiableMultivariateVectorialOptimizer}}. That interface mandates methods for setting and getting a {{VectorialConvergenceChecker}}.
In v2.1, however, that checker is never used! The convergence check is performed using parameters specific to the Levenberg-Marquardt algorithm. Such circumvention of the superclass interface is confusing and leads to totally unexpected behaviour (such as changing the values of the thresholds of the {{VectorialConvergenceChecker}} being ineffective).
In the development version, the default constructor of {{LevenbergMarquardtOptimizer}} sets the the {{VectorialConvergenceChecker}} field to ""null"" and when such is the case, the behaviour is as in v2.1. Although it is documented, this is still confusing since it is impossible to use {{LevenbergMarquardtOptimizer}} through its {{DifferentiableMultivariateVectorialOptimizer}} interface: When using the {{VectorialConvergenceChecker}}, one does not know what parameters to use in order to reproduce the results obtained with the LM-specific convergence check (i.e. how to reproduce the result from v2.1).
Unless I'm missing something, I think that there should be an LM-specific implementation of {{VectorialConvergenceChecker}} that, when given the usual relative and absolute thresholds, can perform a check that will give the same result as the currently specific check (when the ""checker"" field is ""null"").
",,,,,,,,,,,,,MATH-362,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-08-09 18:36:14.162,,,false,,,,,,,,,,,,,,34212,,,Mon Aug 30 13:53:12 UTC 2010,,,,,,0|i0ruof:,160614,,,,,,,,"09/Aug/10 18:36;luc;The problem was identified and discussed as MATH-362. It was decided to let both convergence methods available.

The reason there are two different way is that the Levenberg-Marquardt implementation originally came from Netlib and I kept the way it behaved. I think the general interface with the new generic convergence was set up later and at that time I forgot to implement it properly, so the settings were ignored.

Reporter of issue 362 explicitly asked to keep the ortho-tolerance setting and this setting does not fit with the general scheme.","10/Aug/10 11:37;erans;Sorry I hadn't followed that other report.

{quote}
It was decided to let both convergence methods available. 
{quote}

Switching between two convergence checking procedures, based on whether a field is {{null}} or not, is at best a temporary workaround, but it is not a good solution.

As explained above, from an OOP point-of-view, it is surprising that a class completely circumvents its base class interface.
At least one of the following is wrong:
* {{LevenbergMarquardtOptimizer}} inherits from {{AbstractLeastSquaresOptimizer}}
* {{LevenbergMarquardtOptimizer}} has a second interface for convergence checking
*  {{AbstractLeastSquaresOptimizer}} defines the interface for  convergence checking

{quote}
[...] does not fit with the general scheme.
{quote}

Then maybe the scheme needs to be reviewed so that it is general enough to fit.
Allow me to remind what you said: convergence checking is independent from the optimization algorithm.
But then, in the LM implementation, this doesn't hold...

If it is really impossible to fit LM within the hierarchy it currently belongs to, then it should not belong to it, since one cannot leverage the advantages of ""interface programming"" anyways.
","10/Aug/10 12:10;luc;{quote}
Switching between two convergence checking procedures, based on whether a field is null or not, is at best a temporary workaround, but it is not a good solution.
{quote}

I agree.

{quote}
Then maybe the scheme needs to be reviewed so that it is general enough to fit.
{quote}

Or LevenbergMarquardtOptimizer needs to be changed and the orthogonality concept be finally discarded.

{quote}
Allow me to remind what you said: convergence checking is independent from the optimization algorithm.
But then, in the LM implementation, this doesn't hold...
{quote}

I know, and I am not happy with this. However, I don't want LevenbergMarquardtOptimizer to be special. It _must_ fit. We can take the opportunity of a 3.0 major release to fix this problem too, with some incompatible changes. What would you propose for this ?
","10/Aug/10 15:40;erans;{quote}
What would you propose for this ?
{quote}

I don't know.

However, it seems that this ""non-fitting checker"" case is not isolated. I wanted to replace the original check in ""BrentOptimizer"" (package ""optimization.univariate"") by a call to an appropriate subclass of ""RealConvergenceChecker"", but here too there are more values to be considered than those stored in a pair of ""RealPointValuePair"". The check needs
* the ""current"" point
* the points at both interval ends

but it does not use the ""previous"" point.

So it seems that this also does not fit with the ""converged"" method of the ""RealConvergenceChecker"" interface.

At first sight, I'd say that there should be a more general ""ConvergenceChecker"" (not existing yet) interface. Maybe using generics...
","11/Aug/10 11:09;erans;I'm trying to define a more general ""ConvergenceChecker"" interface. This is an incompatible change.
","30/Aug/10 13:53;erans;Final resolution is delegated to issue [MATH-413|https://issues.apache.org/jira/browse/MATH-413].
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Complex.ZERO.pow(Complex.ONE) gives NaN in unit tests,MATH-402,12470918,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Won't Fix,,axelclk,axelclk,05/Aug/10 16:39,01/Jan/11 01:26,07/Apr/19 20:38,01/Jan/11 01:26,1.2,2.0,2.1,,,,,2.2,,,0,,,,,,,,"Why does this unit test in ComplexTest.java gives NaN?
I expected to get Complex.ZERO as the result?

{code:java} 
   public void testPowZero() {
       TestUtils.assertSame(Complex.NaN,
               Complex.ZERO.pow(Complex.ONE));
...
   }
{code} 

I would suggest something like this for the Complex#pow() method:
{code:java} 
    public Complex pow(Complex x) {
        if (x == null) {
            throw new NullPointerException();
        }
        if (x.imaginary == 0.0) {
          if (real == 0.0 && imaginary == 0.0) {
            if (x.real == 0.0){      	
            	return Complex.ZERO;
            }
          }
          if (x.real == 1.0) {
          	return this;
          }
        }
        return this.log().multiply(x).exp();
    }
{code} 
 ",Issue 15 http://code.google.com/p/symja/issues/detail?id=15,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-08-08 12:15:04.237,,,false,,,,,,,,,,,,,,34170,,,Thu Dec 30 14:49:35 UTC 2010,,,,,,0|i0ruov:,160616,,,,,,,,"07/Aug/10 11:25;axelclk;Sorry for my stupid suggestion in the first post.

It should be something like this:
{code:java}
	public Complex pow(Complex x) {
		if (x == null) {
			throw new NullPointerException();
		}
		if (x.real == 0.0 && x.imaginary == 0.0) {
			if (real == 0.0 && imaginary == 0.0) {
				// 0^0 => NaN
				return Complex.NaN;
			}
			// THIS^0 => 1 for THIS<>0
			return Complex.ONE;
		}
		if (real == 0.0 && imaginary == 0.0) {
			// 0^x => 0 for x<>0
			return Complex.ZERO;
		}
		return this.log().multiply(x).exp();
	}
{code}","08/Aug/10 12:15;psteitz;Thanks for reporting this. If we do make the suggested change, we need to change the formulas in the javadoc to reflect the changed definition.  The value returned is a consequence of how we have defined complex exponentiation, which is spelled out in the javadoc for pow, exp and log.   I am open to changing the definition, but we should make sure that the javadoc is modified to reflect whatever change we make.  Interested in what others think the definition should be.","08/Aug/10 12:26;luc;I know we do not follow C99 in many aspects, but what does C99 standard say about this specific topic ?","08/Aug/10 14:58;psteitz;The latest draft ISO C spec appears now to be public and is here:
﻿http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1494.pdf
The complex specifications are in Annex G.  Does not look like anything has changed since C99x, though I have not done side-by-side comparisons.  It looks to me like what we have in the code now matches the spec; but the spec does not specifically call out (unless I missed it) specialization to real arguments.  What is causing the NaN is the singularity in the log function.   Colt 1.1 looks like it matches our code.  R returns  0 + 0i.
﻿﻿﻿","12/Dec/10 21:51;psteitz;I am leaning toward closing as WONTFIX.  Going once, going twice...",30/Dec/10 14:49;mikl;I'm also leaning towards agreeing with Phil. It also seems like the user itself has implemented the change (see http://code.google.com/p/symja/issues/detail?id=15). Another option is to postpone to 3.0?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect javadoc for RealVector subtract method,MATH-399,12470444,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,julien ruiz,julien ruiz,29/Jul/10 14:05,23/Mar/11 20:16,07/Apr/19 20:38,29/Jul/10 19:29,,,,,,,,,,,0,,,,,,,,"subtract method decription is :

069         * Compute this minus v.
070         * @param v vector to be subtracted
071         * @return this + v
072         * @throws IllegalArgumentException if v is not the same size as this
073         */
(...)
080         * @return this + v

Should be : return this - v

Besides, method might be renamed to substract .",,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-29 19:29:04.765,,,false,,,,,,,,,,,,,,150550,,,Wed Mar 23 20:16:28 UTC 2011,,,,,,0|i0rupj:,160619,,,,,,,,"29/Jul/10 19:29;luc;fixed in subversion repository as of r980544.
thanks for reporting this.

On a side note, it really seems subtract is the proper spelling whereas substract is a common misspelling. I am not a native english speaker, so I checked several references.","30/Jul/10 08:06;julien ruiz;Thanks for solving it.

I really would have bet for substract. But I double checked and it seems I
was wrong indeed (french speaking mistake maybe).

I hope my next contributions will be more valuable...

Julien
2010/7/29 Luc Maisonobe (JIRA) <jira@apache.org>

","23/Mar/11 20:16;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MathUtilsTest uses Math.nextUp from Java 6,MATH-398,12470365,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,jlmuir,jlmuir,28/Jul/10 17:31,23/Mar/11 20:15,07/Apr/19 20:38,28/Jul/10 18:49,2.2,,,,,,,,,,0,,,,,,,,"The class  {{org.apache.commons.math.util.MathUtilsTest}} uses the method {{Math.nextUp(double)}} which is only available in Java 6, not in Java 5.  This makes Commons Math dependent on Java 6 rather than Java 5 which is in conflict with item 5 at the top of [Commons Math Overview|http://commons.apache.org/math/index.html].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-28 18:49:31.514,,,false,,,,,,,,,,,,,,150549,,,Wed Mar 23 20:15:55 UTC 2011,,,,,,0|i0rupr:,160620,,,,,,,,"28/Jul/10 18:49;luc;fixed in subversion repository as of r980154
thanks for reporting this bug","23/Mar/11 20:15;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Bugs in ""BrentOptimizer""",MATH-395,12470120,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,erans,erans,erans,25/Jul/10 21:26,23/Mar/11 20:14,07/Apr/19 20:38,28/Jul/10 12:11,2.1,,,,,,,2.2,,,0,,,,,,,,"I apologize for having provided a buggy implementation of Brent's optimization algorithm (class ""BrentOptimizer"" in package ""optimization.univariate"").
The unit tests didn't show that there was something wrong, although (from the ""changes.xml"" file) I discovered that, at the time, Luc had noticed something weird in the implementation's behaviour.
Comparing with an implementation in Python, I could figure out the fixes. I'll modify ""BrentOptimizer"" and add a test. I also propose to change the name of the unit test class from ""BrentMinimizerTest"" to ""BrentOptimizerTest"".
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:14:58.357,,,false,,,,,,,,,,,,,,34142,,,Wed Mar 23 20:14:58 UTC 2011,,,,,,0|i0ruqf:,160623,,,,,,,,"26/Jul/10 12:19;erans;Bugs corrected in revision 979257.
Not resolving yet because the implementation still does not behave as the Python one. I've added a unit test that indicates the discrepancies (with ""XXX"" markers).
","28/Jul/10 12:11;erans;Last bug fixed in revision 980032.
[This revision also contains the modifications due to the changes in ""AbstractUnivariateRealOptimizer"".]

The test comparing with Python has been removed because a tracing of the execution paths (in Python and Java) showed that the remaining discrepancies were due to different values being used for the ""golden ratio"" constant.
","23/Mar/11 20:14;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Method ""getResult()"" in ""MultiStartUnivariateRealOptimizer""",MATH-393,12470096,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,erans,erans,erans,25/Jul/10 00:14,23/Mar/11 20:14,07/Apr/19 20:38,25/Jul/10 12:31,,,,,,,,2.2,,,0,,,,,,,,"In ""MultiStartUnivariateRealOptimizer"" (package ""optimization""), the method ""getResult"" returns the result of the last run of the ""underlying"" optimizer; this last result might not be the best one, in which case it will not correspond to the value returned by the ""optimize"" method. This is confusing and does not seem very useful. I think that ""getResult"" should be defined as
{code} 
public double getResult() {
    return optima[0];
}
{code}
and similarly
{code}
public double getFunctionValue() {
    return optimaValues[0];
}
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-25 08:38:21.012,,,false,,,,,,,,,,,,,,34156,,,Wed Mar 23 20:14:21 UTC 2011,,,,,,0|i0ruqv:,160625,,,,,,,,"25/Jul/10 08:38;luc;You're right.
Don't forget to fix also the Javadoc in the interface which is also misleading when you fiw this. I have checked other implementations of this interface (BrentMinimizer) and what it returns is really the optimum.","25/Jul/10 12:31;erans;Changed in revision 979032.
","23/Mar/11 20:14;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
calculateYVariance in OLS/GLSMultipleLinearRegression uses residuals not Y vars,MATH-392,12469843,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,markdevaney,markdevaney,21/Jul/10 18:43,23/Mar/11 20:13,07/Apr/19 20:38,22/Aug/10 13:16,2.1,,,,,,,2.2,,,0,,,,,,,,"Implementation of OLS/GLSMultipleLinearRegression is:
@Override
173        protected double calculateYVariance() {
174            RealVector residuals = calculateResiduals();
175            return residuals.dotProduct(residuals) /
176                   (X.getRowDimension() - X.getColumnDimension());
177        }

This gives variance of residuals not variance of the dependent (Y) variable as the documentation suggests.
",,,,,,,,,,,,,,,,,,,,,26/Jul/10 19:41;markdevaney;patch;https://issues.apache.org/jira/secure/attachment/12450508/patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-26 09:55:26.936,,,false,,,,,,,,,,,,,,150545,,,Wed Mar 23 20:13:58 UTC 2011,,,,,,0|i0rur3:,160626,,,,,,,,26/Jul/10 09:55;psteitz;Thank you for reporting this.  Patches welcome!,"26/Jul/10 16:32;markdevaney;Can't test a patch as I'm not able to build current repository version:
math/src/test/java/org/apache/commons/math/optimization/univariate/BrentOptimizerTest.java:[28,39] cannot find symbol
symbol  : class SincFunction

Implementation for both GLS/OLS:

protected double calculateYVariance() {
    return new Variance().evaluate(Y);
}
",26/Jul/10 18:27;luc;There was an error in a file committed this afternoon. It should be OK now.,"26/Jul/10 19:41;markdevaney;corrected implementations of calculateYVariance() for OLS/GLSMultipleRegression

added unit tests for both calculateYVariance implementations

fixed AbstractMultipleRegression.estimateRegressionParametersStandardErrors() to use residuals ","22/Aug/10 13:16;psteitz;Fixed in 987897.   I added calcluate/estimateErrorVariance methods to return what was previously incorrectly reported as ""Y variance.""
Thanks for the patch!","23/Mar/11 20:13;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent behaviour of constructors in ArrayRealVector class,MATH-391,12469795,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,rwerp,rwerp,21/Jul/10 08:57,23/Mar/11 20:13,07/Apr/19 20:38,03/Oct/10 16:43,2.1,,,,,,,2.2,,,0,,,,,,,,"ArrayRealVector(double[] d) allows to construct a zero-length vector, but ArrayRealVector(double[] d, boolean copyArray) doesn't. Both should allow this as zero-length vectors are mathematically well-defined objects and they are useful boundary cases in many algorithms.

This breaks some arithmetic operators (addition) on zero-length real vectors which worked in 2.0 but don't work in 2.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-10-03 01:55:26.017,,,false,,,,,,,,,,,,,,150544,,,Wed Mar 23 20:13:27 UTC 2011,,,,,,0|i0rurb:,160627,,,,,,,,"03/Oct/10 01:55;psteitz;I agree that the code should be consistent.  I agree as well that a zero-dimensional vector is legit.   Can anyone explain why ArrayRealVector(double[] d, boolean copyArray) requires positive length?",03/Oct/10 08:47;luc;Most probably my bad ...,"03/Oct/10 16:43;luc;Fixed in subversion repository as of r1003993 for barnch 2.X and r1003994 for trunk.
Note that the same problem occurred also in ArrayFieldVector but the fix is different. For Field-based vectors, we need to get the field, so either we use a non-empty array and retrieve the field from the first array element or we add a parameter for the field and allow the array to be empty. The two choices are now possible, as new constructors have been added and the javadoc updated to explain this behavior.
Thanks for reporting the issue.","23/Mar/11 20:13;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Simplex Solver is very inaccurate on a large problem, even a very low value for epsilon",MATH-390,12469760,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,pcbouman,pcbouman,20/Jul/10 22:47,20/Jul/10 23:32,07/Apr/19 20:38,20/Jul/10 23:32,2.1,,,,,,,,,,0,,,,,,,,"I'm currently playing with a program for solving a rather simple chess puzzle. The goal is to place 12 knights on a 8x8 board, such that each field is either attacked by a knight, or contains a knight. To solve this problem (and different variants) I want to use a handcrafted Branch and Bound algorithm that uses Linear Programming to calculate an upperbound on the number of fields that can be covered by a certain amount of knights.

The idea is to create variables for each field that has to be covered, and to create variables for each field to contain a knight. A cover variable can only become positive if a corresponding knight variable for an adjacent field is also positive, there is a limit to the amount of knights we may place (so the sum of all knight variables cannot be larger than 12) and the cover variables cannot be larger than one. Also, only the cover variables have a coefficient of one in the objective function, all other variables have zero. Because we want to cover the entire board our goal will be to maximize the objective function, since we want to maximize the number of fields that are covered.

Since a basic chessboard has 64 fields and since it is possible to cover the chessboard with 12 knights, we know there is an integer solution that has value 64. Since we are solving a relaxed variant of the problem, the value should be at least 64. However, when I use the Simplex Solver, I get a value of around 58.6, which is much too low. Even when I relax the constraints in such a fashion that 64 knights may be placed on the board, the solution value remains the same. I've lowered the value of epsilon as much as I can and it still gives the incorrect value. What makes it worse is that the calculation is totally useless as an upperbound (if the value would have been around 70, it would have been an upperbound at least).

I've heard that using the revised simplex method is a lot better with respect to stacked errors, so I am not sure this is really a bug, or just a problem that arises when the two phase simplex method is used for large problems.

I will try to attach a code example that implements the problem (but possibly isn't that readable).","Windows Vista Enterprise
Runtime:
java version ""1.6.0_20""
Java(TM) SE Runtime Environment (build 1.6.0_20-b02)
Java HotSpot(TM) Client VM (build 16.3-b01, mixed mode, sharing)

Compiler:
javac 1.6.0_13",,,,,,,,,,,,,,,,,,,,20/Jul/10 23:31;pcbouman;CorrectLPTestCase.java;https://issues.apache.org/jira/secure/attachment/12449999/CorrectLPTestCase.java,20/Jul/10 22:48;pcbouman;LPTestCase.java;https://issues.apache.org/jira/secure/attachment/12449991/LPTestCase.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150543,,,Tue Jul 20 23:32:12 UTC 2010,,,,,,0|i0rurj:,160628,,,,,,,,"20/Jul/10 22:49;pcbouman;Example of the 8x8 Knight covering Chess problem. The objective value should at least be 64, but it is around 59.","20/Jul/10 23:30;pcbouman;Hmm, it seems I made a programming mistake in the type of the relationship: I used an equality where I should have used a greater-equals. I created a much nicer version of the example, which actually works. Feel free to use it for an example or something.

My bad, I will close the issue.","20/Jul/10 23:31;pcbouman;The correct and more readable example, which actually works.",20/Jul/10 23:32;pcbouman;It seems I made a programming error. I included a correct example to solve the problem.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ODE integrator: different size needed for state vector and tolerance error vector dimension,MATH-388,12469625,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,pparraud,pparraud,19/Jul/10 15:35,23/Mar/11 20:12,07/Apr/19 20:38,23/Jul/10 09:04,,,,,,,,2.2,,,0,,,,,,,,"The user should be allowed to chose a tolerance vector dimension different from the state vector dimension.
For example, using the FirstOrderIntegratorWithJacibians, we don't want to set some tolerance error for the jacobian part in order to not interfere with the main state vector integration.
This is really a problem with the dimension of the tolerance vector, there is no work-around assigning some particular value to the tolerance vector.",,,,,,,,,,,,,,,,,,,,,20/Jul/10 21:51;luc;math-388.patch;https://issues.apache.org/jira/secure/attachment/12449981/math-388.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-07-19 18:52:11.785,,,false,,,,,,,,,,,,,,34186,,,Wed Mar 23 20:12:32 UTC 2011,,,,,,0|i0rurz:,160630,,,,,,,,"19/Jul/10 18:52;luc;This issue is in some sense related to MATH-380.
I intend to completely change the way to solve ODE problems with Jacobians soon using a much better architecture (the current implementation is really bad).
I will however encounter the exact same problem with the new architecture so need to solve MATH-388 before MATH-380.

The simple Initial Value Problem is:
   y(t0) = y0
   y'(t) = f(t,y)
its solution is a function y(t)

Here, we want to integrate at the same time this problem and an extension of this problem which is:
   dy(t0)/dy0      = Jy0
   dy(t0)/dp       = Jp0
   d[dy(t)/dy~0~]/dt = one variational equation
   d[dy(t)/dp]/dt  = another variational equation

So we end up with a very large state vector containing the initial y and also two jacobians. As an example, in some orbit determination problems the size of the initial y vector is 7 and the size of the extended y can be 7 + (7 * 7) + (7 * 2) = 70.

However, we want to compute the step sizes and error estimation only on the first 7 parameters, not the 63 final ones.

The dimension of the state vector is specified by the user who implements the FirstOrderDifferentialEquations method and hence provides a getDimension() method. There is no way now to say: my problem is dimension 70 but error should only be computed on the first 7 components of the vector.

So we need to add a getErrorDimension() or getMainProblemDimension() or something like that. In the example above, this new method would return 7 and the existing getDimension would return 70. The ODE integrators have to be changed (this is a simple change) to use the appropriate dimension in the various loops.

Adding the new method in the FirstOrderDifferentialEquations is bad because it breaks compatibility on user code (the interface is explicitly here to be implemented by users as it represents the problem they want our integrators to solve). I would suggest to extend the interface using something like:

{code:title=ExtendedFirstOrderDifferentialEquations.java}
public interface ExtendedFirstOrderDifferentialEquations
  extends FirstOrderDifferentialEquations {

    /** Return the dimension of the main problem.
      * <p>
      * The main problem represent the first part of an ODE state
      * and the error estimations and adaptive step size computation
      * should be done on this first part only, not on the final part
      * of the state which represent an extension of the main problem.
      * </p>
      * @return dimension of the main problem, must be lesser than or
      * equal to the {@link #getDimension() total dimension}
      */
    int getMainProblemDimension();

}
{code}

Then ODE integrators would have to check if the user provided FirstOrderDifferentialEquations implements only the base interface or the extended interface. In the former case, getDimension() would be used for both state dimension and error dimension, in the later case the appropriate method would be used for each loop.

This is the only solution I see that is backaward compatible. It implies putting in our integrators code statements like:

{code}
  int errorDimension;
  if (equations instanceof ExtendedFirstOrderDifferentialEquations) {
    errorDimension = equations.getDimension();
  } else {
    ExtendedFirstOrderDifferentialEquations extEq =  (ExtendedFirstOrderDifferentialEquations) equations;
    errorDimension = extEq.getMainProblemDimension();
  }
{code}

This is not really elegant, but I don't see another way yet.

One problem is that some adventurous users may have implemented their own ODE integrators and would not include this new feature, because basically the requirements for an integrator are only defined in terms of FirstOrderDifferentialEquations not on the proposed new ExtendedFirstOrderDifferentialEquations. However, this is not an incompatible change: these custom integrator currently use the complete dimension to compute size, and they would still do it. So this problem is probably not important (and I'm pretty sure noone as attempted yet to implement their own integrators from scratch).

What do other developers think ?
","20/Jul/10 21:51;luc;Could you check whether this patch solves the problem or not ?
You should have your ODE implement ExtendedFirstOrderDifferentialEquations instead of FirstOrderDifferentialEquations, so you would need to add an implementation for the getMainDimension method.
I did not have the time to do any verification by myself except running existing tests that do not use this new interface. So there may be some problems with the patch, mainly things like ArrayIndexOutOfBoundsException, so use this with care.

I will wait for your feedback before checking this in the subversion repository.","21/Jul/10 13:27;pparraud;After a few tests, the patch:
*  seems ok for DormandPrince853Integrator,
*  seems ok for  GraggBulirschStoerIntegrator, with 2 fixes :
** in GraggBulirschStoerIntegrator, y0.length changed into scale.length (or mainSetDimension) at lines 712 & 716 ( integrate method),
** in GraggBulirschStoerStepInterpolator, currentState.length changed into scale.length at lines 302 & 306 (estimateErrror method),
* needs to be tested for others integrators.

And, waiting for a new architecture, ODEWithJacobians & ParameterizedODE classes should extend ExtendedFirstOrderDifferentialEquations instead of FirstOrderDifferentialEquations.","23/Jul/10 09:04;luc;fixed in subversion repository as of r919963
Note that ODEWithJacobians & ParameterizedODE classes still don't implement ExtendedFirstOrderDifferentialEquations, they should remain simple at user level, it is the MappingWrapper private class inside FirstOrderIntegratorWithJacobians that implements the extended set of equations.","23/Mar/11 20:12;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MaxIterationsExceededException in SVD (EigenDecompositionImpl.findEigenVectors ) caused by NaN,MATH-383,12468767,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,dimpbx,kushkuley,kushkuley,07/Jul/10 22:18,24/Mar/12 16:16,07/Apr/19 20:38,20/Jul/11 12:19,3.0,,,,,,,3.0,,,0,,,,,,,,"In the following code fragment from EigenDecompositionImpl.findEigenVectors 

....................................................................
    for (int j = 0; j < n; j++) {
            int its = 0;
            int m;
            do {
                for (m = j; m < n - 1; m++) {
                    double delta = Math.abs(realEigenvalues[m]) + Math.abs(realEigenvalues[m + 1]);
                    if (Math.abs(e[m]) + delta == delta) {
                        break;
                    }
                }
          

                if (m != j) {
                    .........................................

the test for ""(Math.abs(e[m]) + delta == delta)"" is not executed  when m is equal to n -1.
As a result  e[m]  == 0 (does happen!) causes variables q and realEigenvalues[m] to become NaN that in turn causes   ""Math.abs(e[m]) + delta == delta)"" to become always false.

My guess (seems to work) is that another test for e[m] == 0 is needed, so that the code becomes

   for (int j = 0; j < n; j++) {
            int its = 0;
            int m;
            do {
                for (m = j; m < n - 1; m++) {
                    double delta = Math.abs(realEigenvalues[m]) + Math.abs(realEigenvalues[m + 1]);
                    if (Math.abs(e[m]) + delta == delta) {
                        break;
                    }
                }
          
               // begin patch 
               if ( m == n - 1 && e[m-1] == 0 )
                       break;
               // end patch


                
               if (m != j) {
                    ......................................... 

or something like that

",Java 6,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-07-26 10:12:23.786,,,false,,,,,,,,,,,,,,67398,,,Wed Jul 20 12:19:35 UTC 2011,,,,,,0|i0rut3:,160635,,,,,,,,26/Jul/10 10:12;psteitz;Thanks for reporting this.,26/Jul/10 14:52;dimpbx;Could you supply with a matrix which causes the problem?  The present unit tests are not helpful enough.,"26/Dec/10 20:03;psteitz;Leaving open, but pushing out to 3.0.  Needs more info and possibly retest when 3.0 changes to SVD are implemented.","20/Jul/11 12:19;luc;Fixed in subversion repository as of r1148714.

This issue was fixed by changing SVD implementation according to issue MATH-611.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong variable in precondition check,MATH-382,12468560,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,erans,erans,erans,05/Jul/10 12:57,23/Mar/11 20:09,07/Apr/19 20:38,05/Jul/10 14:11,2.1,,,,,,,2.2,,,0,,,,,,,,"In the class {{MicrosphereInterpolator}} (package {{analysis.interpolation}}), the method
{code}
  public void setMicropshereElements(int elements)
{code}
is checking the precondition with the instance variable instead of the passed argument.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2011-03-23 20:09:52.596,,,false,,,,,,,,,,,,,,150537,,,Wed Mar 23 20:09:52 UTC 2011,,,,,,0|i0rutb:,160636,,,,,,,,"05/Jul/10 14:11;erans;r960602.
","23/Mar/11 20:09;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inconsistency for methods names within StepInterpolator and StepInterpolatorWithJacobians,MATH-381,12467822,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,pparraud,pparraud,24/Jun/10 16:59,24/Mar/12 16:16,07/Apr/19 20:38,01/Oct/11 13:55,2.1,,,,,,,3.0,,,0,,,,,,,,"There is some inconstency in methods naming :
getInterpolatedY and getInterpolatedYDot in StepInterpolatorWithJacobians do the same as getInterpolatedState and getInterpolatedDerivatives in StepInterpolator, so it will be better if they would have the same name.",,,,,,,,,,MATH-380,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-06-24 20:03:22.171,,,false,,,,,,,,,,,,,,2243,,,Sat Oct 01 13:55:44 UTC 2011,,,,,,0|i0rutj:,160637,,,,,,,,"24/Jun/10 20:03;luc;Agreed.
This will be fixed in the new design which should in fact completely remove the need for StepInterpolatorWithJacobians and replae it by something else.",26/Jul/10 12:09;luc;We need the refactoring of ODE with Jacobians to be done for 2.2,"23/Aug/10 03:22;psteitz;Should this move to 3.0, given compatibility issues?","23/Aug/10 07:27;luc;Yes, I pushed this to 3.0.
However, the refactoring proposed in my previous comment should be done for 2.2. This means that the new implementation should be made available in 2.2 and the old one deprecated. Then in 3.0 the old deprecated implementation should be removed.
I'm working on it.",01/Oct/11 13:55;luc;Fixed in subversion repository as of r1176745,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to (re)initialize dYdY0 for multiple integrate with FirstOrderIntegratorWithJacobians,MATH-380,12467821,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,pparraud,pparraud,24/Jun/10 16:47,24/Mar/12 16:16,07/Apr/19 20:38,01/Oct/11 13:54,2.1,,,,,,,3.0,,,0,,,,,,,,"There is a lack in the method integrate of FirstOrderIntegratorWithJacobians. The jacobian DYDY0 can't be initialized by the user, unlike DFDP with DF0DP.
So, for several successive integrations, the matrix is reinitialized to identity and that is not what we might want.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-06-24 20:01:52.871,,,false,,,,,,,,,,,,,,2244,,,Sat Oct 01 13:54:20 UTC 2011,,,,,,0|i0rutr:,160638,,,,,,,,"24/Jun/10 20:01;luc;You are perfectly right.

The FirstOrderIntegratorWithJacobians class is a brand new one and it clearly has some design flaws.
It will most probably be deprecated in its current form and replaced by a new mechanism, better integrated (sorry for the joke) with the standard ODE solvers.
The ability for user to set an initial value for dydy0 will be present in the new design, but will probably not be back-ported to the current one.
In the meantime, you can save the final value of the jacobian matrix dydy0 after first part of integration, which we could call dy1dy0 as it represents dy(t1)/dy(t0). Start the second part from t1 to t2 that will reset the initial matrix to identity and hence compute compute dy(t2)/dy(t1) and do the multiplication by yourself of the two matrices to really get what you need: dy(t2)/dy(t1) = dy(t2)/dy(t1) * dy(t1)/dy(t0).

Thanks for reporting the issue ","20/Nov/10 21:43;luc;changing target fix version to 3.0.
Fixing this and several other problems requires a complete rewrite of the jacobians computation with ODE, and this rewrite implies user interfaces changes, so it cannot be fixed before 3.0.
","25/Sep/11 15:15;luc;A first attempt to implement Jacobians computation again in ODE has been committed in subversion repository as of r1175409.
This implementation still lacks the ability for step handlers to also retrieve the additional equations and their derivatives.
This implementation is based on the Orekit one described here: [https://www.orekit.org/blog/public/vpommier-ISSFD-2011-extended_propagation.pdf]",01/Oct/11 13:54;luc;fixed in subversion repository as of r1176745.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
weight versus sigma in AbstractLeastSquares,MATH-377,12467197,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,dimpbx,dimpbx,dimpbx,17/Jun/10 09:06,23/Mar/11 20:08,07/Apr/19 20:38,25/Jul/10 19:49,2.1,,,,,,,2.2,,,0,,,,,,,,"In AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.

 Once corrected, getRMS() can even reduce

 public double getRMS() {return Math.sqrt(getChiSquare()/rows);}",,60,60,,0%,60,60,,,,,,,,,,,,,,24/Jun/10 14:51;dimpbx;Math377fix.diff;https://issues.apache.org/jira/secure/attachment/12447947/Math377fix.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-06-19 19:09:44.632,,,false,,,,,,,,,,,,,,34145,,,Wed Mar 23 20:08:36 UTC 2011,,,,,,0|i0ruuf:,160641,,,,,,,,19/Jun/10 19:09;psteitz;It is not clear to me exactly what is being computed in getChiSquare.  Step 0 is to get an actual definition in the javadoc for what it is trying to compute.  I agree it seems odd to be dividing by residual weights; but I could be missing the intent.,"20/Jun/10 13:54;dimpbx;OK, let us define ChiSquare as the sum of the weighted square of the residual in order to be consistent with the rest of the definitions in that class.  That would also be consistent with what users expect from a parameter labeled 'weight' rather than 'sigma'.  If we reach consensus on that definition, I can take care of that issue.","21/Jun/10 13:30;psteitz;I could be missing something, but I see no reason that the weighted sum of squared residuals computed here (after the proposed change) should in general follow a chi-square distribution or be related to a chi-square test statistic of any kind.   Why is it called chi-square?  Sorry if I am missing something simple here.","21/Jun/10 14:00;psteitz;I guess if you assume normalliy distributed errors, it makes sense, so drop the last comment and I am +1 for the change (with definition added to the javadoc).","21/Jun/10 14:35;dimpbx;Indeed, the confusion comes from the fact that, in some textbooks, each residual is divided by 'sigma_i' which leads to a weight of 1/(sigma_i^2).  In CM, we adopted the terminology 'weight' without reference to sigma.  I will change the javadoc accordingly.",24/Jun/10 14:51;dimpbx;Patch to correct issue MATH-377.  The change in getChiSquare let to a tiny update in one of Levenberg-Marquardt unit tests.,"23/Mar/11 20:08;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"One-time initialization in ""DirectSearchOptimizer""",MATH-376,12466988,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,erans,erans,15/Jun/10 12:02,23/Mar/11 20:08,07/Apr/19 20:38,16/Jun/10 13:55,,,,,,,,2.2,,,0,,,,,,,,"In class  ""DirectSearchOptimizer"" (in package ""optimization.direct""), the ""optimize"" method contains this code (at line 270):
{code}
if (startConfiguration == null) {
    // no initial configuration has been set up for simplex
    // build a default one from a unit hypercube
    final double[] unit = new double[startPoint.length];
    Arrays.fill(unit, 1.0);
    setStartConfiguration(unit);
}
{code}

I think that this has the consequence that it is impossible to call ""optimize"" a second time, where one would like to solve a problem with a different dimension.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-06-15 19:27:22.79,,,false,,,,,,,,,,,,,,150534,,,Wed Mar 23 20:08:10 UTC 2011,,,,,,0|i0ruun:,160642,,,,,,,,"15/Jun/10 19:27;luc;This code is here only for the case setStartConfiguration has not been called by the user, which is not the recommended way.
This behaviour is documented in the javadoc of the class. The case of multiple calls with different problems size is explained: in this case users must call setStartConfiguration.","16/Jun/10 09:13;erans;Sorry, I should have read the documentation :-}. It would have me rephrased this issue as follows.
Considering the algorithm operation, is it a serious problem that the automatic default is used as initial simplex?
In the affirmative, instead of providing a default (as in the above excerpt) the code should throw an ""IllegalStateException"", signaling that ""setStartConfiguration"" must be called.
The alternative would be to extend the check:
{code}
if (startConfiguration == null
    ||  startConfiguration.length != startPoint.length) {
    // Re-initialize the default simplex with the appropriate number of vertices.
    // ...
}
{code}
Because it is confusing (and IMO doesn't make much sense) to allow one default run but not more.
","16/Jun/10 10:19;luc;I agree. Go ahead to make the change, and make sure documentation is updated. It may appear in the javadoc of this top level class as well as in the subclasses and perhaps in the user manual (I did not check).","16/Jun/10 13:55;erans;Implemented (second option) in r955230.
","23/Mar/11 20:08;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StatUtils.sum returns NaN for zero-length arrays,MATH-373,12466353,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,rwerp,rwerp,07/Jun/10 14:54,24/Mar/12 16:16,07/Apr/19 20:38,02/Sep/10 04:52,2.1,,,,,,,3.0,,,0,,,,,,,,"StatUtils.sum returns NaN for zero-length arrays, which is:

1. inconsistent with the mathematical notion of sum: in maths, sum_{i=0}^{N-1} a_i will be 0 for N=0. In particular, the identity

sum_{i=0}^{k-1} a_i + sum_{i=k}^{N-1} = sum_{i=0}^{N-1}

is broken for k = 0, since NaN + x = NaN, not x.

2. introduces hard to debug erros (returning a NaN is one of the worst forms of reporting an exceptional condition, as NaNs propagate silently and require manual tracing during the debugging)

3. enforces ""special case"" handling when the user expects that the summed array can have a zero length.

The correct behaviour is, in my opinion, to return 0.0, not NaN in the above case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-06-08 01:37:46.013,,,false,,,,,,,,,,,,,,150531,,,Thu Sep 02 04:52:33 UTC 2010,,,,,,0|i0ruvb:,160645,,,,,,,,"08/Jun/10 01:37;billbarker;I agree with the reasoning here, and we should do it this way in 3.0.  However it is an incompatible change to do in a point release, so I'm going to wait for more feed back from other developers before I make any changes to the current code.

I'm thinking that adding a method to AbstractUnivariateStatistic that looks like:
   protected boolean test( final double[] values,  final int begin,   final int length, final boolean allowEmpty)

that would have the test:
   if(length == 0 && !allowEmpty)
        return false;

The current test method can call the new one with allowEmpty=false for backwards compatibility.  Then we can decide on which statistics should have a zero value on the empty set.
","11/Jun/10 01:57;billbarker;The consensus of the commons-math developers is that, since the current behavior is documented in 2.x, that this will have to wait for 3.0.  Fixing this in 2.x would introduce a too large incompatibility change to include in 2.x.

I can attach a patch against 2.x that fixes this, as long as anybody using the patch understands that it isn't supported.

","11/Jun/10 11:19;sebb@apache.org;Possibly crazy idea: 

if Math 3.0 is going to change package names (which may be necessary), one could introduce the fix using a math3 package name?","11/Jun/10 12:14;erans;IIRC, changing the package name had been suggested and discussed for 2.0.
[One argument is that, to be consistent,  you'd have to change the name at every major release...]
","11/Jun/10 12:37;rwerp;Speaking as a maintainer of client code which uses ACM, I'd rather cope with occasional incompatibilities in the same packages, than have to change ALL my client code to keep up with the package name changes after every release. A reason to change the package name would be if you wanted to use the old and new version side by side, but that would not be a common usage pattern for ACM, I think.","12/Jun/10 04:04;billbarker;As Gilles mentioned, changing the package name for commons-math was discussed and voted on for 2.x.  The result of the vote was to keep the package name, since commons-math won't usually be provided by a third party library.  Since nothing much has changed, I can't see that commons-math would change it's package for version 3.0.",02/Sep/10 04:52;billbarker;This will be fixed in the 3.0 build.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Curve fitting appears unreliable,MATH-372,12465409,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,mprice,mprice,25/May/10 22:16,23/Mar/11 20:06,07/Apr/19 20:38,01/Jun/10 06:18,2.1,,,,,,,,,,0,curve,curvefitter,fitting,,,,,"I've been trying to find a good curve fitting library for Java for the last couple of weeks.  I came across Apache Commons Math and was really excited because I like all things Apache.  The curve fitting API looks good and is fairly easy to use, however, it doesn't seem to be as accurate as it should/could be.

I've produced some code and data that shows that the initial parameter guesses affect the results too much.  Guess low and the curve ends up low, guess high and the curve ends up high.  I wish I had a stronger statistics/math background to make more sense of this.  I've tried playing with the optimizer's options (costRelativeToTolerance, initialStepBoundFactor, maxEvaluations, maxIterations, orthTolerance and parRelativeTolerance) but nothing seems to improve the end result.

I've attached the spreadsheet and Java code.  FYI, in the spreadsheet you'll see an entry in the chart for DataFitX.  It is a COM library used in my company's current software that needs to be replaced.

Any help on this would be greatly appreciated.  If you need more info, let me know and I'll supply it as quickly as I can.

Thanks,
Matt","Win7 x64, Netbeans, JDK 1.6.0.18",28800,28800,,0%,28800,28800,,,,,,,,,,,,,,25/May/10 22:24;mprice;CurveFitterDebug.java;https://issues.apache.org/jira/secure/attachment/12445503/CurveFitterDebug.java,25/May/10 22:24;mprice;CurveFitting.ods;https://issues.apache.org/jira/secure/attachment/12445504/CurveFitting.ods,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2010-05-29 19:01:49.059,,,false,,,,,,,,,,,,,,34140,,,Wed Mar 23 20:06:50 UTC 2011,,,,,,0|i0ruvj:,160646,,,,,,,,25/May/10 22:24;mprice;Java code for curve fitting and a spreadsheet with results.,"29/May/10 19:01;luc;It seems to me there is a sign error in the computation of the second component of the gradient in your code.
Once the sign of this component is changed, the algorithm seems to converge even from the first low guess.
Could you confirm this fixes your problem ?","31/May/10 17:38;mprice;Luc,

Thanks for your help.  I changed the sign for this example it converges beautifully.  I'll test this fix with a few more scenarios that gave me trouble. 

I really appreciate the help.  This issue was going to be a show stopper and I couldn't be happier that the problem is on my end.  Makes me want to contribute some money to Apache :)

Thanks,
Matt","01/Jun/10 06:18;luc;OK, so I'm going to set the ""invalid"" resolution tag for this issue. If you find another problem, please open a new issue so we can track it down.
","10/Jun/10 18:56;mprice;A quick update...

I've successfully gotten linear and non-linear regression working via Apache Commons Math for linear, point-to-point, log-log, 4PL and 5PL regression models.  The troubles I had were due to errors in complex derivatives.  My aim is to replace software that is currently using a commercial product (DataFitX).  From what I can tell, Apache Commons Math is faster and more accurate than DataFitX.  Good job Apache :)

P.S. Wolfram Alpha is a great website for solving your regression model's derivatives (www.wolframalpha.com)",23/Mar/11 20:06;luc;Closing issue that was declared invalid long ago,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PearsonsCorrelation.getCorrelationPValues() precision limited by machine epsilon,MATH-371,12464458,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,kchilds,kchilds,13/May/10 19:48,23/Mar/11 20:05,07/Apr/19 20:38,16/May/10 23:49,2.0,,,,,,,2.1,,,0,,,,,,,,"Similar to the issue described in MATH-201, using PearsonsCorrelation.getCorrelationPValues() with many treatments results in p-values that are continuous down to 2.2e-16 but that drop to 0 after that.

In MATH-201, the problem was described as such:
> So in essence, the p-value returned by TTestImpl.tTest() is:
> 
> 1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t))
> 
> For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0. When 
> cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because:
> 
> 1.0 - 1.0 + 0.0 = 0.0

The solution in MATH-201 was to modify the p-value calculation to this:
> p = 2.0 * cumulativeProbability(-t)

Here, the problem is similar.  From PearsonsCorrelation.getCorrelationPValues():
  p = 2 * (1 - tDistribution.cumulativeProbability(t));

Directly calculating the p-value using identical code as PearsonsCorrelation.getCorrelationPValues(), but with the following change seems to solve the problem:
  p = 2 * (tDistribution.cumulativeProbability(-t));




",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-05-16 14:10:39.057,,,false,,,,,,,,,,,,,,34169,,,Wed Mar 23 20:05:43 UTC 2011,,,,,,0|i0ruvr:,160647,,,,,,,,16/May/10 14:10;psteitz;Thanks for reporting this.,16/May/10 23:49;psteitz;Fixed in r944939.  Thanks!,"23/Mar/11 20:05;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NaN in ""equals"" methods",MATH-370,12463970,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,erans,erans,erans,07/May/10 11:32,23/Mar/11 20:05,07/Apr/19 20:38,01/Sep/10 13:47,,,,,,,,3.0,,,0,,,,,,,,"In ""MathUtils"", some ""equals"" methods will return true if both argument are NaN.
Unless I'm mistaken, this contradicts the IEEE standard.

If nobody objects, I'm going to make the changes.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-05-07 14:31:34.328,,,false,,,,,,,,,,,,,,150530,,,Wed Mar 23 20:05:24 UTC 2011,,,,,,0|i0ruvz:,160648,,,,,,,,"07/May/10 14:31;luc;I really think this is intentional and the fact that NaN != Nan was known to the people who did that.
For example, the javadoc for Complex.equals ([http://commons.apache.org/math/apidocs/org/apache/commons/math/complex/Complex.html#equals%28java.lang.Object%29]) explicitly warns about behavior for NaN.
So on the one one changing this would break compatibility with respect to a documented behavior and on the other hand it would bring compatibility with IEEE arithmetic.
I'm sure there is a good reason for the current status, but I don't know it.","07/May/10 17:00;erans;The Javadocs for ""Float"" and ""Double"" also indicate the special behaviour for NaN in a number object.  Nevertheless, the _primitive_ types are compliant with IEEE. Hence, there is an inconsistency between the ""equals"" methods in ""MathUtils"" and the behaviour of the JVM.
",08/May/10 01:41;psteitz;-1 for the change.  It will break some internal things and likely some user applications that depend on the documented behavior.  The reason that these methods exist is to be able to handle NaNs and treat them as equal.,"08/May/10 11:10;erans;But _why_?
By this I mean, how to explain to someone that CM does not comply with IEEE?

Also, please point me to the internal things likely to break, so that I have a chance to understand the usefulness of the behaviour.
","08/May/10 14:00;psteitz;The method Mathutils.equals(double, double) was implemented for the reason that in some cases, users may wish to treat doubles with NaN values as equal.  The internal classes that use it are the equals methods for statistics and statistical aggregates.  When these classes were defined, we decided that we would treat two statistics or statistical aggregates as equal iff they had the same values, including NaNs.  So for example, two StatisticalSummaryValues instances are equal iff they report the same values for all statistics, with NaNs treated as ""the same value.""  The MathUtils.equals(double, double) method was introduced as a convenience method to do this kind of comparison.  I guess we could (deprecate and) change the name if it makes it look like all of our computations are defining equals that way, which is certainly not the case.  In general, we do try to follow IEEE754.  We agreed early on that we would carefully document how NaNs are treated throughout Commons Math.  We have slipped a little over the years, but in the case of this method and the classes that use it, the documentation is clear.

Sorry I misunderstood the point of the issue here.  I now get your point that naming the method ""equals"" makes it look like we are re-defining equals for primitive doubles, which is not our intent.  So maybe we deprecate the equals method and rename something like ""equalsIncludingNaN"" or something simpler?","10/May/10 10:14;erans;Well, the first impression I got from looking at those methods in ""MathUtils"" was indeed that they provide the ""right"" way to compare primitive doubles. Now, the right way for most users dealing with numerical codes should arguably be IEEE754-compliant.  So, unless we can explain that the way CM handles NaN is somehow more useful than what the IEEE standard defines, CM should at least provide ""equals"" utility methods that stick to the standard.
I understand the usefulness of the current implementation in the examples which you gave, but as a matter of principle this usage is second in priority (from a user perspective): it is mainly internal to CM.
Maybe, we should create a ""util.internal"" package where (CM) developer-utilities methods would live, whereas ""util"" would contain the functions which users are most likely looking for.  I think that it would be clearer (e.g. it would avoid dealing with long names such as ""equalsIncludingNaN"").  What do you think?
","10/May/10 10:32;psteitz;I use the equals method and the array version in some of my  code that uses [math] (pretty much the same basic use case as above) and I suspect it is possible that other users do as well, so I would prefer to keep a renamed version of it available publicly, if consensus is we need to rename it.  The fact that it is defined at all should be a flag to users that there is something different going on, at least in the <double, double> case.  Note that the <double, double, double> and <double, double, int> versions are also ""different"" from the JDK; but there is no possibility for confusion of intent for them.","10/May/10 10:55;erans;I did not say that there should be private methods, unavailable to users of CM. Just that different packages would make it clear that some methods intend to implement common/standard functions while others are more special-purpose or could deviate from standard definitions.
","11/May/10 09:52;erans;OK, maybe it is not necessary to introduce a new package.
Then, let's implement the proposed name change (""equalsIncludingNaN"").

> Note that the <double, double, double> and <double, double, int> versions are also ""different"" from the JDK;
> but there is no possibility for confusion of intent for them.

I do not agree. The point is there will be a confusion because the JDK treats ""Double"" and ""double"" _differently_ wrt NaN whereas those ""equals"" methods handle NaN _similarly_ to what happens with ""Double"".

In my (maybe naive) opinion, all this NaN business is not really important because the equality test itself doesn't make sense when a NaN is passed as arguments (meaning something went wrong previously).
However, there is this IEEE754 _standard_ saying that NaN is not equal to NaN. Hence, to avoid confusion, the ""equals"" methods should comply with it.  And, for all of them, we can add the special-purpose variant in the longer-named ones.

Agreed?
[If so, I'm willing to make the changes and hunt for all the occurrences of ""equals"" with the current semantics and replace them with ""equalsIncludingNaN"".]
","11/May/10 10:15;psteitz;I am fine with deprecating and changing the name of MathUtils.equals(double, double) to ""equalsIncludingNaN"" or ""equalsN"".   I see no reason to change the names or modify the behavior of any of the other equals methods in MathUtils.","11/May/10 11:15;erans;> I see no reason to change the names or modify the behavior of any of the other equals methods in MathUtils. 

Maybe because the other ""equals"" methods also consider that NaN == NaN which contradicts IEEE !

Where people need to treat all NaN values as equal, they will use one of the ""equalsIncludingNaN"", otherwise they will use one of the ""equals"" methods (and get an IEEE754-compliant result wrt to NaN values).
",11/May/10 18:32;psteitz;Which methods exactly are you talking about?  ,"11/May/10 19:42;erans;{code}
public static boolean equals(double x, double y) {
    return (Double.isNaN(x) && Double.isNaN(y)) || x == y;
}
{code}

{code}
public static boolean equals(double x, double y, double eps) {
  return equals(x, y) || (Math.abs(y - x) <= eps);
}
{code}

{code}
public static boolean equals(double[] x, double[] y) {
    if ((x == null) || (y == null)) {
        return !((x == null) ^ (y == null));
    }
    if (x.length != y.length) {
        return false;
    }
    for (int i = 0; i < x.length; ++i) {
        if (!equals(x[i], y[i])) {
            return false;
        }
    }
    return true;
}
{code}

{code}
public static boolean equals(double x, double y, int maxUlps) {
    // Check that ""maxUlps"" is non-negative and small enough so that the
    // default NAN won't compare as equal to anything.
    assert maxUlps > 0 && maxUlps < NAN_GAP;

    long xInt = Double.doubleToLongBits(x);
    long yInt = Double.doubleToLongBits(y);

    // Make lexicographically ordered as a two's-complement integer.
    if (xInt < 0) {
        xInt = SGN_MASK - xInt;
    }
    if (yInt < 0) {
        yInt = SGN_MASK - yInt;
    }

    return Math.abs(xInt - yInt) <= maxUlps;
}
{code}

The first assumes that NaN == NaN; the next two use the first; the last one also returns {{true}} when both arguments are NaN.

I propose that these ""equals"" will return {{false}} when one or the other argument is NaN (or contains a NaN for the array variant), and to create for each one an ""equalsIncludingNaN"" variant that will behave as the current code of ""equals"".
All occurrences of ""equals"" currently in CM will be replaced by ""equalsIncludingNaN"" so that no semantics change will happen.
CM will stay consistent and be compliant.","11/May/10 20:42;erans;Actually, I think that the first of the above ""equals"" should be replaced by:

{code}
public static boolean equals(double x, double y) {
    return equals(x, y, 1);
}
{code}

I.e. calling the fourth form (when it is replaced by the IEEE-compliant version).
","15/May/10 19:09;psteitz;Looking carefully at the uses inside [math] and reviewing my own uses, I agree now that the original definitions of equals(double, double) and equals(double, double, double) were incorrect, or I guess I should say ""unfortunate.""  Regarding equals(double, double), I don't see why we need that at all - just deprecate and replace by ""equalsN"" or somesuch or remove it altogether in 3.0. The uses of equals(double, double, double)  in the linear package should NOT identify NaNs, so these would benefit from changing behavior.  The uses in the stats package of the array version are OK, I think - i.e., I think it is a legitimate definition of equals for a stats reporting object to identify stats that report NaNs in the same places.

So the question is what to do.  I don't think we can change the contracts or delete methods in point releases, so the reasonable thing to do is to 

1) deprecate equals(double, double) 
2) Make a note in the javadoc for equals(double[], double[]) and equals(double, double, double) indicating that in version 3.0 these methods will handle NaNs differently.
3) Fix equals(double, double, int).  This method does not use equals(double, double) and does not specify how NaNs are handled in its javadoc, so it can be fixed.","20/May/10 09:57;erans;> [...] Regarding equals(double, double), I don't see why we need that at all [...]

As I noted in the previous comment, this version of an equality test seems to make sense, from a numerical perspective, if it's implemented by using the ""equals(double,double,int)"" method (introduced according to [MATH-264|https://issues.apache.org/jira/browse/MATH-264]) where the third argument is set to 1): It means that there is no floating point number in the range of real numbers defined by the first and second arguments.

Practically, I've now modified all the ""equals"" methods (so that NaN != NaN) and for each one, I've also implemented an ""equalsIncludingNaN"" version (for which NaN == NaN).
In the ""stat"" package, wherever ""equals"" was used, I replaced it with ""equalsIncludingNaN"".
I modified the junit tests (""mutatis mutandis"") and they all pass.

I don't think it's worth waiting v3.0 to introduce those changes, for the following reasons:
# The change is non-breaking wrt usage of the code in package ""stat"", or any other code that was using ""equals"".
# No functionality is removed (i.e. the ""equalsIncludingNaN"" provide the same behaviour as previously provided by the corresponding ""equals"" methods).
# For a direct user of ""equals"", the change is, in principle, breaking but only in the single case where he relies on NaNs being equal. How likely is it that expecting such a behaviour is *not* wrong (i.e. comparing the results of computations that produced meaningless values)?

To further alleviate the (marginal) risk of the last point, couldn't we make a poll on the ""user"" ML, so that people can make a case if they really need the time to a major version change in order to adapt to this modification?
","23/May/10 13:46;psteitz;Regarding equals(double, double), it looks like what you are proposing is yet another version of equals that is not exactly what is defined in the spec.  Do I have that right?  In that case, like the current version, it should have a different name.  Or does the spec imply that equals should behave this way and the JDK not quite deliver it?

I sympathize with the desire to change the definitions of the other methods now; but while we did not quite live up to this in 2.1, .x versions are supposed to be drop-in replacements, so we should not be changing behavior of methods unless they are not meeting their documented contracts and in this case, what we are seeing as ""bugged"" are the contracts, so I think we need to deprecate  equals(double, double) and fix the others in 3.0, with the exception of equals(double, double, int), which does not specify NaN behavior.  We can introduce the new ""equalsN"" versions now and note the changed behavior in the javadoc for the ""equals"" versions.  We can also introduce the new version of equals(double, double) that you are describing with a new name.","09/Jun/10 10:47;erans;Changes partially implemented in r952949. [Hopefully not breaking compatibility.]
Methods ""equalsIncludingNaN"" have been added and used internally (in the classes of the ""stat.descriptive"" package) instead of the now deprecated ""equals"" methods. Those should change semantics in release 3.0.
","01/Sep/10 13:47;erans;Revision 991535.
","23/Mar/11 20:05;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,
"BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException",MATH-369,12463568,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,sasunpundev@abv.bg,sasunpundev@abv.bg,03/May/10 15:48,23/Mar/11 20:05,07/Apr/19 20:38,03/May/10 18:43,2.1,,,,,,,,,,0,,,,,,,,"Method 

    BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)  

invokes 

    BisectionSolver.solve(double min, double max) 

which throws NullPointerException, as member variable

    UnivariateRealSolverImpl.f 

is null.

Instead the method:

    BisectionSolver.solve(final UnivariateRealFunction f, double min, double max)

should be called.

Steps to reproduce:

invoke:

     new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5);

NullPointerException will be thrown.


","Windows XP Profesional
java version ""1.6.0_20""
Java(TM) SE Runtime Environment (build 1.6.0_20-b02)
Java HotSpot(TM) Client VM (build 16.3-b01, mixed mode, sharing)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-05-03 18:43:59.213,,,false,,,,,,,,,,,,,,34165,,,Wed Mar 23 20:05:06 UTC 2011,,,,,,0|i0ruw7:,160649,,,,,,,,"03/May/10 18:43;luc;Fixed in subversion repository as of r940565.
Thanks for the report and for the fix.","23/Mar/11 20:05;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenMapRealVector.getSparcity should be getSparsity,MATH-368,12463289,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,billbarker,ashuang,ashuang,29/Apr/10 03:41,23/Mar/11 20:04,07/Apr/19 20:38,09/May/10 23:07,2.1,2.2,,,,,,2.2,,,0,,,,,,,,"The term for describing the ratio of nonzero elements to zero elements in a matrix/vector is sparsity, not sparcity.  Suggest renaming getSparcity() to getSparsity()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-05-09 23:07:24.751,,,false,,,,,,,,,,,,,,150529,,,Wed Mar 23 20:04:17 UTC 2011,,,,,,0|i0ruwf:,160650,,,,,,,,"09/May/10 23:07;billbarker;The policy of this project is to not remove methods from the public API in a point release.  However, the misspelled method has been deprecated and the correctly spelled method has been added.","23/Mar/11 20:04;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractRealVector.sparseIterator fails when vector has exactly one non-zero entry,MATH-367,12462804,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,billbarker,ashuang,ashuang,22/Apr/10 18:31,23/Mar/11 20:03,07/Apr/19 20:38,10/May/10 01:17,2.1,2.2,,,,,,2.2,,,0,,,,,,,,"The following program:
===
import java.util.Iterator;
import org.apache.commons.math.linear.*;

public class SparseIteratorTester
{
    public static void main(String[] args) {
        double vdata[] = { 0.0, 1.0, 0.0 };
        RealVector v = new ArrayRealVector(vdata);
        Iterator<RealVector.Entry> iter = v.sparseIterator();
        while(iter.hasNext()) {
            RealVector.Entry entry = iter.next();
            System.out.printf(""%d: %f\n"", entry.getIndex(), entry.getValue());
        }   
    }       
} 
===
generates this output:

1: 1.000000
Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.commons.math.linear.ArrayRealVector.getEntry(ArrayRealVector.java:995)
	at org.apache.commons.math.linear.AbstractRealVector$EntryImpl.getValue(AbstractRealVector.java:850)
	at test.SparseIteratorTester.main(SparseIteratorTester.java:13)
===

This patch fixes it, and simplifies AbstractRealVector.SparseEntryIterator  (sorry, i don't see any form entry for attaching a file)
===
Index: src/main/java/org/apache/commons/math/linear/AbstractRealVector.java
===================================================================
--- src/main/java/org/apache/commons/math/linear/AbstractRealVector.java	(revision 936985)
+++ src/main/java/org/apache/commons/math/linear/AbstractRealVector.java	(working copy)
@@ -18,6 +18,7 @@
 package org.apache.commons.math.linear;
 
 import java.util.Iterator;
+import java.util.NoSuchElementException;
 
 import org.apache.commons.math.FunctionEvaluationException;
 import org.apache.commons.math.MathRuntimeException;
@@ -875,40 +876,25 @@
         /** Dimension of the vector. */
         private final int dim;
 
-        /** Temporary entry (reused on each call to {@link #next()}. */
-        private EntryImpl tmp = new EntryImpl();
-
-        /** Current entry. */
+        /** Last entry returned by #next(). */
         private EntryImpl current;
 
-        /** Next entry. */
+        /** Next entry for #next() to return. */
         private EntryImpl next;
 
         /** Simple constructor. */
         protected SparseEntryIterator() {
             dim = getDimension();
             current = new EntryImpl();
-            if (current.getValue() == 0) {
-                advance(current);
-            }
-            if(current.getIndex() >= 0){
-                // There is at least one non-zero entry
-                next = new EntryImpl();
-                next.setIndex(current.getIndex());
+            next = new EntryImpl();
+            if(next.getValue() == 0)
                 advance(next);
-            } else {
-                // The vector consists of only zero entries, so deny having a next
-                current = null;
-            }
         }
 
-        /** Advance an entry up to the next non null one.
+        /** Advance an entry up to the next nonzero value.
          * @param e entry to advance
          */
         protected void advance(EntryImpl e) {
-            if (e == null) {
-                return;
-            }
             do {
                 e.setIndex(e.getIndex() + 1);
             } while (e.getIndex() < dim && e.getValue() == 0);
@@ -919,22 +905,17 @@
 
         /** {@inheritDoc} */
         public boolean hasNext() {
-            return current != null;
+            return next.getIndex() >= 0;
         }
 
         /** {@inheritDoc} */
         public Entry next() {
-            tmp.setIndex(current.getIndex());
-            if (next != null) {
-                current.setIndex(next.getIndex());
-                advance(next);
-                if (next.getIndex() < 0) {
-                    next = null;
-                }
-            } else {
-                current = null;
-            }
-            return tmp;
+            int index = next.getIndex();
+            if(index < 0)
+                throw new NoSuchElementException();
+            current.setIndex(index);
+            advance(next);
+            return current;
         }
 
         /** {@inheritDoc} */
",,,,,,,,,,,,,,,,,,,,,22/Apr/10 18:32;ashuang;AbstractRealVector_sparseIterator_patch.txt;https://issues.apache.org/jira/secure/attachment/12442607/AbstractRealVector_sparseIterator_patch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-05-10 01:17:14.417,,,false,,,,,,,,,,,,,,34143,,,Wed Mar 23 20:03:42 UTC 2011,,,,,,0|i0ruwn:,160651,,,,,,,,22/Apr/10 18:32;ashuang;patch fixing the bug,"10/May/10 01:17;billbarker;I've applied your patch (with a couple of style tweaks).  It should be available in the next release of commons-math.

Thank you for your contribution.","23/Mar/11 20:03;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Issue with ""SmoothingBicubicSplineInterpolator""",MATH-365,12462547,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,erans,erans,20/Apr/10 14:21,23/Mar/11 20:02,07/Apr/19 20:38,21/Apr/10 14:35,2.1,,,,,,,2.2,,,0,,,,,,,,"I figured out that the name of this class is misleading as the implementation doesn't perform the intended smoothing.

In order to solve this issue, I propose to:
# deprecate the ""SmoothingBicubicSplineInterpolator"" class
# create a ""BicubicSplineInterpolator"" class (similar to the above class but with the useless code removed)
# remove the ""SmoothingBicubicSplineInterpolatorTest"" class
# add a ""BicubicSplineInterpolatorTest"" with essentially the same contents as the above one

Then I would also add a new ""SmoothingPolynomialBicubicSplineInterpolator"" where I used the ""PolynomialFitter"" class to smooth the input data along both dimensions before the interpolating function is computed.

Does someone object to these changes?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-04-20 18:56:47.416,,,false,,,,,,,,,,,,,,150527,,,Wed Mar 23 20:02:52 UTC 2011,,,,,,0|i0rux3:,160653,,,,,,,,"20/Apr/10 18:56;luc;removing the test class would badly impact test coverage, so it would be better to simply deprecae it also and to remove the library class and its associated test class together when releasing 3.0","21/Apr/10 14:35;erans;revision 936295.
","23/Mar/11 20:02;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it,MATH-362,12461240,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,roman.werpachowski,roman.werpachowski,06/Apr/10 11:38,23/Mar/11 20:02,07/Apr/19 20:38,29/May/10 18:16,2.0,2.1,,,,,,2.2,,,0,,,,,,,,LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it. This makes it hard to specify custom stopping criteria for the optimizer.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-04-06 16:49:36.856,,,false,,,,,,,,,,,,,,34181,,,Wed Mar 23 20:02:00 UTC 2011,,,,,,0|i0ruxr:,160656,,,,,,,,"06/Apr/10 16:49;luc;Ooops. You are right.
The Levenberg-Marquardt optimizer uses specific convergence parameters which can be set by   setInitialStepBoundFactor, setCostRelativeTolerance, setParRelativeTolerance and setOrthoTolerance.
The most important convergence tuning are either setCostRelativeTolerance for a convergence on the cost itself or setParRelativeTolerance for a convergence on the parameters.

I'm not sure how to solve this. Do the existing tuning parameters fit your needs or not ? Some convergence criteria can be expressed with both methods, but not all. Should we keep both setting as alternate methods or should we remove one and rely on the remaining one ?
","06/Apr/10 17:20;roman.werpachowski;I would keep using orthoTolerance as it is used now:

{quote}
292                if (maxCosine <= orthoTolerance) \{
293                    // convergence has been reached
294                    return new VectorialPointValuePair(point, objective);
295                \}
{quote}

and then use costRelativeTolerance & parRelativeTolerance if and only if the convergence checker is null, otherwise use the convergence checker and ignore {costRelativeTolerance, parRelativeTolerance}.

What I am missing now is the ability to bail out if the absolute distance from the target falls below some value (""close enough"").","24/May/10 20:50;mprice;I've spent that last few days trying to find a good curve fitting library for Java and got excited when I learned of Commons Math.  Unfortunately, its curve fitting is very unreliable.  I'm hoping that this bug is what is causing the problems that I'm seeing.  I'm comparing data from NIST and results from DataFitX and it is apparent that Commons Math is not yet up to the task.  My fingers are crossed that its quality in the curve fitting area will be improved in the near future.  Keep up the good work Apache.

I've opened an issue about the problems I'm seeing, https://issues.apache.org/jira/browse/MATH-372","25/May/10 06:37;roman.werpachowski;Double check how you use it, Matt. I have succesfully used this curve fitting in production.","25/May/10 17:49;luc;Matt, could you please describe the problem you encounter more precisely (i.e. with numerical examples) and preferably in a new JIRA issue ? We will check if the two problems are related and link the issues afterwards if it appears they are.

Thanks","25/May/10 19:10;mprice;It's good to see such quick responses.  I'll open a new JIRA issue and spend some time putting together code, data and a detailed description of the problem I'm seeing.  Thanks Apache for all your hard work.

I've opened an issue regarding the problem, https://issues.apache.org/jira/browse/MATH-372","29/May/10 18:16;luc;Fixed in subversion repository as of r949433.
Thanks for reporting the issue",29/May/10 18:27;roman.werpachowski;Thank you.,"23/Mar/11 20:02;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SmoothingBicubicSplineInterpolatorTest.testPreconditions()  assigns wzval but does not use it - test bug?,MATH-360,12460242,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,25/Mar/10 15:22,03/Apr/10 03:01,07/Apr/19 20:38,25/Mar/10 17:31,,,,,,,,2.1,,,0,,,,,,,,"Findbugs points out that SmoothingBicubicSplineInterpolatorTest.testPreconditions()  assigns wzval but does not use it - is this a test bug?

{code}
double[][] wzval = new double[xval.length][yval.length + 1];
try {
    p = interpolator.interpolate(xval, wyval, zval); // <== should the last param be wzval ??
    Assert.fail(""an exception should have been thrown"");
} catch (IllegalArgumentException e) {
    // Expected
}
wzval = new double[xval.length - 1][yval.length];

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150524,,,2010-03-25 15:22:42.0,,,,,,0|i0ruy7:,160658,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Normal Distribution implementation gives false cumulative probabilities,MATH-359,12460241,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Duplicate,,joris,joris,25/Mar/10 15:21,14/Apr/10 00:00,07/Apr/19 20:38,14/Apr/10 00:00,,,,,,,,,,,0,,,,,,,,"Package: org.apache.commons.math.distribution
Class: NormalDistributionImpl

For a given mean and standard deviation, the class NormalDistributionImpl implements a normal distribution. Per definition, the function cumulativeProbability(double x) should return a value on the interval <0,1> (0 and 1 excluded), for any real value of x. However, the following test case shows that the method cumulativeProbability(double x) gives for some values wrong results:


NormalDistributionImpl ncdf=new NormalDistributionImpl(0.06848215242239623,0.21287763557454142);
try{
	System.out.println(""Test: ""+ncdf.cumulativeProbability(2.636630902183101));
}catch(MathException e){ System.out.println(""Exception has occurred: ""+e);}

Result:
Test: 1.0000000000000064


Only in the case where x=Double.POSITIVE_INFINITY,  cumulativeProbability(double x) should return 1. For all other values of x, the result should be <1.
The weird result from the above test case is quite likely caused by the data type double. The 2 most straight forward ways to fix this behavior:
1. Use a more accurate data type
2. Build in checks which prevent bad results like:
if(x==Double.POSITIVE_INFINITY)
   return 1;
else if(x==Double.NEGATIVE_INFINITY)
   return 0;
else if(result >=1)
   return 0.9999999999999; //A constant value which is stored correctly by a double
else if(result <=0)
   return 0.0000000000001;
Nevertheless, I believe that this issue should be noted in the Javadoc of the NormalDistributionImpl class. ",Ubuntu Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-04-02 11:21:30.893,,,false,,,,,,,,,,,,,,150523,,,Wed Apr 14 00:00:44 UTC 2010,,,,,,0|i0ruyf:,160659,,,,,,,,"02/Apr/10 11:21;roman.werpachowski;The problem arises from the way Normal Gaussian CDF is computed in ACM: via an iterative computation of the Gamma function. It is better, I think, to use a specialized approximation for the Normal Gaussian CDF, such as (highly accurate) http://www.netlib.org/specfun/erf. I checked that for the arguments given it simply returns 1.",13/Apr/10 07:53;cwinter;This issue is has the reason as [Math-282|https://issues.apache.org/jira/browse/MATH-282] and is resolved in Version 2.1. Thus it can be closed.,14/Apr/10 00:00;psteitz;Duplicates MATH-282,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ODE integrator goes past specified end of integration range,MATH-358,12460137,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,luc,luc,luc,24/Mar/10 17:25,03/Apr/10 03:01,07/Apr/19 20:38,24/Mar/10 22:13,2.0,,,,,,,2.1,,,0,,,,,,,,"End of integration range in ODE solving is handled as an event.
In some cases, numerical accuracy in events detection leads to error in events location.
The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.
{code}
  public void testMissedEvent() throws IntegratorException, DerivativeException {
          final double t0 = 1878250320.0000029;
          final double t =  1878250379.9999986;
          FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {
            
            public int getDimension() {
                return 1;
            }
            
            public void computeDerivatives(double t, double[] y, double[] yDot)
                throws DerivativeException {
                yDot[0] = y[0] * 1.0e-6;
            }
        };

        DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,
                                                                               1.0e-10, 1.0e-10);

        double[] y = { 1.0 };
        integrator.setInitialStepSize(60.0);
        double finalT = integrator.integrate(ode, t0, y, t, y);
        Assert.assertEquals(t, finalT, 1.0e-6);
    }

{code}",Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,34176,,,Wed Mar 24 22:13:19 UTC 2010,,,,,,0|i0ruyn:,160660,,,,,,,,24/Mar/10 22:13;luc;fixed in subversion repository as of r927202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compiler dependency in RandomDataImpl.getPoisson(double mean),MATH-354,12459572,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,gmicha,gmicha,18/Mar/10 21:08,03/Apr/10 03:05,07/Apr/19 20:38,18/Mar/10 22:34,2.0,,,,,,,2.1,,,0,,,,,,,,"Hi,

in RandomDataImpl.getPoisson(double mean) I got the following problem in the case mean>= 6.0:

in the branch if (u <= c1):

if (x < -mu)
    w = Double.POSITIVE_INFINITY;

implicits that (int) (mu+ x) < 0

I found that for some compiler/run-time environments the subsequent update of the ""accept"" value then fails, as by the right hand side of the comparison leads to an Exception in MathUtils.factorialLog((int) (mu + x)). Some compiler/jre combinations, however, skip evaluating the right side as by isInfinity(w).

To ensure stability, I currently worked around by an explicit if(Double.isInfinity(w)) branch, however, I would like to ask whether there is a more elegant way to ensure general functionality of that method.

Thank you, micha. ",jre1.5/jre1.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-03-18 22:34:20.331,,,false,,,,,,,,,,,,,,34157,,,Thu Mar 18 22:34:20 UTC 2010,,,,,,0|i0ruzb:,160663,,,,,,,,18/Mar/10 22:34;psteitz;The method has been rewritten in version 2.1.  The current version no longer includes the compiler-dependent code referenced in this issue.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jacobian rank determination in LevenbergMarquardtOptimizer is not numerically robust,MATH-352,12458613,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,geneg102,geneg102,10/Mar/10 03:58,23/Mar/11 20:01,07/Apr/19 20:38,06/Jun/10 14:04,2.0,,,,,,,2.2,,,0,commons-math,DifferentiableMultivariateVectorialOptimizer,Levenberg,Marquardt,optimizer,qr,rank,"LevenbergMarquardtOptimizer is designed to handle singular jacobians,  i.e. situations when some of the fitted parameters depend on each other. The check for that condition is in LevenbergMarquardtOptimizer.qrDecomposition uses precise comparison to 0.

    if (ak2 == 0 ) {
                rank = k;
                return;
        }

A correct check would be comparison with a small epsilon. Hard coded 2.2204e-16 is used elsewhere in the same file for similar purpose.

final double QR_RANK_EPS = Math.ulp(1d); //2.220446049250313E-16
....
    if (ak2  < QR_RANK_EPS) {
                rank = k;
                return;
        }

Current exact equality check is not tolerant of the real world poorly conditioned situations. For example I am trying to fit a cylinder into sample 3d points. Although theoretically cylinder has only 5 independent variables, derivatives for optimizing function (signed distance) for such minimal parametrization are complicated and it  it much easier to work with a 7 variable parametrization (3 for axis direction, 3 for axis origin and 1 for radius). This naturally results in rank-deficient jacobian, but because of the numeric errors the actual ak2 values for the dependent rows ( I am seeing values of 1e-18 and less), rank handling code does not kick in.
Keeping these tiny values around then leads to huge corrections for the corresponding very slowly changing parameters, and consequently to numeric errors and instabilities. I have noticed the problem because tiny shift in the initial guess (on the order of 1e-12 in the axis component and origins) resulted in significantly different finally converged answers (origins and radii differing by as much as 0.02) which I tracked to loss of precision due to numeric error with root cause described above.
Providing a cutoff as suggested fixes the issue. After the fix, small perturbations in the initial guess had practically no effect to the converged result - as expected from a robust algorithm.
",commons-math 2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-06-06 14:04:42.316,,,false,,,,,,,,,,,,,,150520,,,Wed Mar 23 20:01:13 UTC 2011,,,,,,0|i0ruzr:,160665,,,,,,,,"06/Jun/10 14:04;luc;Fixed in subversion repository as of r951864.
A setQRRankingThreshold has been added as proposed in [http://markmail.org/message/p2j76cnwsyehl7u6].
Thanks for reporting the issue.","23/Mar/11 20:01;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver fails to solve feasible problem instance ,MATH-351,12458608,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,thomamark,thomamark,10/Mar/10 02:47,23/Mar/11 20:00,07/Apr/19 20:38,28/Sep/10 06:43,2.0,,,,,,,2.2,,,0,linear_programming,simplex,,,,,,"SimplexSolver throws an UnboundedSolutionException on a problem instance I can optimally solve with Excel's Solver. I've kept the parameters between the two programs the same as far as I can tell  (i.e. both have a precision/epsilon value of 1e-6 and a maxIterations value of 1000). I will attach a JUnit test  with an example problem on which SimplexSolver fails. I will also attach an Excel spreadsheet wtih the same data and successful Solver setup in place.

I don't know a whole lot about linear programming or Simplex, but the problem I'm attempting to solve does appear to have a fairly sparse coefficient matrix, which may be part of the problem.

It's surprisingly difficult to find a Java-based linear programming library, so I was ecstatic when I found this. Let me know how I can help!

Thanks!","Windows Vista Home Premium Version 6.0 Service Pack 1, Build 6001",,,,,,,,,,,,,,,,,,,,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image001.wmz;https://issues.apache.org/jira/secure/attachment/12440453/ASF.LICENSE.NOT.GRANTED--image001.wmz,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image017.gif;https://issues.apache.org/jira/secure/attachment/12440454/ASF.LICENSE.NOT.GRANTED--image017.gif,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image018.wmz;https://issues.apache.org/jira/secure/attachment/12440455/ASF.LICENSE.NOT.GRANTED--image018.wmz,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image019.gif;https://issues.apache.org/jira/secure/attachment/12440456/ASF.LICENSE.NOT.GRANTED--image019.gif,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image020.wmz;https://issues.apache.org/jira/secure/attachment/12440457/ASF.LICENSE.NOT.GRANTED--image020.wmz,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image021.gif;https://issues.apache.org/jira/secure/attachment/12440458/ASF.LICENSE.NOT.GRANTED--image021.gif,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image022.wmz;https://issues.apache.org/jira/secure/attachment/12440459/ASF.LICENSE.NOT.GRANTED--image022.wmz,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image023.gif;https://issues.apache.org/jira/secure/attachment/12440460/ASF.LICENSE.NOT.GRANTED--image023.gif,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image024.wmz;https://issues.apache.org/jira/secure/attachment/12440461/ASF.LICENSE.NOT.GRANTED--image024.wmz,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image025.gif;https://issues.apache.org/jira/secure/attachment/12440462/ASF.LICENSE.NOT.GRANTED--image025.gif,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image026.wmz;https://issues.apache.org/jira/secure/attachment/12440463/ASF.LICENSE.NOT.GRANTED--image026.wmz,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image027.gif;https://issues.apache.org/jira/secure/attachment/12440464/ASF.LICENSE.NOT.GRANTED--image027.gif,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image028.wmz;https://issues.apache.org/jira/secure/attachment/12440465/ASF.LICENSE.NOT.GRANTED--image028.wmz,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image029.gif;https://issues.apache.org/jira/secure/attachment/12440466/ASF.LICENSE.NOT.GRANTED--image029.gif,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image030.wmz;https://issues.apache.org/jira/secure/attachment/12440467/ASF.LICENSE.NOT.GRANTED--image030.wmz,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--image031.gif;https://issues.apache.org/jira/secure/attachment/12440468/ASF.LICENSE.NOT.GRANTED--image031.gif,01/Apr/10 07:38;jtas;ASF.LICENSE.NOT.GRANTED--oledata.mso;https://issues.apache.org/jira/secure/attachment/12440469/ASF.LICENSE.NOT.GRANTED--oledata.mso,10/Mar/10 02:52;thomamark;SimplexFail.xlsx;https://issues.apache.org/jira/secure/attachment/12438360/SimplexFail.xlsx,10/Mar/10 02:52;thomamark;TestSimplexFail.java;https://issues.apache.org/jira/secure/attachment/12438359/TestSimplexFail.java,19.0,,,,,,,,,,,,,,,,,,,2010-03-31 08:41:25.099,,,false,,,,,,,,,,,,,,150519,,,Wed Mar 23 20:00:48 UTC 2011,,,,,,0|i0ruzz:,160666,,,,,,,,10/Mar/10 02:52;thomamark;The JUnit test showing an example of a problem which fails with an UnboundedSolutionException when using SimplexSolver and the Excel spreadsheet containing the optimal solution. The two together prove (I believe) that there is something wrong with SimplexSolver.,"31/Mar/10 08:41;jtas;To put it simply, you have defined a problem of the following form:

max (0*x1 + 0*x2 + 0*x3 +...) subject to the following constraints:

x1 + x2 + ... >= some number

x2 + x3 + ... >= some number

etc...(you have defined inequalities of a similar form)

x1>=0, x2>=0,....

Ofcourse such a problem is unbounded; i.e. the solution is x1 = infinitiy, x2 = infinity,.... etc..
Maybe I don't understand the problem well, but to me it seems that the Excel program gives a wrong answer.

Jurgen
","31/Mar/10 08:59;jtas;I see what you have done wrong in your Java code. The coefficients in your objective function are all zero, while they need to be all one. If you change this, you get the same answer as Excel.

Jurgen","31/Mar/10 16:27;thomamark;Good catch. Not sure how I missed this. I'm still getting an UnboundedSolutionException, even after I set the coefficients all to 1, however. Note that I'm still using the 2.0 code which was noted above to have a bug in it. Are you using 2.1?

Thanks Jurgen!

--Mark","01/Apr/10 07:38;jtas;You are welcome!

 

I am using the trunk version of the code. I also found that 2.0 contained some errors. In the new versions I have not found anything strange. 

 

Interesting to me is the use of epsilon. I have found that the default value of 1e-6 works fine for most problems. However, consider the following problem:

 

 

 

The answer to this problem is trivial to find; i.e.  and .  However, for the case when we get a wrong answer; i.e. for   we find that and  (the second constraint is not satisfied), and even for  no feasible solution could be found. 

 

Too me the results of this very simple problem are obvious. But how do you determine the threshold for epsilon for general problems? Do you analyze the condition number of the constraints matrix?

 

Jurgen

","01/Apr/10 18:19;luc;Jurgen, your last comment on this issue is impossible to read, the images containing the equations are not inlined in the text (at least not with firefox on Linux). Could you rewrite it in simple raw text ?

Mark, do you agree to close the issue as invalid ?","01/Apr/10 20:38;thomamark;I haven't had a chance yet to verify for myself that this works on 2.1, but if it works for you, Luc and you consider the epsilon discussion a separate issue, I'm happy with closing this as invalid (especially if it will lead to an earlier release of 2.1 =).

For other readers, it appears that there were bug fixes for Simplex which weren't in 2.0, but have been incorporated into the trunk (see: http://issues.apache.org/jira/browse/MATH-302). If this is so, we'll want to leave this link in.

Thanks guys and sorry for the mix up.
Mark",28/Sep/10 06:43;luc;solving as invalid as per last comments,23/Mar/11 20:00;luc;Closing issue which was declared invalid long ago,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Regression in package ""regression""",MATH-350,12458567,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Incomplete,,erans,erans,09/Mar/10 18:21,12/Mar/10 12:27,07/Apr/19 20:38,12/Mar/10 12:27,2.0,,,,,,,,,,0,,,,,,,,"There is a regression in class ""OLSMultipleLinearRegression"".",,,,,,,,,,,,,,,,,,,,,11/Mar/10 11:14;erans;OLSRegressionCompare20To21Test.java;https://issues.apache.org/jira/secure/attachment/12438501/OLSRegressionCompare20To21Test.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150518,,,Fri Mar 12 12:27:35 UTC 2010,,,,,,0|i0rv07:,160667,,,,,,,,"11/Mar/10 11:14;erans;Unit test from the user who discovered the issue. Copying it into the
  src/test/java/org/apache/commons/math/stat/regression
directory of the CM source tree, and running the tests will result in the following output:

{noformat}
Running org.apache.commons.math.stat.regression.OLSRegressionCompare20To21Test
Test parameters
Test variance
Expected 2.7104787386850834E-26 Actual 1.091646231028372E-25
Constructed a polynomial of degree 2
System will model various degrees of polynomial and use an Ftest to find the best model


Testing trend of degree 1
OLSRegressionCompare20To21Test: model statistic: 1545.9235701200896 threshold 3.9381110982233225
Model with degree 1 is better - keep testing higher order models

Testing trend of degree 2
OLSRegressionCompare20To21Test: model statistic: 8.9522618877706E31 threshold 3.939126144339758
Model with degree 2 is better - keep testing higher order models

Testing trend of degree 3
OLSRegressionCompare20To21Test: model statistic: 109.12572361790322 threshold 3.940162735266305
Model with degree 3 is better - keep testing higher order models

Testing trend of degree 4
OLSRegressionCompare20To21Test: model statistic: -40.823878611285174 threshold 3.941221564253479
Model with degree 4 is rejected - keeping simple model and exiting
Best model found with degree = 3
Coeff 0 = 3.000000000000148
Coeff 1 = 1.2000000000000006
Coeff 2 = 0.3399999999999999
Coeff 3 = 1.0632735348660702E-18
Test residuals
Test standard errors
Test hat matrix
Test parameter variance
{noformat}

So, although the last coefficient is nearly zero, it is conceptually wrong to return a fit with a polynomial of degree 3 whereas the input data was generated from a polynomial of degree 2.
[CM 2.0 behaved properly in this respect.]
","11/Mar/10 11:24;erans;Examining the history:
{noformat}
r825925 | luc | 2009-10-16 17:11:47 +0200 (Fri, 16 Oct 2009) | 1 line

replaced custom linear solve computation by use of the linear package features
{noformat}

The diff shows:
{noformat}
-        return solveUpperTriangular(qr.getR(), qr.getQ().transpose().operate(Y));
+        return qr.getSolver().solve(Y);
{noformat}
","12/Mar/10 12:27;erans;The initial reporter identified changes (of the order of 1e-13) in the values computed by the current code and those from 2.0. However we cannot assert which ones are ""better"" at this point.
The provided test does not clearly points to some deficiency in the CM source code.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Dangerous code in ""PoissonDistributionImpl""",MATH-349,12458249,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,erans,erans,05/Mar/10 15:15,23/Mar/11 19:59,07/Apr/19 20:38,26/Sep/10 21:55,,,,,,,,3.0,,,0,,,,,,,,"In the following excerpt from class ""PoissonDistributionImpl"":

{code:title=PoissonDistributionImpl.java|borderStyle=solid}
    public PoissonDistributionImpl(double p, NormalDistribution z) {
        super();
        setNormal(z);
        setMean(p);
    }
{code}

(1) Overridable methods are called within the constructor.
(2) The reference ""z"" is stored and modified within the class.

I've encountered problem (1) in several classes while working on issue 348. In those cases, in order to remove potential problems, I copied/pasted the body of the ""setter"" methods inside the constructor but I think that a more elegant solution would be to remove the ""setters"" altogether (i.e. make the classes immutable).
Problem (2) can also create unexpected behaviour. Is it really necessary to pass the ""NormalDistribution"" object; can't it be always created within the class?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-03-06 15:49:19.792,,,false,,,,,,,,,,,,,,150517,,,Wed Mar 23 19:59:37 UTC 2011,,,,,,0|i0rv0f:,160668,,,,,,,,"06/Mar/10 15:49;psteitz;The reason this constructor exists is to allow users to plug in an alternative normal distribution implementation to be used in computing normal approximations.  I don't see 1) as a serious issue, but I am +1 on deprecating the setters with aim to make this class immutable in 3.0.  2) is a harder problem, as there is no requirement that a NormalDistribution be clonable.   I see three solutions, none of which are particularly appealing:

1) leave as is and specify in the javadoc that z is going to be modified
2) change the implementation to avoid changing the parameters of z 
3) deprecate the constructor altogether

I vote for 1) + 3) - update the javadoc, but deprecate.  If we get complaints before 3.0, we can reconsider; otherwise eliminate in 3.0","08/Mar/10 16:48;erans;Problem (1)
In principle, it is not safe to call overridable methods in the constructor, so, if only to promote coding quality, can I implement private (""helper"") setter methods that shall be called from within the constructors and from the public setters?

Problem (2)
In ""PoissonDistributionImpl.java"", there is this code:

{code:title=PoissonDistributionImpl.java|borderStyle=solid}

public PoissonDistributionImpl(double p, NormalDistribution z) {
    super();
    setNormal(z);
    setMean(p);
}

public void setMean(double p) {
    // ...
    this.mean = p;
    normal.setMean(p);
    normal.setStandardDeviation(Math.sqrt(p));
}

public void setNormal(NormalDistribution value) {
    normal = value;
}
{code}

In the constructor, the code makes sure that the ""mean"" of this class and the mean of the ""normal"" object are consistent.
But there is no such guarantee anymore when calling the ""setNormal"" method. [The comment warns the user that he is responsible for setting the right parameter in ""value"" but this is far from fool-proof...]

","08/Mar/10 17:45;psteitz;I agree that both (1) and (2) are problems.  The private helpers would be OK to address (1), but I am also OK with deprecating the setters and just setting the fields directly (per MATH-348) in the constructor.  As stated above, I am also +1 on deprecating the pluggability of the normal impl, which will (eventually) address (2).  I guess to address (2) now we either need to modify setNormal to update the parameters of the normal instance as setMean does or change the normal approximation implementation to not depend on the parameters of the distribution (which addresses the problem of z being updated).  Its probably easiest and best in the long term to take the first approach, documenting the fact that z is going to be updated (both in setNormal and constructor javadoc).","09/Mar/10 13:02;erans;Issue is partially fixed (removed the consistency problem) in revision 920852.
Setters were marked as deprecated.
","09/Mar/10 13:46;psteitz;Issue (2) in the bug description remains open.  I think we should leave this issue open until we have a solution for this.  As stated above, I can see two ways to fix this:

a) modify the normal approximation implementation so that it does not change the parameters of z
b) eliminate pluggability of z (i.e., deprecate and then remove the constructor that accepts a normal instance as a parameter)

a) could be accomplished in 2.x, but it would complicate the code and could be bad for numerics.  My vote is for b) - add a warning (about safety as well as deprecation), deprecate now and if we get no complaints before 3.0, remove then.   Leave the issue open with fix version 3.0.","09/Mar/10 14:07;erans;The same kind of problem also exists in ""ChiSquaredDistributionImpl.java"".
In both classes, the constructor that takes the distribution as a parameter has been deprecated.
","09/Mar/10 15:22;psteitz;Sorry,  I missed the constructor deprecations in r920852",09/Mar/10 15:25;psteitz;Leaving open until deprecated constructors are removed in 3.0.,25/Sep/10 23:07;erans;{{PoissonDistributionImpl}} made immutable in revision 1001331.,26/Sep/10 21:54;erans;{{ChiSquaredDistributionImpl}} made immutable in revision 1001533.,"26/Sep/10 21:55;erans;Removed deprecated code.  Classes are now immutable.
","23/Mar/11 19:59;luc;Closing issue as it was included in version 2.2, which has been released",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Brent solver shouldn't need strict ordering of min, max and initial",MATH-347,12457428,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,aktasv,aktasv,25/Feb/10 14:33,03/Apr/10 03:06,07/Apr/19 20:38,01/Mar/10 19:39,2.0,,,,,,,,,,0,,,,,,,,"The ""solve(final UnivariateRealFunction f, final double min, final double max, final double initial)"" function calls verifySequence() which enforces a strict ordering of min, max and initial parameters. I can't see why that is necessary - the rest of solve() seems to be able to handle ""initial == min"" and ""initial == min"" cases just fine. In fact, the JavaDoc suggests setting initial to min when not known but that is not possible at the moment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-03-01 19:39:24.285,,,false,,,,,,,,,,,,,,150515,,,Mon Mar 01 19:39:24 UTC 2010,,,,,,0|i0rv0v:,160670,,,,,,,,"25/Feb/10 14:36;aktasv;Also would like to add that when this constraint is removed it should be possible to make the ""solve(final UnivariateRealFunction f, final double min, final double max)"" call the other solve() function without any additional logic.
 
","01/Mar/10 19:39;luc;Fixed in subversion repository as of r917668.
Thanks for reporting the issue",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brent solver returns the wrong value if either bracket endpoint is root,MATH-344,12457210,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,aktasv,aktasv,23/Feb/10 20:23,03/Apr/10 03:07,07/Apr/19 20:38,23/Feb/10 21:10,2.0,,,,,,,,,,0,,,,,,,,"The solve(final UnivariateRealFunction f, final double min, final double max, final double initial) function returns yMin or yMax if min or max are deemed to be roots, respectively, instead of min or max.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-02-23 21:10:00.408,,,false,,,,,,,,,,,,,,34150,,,Tue Feb 23 21:10:00 UTC 2010,,,,,,0|i0rv1j:,160673,,,,,,,,"23/Feb/10 21:10;luc;Fixed in subversion repository as of r915522
thanks for reporting the issue",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign,MATH-343,12457208,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,aktasv,aktasv,23/Feb/10 20:21,03/Apr/10 03:44,07/Apr/19 20:38,23/Feb/10 21:02,2.0,,,,,,,2.1,,,0,,,,,,,,"Javadoc for ""public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)"" claims that ""if the values of the function at the three points have the same sign"" an IllegalArgumentException is thrown. This case isn't even checked.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-02-23 21:02:31.079,,,false,,,,,,,,,,,,,,34172,,,Wed Feb 24 11:34:29 UTC 2010,,,,,,0|i0rv1r:,160674,,,,,,,,"23/Feb/10 21:02;luc;Fixed in subversion repository as of r915517
Thanks for reporting the issue","23/Feb/10 22:56;aktasv;Thanks for the quick turnaround. One comment: I'm not sure whether the check before throwing the IllegalArgumentException is necessary. You can have only the following situations (given that min <= initial <= max and assuming neither min nor max is a root):

    * yMin and yMax have the same sign:
        ** yInitial has a different sign: Handled on line 121 (function is not monotonous between min and max)
        ** yInitial has the same sign: Falls through to line 136 and yMin * yMax > 0 by definition
    * yMin and yMax do not have the same sign:
        ** yInitial has the same sign as yMax: Handled on line 121
        ** yInitial has the same sign as yMin: Handled on line 133

In this case I'd say code between lines 131 and 142 should be replaced by the throw statement on line 137.","24/Feb/10 11:34;luc;You are right.
I have removed the unreachable code and committed it in the subversion repository
Thanks again",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SVD crashes when applied to a strongly rectangular matrix (typical case of least-squares problem),MATH-342,12456954,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,dimpbx,dimpbx,dimpbx,21/Feb/10 21:41,03/Apr/10 03:45,07/Apr/19 20:38,21/Feb/10 21:50,2.0,,,,,,,2.1,,,0,,,,,,,,"When SVD is applied to a strongly rectangular matrix (number of rows way larger than number of columns, typical case of least-squares problem), finite precision arithmetics shows up:
 - in EigenDecompositionImpl.isSymmetric: a by-definition symmetric matrix returns false;
 - in EigenDecompositionImpl.findEigenVectors: too many iterations exception ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150513,,,Sun Feb 21 21:50:20 UTC 2010,,,,,,0|i0rv1z:,160675,,,,,,,,"21/Feb/10 21:50;dimpbx;The two identified troublesome behaviors of EigenDecomposition are corrected.  Besides the regular unit tests, the two classes SingularValueDecompositionimpl and EigenDecompositionImpl have now been successfully tested over 300k+ systems coming from some astronomical application.  No crash reported!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test for firsst Derivative in PolynomialFunction ERROR,MATH-341,12455491,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,rfball,rfball,05/Feb/10 20:32,09/Feb/10 23:21,07/Apr/19 20:38,09/Feb/10 20:08,2.0,,,,,,,,,,0,,,,,,,,"I have written the attached test using our data for generating a curve function

However the first derivative test fails see: testfirstDerivativeComparisonFullPower

Either my test is in error or there is a bug in PolynomialFunction class.


Roger Ball
Creoss Business Solutions ",,,,,,,,,,,,,,,,,,,,,05/Feb/10 20:34;rfball;FirstDerivativePolyNomTest.java;https://issues.apache.org/jira/secure/attachment/12435005/FirstDerivativePolyNomTest.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-02-05 21:50:33.314,,,false,,,,,,,,,,,,,,34204,,,Tue Feb 09 23:21:27 UTC 2010,,,,,,0|i0rv27:,160676,,,,,,,,05/Feb/10 20:34;rfball;FirstDerivativePolyNomTest.java,"05/Feb/10 21:50;luc;I think there are no bugs here.
The first part of the test (method testfirstDerivativeComparison) runs without error.
The second part of the test (method testfirstDerivativeComparisonFullPower) prints coefficients that do not match.

However, the coefficients computed seem wrong to me because
* the getCoefficients method returns an array in increasing degree order (i.e. coeff[0] is the constant term)
* the coefficients array length is d+1 when d is the degree

This implies that the g_coeff array which should contain the coefficients of the derivative must have length myPolyNom.degree() and not myPolyNom.degree()-1 and the following loop should match.  Also the computation of the coefficients of the derivative shoud be:

  g_coeff[i] = f_coeff[i+1] * (i + 1);

instead of

  g_coeff[i] = f_coeff[i]*(myPolyNom.degree()-i);

With these changes, the automatic computation of derivative works.

A first comment on this case is that using polynomial fitting for such functions with large numbers of points shows very large Gibbs oscillations near the interval ends. One way to see this is to draw the curve from the sample points and from evaluation of myPolyNom with x varying from 5 to 30 with a 0.1 step. You will see that the polynomial fits the sample points perfectly, but near interval ends it has HUGE oscillations.

Another comment is that extracting the coefficients from the lagrangian form should be used with caution. Unfortunately, this is written only in the protected computationCoefficients() method javadoc, not in the public getCoefficients() method javadoc ... The computation is ill-conditioned and in fact the coefficients returned in your case are really bad. This could be seen by evaluating the original lagrangian form polynomial and the one reconstructed from the coefficients. The original does match the expected points, the reconstructed one does not.

So I think there is only a documentation problem: we should warn the user about extracting coefficients in this implementation.

Do you agree with this analysis ?","09/Feb/10 17:59;rfball;Regarding: 
""So I think there is only a documentation problem: we should warn the user about extracting coefficients in this implementation.

Do you agree with this analysis ?""

Certainly a documentation problem. However, if the coefficients do not result in a curve expression that is valid of the entire range of values for which the curve expression is derived then the curve expression is not really useful in a practical sense. I also the erractic occillations in curve. These occillations are HUGE when comparred to the data. Therefore the expression is not all together valid.","09/Feb/10 20:00;luc;I will fix the documentation.

The coefficients are valid when the number of points is reduced (for example consider only the 8 or 10 last points, it should work). Also note that polynomials in lagrangian form should really stay in this form and should not be converted to the canonical sum of monomials. It IS ill-conditioned so attempting this for high degree is not advised.

The oscillations you see are not an implementation problem, the mathematical solution is really like this. This is a theoretical problem known as Runge's phenomenon. See for example [http://demonstrations.wolfram.com/RungesPhenomenon/] and especially the high degree versions at the bottom of the page. You will see huge interpolation error at the end of the interval. When using polynomial interpolation, increasing the degree and the number of points but keeping them equidistant does NOT reduce maximal error (in fact for some functions error will tend towards infinity). The expression perfectly fits the sample points you provide but it has no information of what to do between them.

Polynomial interpolation should really be used with care. If you want smoother behavior, you should use several lower degrees polynomials each covering a subrange of your data.
","09/Feb/10 20:08;luc;added a warning in getCoefficients() method documentation
fixed in subversion repository as of r908190
thanks for reporting the problem","09/Feb/10 21:57;rfball;Looking at ""See for example http://demonstrations.wolfram.com/RungesPhenomenon/ "" it seems that if you select Chebyshev sample points the error drops dramatically all along the curve. Is this an option in the apache math library?","09/Feb/10 23:21;luc;You select the points yourself as you provides the x and y arrays, so yes, you can choos points at Chebyshev abscissas.
The array even don't need to be sorted. However, you cannot have two points with the same x value.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BigFraction numerator constrainted by int size during multiplication,MATH-340,12455227,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mjtaylor,mjtaylor,03/Feb/10 20:17,03/Apr/10 03:45,07/Apr/19 20:38,03/Feb/10 21:21,2.0,,,,,,,2.1,,,0,,,,,,,,"When multiplying two BigFraction objects with numerators larger than will fit in an java-primitive int the result of BigFraction.ZERO is incorrectly returned.


Test Case:

	        BigFraction fractionA = new BigFraction(0.00131);
	        BigFraction fractionB = new BigFraction(.37).reciprocal();
	        BigFraction errorResult = fractionA.multiply(fractionB);
	        System.out.println(""Error Result: "" + errorResult);
		BigFraction correctResult = new BigFraction(fractionA.getNumerator().multiply(fractionB.getNumerator()), fractionA.getDenominator().multiply(fractionB.getDenominator()));
	        System.out.println(""Correct Result: "" + correctResult);
",All,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-02-03 21:21:06.956,,,false,,,,,,,,,,,,,,34197,,,Wed Feb 03 21:21:06 UTC 2010,,,,,,0|i0rv2f:,160677,,,,,,,,"03/Feb/10 21:21;luc;Fixed in subversion repository as of r906251.
Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong parameter for first step size guess for Embedded Runge Kutta methods,MATH-338,12446898,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,vincent.morand,vincent.morand,28/Jan/10 10:59,03/Apr/10 03:54,07/Apr/19 20:38,28/Jan/10 15:17,2.0,,,,,,,2.1,,,0,,,,,,,,"In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator.

Here, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...)

The problem comes from the array ""scale"" that is used as a parameter in the call off initializeStep(..)

Following the theory described by Hairer in his book ""Solving Ordinary Differential Equations 1 : Nonstiff Problems"", the scaling should be :

sci = Atol i + |y0i| * Rtoli

Whereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli

Note that the Gragg-Bulirsch-Stoer integrator uses the good implementation ""sci = Atol i + |y0i| * Rtoli  "" when he performs the call to the same method initializeStep(..)

In the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user.
But in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...)


To fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator
For exemple :

 final double[] scale= new double[y0.length];;
          
          if (vecAbsoluteTolerance == null) {
              for (int i = 0; i < scale.length; ++i) {
                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));
                scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;
              }
            } else {
              for (int i = 0; i < scale.length; ++i) {
                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));
                scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;
              }
            }
          
          hNew = initializeStep(equations, forward, getOrder(), scale,
                           stepStart, y, yDotK[0], yTmp, yDotK[1]);



Sorry for the length of this message, looking forward to hearing from you soon

Vincent Morand




",Eclipse sous Red Hat 5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-01-28 15:17:46.543,,,false,,,,,,,,,,,,,,150511,,,Thu Jan 28 15:17:46 UTC 2010,,,,,,0|i0rv2v:,160679,,,,,,,,"28/Jan/10 15:17;luc;Fixed in subversion repository as of r904112.
Note that I have changed slightly the fix you proposed: the call to Math.max was not needed because both arguments were the same (in GBS integrator, they are different).
Note that instead of letting the integrator guess the first step by itself, you can provide it yourself by calling setInitialStepSize. This setting must be done before the call to integrate, which is called by Orekit propagate method if you happen to use Orekit for your application ;-) For such applications, an initial step of the order of magnitude of 1/100 of the keplerian period is a fair bet, it will be adjusted by the integrator if inconsistent with your accuracy settings.

Thanks for the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fraction.hashCode() inconsistent with Fraction.equals(),MATH-335,12446819,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,sebb@apache.org,sebb@apache.org,27/Jan/10 15:31,03/Apr/10 20:33,07/Apr/19 20:38,14/Mar/10 01:38,,,,,,,,2.1,,,0,,,,,,,,"Fraction.hashCode() is inconsistent with Fraction.equals().

hashCode() uses getNumerator() (which is not final) rather than directly accessing the field.

Seems to me that hashCode() has no reason to use the getters - or if it does, then equals should do so too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-01-27 16:40:24.405,,,false,,,,,,,,,,,,,,150508,,,Sun Mar 14 01:38:48 UTC 2010,,,,,,0|i0rv3j:,160682,,,,,,,,27/Jan/10 16:40;luc;Agreed. I would prefer it uses the fields directly.,"14/Mar/10 01:38;sebb@apache.org;URL: http://svn.apache.org/viewvc?rev=922715&view=rev
Log:
MATH-335 Fraction.hashCode() inconsistent with Fraction.equals()
Change hashCode() to use fields directly
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
eigenvectors computation is wrong when several vectors share the same eigenvalue,MATH-333,12446568,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,dimpbx,luc,luc,25/Jan/10 17:56,03/Apr/10 20:34,07/Apr/19 20:38,16/Feb/10 17:43,,,,,,,,2.1,,,0,,,,,,,,"A typical example is to decompose identity matrix. There is only one eigenvalue: 1.0. Instead of returning n different eigenvectors forming a complete vector base, the same vector is returned n times.",,,,,,,,,,,,,,,,,,,,,10/Feb/10 22:52;dimpbx;EigenDecompositionImpl.java;https://issues.apache.org/jira/secure/attachment/12435510/EigenDecompositionImpl.java,11/Feb/10 07:30;dimpbx;MATH-333_320.patch;https://issues.apache.org/jira/secure/attachment/12435555/MATH-333_320.patch,10/Feb/10 22:52;dimpbx;SingularValueDecompositionImpl.java;https://issues.apache.org/jira/secure/attachment/12435511/SingularValueDecompositionImpl.java,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2010-02-10 22:52:28.792,,,false,,,,,,,,,,,,,,150506,,,Thu Feb 11 07:30:47 UTC 2010,,,,,,0|i0rv3z:,160684,,,,,,,,02/Feb/10 22:12;luc;This issue should really be fixed for 2.1,"10/Feb/10 22:52;dimpbx;EigenDecomposition is the Java (strict) translation of some parts of the
LAPACK library, as obscure as the original.  It is likely efficient when
it works properly but it is a debugging nightmare when it does not.  Far
away from most OOP paradigms, every useful quantity is stored in a huge
array which is used as a working area.

Part of the LAPACK complexity as far as EigenDecomposition is concerned
is caused by its wish to handle almost any kind of matrix.  So, lots of
routines are introduced to handle these different cases.  If one tries to
follow the path of a real symmetric matrix in that jungle, one still ends
up with a lot of routines and thus with a lot of Java methods.  However,
if one decides from the beginning that we will deal with real symmetric
matrices only, there are way simpler alternatives to the LAPACK collection.

Even without changing the current methods, a bit of mathematical consistency
might be welcome: the class EigenDecomposition is clearly described as sol-
ving an eigen problem for a real symmetic matrix.  Fine.  But if it is so,
there is no point in keeping imaginaryEigenvalues: the eigen values of a
real symmetric matrix are real (LAPACK legacy)!!!  The only reason I see for
keeping the imaginary part is because solver() needs it.
At the cost of a second eigen decomposition, SVD now returns the correct
number of singular values (and left and right singular vectors) even when
the matrix is singular.

A few signs in the eigen vectors of testMath308 must be changed to
successfully pass to the tests.

testTruncated and testMath320A (in SingularValueSolverTest) have been
removed since SVD always return the maximum number of singular values.
testMatricesValues2 (from SingularValueDecompositionImplTest) is
removed because the SVD decomposition is not unique.
",11/Feb/10 07:30;dimpbx;The attached file is the result 'svn diff' from trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable) ",MATH-329,12445486,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,williausrohr,williausrohr,14/Jan/10 08:01,03/Apr/10 20:35,07/Apr/19 20:38,16/Jan/10 20:02,2.0,,,,,,,2.1,,,0,,,,,,,,"Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change

Frequency.java

   /**
      * Returns the percentage of values that are equal to v
     * @deprecated replaced by {@link #getPct(Comparable)} as of 2.0
     */
    @Deprecated
    public double getPct(Object v) {
        return getCumPct((Comparable<?>) v);
    }",,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-01-16 20:02:15.55,,,false,,,,,,,,,,,,,,34195,,,Sat Jan 16 20:02:15 UTC 2010,,,,,,0|i0rv4v:,160688,,,,,,,,16/Jan/10 20:02;psteitz;Fixed in r900016.  Thanks for reporting this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Is ""NoFeasibleSolutionException"" Bug not Fixed Yet?",MATH-328,12445022,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Incomplete,,shon2906,shon2906,08/Jan/10 11:53,03/Apr/10 20:36,07/Apr/19 20:38,13/Mar/10 20:22,2.1,,,,,,,2.1,,,0,,,,,,,,"I received a e-mail which introduces URL ""https://svn.apache.org/repos/asf/commons/proper/math/trunk"".
I downloaded Source Files in  Package ""org\apache\commons\math\optimization\linear"" 
and compiled these files(make a new commons-math-2.?.jar = Simplex Patch Version)

Lastly I  tested a fixed Simplex module, But didn't get a feasible solution.
Exception Message is as follow

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
org.apache.commons.math.optimization.linear.NoFeasibleSolutionException: no feasible solution
        at org.apache.commons.math.optimization.linear.SimplexSolver.solvePhase1(SimplexSolver.java:166)
        at org.apache.commons.math.optimization.linear.SimplexSolver.doOptimize(SimplexSolver.java:176)
        at org.apache.commons.math.optimization.linear.AbstractLinearOptimizer.optimize(AbstractLinearOptimizer.java:106)
        at SimplexSolverTest.test111(SimplexSolverTest.java:711)
        at SimplexSolverTest.main(SimplexSolverTest.java:730
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



Test Data is as follow
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
double[] ob = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1000,0,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,0};
		double[] c1  = {115239312,65417490,111006072,63657738,26800758,120283110,137807244,90077436,25759098,20452338,40821822,136261944,54779832,42682500,138210804,27299808,144635238,105106248,46103364,142549164,94178844,129443166,127867176,152840844,104050710,122083578,25506900,110386026,79515846,149110056,98889318,87030882,128243016,112193730,109948500,134574300,72316818,174011850,139884606,170461962,178817040,45444960,119593602,82012374,155913894,19837854,135737676,77510844,168166152,63235476,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c2  = {115239312,65417490,111006072,63657738,26800758,120283110,137807244,90077436,25759098,20452338,40821822,136261944,54779832,42682500,138210804,27299808,144635238,105106248,46103364,142549164,94178844,129443166,127867176,152840844,104050710,122083578,25506900,110386026,79515846,149110056,98889318,87030882,128243016,112193730,109948500,134574300,72316818,174011850,139884606,170461962,178817040,45444960,119593602,82012374,155913894,19837854,135737676,77510844,168166152,63235476,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c3  = {8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,11.1,14.4,7.5,4.5,3.6,9.3,12.6,5.4,1.2,6.6,8.1,9.6,8.1,8.1,13.8,0.6,14.1,6.6,15,10.2,7.2,6,1.2,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c4  = {8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,8.4,11.1,14.4,7.5,4.5,3.6,9.3,12.6,5.4,1.2,6.6,8.1,9.6,8.1,8.1,13.8,0.6,14.1,6.6,15,10.2,7.2,6,1.2,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c5  = {-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-48.9,-45.6,-52.5,-55.5,-56.4,-50.7,-47.4,-54.6,-58.8,-53.4,-51.9,-50.4,-51.9,-51.9,-46.2,-59.4,-45.9,-53.4,-45,-49.8,-52.8,-54,-58.8,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c6  = {-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-51.6,-48.9,-45.6,-52.5,-55.5,-56.4,-50.7,-47.4,-54.6,-58.8,-53.4,-51.9,-50.4,-51.9,-51.9,-46.2,-59.4,-45.9,-53.4,-45,-49.8,-52.8,-54,-58.8,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c7  = {-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c8  = {-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c9  = {24,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,24,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c10 = {24,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,24,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c11 = {-9,21,21,21,21,21,21,21,21,21,21,21,21,21,-9,-9,-9,-9,-9,21,21,21,-9,-9,-9,-9,-9,21,21,-9,-9,-9,21,-9,-9,21,21,-9,-9,-9,21,21,21,21,21,21,21,21,21,21,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c12 = {-9,21,21,21,21,21,21,21,21,21,21,21,21,21,-9,-9,-9,-9,-9,21,21,21,-9,-9,-9,-9,-9,21,21,-9,-9,-9,21,-9,-9,21,21,-9,-9,-9,21,21,21,21,21,21,21,21,21,21,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0};
		double[] c13 = {-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,24,24,24,24,24,-6,-6,-6,24,-6,-6,24,24,-6,-6,24,24,24,-6,24,-6,-6,-6,-6,-6,24,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0};
		double[] c14 = {-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,24,24,24,24,24,-6,-6,-6,24,-6,-6,24,24,-6,-6,24,24,24,-6,24,-6,-6,-6,-6,-6,24,-6,-6,-6,-6,-6,-6,-6,-6,-6,-6,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0};
		double[] c15 = {-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,27,27,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,27,27,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0};
		double[] c16 = {-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,27,27,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,27,27,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,-3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0};
		double[] c17 = {-21,9,9,9,9,9,9,9,9,9,9,9,9,-21,9,9,-21,-21,9,9,9,9,-21,-21,-21,9,9,9,9,9,9,9,9,9,-21,-21,-21,-21,9,9,9,9,9,9,9,9,9,9,9,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0};
		double[] c18 = {-21,9,9,9,9,9,9,9,9,9,9,9,9,-21,9,9,-21,-21,9,9,9,9,-21,-21,-21,9,9,9,9,9,9,9,9,9,-21,-21,-21,-21,9,9,9,9,9,9,9,9,9,9,9,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0};
		double[] c19 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0};
		double[] c20 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0};
		double[] c21 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0};
		double[] c22 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0};
		double[] c23 = {-76826208,-43611660,-74004048,-42438492,-17867172,-80188740,-91871496,-60051624,-17172732,-13634892,-27214548,-90841296,-36519888,-28455000,-92140536,-18199872,-96423492,-70070832,-30735576,-95032776,-62785896,-86295444,-85244784,-101893896,-69367140,-81389052,-17004600,-73590684,-53010564,-99406704,-65926212,-58020588,-85495344,-74795820,-73299000,-89716200,-48211212,-116007900,-93256404,-113641308,-119211360,-30296640,-79729068,-54674916,-103942596,-13225236,-90491784,-51673896,-112110768,-42156984,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c24 = {1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c25 = {0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c26 = {0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c27 = {0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c28 = {0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c29 = {0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c30 = {0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c31 = {0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c32 = {0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c33 = {0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c34 = {0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c35 = {0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c36 = {0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c37 = {0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c38 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c39 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c40 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c41 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c42 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c43 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c44 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c45 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c46 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c47 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c48 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c49 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c50 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c51 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c52 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c53 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c54 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c55 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c56 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c57 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c58 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c59 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c60 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c61 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c62 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c63 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c64 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c65 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c66 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c67 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c68 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c69 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c70 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c71 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c72 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
		double[] c73 = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
	
		
		
		
		LinearObjectiveFunction f = new LinearObjectiveFunction(ob, 0 );
	  	Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
	
	  	constraints.add(new LinearConstraint(c1 , Relationship.GEQ,3000000000.0d));
		constraints.add(new LinearConstraint(c2 , Relationship.LEQ,3000000000.0d));
		constraints.add(new LinearConstraint(c3 , Relationship.LEQ,1200.0d));
		constraints.add(new LinearConstraint(c4 , Relationship.GEQ,1200.0d));
		constraints.add(new LinearConstraint(c5 , Relationship.LEQ,0.0d));
		constraints.add(new LinearConstraint(c6 , Relationship.GEQ,0.0d));
		constraints.add(new LinearConstraint(c7 , Relationship.LEQ,0.0d));
		constraints.add(new LinearConstraint(c8 , Relationship.GEQ,0.0d));
		constraints.add(new LinearConstraint(c9 , Relationship.LEQ,0.0d));
		constraints.add(new LinearConstraint(c10, Relationship.GEQ,0.0d));
		constraints.add(new LinearConstraint(c11, Relationship.LEQ,0.0d));
		constraints.add(new LinearConstraint(c12, Relationship.GEQ,0.0d));
		constraints.add(new LinearConstraint(c13, Relationship.LEQ,0.0d));
		constraints.add(new LinearConstraint(c14, Relationship.GEQ,0.0d));
		constraints.add(new LinearConstraint(c15, Relationship.LEQ,0.0d));
		constraints.add(new LinearConstraint(c16, Relationship.GEQ,0.0d));
		constraints.add(new LinearConstraint(c17, Relationship.LEQ,0.0d));
		constraints.add(new LinearConstraint(c18, Relationship.GEQ,0.0d));
		constraints.add(new LinearConstraint(c19, Relationship.LEQ,0.0d));
		constraints.add(new LinearConstraint(c20, Relationship.GEQ,0.0d));
		constraints.add(new LinearConstraint(c21, Relationship.LEQ,0.0d));
		constraints.add(new LinearConstraint(c22, Relationship.GEQ,0.0d));
		constraints.add(new LinearConstraint(c23, Relationship.GEQ,0.0d));
		constraints.add(new LinearConstraint(c24, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c25, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c26, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c27, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c28, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c29, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c30, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c31, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c32, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c33, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c34, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c35, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c36, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c37, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c38, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c39, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c40, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c41, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c42, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c43, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c44, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c45, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c46, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c47, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c48, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c49, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c50, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c51, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c52, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c53, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c54, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c55, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c56, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c57, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c58, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c59, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c60, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c61, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c62, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c63, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c64, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c65, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c66, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c67, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c68, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c69, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c70, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c71, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c72, Relationship.GEQ,1.0d));
		constraints.add(new LinearConstraint(c73, Relationship.GEQ,1.0d));
	
	
	 	try {
		  	SimplexSolver solver = new SimplexSolver();
		  	
		  	RealPointValuePair solution = solver.optimize(f, constraints, GoalType.MINIMIZE, false);
		 
		  	double[] p = solution.getPoint();
		
		  	for(int i=0; i < p.length; i++) {
		  		System.out.println(p[i]);
		  	}
		}catch(Exception e) {
			e.printStackTrace();
		}
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


for reference,  I  need to use  a few hundred contraints(The data presented is part of the required data)",Windows XP commons Math 2.0 jre 1.6 ,,,,,,,,,,,,,,,,,,,,08/Jan/10 11:56;shon2906;error_data.txt;https://issues.apache.org/jira/secure/attachment/12429742/error_data.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2010-01-08 22:33:00.708,,,false,,,,,,,,,,,,,,34162,,,Tue Jan 26 22:08:55 UTC 2010,,,,,,0|i0rv53:,160689,,,,,,,,08/Jan/10 11:56;shon2906;test data file,"08/Jan/10 22:33;bmccann;I can't tell whether this is a bug or is giving you the correct solution.  Very often there is not a feasible solution, so this may well be functioning correctly.  It would be helpful to run this same problem through another linear programming library and see if it gives the same solution.  Also, if possible, smaller examples are usually much easier to debug.  I'd suggest perhaps using a free trial of Lindo and seeing if it returns that there is no feasible solution.","11/Jan/10 01:15;shon2906;I Already tried to confirm whether other software(Lindo's What's Best) solves it or not,
But Lindo's S/W didn't give us ""No Feasible Solution"".
It presents solution.

In fact attached data is part of the real test data(using Lindo's S/W)
",26/Jan/10 22:08;luc;Could you provide the solution found by Lindo on this test case ?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Maximal number of iterations (540) exceeded,MATH-327,12444750,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,mansoorkhan,mansoorkhan,05/Jan/10 21:49,24/Mar/12 16:16,07/Apr/19 20:38,20/Jul/11 12:18,,,,,,,,3.0,,,0,,,,,,,,"I have a matrix of size 49x19 and when I apply SVD on this matrix it raises the following exception. The problem which I am facing is that SVD works for some matrix and doesn't work for others. I have no clue what is the possible reason.

Exception::
CorrespondenceAnalysis: org.apache.commons.math.MaxIterationsExceededException: Maximal number of iterations (540) exceeded 
[org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:881), org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:651), org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:243), org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:202), org.apache.commons.math.linear.SingularValueDecompositionImpl.<init>(SingularValueDecompositionImpl.java:114),


RealMatrix m = [[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573]]

RealMatrix rcp = MatrixUtils.createRealMatrix(CP);	
SingularValueDecomposition svd = new SingularValueDecompositionImpl(rcp);		

RealMatrix U = svd.getU();
RealMatrix S = svd.getS();
RealMatrix Vt = svd.getVT();
double[] singularValues = svd.getSingularValues();",Windows 7 (32-bit)  JDK 1.6_16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2010-01-05 22:40:06.503,,,false,,,,,,,,,,,,,,34192,,,Wed Jul 20 12:18:41 UTC 2011,,,,,,0|i0rv5b:,160690,,,,,,,,"05/Jan/10 22:40;axelclk;Maybe that Math-320 isn't completely solved.
For the getU() method from the latest SVN sources I'm getting NaN values:

{code:java}
    @Test
    public void testMath327() {
      double[][] arr = {
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000,
              1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000,
              0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142,
              0.95238096, 1.00000000, 0.93333334, 0.96428573 },
          { 1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000,
              1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000,
              0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142,
              0.95238096, 1.00000000, 0.93333334, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000,
              1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000,
              0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142,
              0.95238096, 1.00000000, 0.93333334, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000,
              1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000,
              0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142,
              0.95238096, 1.00000000, 0.93333334, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000,
              1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000,
              0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142,
              0.95238096, 1.00000000, 0.93333334, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000,
              1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000,
              0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142,
              0.95238096, 1.00000000, 0.93333334, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000,
              1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000,
              0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142,
              0.95238096, 1.00000000, 0.93333334, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000,
              1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143,
              0.95238096, 1.00000000, 1.00000000, 0.96428573 },
          { 1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000,
              1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000,
              0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142,
              0.95238096, 1.00000000, 0.93333334, 0.96428573 },
          { 1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000,
              1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000,
              0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142,
              0.95238096, 1.00000000, 0.93333334, 0.96428573 } };

      RealMatrix rcp = new Array2DRowRealMatrix(arr);
      SingularValueDecomposition svd = new SingularValueDecompositionImpl(rcp);

      System.out.println(svd.getU());
      System.out.println(svd.getS());
      System.out.println(svd.getVT());
      
//      System.out.println(svd.getU().multiply(svd.getS()).multiply(svd.getVT()));
      double[] singularValues = svd.getSingularValues();
      System.err.println();
    }
{code}","05/Jan/10 23:21;luc;I got the NaN values too with the original values, but not the exception.

Could you try the latest version for subversion repository ?

It seems there are only a small number of different values in the initial matrix (about 14, depending on whether you consider 0.95000000 and 0.94999999 be the same 19.0/20.0 or not). The low print accuracy used by the default print leads to a problem: do we run the same test as the original one ? Could you provide the numbers to full precision ?

With the original low accuracy data, the matrix is singular : it has 4 non-null singular values only: 30.157767108847377, 0.27095453462617336, 6.218305311492343E-8 and 7.32410687763558E-15. I'm pretty sure the last one should really be zero, and not sure about the third one.
So the problem may be related to MATH-320, and as Axel noticed, U is still wrong in this case on the last column, which corresponds to the very small but non-zero last singular value.","28/Jan/10 20:17;luc;The matrix is really exactly rank 2, regardless of the exact values of the elements, as there are only two different rows.
These rows are at indices 0 and 4. All other rows are copies of these ones.
The issue may be related to numerical problems, which would explain why we get the spurious 6.218305311492343E-8 and 7.32410687763558E-15 singular values.","28/Sep/10 07:22;luc;Checking again this issue with current version in branch 2.X on subversion repository.
Using the original low accuracy values and making sure we simply have two different rows, I get the two largest singular values 30.157767108847384 and 0.2709545346259456, a large set of 0 values and a last non null values at 3.547702387229884E-7.
Reconstructing the matrix from its decomposition and subtracting the initial matrix gives a norm of 7.9e-7.

I suspect there are numerical problems here, enhanced by the large number of identical rows and initial low accuracy of the numbers in the issue report.
The current implementation of SVD uses directly A.At and At.A (where t is the transpose) and eigen decomposition. I think in this case lots of precision is lost. I would be happy with singular values and reconstruction errors at about 1.0e-10 or something like that, but a result larger than 1.0e-7 seems a real problem to me.","26/Dec/10 19:59;psteitz;Pushing out to 3.0, when we may need to reimplement SVD to deal with this and other numerical stability problems.","25/Mar/11 13:54;pkfeldman;Another unit test:
A [7 x 168] =
0.003000302	-0.005529946	0.01538155	-0.001403205	-0.018213096	0.000076204	-0.000023713	0.000093626	0.000143789	-0.000092728	-0.00009325	-0.000471574	0.000256266	-0.000235424	-0.000559821	-0.000181869	0.000070384	0.000005498	0.000383238	0.000187495	0.000744314	-0.000299571	0.000655884	-0.000367504	-0.000296129	0.001865159	0.002139813	0.001156764	-0.000274115	0.00194246	-0.001067201	-0.000965123	-0.000838439	-0.000233388	-0.000400203	0.000097493	0.000236697	-0.000612503	0.000248345	0.000058795	0.000121681	0.000441167	0.000328212	0.000333702	0.000739353	0.000205686	0.00011173	0.001217128	0.000488424	0.000382149	0.000308471	-0.000490277	0.000345624	-0.000834169	0.000216899	0.000433887	-0.000369019	0.000646984	-0.000300974	0.000373483	0.000647662	-0.000253848	0.000223624	0.000634534	0.000747524	-0.000077809	-0.000353221	0.000240388	0.000414233	0.000207479	0.000041459	-0.000305439	-0.000105042	0.00019835	0.000413267	0.000026096	-0.00046518	-0.000128547	-0.000043406	0.00005626	0.00000624	-0.000126197	-0.000020413	-0.000151383	-0.000024204	-0.000030944	0.000022925	-0.000064255	0.000240003	0.000262614	0.000044858	-0.000320061	-0.000030874	0.000131975	0.000079968	-0.000113183	0.000073552	-0.000008934	0.00004173	0.000052111	-0.000061729	0.000044233	0.00031579	0.000136546	0.000069494	0.000086781	-0.000040722	0.000040594	0.000105801	0.000037761	0.000010309	-0.000152556	0.000021932	0.000034306	0.000012486	-0.000005934	-0.000046878	0.000023209	0.000008168	0.000008117	-0.000016579	-0.00000505	0.000004528	-0.00000555	-0.000000095	-0.00000136	-0.000005205	-0.000002595	0.000000285	-0.000001563	-0.000000953	-0.000000364	-0.000001132	-0.000000645	-0.000000775	-0.000000144	-0.000000641	-0.00000031	-0.000000023	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0
0.002969656	-0.005304577	0.015172372	-0.001362703	-0.01832869	0.000054703	-0.000122829	-0.00003114	0.000041451	0.000025999	-0.000135931	-0.000186937	0.000239001	-0.000051824	-0.000383595	-0.00012191	-0.000021933	-0.000024977	0.000385048	0.000119015	0.000599602	-0.000358632	0.000726135	-0.000131523	-0.000116199	0.001494981	0.001850129	0.000724508	-0.000261562	0.001491656	-0.000748772	-0.000627043	-0.000597073	0.000000987	-0.000220861	-0.000078266	0.000303396	-0.000299759	0.000036112	0.000047285	0.000101568	0.000350496	0.000181929	0.000083279	0.00025771	0.000026141	-0.000010124	0.000382831	0.000080824	0.000021461	0.000143501	-0.000042618	0.000111455	-0.000339422	-0.000006761	0.000101824	-0.000082054	-0.000060832	-0.000124609	0.000138409	0.000029263	-0.000077077	0.000092533	-0.000011487	0.000023891	-0.000064685	0.000033676	0.000077876	0.000019453	0.000176876	-0.000211758	0.000035019	0.000043861	-0.000043324	0.000183336	-0.000029838	-0.000109527	0.000065992	-0.000221244	-0.000014366	-0.000032553	-0.000062429	0.00004144	0.000035076	0.000062257	0.000033082	0.000005334	0.000073079	-0.000159208	-0.000239372	-0.000056981	0.000281376	-0.000009754	-0.000031414	-0.000108677	0.000193121	-0.000082145	0.000036054	-0.000074536	-0.000005391	0.000091036	-0.00005061	-0.000360727	-0.000218139	-0.000109819	-0.000100537	0.000092986	-0.00010101	-0.000154437	-0.000115903	-0.000022312	0.000308967	-0.000020773	-0.000103365	-0.000013504	0.000011012	0.000171955	-0.000173067	-0.000026495	-0.000048153	0.000161432	0.000028197	-0.000036547	0.000067851	0.000013284	0.000040178	-0.000008893	0.000015076	-0.000015242	0.000013819	0.000003256	0.000000331	0.000014678	0.00000763	0.000002552	0.000004191	0.000004197	0.000000411	-0.000000075	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0
0.002887098	-0.005162996	0.014815235	-0.001285135	-0.017864999	0.00000749	-0.000086847	-0.000083018	0.000105148	0.000081389	-0.000091836	-0.000139615	0.000144603	0.000009173	-0.000390802	-0.000178339	-0.000026705	0.000019374	0.000339385	0.000106817	0.000534724	-0.000295593	0.000630123	-0.00006547	-0.000158208	0.001252922	0.001515506	0.000467301	-0.000238628	0.001181597	-0.000586928	-0.000454072	-0.000436001	0.00011785	-0.000120203	-0.000050288	0.000131089	-0.00013403	-0.000094773	0.000109751	0.000052716	0.000224419	0.000031617	-0.000005773	0.000039057	-0.000067576	-0.000073733	-0.000022599	-0.00008645	-0.000176968	-0.000001081	0.000065436	-0.000057159	0.000019626	-0.000076696	-0.000085582	0.000027081	-0.000237917	0.000027559	-0.000027321	-0.000138684	-0.000010139	0.000026879	-0.000280673	-0.000163613	-0.00006582	0.00008998	-0.000072257	-0.000129705	0.000144026	-0.000195254	0.000131956	0.000008668	-0.00004325	-0.000036238	-0.000014767	0.000003249	0.000103041	-0.000106909	-0.000013922	-0.000025123	0.000014109	0.000077397	0.000074812	0.000097137	0.000008743	-0.000015867	0.000078784	-0.000160333	-0.000220779	-0.000096102	0.000276578	0.000044677	-0.000166	-0.000113381	0.000126259	-0.000094119	0.000046776	0.000006033	-0.000019026	0.000047752	-0.000077061	-0.000245975	-0.000149312	-0.000033577	-0.000036015	0.000006292	-0.000016404	-0.000042226	0.000005486	-0.000030862	-0.000047629	-0.000008657	0.000011517	-0.00001011	-0.000010266	0.000024103	0.000041587	-0.000072901	0.00008128	-0.000182947	-0.000016937	0.000031871	-0.000074912	0.000004791	-0.000132392	0.00002209	-0.000042783	0.000027412	-0.000036629	-0.000013631	0.000006481	-0.000035375	-0.00002774	-0.000009908	-0.000015946	-0.000012614	-0.000001499	0.000000198	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0
0.002831307	-0.005036083	0.014495839	-0.001222631	-0.01743462	-0.000021877	-0.000089791	-0.00008631	0.000101769	0.000084557	-0.000093566	-0.000003847	0.000103268	0.000007839	-0.000342041	-0.000176391	-0.000013103	0.000043794	0.000266074	0.00011671	0.000488465	-0.00024847	0.000460824	-0.000028753	-0.00021999	0.001021342	0.001224627	0.000221835	-0.00029908	0.000917751	-0.000398617	-0.000299065	-0.000313377	0.000213555	-0.000069759	-0.000065952	0.000058733	-0.00002764	-0.000131624	0.000092763	0.000093374	0.000102217	-0.000039354	-0.000046958	-0.000102751	-0.000104948	-0.000110721	-0.000264731	-0.000183338	-0.000194462	-0.000097041	0.000096354	-0.000152521	0.000223908	-0.00014148	-0.00020514	0.000060268	-0.000291261	0.00009264	-0.000133008	-0.000194146	-0.000000404	0.000022513	-0.000420375	-0.000274199	-0.000004157	0.000118682	-0.000145176	-0.000187359	0.000118614	-0.000190098	0.000157552	-0.000000434	0.000005534	-0.000122158	-0.000035336	0.000091302	0.000070502	-0.000035075	0.000026897	-0.000026027	0.000068863	0.000030758	0.000118906	0.000052865	-0.000026329	-0.00000826	0.000101079	-0.000197935	-0.000140233	-0.000114663	0.000170086	0.000034608	-0.000173948	-0.000106214	0.000022543	-0.000095437	0.000028632	0.000052431	0.000005499	-0.000007454	-0.000044578	-0.000000059	0.000043577	0.000018142	0.000032244	-0.000026283	0.000068926	0.000042058	0.000008931	-0.000011774	-0.000111127	-0.000011769	0.000009934	0.000014911	-0.000079832	-0.000083897	0.000157805	0.000059901	-0.000004452	-0.000123879	-0.000055225	0.000008354	-0.000052262	-0.000087207	0.000107662	0.000039901	0.000035044	0.000016752	0.000021868	0.000026419	-0.000009502	0.000036966	0.000039395	0.000019642	0.000030458	0.000013378	0.000008617	-0.000000132	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0
0.002756109	-0.004887643	0.014196414	-0.00116362	-0.01703753	0.000004375	-0.000097905	-0.000069192	0.000084888	0.000113941	-0.000104082	0.000006669	0.000086462	-0.000044821	-0.000238952	-0.000135702	0.000022235	0.000037076	0.000177186	0.000152045	0.000381545	-0.000224624	0.000260298	-0.000069067	-0.000207512	0.000794191	0.00090326	0.000111248	-0.00026504	0.00064074	-0.000259072	-0.000186556	-0.000222422	0.000225984	-0.000048346	-0.000070009	0.000011544	0.000062158	-0.000140825	0.000057386	0.000083756	0.000017278	-0.000134431	-0.000146456	-0.000251551	-0.000135834	-0.000087733	-0.000452141	-0.000243808	-0.000136996	-0.000212436	0.000144639	-0.000265376	0.000421396	-0.000148677	-0.000262259	0.000043509	-0.000298101	0.000123116	-0.000182754	-0.000189525	0.000046441	-0.000032655	-0.000384988	-0.000406727	0.000091405	0.000189811	-0.000144187	-0.000117633	0.000105068	-0.000075964	0.000148894	0.00004975	0.000094222	-0.000219824	-0.000073299	0.000100216	0.000046446	-0.00002118	0.000072675	0.00004038	0.000129741	-0.000046565	0.00011456	-0.000035221	-0.0000787	0.000024227	0.000092911	-0.000123184	-0.000023855	-0.000078062	0.000047717	0.000046576	-0.000088641	-0.00002932	-0.000090071	-0.000041614	0.000003208	0.000036034	0.000008077	0.000009485	-0.000014586	0.000140279	0.000173089	0.000056949	0.000011312	-0.000016935	0.0000546	0.000071702	0.000054964	0.000038482	-0.000020307	0.000019496	-0.000076559	0.000037942	-0.000004126	-0.000097085	0.000158935	0.000084689	-0.000032025	0.000027741	-0.000001712	0.000047726	0.000067935	0.000062807	0.000042019	-0.000107553	0.000003783	-0.000068377	0.000033799	-0.000008086	-0.000014471	-0.000014571	-0.000012693	-0.000021009	-0.000043842	-0.000001391	-0.00002082	0.000000039	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0
0.002713439	-0.004752147	0.013965534	-0.001108472	-0.01672038	0.000009941	-0.000112401	-0.00006527	0.000078248	0.000152419	-0.000116667	0.000033126	0.000064384	-0.00007936	-0.000147899	-0.000068426	0.000033315	0.00002579	0.000120293	0.000215288	0.000323611	-0.000171217	0.000157969	-0.00005578	-0.00015296	0.000572407	0.000688925	0.000060766	-0.00020446	0.00048807	-0.00014862	-0.00009754	-0.000180141	0.000210196	-0.000036916	-0.000060428	0.00000925	0.000081098	-0.000136197	0.000013623	0.000080662	-0.000003224	-0.000165756	-0.00019215	-0.000308542	-0.000137013	-0.000061222	-0.000510196	-0.000248039	-0.000057964	-0.000224912	0.000125232	-0.000278473	0.000514189	-0.000154484	-0.000254711	-0.000009644	-0.000268256	0.000156188	-0.000188104	-0.000139626	0.000072911	-0.00008888	-0.000233338	-0.00046348	0.000144267	0.000185848	-0.000069423	-0.000046224	0.000076921	0.000062911	0.000085604	0.000134094	0.000122251	-0.000171917	-0.000080133	0.000124002	-0.000011948	-0.00006699	0.000122377	0.000084481	0.000108265	-0.000117191	0.000053756	-0.000096093	-0.000050405	0.000012092	0.000013554	-0.000033783	0.00004524	0.000030117	-0.000098499	0.000016659	0.000051857	0.000022419	-0.000134127	0.000007665	-0.000040693	-0.000026113	-0.000036273	0.000006864	0.000076699	0.000169231	0.000164427	0.000030649	0.000033798	-0.00002345	-0.00001429	0.000047296	0.00007971	0.000069608	0.000062877	0.000017838	-0.000017769	0.00002631	0.000071317	-0.000130968	0.000042289	0.000067647	-0.000064236	0.000128106	0.000063577	0.000000735	0.000037498	0.000047856	-0.000035523	0.000035664	0.000029723	0.000045477	-0.000025745	-0.000041613	0.000047659	-0.000017681	-0.000030828	0.00001873	0.000035826	-0.000004717	0.000033014	0.000000109	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0
0.002654198	-0.004617799	0.013738668	-0.00106814	-0.016441538	0.000020623	-0.000085095	-0.00006464	0.000053609	0.000186066	-0.000107783	0.000031834	0.000022983	-0.000074228	-0.000068779	-0.000039889	0.000029619	0.000041417	0.000044411	0.000255179	0.000198631	-0.000132924	0.000082586	-0.00006463	-0.000063918	0.000359364	0.000438507	0.00003992	-0.000101309	0.000328214	-0.000061309	0.000003358	-0.000096576	0.000163887	-0.000022349	-0.000050092	-0.000012869	0.000115162	-0.000111409	-0.000009761	0.000077735	0.000014956	-0.000206665	-0.000207496	-0.000314351	-0.000161785	-0.000017807	-0.000524724	-0.000276115	0.000019489	-0.000205149	0.00009544	-0.000241665	0.000578938	-0.000166919	-0.00022578	-0.000108395	-0.000231887	0.000217214	-0.000205621	-0.000083071	0.000108178	-0.000157203	0.000010994	-0.000467108	0.000166157	0.000150765	-0.000021731	0.00002444	0.000040855	0.000246309	-0.000001378	0.000270913	0.000055249	-0.000110662	-0.000061757	0.000080507	-0.000087269	-0.000080525	0.000103341	0.000076934	0.000031422	-0.000094885	0.000004692	-0.000073083	0.000050076	-0.000016419	-0.000076807	0.00008026	0.000072597	0.000083693	-0.00015694	0.00000627	0.00009672	0.000092051	-0.000114397	0.000057658	-0.000030369	-0.000037034	-0.000059939	-0.000030038	0.000095397	0.000100554	0.000056162	0.000021134	0.00002121	0.00001418	-0.000050626	0.00000043	0.000054517	0.000023516	0.000065812	0.00000895	0.000094756	-0.000010199	0.000131197	-0.00010293	-0.000061066	0.000003383	0.00000012	0.000065144	0.000028177	-0.000085212	-0.00000508	0.000002609	-0.000083369	0.0000428	-0.000049929	0.000005855	-0.000043563	0.000045236	-0.000031818	0.000035129	0.000036617	-0.000019773	0.000008113	0.000001782	-0.000039179	-0.000000243	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0

b [7 x 1] =
-0.1
-0.066666667
-0.033333333
0.000000001
0.008333333
0.016666667
0.025

The SVD solve in ACM 2.2 throws the MaxIterationsExceededError. 
In R, the solve works:
pinv <- with(svd(A), v %*% diag(1/d) %*% t(u))
x <- pinv %*% b
","25/Mar/11 14:09;luc;Thanks for the new test case. Could you run it again with an increased print accuracy and exponential format please ? Something like new DecimalFormat(""0.###############E00"") will be fine.","20/Jul/11 12:18;luc;Fixed in subversion repository as of r1148714.

This issue was fixed by changing SVD implementation according to issue MATH-611.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways),MATH-326,12444282,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,jake.mannix,jake.mannix,29/Dec/09 00:09,03/Apr/10 20:37,07/Apr/19 20:38,29/Dec/09 12:26,2.0,,,,,,,2.1,,,0,,,,,,,,"the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries.

The current implementation in ArrayRealVector has a typo:

{code}
    public double getLInfNorm() {
        double max = 0;
        for (double a : data) {
            max += Math.max(max, Math.abs(a));
        }
        return max;
    }
{code}

the += should just be an =.

There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness).

Worse, the implementation in OpenMapRealVector is not even positive semi-definite:

{code}   
    public double getLInfNorm() {
        double max = 0;
        Iterator iter = entries.iterator();
        while (iter.hasNext()) {
            iter.advance();
            max += iter.value();
        }
        return max;
    }
{code}

I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():

{code}
  public double getLInfNorm() {
    double norm = 0;
    Iterator<Entry> it = sparseIterator();
    Entry e;
    while(it.hasNext() && (e = it.next()) != null) {
      norm = Math.max(norm, Math.abs(e.getValue()));
    }
    return norm;
  }
{code}

Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-12-29 12:26:59.335,,,false,,,,,,,,,,,,,,34173,,,Tue Dec 29 12:26:59 UTC 2009,,,,,,0|i0rv5j:,160691,,,,,,,,"29/Dec/09 12:26;luc;Fixed in subversion repository as of r894367.
For consistency, the implementation of L1 and L2 norms have also been pushed upward in the abstract class.
Thanks for reporting and providing a fix to this bug",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"during ODE integration, the last event in a pair of very close event may not be detected",MATH-322,12442588,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,06/Dec/09 23:01,03/Apr/10 20:48,07/Apr/19 20:38,06/Dec/09 23:06,2.0,,,,,,,2.1,,,0,,,,,,,,"When an events follows a previous one very closely, it may be ignored. The occurrence of the bug depends on the side of the bracketing interval that was selected. For example consider a switching function that is increasing around first event around t = 90, reaches its maximum and is decreasing around the second event around t = 135. If an integration step spans from 67.5 and 112.5, the switching function values at start and end of step will  have opposite signs, so the first event will be detected. The solver will find the event really occurs at 90.0 and will therefore truncate the step at 90.0. The next step will start from where the first step ends, i.e. it will start at 90.0. Let's say this step spans from 90.0 to 153.0. The switching function switches once again in this step.

If the solver for the first event converged to a value slightly before 90.0 (say 89.9999999), then the switch will not be detected because g(89.9999999) and g(153.0) are both negative.

This bug was introduced as of r781157 (2009-06-02) when special handling of events very close to step start was added.",All,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,34217,,,Sun Dec 06 23:06:04 UTC 2009,,,,,,0|i0rv6f:,160695,,,,,,,,06/Dec/09 23:06;luc;fixed in subversion repository as of r887794,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NaN singular value from SVD,MATH-320,12440296,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,dieterv77,dieterv77,10/Nov/09 15:42,23/Jun/11 20:02,07/Apr/19 20:38,31/Dec/09 17:55,2.0,,,,,,,2.1,,,0,,,,,,,,"The following jython code
Start code

from org.apache.commons.math.linear import *
 
Alist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]]
 
A = Array2DRowRealMatrix(Alist)
 
decomp = SingularValueDecompositionImpl(A)
 
print decomp.getSingularValues()

End code

prints
array('d', [11.218599757513008, 0.3781791648535976, nan])
The last singular value should be something very close to 0 since the matrix
is rank deficient.  When i use the result from getSolver() to solve a system, i end 
up with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution.

Does this SVD implementation require that the matrix be full rank?  If so, then i would expect
an exception to be thrown from the constructor or one of the methods.


","Linux (Ubuntu 9.10) java version ""1.6.0_16""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-11-10 20:23:02.883,,,false,,,,,,,,,,,,,,34206,,,Thu Jun 23 20:02:56 UTC 2011,,,,,,0|i0rv6n:,160696,,,,,,,,"10/Nov/09 20:23;luc;This is a real new bug, thanks for reporting it.

Before I look more precisely at it, could you do a quick check for me ?

If at the end of the SingularValueDecompositionImpl constructor, around line 118 in the java source file you change from:
{noformat}
  singularValues[i] = Math.sqrt(singularValues[i]);
{noformat}
to
{noformat}
  singularValues[i] = Math.sqrt(Math.max(0, singularValues[i]));
{noformat}

does the problem still appear on singular values and does the solver work properly ?","11/Nov/09 13:37;dieterv77;Yes, making that change fixes the singular values, printing the singular values now gives

array('d', [11.218599757513008, 0.3781791648535976, 0.0])

The unittests for the project still pass as well.

However, now the solve fails with a SinularMatrixException

Traceback (most recent call last):
  File ""testdecomp.py"", line 14, in <module>
    soln = solver.solve([5.0, 6.0,7.0])
	at org.apache.commons.math.linear.SingularValueDecompositionImpl$Solver.solve(SingularValueDecompositionImpl.java:371)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)

org.apache.commons.math.linear.SingularMatrixException: org.apache.commons.math.linear.SingularMatrixException: matrix is singular

This confuses me, i guess i'm assuming incorrectly that if the solve method can solve in the least squares sense, then it should be
able to handle singular matrices.  Is that just a restriction on the current solve methods and if so, are there plans to relax that restriction?

thanks very much for your time","11/Nov/09 14:21;luc;The method should solve the problem in the least square sense.
The fact it does not do it is not a restriction, it's a bug.
I'll have a look at it","17/Dec/09 18:43;axelclk;
Is this a similar problem for the getU() method?

  public void testU() {
      double[][] testMatrix = {
          { 1.0 , 2.0 },
          { 1.0 , 2.0 } };
      SingularValueDecompositionImpl svd =
          new SingularValueDecompositionImpl(MatrixUtils.createRealMatrix(testMatrix));
// wrong result:
      assertEquals(""Array2DRowRealMatrix{{-0.7071067811865472,NaN},{-0.7071067811865475,NaN}}"", svd.getU().toString());
    }","17/Dec/09 21:21;luc;The two issues are probably related.
I'll look at both cases.","30/Dec/09 23:27;luc;A first round on fixing this bug has been committed in the subversion repository as of r894735.
Axel example is confirmed to be an occurrence of the same bug as Dieter example.

The SVD is now computed either as a compact SVD (only positive singular values considered) or as a truncated SVD
(max number of singular values to consider is user-specified). The solver simply applies the pseudo-inverse.

The fix is not considered complete yet because I think that the results provided by the solver are not really the ones
that give the smallest residuals. See for example the commented out parts of testMath320A in
SingularValueSolverTest.  Could you check this, please ?","31/Dec/09 11:57;axelclk;This statement should print the values of the original matrix approximately:
{code:java} 
  System.out.println(svd.getU().multiply(svd.getS()).multiply(svd.getVT()));
{code} 

This is true for
{code:java} 
    public void testMath320A() {
{code} 
but not for
{code:java} 
    public void testMath320B() {
{code} 

For reference values try wolfram alpha:
N[SingularValueDecomposition[{{1,2},{1,2}}]]","31/Dec/09 14:47;luc;Thanks for the hint Axel!
The print statement is even not satisfying for testMath320A, the approximation is really too bad. I would expect about 13 exact figures, not 1 or 2.
The problem seems to be related to matrix U which is not correct. In fact, it is even not unitary (i.e. U^T^.U is not the identity matrix).
I'll look at this.","31/Dec/09 17:55;luc;This should be fixed in subversion repository now (r894908).
Thanks for reporting the bug and sorry for the delay.","23/Jun/11 20:02;gsteri1;Did anyone notice that the 3rd eigenvalue is negative? On my box the eigenvalue is -2.1028862676867717E-14. I am not sure what the fix was, but whatever problems existed still persist. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong result in eigen decomposition,MATH-318,12440038,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,luc,luc,06/Nov/09 15:09,03/Apr/10 20:49,07/Apr/19 20:38,06/Nov/09 15:12,2.0,,,,,,,2.1,,,0,,,,,,,,"Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0
{code}
    public void testMathpbx02() {

        double[] mainTridiagonal = {
        	  7484.860960227216, 18405.28129035345, 13855.225609560746,
        	 10016.708722343366, 559.8117399576674, 6750.190788301587, 
        	    71.21428769782159
        };
        double[] secondaryTridiagonal = {
        	 -4175.088570476366,1975.7955858241994,5193.178422374075, 
        	  1995.286659169179,75.34535882933804,-234.0808002076056
        };

        // the reference values have been computed using routine DSTEMR
        // from the fortran library LAPACK version 3.2.1
        double[] refEigenValues = {
        		20654.744890306974412,16828.208208485466457,
        		6893.155912634994820,6757.083016675340332,
        		5887.799885688558788,64.309089923240379,
        		57.992628792736340
        };
        RealVector[] refEigenVectors = {
        		new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),
        		new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),
        		new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),
        		new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),
        		new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),
        		new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),
        		new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})
        };

        // the following line triggers the exception
        EigenDecomposition decomposition =
            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);

        double[] eigenValues = decomposition.getRealEigenvalues();
        for (int i = 0; i < refEigenValues.length; ++i) {
            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);
            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {
                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);
            } else {
                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);
            }
        }

    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,34177,,,Fri Nov 06 15:12:47 UTC 2009,,,,,,0|i0rv73:,160698,,,,,,,,"06/Nov/09 15:12;luc;fixed in subversion repository as of r833433.
Thanks again to Dimitri would found and fixed this bug.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nextExponential parameter check bug - patch supplied,MATH-309,12439175,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,mikl,mikl,27/Oct/09 13:20,03/Apr/10 20:52,07/Apr/19 20:38,31/Oct/09 02:33,1.0,1.1,1.2,2.0,,,,2.1,,,0,,,,,,,,"Index: src/main/java/org/apache/commons/math/random/RandomDataImpl.java
===================================================================
--- src/main/java/org/apache/commons/math/random/RandomDataImpl.java	(revision 830102)
+++ src/main/java/org/apache/commons/math/random/RandomDataImpl.java	(working copy)
@@ -462,7 +462,7 @@
      * @return the random Exponential value
      */
     public double nextExponential(double mean) {
-        if (mean < 0.0) {
+        if (mean <= 0.0) {
             throw MathRuntimeException.createIllegalArgumentException(
                   ""mean must be positive ({0})"", mean);
         }",Ubuntu 9.04,300,300,,0%,300,300,,,,,,,,,,,,,,27/Oct/09 13:21;mikl;patch_random_exp;https://issues.apache.org/jira/secure/attachment/12423317/patch_random_exp,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-31 02:33:15.66,,,false,,,,,,,,,,,,,,34214,,,Sat Oct 31 02:33:15 UTC 2009,,,,,,0|i0rv93:,160707,,,,,,,,31/Oct/09 02:33;psteitz;Fixed in r831510.  Thanks for reporting this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundException in EigenDecompositionImpl,MATH-308,12439042,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,25/Oct/09 22:25,14/Apr/10 00:02,07/Apr/19 20:38,03/Nov/09 22:19,2.0,,,,,,,2.1,,,0,,,,,,,,"The following test triggers an ArrayIndexOutOfBoundException:

{code:java}
    public void testMath308() {

        double[] mainTridiagonal = {
            22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437
        };
        double[] secondaryTridiagonal = {
            13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225
        };

        // the reference values have been computed using routine DSTEMR
        // from the fortran library LAPACK version 3.2.1
        double[] refEigenValues = {
            14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002
        };
        RealVector[] refEigenVectors = {
            new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),
            new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),
            new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),
            new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),
            new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })
        };

        // the following line triggers the exception
        EigenDecomposition decomposition =
            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);

        double[] eigenValues = decomposition.getRealEigenvalues();
        for (int i = 0; i < refEigenValues.length; ++i) {
            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);
            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {
                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);
            } else {
                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);
            }
        }

    }
{code}

Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:

{noformat}
java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545)
	at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072)
	at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894)
	at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658)
	at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246)
	at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205)
	at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)
{noformat}

I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.",linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,34207,,,Tue Nov 03 22:19:04 UTC 2009,,,,,,0|i0rv9b:,160708,,,,,,,,"03/Nov/09 22:19;luc;fixed in subversion repository as of r832577
Many thanks to Dimitri who debugged this with a careful step by step comparison between the original lapack fortran and our translation in Java.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BigReal/Fieldelement divide without setting a proper scale -> exception: no exact representable decimal result,MATH-307,12438999,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,joan,joan,24/Oct/09 14:36,14/Apr/10 00:03,07/Apr/19 20:38,24/Jan/10 10:35,2.0,,,,,,,2.1,,,0,,,,,,,,"BigReal implements the methode divide of Fieldelement. The problem is that there is no scale defined for the BigDecimal so the class will throw an error when the outcome is not a representable decimal result. 
(Exception: no exact representable decimal result)

The workaround for me was to copy the BigReal and set the scale and roundingMode the same as version 1.2.

Maybe is it possible to set the scale in FieldMatrix and implements it also a divide(BigReal b, int scale, int roundMode) ?? 

",independent,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-12-27 20:04:40.185,,,false,,,,,,,,,,,,,,150490,,,Sun Jan 24 10:35:39 UTC 2010,,,,,,0|i0rv9j:,160709,,,,,,,,"27/Dec/09 20:04;luc;I have added rounding mode and scale fields in BigReal in subversion tree as of r894107.
Could you please check if this solves the issue for you ?","27/Dec/09 21:00;joan;
Thank you for your reply and effort.

If this method is replaced for the implementation of scale and roundingMode it will certainly work:

old method:

  public BigReal divide(BigReal a) throws ArithmeticException {
       return new BigReal(d.divide(a.d));
   }

suggested method:

  public BigReal divide(BigReal a) throws ArithmeticException {
       return new BigReal(d.divide(a.d, scale, roundingMode));
   }

Joan ","27/Dec/09 21:00;joan;
Thank you for your reply and effort.

If this method is replaced for the implementation of scale and 
roundingMode it will certainly work:

   public BigReal divide(BigReal a) throws ArithmeticException {
        return new BigReal(d.divide(a.d));
    }

is replaced by:

   public BigReal divide(BigReal a) throws ArithmeticException {
        return new BigReal(d.divide(a.d, scale, roundingMode));
    }

Joan

",27/Dec/09 21:08;luc;Oops. Sorry for having forgotten this ... I've checked it in now.,24/Jan/10 10:35;luc;Fixed since 2009-12-27 as of r894109,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Method 'divide' in class 'Complex' uses a false formula for a special case resulting in erroneous division by zero.,MATH-306,12438998,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,bowman2001,bowman2001,24/Oct/09 14:23,14/Apr/10 00:05,07/Apr/19 20:38,27/Oct/09 01:33,1.1,1.2,2.0,,,,,2.1,,,0,,,,,,,,"The formula that 'divide' wants to implement is

( a + bi )  /  ( c + di )  =  ( ac + bd + ( bc - ad ) i )  /  ( c^2 + d^2 )

as correctly written in the description.

When c == 0.0 this leads to the special case

( a + bi )  /  di  = ( b / d ) - ( a / d ) i

But the corresponding code is:

if (c == 0.0) {
    return createComplex(imaginary/d, -real/c);
}

The bug is the last division -real/c, which should obviously be -real/d.",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-10-24 15:39:35.426,,,false,,,,,,,,,,,,,,150489,,,Tue Oct 27 01:33:42 UTC 2009,,,,,,0|i0rv9r:,160710,,,,,,,,24/Oct/09 14:31;bowman2001;Other layout and an additional formula conversion for better readability,"24/Oct/09 15:39;psteitz;Thanks for reviewing the code.

The code is misleading, but should not lead to incorrect results.  The line you refer to above is actually in a block that will never get executed - so should be deleted.
 {code}
        if (Math.abs(c) < Math.abs(d)) {
            if (d == 0.0) {  <---- impossible to have abs(c) < 0
                return createComplex(real/c, imaginary/c);
            }
            double q = c / d;
            double denominator = c * q + d;
            return createComplex((real * q + imaginary) / denominator,
                (imaginary * q - real) / denominator);
        } else {
            if (c == 0.0) {  <-- to get here, we need c = d = 0,  but this is handled above.
                return createComplex(imaginary/d, -real/c);  <-- incorrect fmla is harmless because never executed.
            }
{code}


Interesting that static analyzers did not catch this as dead code.  Unless I am missing something, both of the if(* == 0) tests in the block above can be removed (with no effect on results).

Appreciate comments on this, but leaning toward code cleanup and closing as invalid.
","24/Oct/09 16:15;bowman2001;Yes, it looks like a twofold check for an already solved problem. By splitting the code up for the two cases
1. Math.abs(c) < Math.abs(d) 
2. the other way around
the denominator of q is always non-zero, which would habe been a problem and may have been the reason to apply those extra checks.

The special cases c = 0 and d = 0 lead always to q = 0, in the respective code parts. 
Which is no problem, because there is no division by q. 
Effective code, if the extra checks are omitted.

Thank you Phil, I encourage the code cleanup you proposed.","24/Oct/09 17:10;psteitz;Thanks, and thanks again for reviewing the code.   We really appreciate that.",27/Oct/09 01:33;psteitz;Removed dead code in r830044.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in  KMeansPlusPlusClusterer unittest,MATH-305,12438781,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,erikvaningen,erikvaningen,22/Oct/09 06:35,30/Nov/09 08:12,07/Apr/19 20:38,27/Nov/09 21:46,2.0,,,,,,,2.1,,,0,,,,,,,,"When running this unittest, I am facing this NPE:
java.lang.NullPointerException
	at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91)

This is the unittest:


package org.fao.fisheries.chronicles.calcuation.cluster;

import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertTrue;

import java.util.Arrays;
import java.util.List;
import java.util.Random;

import org.apache.commons.math.stat.clustering.Cluster;
import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint;
import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer;
import org.fao.fisheries.chronicles.input.CsvImportProcess;
import org.fao.fisheries.chronicles.input.Top200Csv;
import org.junit.Test;

public class ClusterAnalysisTest {


	@Test
	public void testPerformClusterAnalysis2() {
		KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>(
				new Random(1746432956321l));
		EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] {
				new EuclideanIntegerPoint(new int[] { 1959, 325100 }),
				new EuclideanIntegerPoint(new int[] { 1960, 373200 }), };
		List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1);
		assertEquals(1, clusters.size());

	}

}
","java 6, eclipse, apache commons math trunk",14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-11-27 16:57:43.788,,,false,,,,,,,,,,,,,,34189,,,Mon Nov 30 08:12:50 UTC 2009,,,,,,0|i0rv9z:,160711,,,,,,,,27/Nov/09 16:57;psteitz;Thanks for reporting this. ,27/Nov/09 21:46;psteitz;The problem was due to overflow in MathUtils.distance() due to bad typing.  Fixed in r885027.,30/Nov/09 08:12;erikvaningen;I have tested the fix and I can confirm that it is working in my environment. Thanks a lot!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CurveFitter.fit(ParametricRealFunction, double[]) always returns the same value as the initial guess when used with the LevenbergMarquardtOptimizer",MATH-304,12438499,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,ddrummond,ddrummond,19/Oct/09 21:29,14/Apr/10 00:06,07/Apr/19 20:38,27/Dec/09 17:33,2.0,,,,,,,2.1,,,0,,,,,,,,"CurveFitter.fit(ParametricRealFunction, double[]) always returns the same value as the initial guess when used with the LevenbergMarquardtOptimizer and the length of the initial guess array is 1.  Here is my example code:

{code:title=CurveFitter with LevenbergMarquardtOptimizer|borderStyle=solid}
  LevenbergMarquardtOptimizer optimizer = new LevenbergMarquardtOptimizer();
  CurveFitter fitter = new CurveFitter(optimizer);
  fitter.addObservedPoint(2.805d, 0.6934785852953367d);
  fitter.addObservedPoint(2.74333333333333d, 0.6306772025518496d);
  fitter.addObservedPoint(1.655d, 0.9474675497289684);
  fitter.addObservedPoint(1.725d, 0.9013594835804194d);
  SimpleInverseFunction sif = new SimpleInverseFunction(); // Class provided below
  double[] initialguess = new double[1];
  initialguess[0] = 1.0d;
  double[] bestCoefficients = fitter.fit(sif, initialguess); // <---- ALWAYS RETURNS A VALUE OF initialguess !

    /**
     * This is my implementation of ParametricRealFunction
     * Implements y = ax^-1 + b for use with an Apache CurveFitter implementation
      */
    private class SimpleInverseFunction implements ParametricRealFunction
    {
        public double value(double x, double[] doubles) throws FunctionEvaluationException
        {
            //y = ax^-1 + b
            //""double[] must include at least 1 but not more than 2 coefficients.""
            if(doubles == null || doubles.length ==0 || doubles.length > 2) throw new FunctionEvaluationException(doubles);
            double a = doubles[0];
            double b = 0;
            if(doubles.length >= 2) b = doubles[1];
            return a * Math.pow(x, -1d) + b;
        }
        public double[] gradient(double x, double[] doubles) throws FunctionEvaluationException
        {
            //derivative: -ax^-2
            //""double[] must include at least 1 but not more than 2 coefficients.""
            if(doubles == null || doubles.length ==0 || doubles.length > 2) throw new FunctionEvaluationException(doubles);
            double a = doubles[0];
            double b = 0;
            if(doubles.length >= 2) b = doubles[1];
            double derivative = -a * Math.pow(x, -2d);
            double[]gradientVector = new double[1];
            gradientVector[0] = derivative;
            return gradientVector; 
        }
    }
{code} 
","Java, Ubuntu 9.04 (64 bit)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-12-27 17:33:58.715,,,false,,,,,,,,,,,,,,150488,,,Sun Dec 27 17:33:58 UTC 2009,,,,,,0|i0rva7:,160712,,,,,,,,"27/Dec/09 17:33;luc;The problem is not in the solver but in the implementation of the gradient method in your SimpleInverseFunction class. The value of the gradient is wrong. The gradient vector is computed with respect to the parameters (which is the reason why lengths must match), not with respect to the independent variable x. So for a function with one parameter p[0] / x, the gradient is { 1/x } and not { -p[0]/x^2 }.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CurveFitter.fit(ParametricRealFunction, double[]) used with LevenbergMarquardtOptimizer throws ArrayIndexOutOfBoundsException when double[] length > 1 (AbstractLeastSquaresOptimizer.java:187)",MATH-303,12438498,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Invalid,,ddrummond,ddrummond,19/Oct/09 21:20,14/Apr/10 00:07,07/Apr/19 20:38,27/Dec/09 17:19,2.0,,,,,,,2.1,,,0,,,,,,,,"CurveFitter.fit(ParametricRealFunction, double[]) throws ArrayIndexOutOfBoundsException at AbstractLeastSquaresOptimizer.java:187 when used with the  LevenbergMarquardtOptimizer  and the length of the initial guess array is greater than 1.  The code will run if the initialGuess array is of length 1, but then CurveFitter.fit() just returns the same value as the initialGuess array (I'll file this as a separate issue).  Here is my example code:
{code:title=CurveFitter with LevenbergMarquardtOptimizer and SimpleInverseFunction|borderStyle=solid}
  LevenbergMarquardtOptimizer optimizer = new LevenbergMarquardtOptimizer();
  CurveFitter fitter = new CurveFitter(optimizer);
  fitter.addObservedPoint(2.805d, 0.6934785852953367d);
  fitter.addObservedPoint(2.74333333333333d, 0.6306772025518496d);
  fitter.addObservedPoint(1.655d, 0.9474675497289684);
  fitter.addObservedPoint(1.725d, 0.9013594835804194d);
  SimpleInverseFunction sif = new SimpleInverseFunction(); // Class provided below
  double[] initialguess = new double[2];
  initialguess[0] = 1.0d;
  initialguess[1] = .5d;
  double[] bestCoefficients = fitter.fit(sif, initialguess); // <---- throws exception here

    /**
     * This is my implementation of ParametricRealFunction
     * Implements y = ax^-1 + b for use with an Apache CurveFitter implementation
      */
    private class SimpleInverseFunction implements ParametricRealFunction
    {
        public double value(double x, double[] doubles) throws FunctionEvaluationException
        {
            //y = ax^-1 + b
            //""double[] must include at least 1 but not more than 2 coefficients.""
            if(doubles == null || doubles.length ==0 || doubles.length > 2) throw new FunctionEvaluationException(doubles);
            double a = doubles[0];
            double b = 0;
            if(doubles.length >= 2) b = doubles[1];
            return a * Math.pow(x, -1d) + b;
        }
        public double[] gradient(double x, double[] doubles) throws FunctionEvaluationException
        {
            //derivative: -ax^-2
            //""double[] must include at least 1 but not more than 2 coefficients.""
            if(doubles == null || doubles.length ==0 || doubles.length > 2) throw new FunctionEvaluationException(doubles);
            double a = doubles[0];
            double b = 0;
            if(doubles.length >= 2) b = doubles[1];
            double derivative = -a * Math.pow(x, -2d);
            double[]gradientVector = new double[1];
            gradientVector[0] = derivative;
            return gradientVector; 
        }
    }

{code} 

This is the resulting stack trace:

java.lang.ArrayIndexOutOfBoundsException: 1
	at org.apache.commons.math.optimization.general.AbstractLeastSquaresOptimizer.updateJacobian(AbstractLeastSquaresOptimizer.java:187)
	at org.apache.commons.math.optimization.general.LevenbergMarquardtOptimizer.doOptimize(LevenbergMarquardtOptimizer.java:241)
	at org.apache.commons.math.optimization.general.AbstractLeastSquaresOptimizer.optimize(AbstractLeastSquaresOptimizer.java:346)
	at org.apache.commons.math.optimization.fitting.CurveFitter.fit(CurveFitter.java:134)
	at com.yieldsoftware.analyticstest.tasks.ppcbidder.CurveFittingTest.testFitnessRankCurveIntercept(CurveFittingTest.java:181)","Java, Linux Ubuntu 9.04 (64 bit)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-12-27 17:19:25.907,,,false,,,,,,,,,,,,,,150487,,,Sun Dec 27 17:19:25 UTC 2009,,,,,,0|i0rvaf:,160713,,,,,,,,"27/Dec/09 17:19;luc;The problem is not in the solver but in the implementation of the gradient method in your SimpleInverseFunction class. The length of the returned array must match the length of the second argument to the method (which is called parameters in the interface and doubles in your class). In your implementation, the array always has length 1 since it is created by statement:
{code}
double[]gradientVector = new double[1];
{code}

Also note that the value of the gradient is wrong. The gradient vector is computed with respect to the parameters (which is the reason why lengths must match), not with respect to the independent variable x. So for a function with two parameters p[0] / x + p[1], the gradient is { 1/x, 1 } and not { -p[0]/x^2, 0 }.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bugs in Simplex Implementation,MATH-302,12438085,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,cwinter,cwinter,14/Oct/09 13:58,14/Apr/10 00:09,07/Apr/19 20:38,15/Oct/09 08:56,2.0,,,,,,,2.1,,,0,,,,,,,,"Simplex routine may return infeasible solution:
{code:title=Bug1.java|borderstyle=solid}
import java.util.ArrayList;
import org.apache.commons.math.linear.ArrayRealVector;
import org.apache.commons.math.optimization.GoalType;
import org.apache.commons.math.optimization.OptimizationException;
import org.apache.commons.math.optimization.linear.*;

public class Bug1 {
    
    public static void main(String[] args) throws OptimizationException {
        
        LinearObjectiveFunction c = new LinearObjectiveFunction(new double[7], 0.0d);
        
        ArrayList<LinearConstraint> cnsts = new ArrayList<LinearConstraint>(5);
        LinearConstraint cnst;
        cnst = new LinearConstraint(new double[] {1.00d, 1.00d, 0.00d, 0.00d, 0.0d, 0.00d, 0.00d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.00d, 1.00d, 1.00d, 1.0d, 0.00d, 0.00d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.00d, 0.00d, 0.00d, 0.0d, 1.00d, 1.00d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.54d, 0.00d, 0.34d, 0.00d, 0.0d, 0.12d, 0.00d}, Relationship.EQ, 0.54d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.54d, 0.00d, 0.34d, 0.0d, 0.00d, 0.12d}, Relationship.EQ, 0.34d);
        cnsts.add(cnst);
        System.out.println(""Constraints:"");
        for(LinearConstraint con : cnsts) {
            System.out.println(con.getCoefficients().toString() + "" "" + con.getRelationship() + "" "" + con.getValue());
        }
        
        SimplexSolver simplex = new SimplexSolver();
        double[] sol = simplex.optimize(c, cnsts, GoalType.MINIMIZE, true).getPointRef();
        System.out.println(""Solution:\n"" + new ArrayRealVector(sol));
        System.out.println(""Third constraint is violated!"");
    }
}
{code}

or may find no solution where some exist:
{code:title=Bug1.java|borderstyle=solid}
import java.util.ArrayList;
import org.apache.commons.math.linear.ArrayRealVector;
import org.apache.commons.math.optimization.GoalType;
import org.apache.commons.math.optimization.OptimizationException;
import org.apache.commons.math.optimization.linear.*;

public class Bug2 {
    
    public static void main(String[] args) throws OptimizationException {
        
        LinearObjectiveFunction c = new LinearObjectiveFunction(new double[13], 0.0d);
        
        ArrayList<LinearConstraint> cnsts = new ArrayList<LinearConstraint>(5);
        LinearConstraint cnst;
        cnst = new LinearConstraint(new double[] {1.00d, 1.00d, 1.0d, 0.00d, 0.00d, 0.00d, 0.0d, 0.0d, 0.0d, 0.0d, 0.00d, 0.00d, 0.0d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.00d, 0.0d, 1.00d, 1.00d, 1.00d, 1.0d, 0.0d, 0.0d, 0.0d, 0.00d, 0.00d, 0.0d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.00d, 0.0d, 0.00d, 0.00d, 0.00d, 0.0d, 1.0d, 1.0d, 1.0d, 0.00d, 0.00d, 0.0d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.00d, 0.0d, 0.00d, 0.00d, 0.00d, 0.0d, 0.0d, 0.0d, 0.0d, 1.00d, 1.00d, 1.0d}, Relationship.EQ, 1.0d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.54d, 0.00d, 0.0d, 0.32d, 0.00d, 0.00d, 0.0d, 0.1d, 0.0d, 0.0d, 0.02d, 0.00d, 0.0d}, Relationship.EQ, 0.54d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.54d, 0.0d, 0.00d, 0.32d, 0.00d, 0.0d, 0.0d, 0.1d, 0.0d, 0.00d, 0.02d, 0.0d}, Relationship.EQ, 0.32d);
        cnsts.add(cnst);
        cnst = new LinearConstraint(new double[] {0.00d, 0.00d, 0.0d, 0.00d, 0.00d, 0.32d, 0.0d, 0.0d, 0.0d, 0.0d, 0.00d, 0.00d, 0.0d}, Relationship.EQ, 0.1d);
        cnsts.add(cnst);
        System.out.println(""Constraints:"");
        for(LinearConstraint con : cnsts) {
            System.out.println(con.getCoefficients().toString() + "" "" + con.getRelationship() + "" "" + con.getValue());
        }
        
        System.out.println(""verifying a known solution:"");
        ArrayRealVector sol = new ArrayRealVector(new double[] {4.0d/9.0d, 5.0d/9.0d, 0.0d, 11.0d/16.0d, 0.0d, 5.0d/16.0d, 0.0d, 4.0d/5.0d, 0.0d, 1.0d/5.0d, 0.0d, 1.0d, 0.0d});
        System.out.println(""sol = "" + sol);
        for(LinearConstraint con : cnsts) {
            System.out.println(sol.dotProduct(con.getCoefficients()) + "" = "" + con.getValue());
        }
        
        SimplexSolver simplex = new SimplexSolver();
        double[] newsol = simplex.optimize(c, cnsts, GoalType.MINIMIZE, true).getPointRef();
        System.out.println(""Solution:\n"" + new ArrayRealVector(newsol));
    }
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-10-14 20:16:39.548,,,false,,,,,,,,,,,,,,34159,,,Thu Oct 15 08:56:56 UTC 2009,,,,,,0|i0rvan:,160714,,,,,,,,"14/Oct/09 20:16;bmccann;These are likely bugs that have already been fixed in the Subversion repository.  Can you try again with the latest version compiled from SVN?

I've verified that the first problem gives valid output with the SVN version:
Constraints:
{1; 1; 0; 0; 0; 0; 0} = 1.0
{0; 0; 1; 1; 1; 0; 0} = 1.0
{0; 0; 0; 0; 0; 1; 1} = 1.0
{0.54; 0; 0.34; 0; 0; 0.12; 0} = 0.54
{0; 0.54; 0; 0.34; 0; 0; 0.12} = 0.34
Solution:
{0.37; 0.63; 0.65; 0; 0.35; 1; 0}


I haven't completely checked the second one, but it appears to be correct as well with the SVN version:
Constraints:
{1; 1; 1; 0; 0; 0; 0; 0; 0; 0; 0; 0; 0} = 1.0
{0; 0; 0; 1; 1; 1; 1; 0; 0; 0; 0; 0; 0} = 1.0
{0; 0; 0; 0; 0; 0; 0; 1; 1; 1; 0; 0; 0} = 1.0
{0; 0; 0; 0; 0; 0; 0; 0; 0; 0; 1; 1; 1} = 1.0
{0.54; 0; 0; 0.32; 0; 0; 0; 0.1; 0; 0; 0.02; 0; 0} = 0.54
{0; 0.54; 0; 0; 0.32; 0; 0; 0; 0.1; 0; 0; 0.02; 0} = 0.32
{0; 0; 0; 0; 0; 0.32; 0; 0; 0; 0; 0; 0; 0} = 0.1
Solution:
[0.3703703703703704, 0.5925925925925926, 0.037037037037037035, 0.6875, 0.0, 0.3125, 0.0, 0.9999999999999999, 0.0, 0.0, 1.0, 0.0, 0.0]","15/Oct/09 07:41;cwinter;Alright. It works with the version from SVN.
Thanks.",15/Oct/09 08:56;luc;Already fixed in subversion repository by previous changes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Erf(z) should return 1.0 for z 'large' but  fails with a MaxIterationsExceededException for z > 26.0.,MATH-301,12437655,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,milac,milac,08/Oct/09 22:47,09/Dec/10 11:48,07/Apr/19 20:38,08/Mar/10 22:59,2.0,,,,,,,2.1,,,0,,,,,,,,"Erf(z) should return 1.0 for z 'large' but fails with a MaxIterationsExceededException for z > 26.0.

Sample code
-----------------

import org.apache.commons.math.MathException;
import org.apache.commons.math.special.Erf;

public class TestErf {

    public TestErf() {
    }

    public static void main(String[] args) {
        double z = Double.NEGATIVE_INFINITY;
        try {
            for(int i=0; i<100; i++) {
                z = i;
                System.out.println(""z = "" + z + ""  erf(z) = "" + Erf.erf(z));
            }

            System.out.flush();
        } catch (MathException mex) {
            System.out.println(""z failed = "" + z);
            mex.printStackTrace();
        }
    }
}

Output
---------

z = 0.0  erf(z) = 0.0
z = 1.0  erf(z) = 0.842700792949715
z = 2.0  erf(z) = 0.9953222650189528
z = 3.0  erf(z) = 0.9999779095030024
z = 4.0  erf(z) = 0.9999999845827416
z = 5.0  erf(z) = 0.9999999999984622
z = 6.0  erf(z) = 0.9999999999999997
z = 7.0  erf(z) = 1.000000000000001
z = 8.0  erf(z) = 0.9999999999999986
z = 9.0  erf(z) = 1.000000000000003
z = 10.0  erf(z) = 1.0000000000000115
z = 11.0  erf(z) = 1.0000000000000016
z = 12.0  erf(z) = 0.9999999999999941
z = 13.0  erf(z) = 0.9999999999999846
z = 14.0  erf(z) = 1.0000000000000024
z = 15.0  erf(z) = 0.9999999999999805
z = 16.0  erf(z) = 0.9999999999999988
z = 17.0  erf(z) = 0.9999999999999949
z = 18.0  erf(z) = 0.9999999999999907
z = 19.0  erf(z) = 0.9999999999999731
z = 20.0  erf(z) = 0.9999999999999862
z = 21.0  erf(z) = 0.9999999999999721
z = 22.0  erf(z) = 1.000000000000017
z = 23.0  erf(z) = 1.0000000000000577
z = 24.0  erf(z) = 1.000000000000054
z = 25.0  erf(z) = 1.0000000000000262
z = 26.0  erf(z) = 1.0000000000000735
z failed = 27.0
org.apache.commons.math.MaxIterationsExceededException: Maximal number of iterations (10,000) exceeded
        at org.apache.commons.math.special.Gamma.regularizedGammaP(Gamma.java:181)
        at org.apache.commons.math.special.Erf.erf(Erf.java:51)
        at org.fhcrc.math.minimization.TestErf.main(TestErf.java:23)",MacOS and Linux,,,,,,,,,,,,,,,,MATH-456,MATH-282,,,29/Oct/09 21:40;njawalkar;erf.txt;https://issues.apache.org/jira/secure/attachment/12423626/erf.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-10-29 21:39:58.0,,,false,,,,,,,,,,,,,,34208,,,Mon Mar 08 22:59:28 UTC 2010,,,,,,0|i0rvav:,160715,,,,,,,,"29/Oct/09 21:39;njawalkar;Hi,

Here's a patch that uses a different algorithm to calculate erf(x). It's adapted from ""Handbook of Mathematical Functions: with Formulas, Graphs, and Mathematical Tables, by Milton Abramowitz and Irene A. Stegun"".

It no longer throws a MathException if x is too large or small. I've added one test to check erf(x) for 20<=x<40, and all previous tests pass also.",15/Feb/10 18:41;psteitz;This issue and MATH-282 are both caused by problems handling extreme values in Gamma.,"20/Feb/10 15:14;psteitz;From commons-user, another example of inaccurate results for extreme erf values:

NormalDistribution normDist = new NormalDistributionImpl(40,1.5)
;
try{
System.out.println(""cummulative probability::
""+normDist.cumulativeProbability(0.908789));
}
catch(MathException e){
e.printStackTrace();
}

*Result:*
cummulative probability:: *-8.104628079763643E-15*",08/Mar/10 22:59;psteitz;Fixed in r920558.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver returns no feasible solution,MATH-299,12436311,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,davidwilcox,davidwilcox,22/Sep/09 14:43,14/Apr/10 00:11,07/Apr/19 20:38,24/Sep/09 22:36,2.0,,,,,,,2.1,,,0,,,,,,,,"I am trying to optimize this:

maximize: v

v <= a1*p1 + a2*p2 + a3*p3 + a4*p4
v <= b1*p1 + b2*p2 + b3*p3 + b4*p4
v <= c1*p1 + c2*p2 + c3*p3 + c4*p4
v <= d1*p1 + d2*p2 + d3*p3 + d4*p4

p1 + p2 + p3 + p4 = 1

where a1-d4 are constant specified below by the code (i didn't want to copy and paste them up here. you can look below to see what they are in the objective function). 

		LinearObjectiveFunction f = new LinearObjectiveFunction(		
				new double[] { 1,
						0, 
						0, 0, 0}, 0 );
		Collection<LinearConstraint>  constraints = new ArrayList<LinearConstraint> ();


		constraints.add(new LinearConstraint(new double[] { -1, 
				1.7316145027890766, 
				 1.3584341412980305,
				 0.9305633063383639,
				 1.687117394945513
		},
		Relationship.GEQ, 0));

		constraints.add(new LinearConstraint(new double[] { -1, 
				0.6617060079461883, 
				 1.4862459822191323,
				 0.7692647272328988,
				 0.7329140944025636
		},
		Relationship.GEQ, 0));

		constraints.add(new LinearConstraint(new double[] { -1, 
				1.3255966888982322, 
				286.21607948837584,
				1.135907611434458,
				0.9803367440299271
		},
		Relationship.GEQ, 0));

		constraints.add(new LinearConstraint(new double[] { -1, 
				0.5428682596573682, 
				1.5745685116536952,
				1.4834419186882808,
				1.2884923232048968
		},
		Relationship.GEQ, 0));


		constraints.add(new LinearConstraint(new double[] {0, 1, 1, 1, 1},
				Relationship.EQ, 1));
		RealPointValuePair solution = null;
		try {
		
			solution = new SimplexSolver().optimize(f, constraints,
					GoalType.MAXIMIZE, true);
		}
		catch (OptimizationException e) {
			e.printStackTrace();
		}

I get this error back from the SimplexSolver.

org.apache.commons.math.optimization.linear.NoFeasibleSolutionException: no feasible solution
	at org.apache.commons.math.optimization.linear.SimplexSolver.solvePhase1(SimplexSolver.java:177)
	at org.apache.commons.math.optimization.linear.SimplexSolver.doOptimize(SimplexSolver.java:187)
	at org.apache.commons.math.optimization.linear.AbstractLinearOptimizer.optimize(AbstractLinearOptimizer.java:106)
	at Runner.main(Runner.java:101)

One interesting thing to note is that if you round all the numbers to the nearest 100's place, it works. If you keep it with the floating point precision shown here, it doesn't.
		",Windows XP commons Math 2.0 jre 1.6.0.16,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-09-22 18:43:43.038,,,false,,,,,,,,,,,,,,34188,,,Thu Sep 24 22:36:26 UTC 2009,,,,,,0|i0rvbb:,160717,,,,,,,,"22/Sep/09 18:43;luc;I have tried to solve your problem with lp_solve to check whether it find a solution. Here is what I have set up using lp_solve lp format:
{noformat:title=math-299.lp}
max: v;

-v + 1.7316145027890766 p1 +   1.3584341412980305 p2 + 0.9305633063383639 p3 + 1.687117394945513  p4 >= 0;
-v + 0.6617060079461883 p1 +   1.4862459822191323 p2 + 0.7692647272328988 p3 + 0.7329140944025636 p4 >= 0;
-v + 1.3255966888982322 p1 + 286.21607948837584   p2 + 1.135907611434458  p3 + 0.9803367440299271 p4 >= 0;
-v + 0.5428682596573682 p1 +   1.5745685116536952 p2 + 1.4834419186882808 p3 + 1.2884923232048968 p4 >= 0;

p1 + p2 + p3 + p4 = 1;

free p1, p2, p3, p4;

{noformat}

Running lp_solve on this file returns the following error message:
{noformat}
%  lp_solve < math-299.lp
This problem is unbounded
{noformat}

Could you check the constants and check if the problem really has a feasible finite solution ?","22/Sep/09 21:21;bmccann;We might be better than lp_solve on this one!  This worked fine for me and gave me the same answer as Lindo (1.398257)
David, did you use Commons Math 2.0?  The seemed to work fine for me when I used Commons Math compiled from the Subversion repository.  I'd recommend using the Subversion version because we've fixed numerous bugs that were present in the 2.0 release.

    @Test
    public void testMath299() throws OptimizationException {
        LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 1, 0, 0, 0, 0}, 0 );
        Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint> ();
        constraints.add(new LinearConstraint(new double[] { -1, 1.7316145027890766, 1.3584341412980305, 0.9305633063383639, 1.687117394945513 }, Relationship.GEQ, 0));
        constraints.add(new LinearConstraint(new double[] { -1, 0.6617060079461883, 1.4862459822191323, 0.7692647272328988, 0.7329140944025636 }, Relationship.GEQ, 0));
        constraints.add(new LinearConstraint(new double[] { -1, 1.3255966888982322, 286.21607948837584, 1.135907611434458, 0.9803367440299271 }, Relationship.GEQ, 0));
        constraints.add(new LinearConstraint(new double[] { -1, 0.5428682596573682, 1.5745685116536952, 1.4834419186882808, 1.2884923232048968 }, Relationship.GEQ, 0));
        constraints.add(new LinearConstraint(new double[] {0, 1, 1, 1, 1}, Relationship.EQ, 1));
        RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);
        Assert.assertEquals(1.398257, solution.getValue(), .0001);
    }
","24/Sep/09 18:15;luc;David, do you consider the version in the subversion repository fixes this bug ?
Can we change its state to resolved ?","24/Sep/09 19:01;davidwilcox;I changed to the subversion repository and it seems fixed.


",24/Sep/09 22:36;luc;already fixed in subversion by previous changes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EmpiriicalDisributionImpl.getUpperBounds does not return upper bounds on data bins,MATH-298,12436158,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,20/Sep/09 14:46,14/Apr/10 00:12,07/Apr/19 20:38,21/Sep/09 01:32,1.0,1.1,1.2,2.0,,,,2.1,,,0,,,,,,,,"Per the javadoc, the getUpperBounds method in the EmpiricalDistribution should return upper bounds for the bins used in computing the empirical distribution and the bin statistics.  What the method actually returns is the upper bounds of the subintervals of [0,1] used in generating data following the empirical distribution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150485,,,Mon Sep 21 01:32:21 UTC 2009,,,,,,0|i0rvbj:,160718,,,,,,,,21/Sep/09 01:32;psteitz;Fixed in r817128.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Eigenvector computation incorrectly returning vectors of NaNs,MATH-297,12436157,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,20/Sep/09 14:33,14/Apr/10 00:16,07/Apr/19 20:38,25/Jan/10 17:58,2.0,,,,,,,2.1,,,0,,,,,,,,"As reported by Axel Kramer on commons-dev, the following test case succeeds, but should fail:

{code}
public void testEigenDecomposition() {
    double[][] m = { { 0.0, 1.0, -1.0 }, { 1.0, 1.0, 0.0 }, { -1.0,0.0, 1.0 } };
    RealMatrix rm = new Array2DRowRealMatrix(m);
    assertEquals(rm.toString(),
        ""Array2DRowRealMatrix{{0.0,1.0,-1.0},{1.0,1.0,0.0},{-1.0,0.0,1.0}}"");
    EigenDecompositionImpl ed = new EigenDecompositionImpl(rm,
        MathUtils.SAFE_MIN);
    RealVector rv0 = ed.getEigenvector(0);
    assertEquals(rv0.toString(), ""{(NaN); (NaN); (NaN)}"");
  }
{code}

ed.getRealEigenvalues() returns the correct eigenvalues (2, 1, -1), but all three eigenvectors contain only NaNs.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-10-18 23:33:05.76,,,false,,,,,,,,,,,,,,34221,,,Mon Jan 25 17:58:04 UTC 2010,,,,,,0|i0rvbr:,160719,,,,,,,,"18/Oct/09 23:33;billbarker;This is now fixed as of R826550.  Unfortunately, I lack karma to resolve this issue, so will have to hope that somebody else will resolve it for me.","19/Oct/09 18:14;axelclk;If I'm expanding my testcase to the snippet below, I'm  now getting an eigenvector with all ""negative values"" at index 1.
I think this should be avoided. 
See also the solution computed by Ted Dunning on the mailing list: 
http://www.mail-archive.com/dev@commons.apache.org/msg12038.html

    double[][] m = { { 0.0, 1.0, -1.0 }, { 1.0, 1.0, 0.0 }, { -1.0, 0.0, 1.0 } };
    RealMatrix rm = new Array2DRowRealMatrix(m);
    assertEquals(rm.toString(),
        ""Array2DRowRealMatrix{{0.0,1.0,-1.0},{1.0,1.0,0.0},{-1.0,0.0,1.0}}"");
    EigenDecompositionImpl ed = new EigenDecompositionImpl(rm,
        MathUtils.SAFE_MIN);
    RealVector rv0 = ed.getEigenvector(0);
    RealVector rv1 = ed.getEigenvector(1);
    RealVector rv2 = ed.getEigenvector(2);
    assertEquals(rv0.toString(), ""{0,58; 0,58; -0,58}"");
    assertEquals(rv1.toString(), ""{-0; -0,71; -0,71}"");
    assertEquals(rv2.toString(), ""{0,82; -0,41; 0,41}"");
","27/Oct/09 03:14;bjohnson;There still seems to be a problem with the decomposition of some matrices.  For example, the decomposition of the identity matrix {noformat} {{1,0},{0,1}}{noformat}
 yields the correct eigenvalues, but NaN for all the eigenvector elements.
Crucially, the ""isIncludedColumn"" in the EigenDecompositionImplTest.java file always returns true (at least on my system) when the calculated eigenvectors have NaN elements, so is useless as a test for this problem.

Also, I discovered this problem when getting NaN values doing an SVD of certain matrices (where each row has only one non-zero value).  Since the SVD algorithm uses the EigenDecompositionImpl code, this seems to be a result of this current  bug.  (And ironically, I just told my students that one reason people love the SVD is that it essentially never fails).





",27/Oct/09 08:43;luc;I will look at this issue one issu MATH-308 has been solved.,"27/Oct/09 11:48;bjohnson;Thanks.

The problem seems to be a divide by zero error with the variable diP1 in the following code, but this is probably as far as I'll get in debugging it:
{noformat}
   private void stationaryQuotientDifferenceWithShift(final double[] d, final double[] l,
                                                       final double lambda) {
        final int nM1 = d.length - 1;
        double si = -lambda;
        for (int i = 0, sixI = 0; i < nM1; ++i, sixI += 6) {
            final double di   = d[i];
            final double li   = l[i];
            final double diP1 = di + si;
            final double liP1 = li * di / diP1;
            work[sixI]        = si;
            work[sixI + 1]    = diP1;
            work[sixI + 2]    = liP1;
            si = li * liP1 * si - lambda;
        }
        work[6 * nM1 + 1] = d[nM1] + si;
        work[6 * nM1]     = si;
    }
{noformat}","27/Oct/09 18:39;bjohnson;..., and I note that what I presume to be the original Fortran code (in dlar1v.f of LAPACK) has several sections of code marked:

*        Runs a slower version of the above loop if a NaN is detected

Hope this helps in resolving the issue,

Bruce
","03/Nov/09 07:23;jake.mannix;Doing the most trivial fix possible (dividing by the SAFE_MIN if diPl == 0)  on the line which leads to NaN appears to correctly fix this for the particular case of decomposing diagonal matrices (at least for 2x2 and 3x3 cases I checked with a unit test). 

I'm not sure if this ""fixes"" the problem, because I'm not sure if I really grok the algorithm's implementation in this code.

We could see better if I knew how often this pops up (any matrix whose tri-diagonal form is actually diagonal?)...","03/Nov/09 07:42;jake.mannix;Ok, well sadly it's easy to find an example which *isn't* fixed by just removing that one divide-by-zero: 
{code} { {0, 1, 0}, {1, 0, 0}, {0, 0, 1} } {code} leads to perfectly reasonable eigenvalues (1, 1, -1), but NaN again rears its ugly head because findEigenVectors() assumes that, among other things, that the main diagonal does not start with a zero, and then divides by it.

Not sure what the proper solution is to this, but a non-shifted LDL^t decomposition is a lot easier to understand to me than the other place where the NaN pops up, so maybe I can figure this one out on the plane ride down to ApacheCon tomorrow.","29/Nov/09 21:27;luc;Another step towards the solution has been checked in as of r885268.
Just as Bruce noticed one month ago (thanks), there was an improvement in dstqds and dqds algorithms implemented in DLAR1V that are not in Dhillon's thesis.
The problem is still not completely solved as for example in dimension 3 the decomposition of identity leads to 3 times the same
vector (0, 0, 1) instead of giving (1, 0, 0), (0, 1, 0) and (0, 0, 1).","25/Jan/10 17:58;luc;The original issue as been solved 2009-11-29 as of r885268.
The remaining problem identified in the last comments has been moved in a separate JIRA issue: MATH-333",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoessInterpolator.smooth() not working correctly,MATH-296,12435875,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,romeop,romeop,16/Sep/09 21:14,14/Apr/10 00:18,07/Apr/19 20:38,26/Jan/10 21:54,2.0,,,,,,,2.1,,,0,,,,,,,,"I have been comparing LoessInterpolator.smooth output with the loessFit output from R (R-project.org, probably the most widely used loess implementation) and have had strangely different numbers. I have created a small set to test the difference and something seems to be wrong with the smooth method but I do no know what and I do not understand the code.
*Example 1*
|x-input: |1.5| 3.0| 6| 8| 12|13| 22| 24|28|31|
|y-input: |3.1|6.1|3.1|2.1|1.4|5.1|5.1|6.1|7.1|7.2|
|Output LoessInterpolator.smooth():|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|NaN|
|Output from loessFit() from R: |3.191178027520974|3.0407201231474037|2.7089538903778636|2.7450823274490297|4.388011000549519|4.60078952381848|5.2988217587114805|5.867536388457898|6.7797794777879705|7.444888598397342|

*Example 2 (same x-values, y-values just floored)*
|x-input: |1.5| 3.0| 6| 8| 12|13| 22| 24|28|31|
|y-input: |3|6|3|2|1|5|5|6|7|7|
|Output LoessInterpolator.smooth(): |3|6|3|2|0.9999999999999005|5.0000000000001705|5|5.999999999999972|7|6.999999999999967|
|Output from loessFit() from R: |3.091423927353068|2.9411521572524237|2.60967950675505|2.7421759322272248|4.382996912300442|4.646774316632562|5.225153658563424|5.768301917477015|6.637079139313073|7.270482144410326|

As you see the output is practically the replicated y-input.
At this point this funtionality is critical for us but I could not find any other suitable java-implementation. Help. Maybe this strange behaviour gives someone a clue?

",Java 1.6 on Vista,,,,,,,,,,,,,,,,,,,,26/Jan/10 14:27;jkff;MATH-296.2.patch;https://issues.apache.org/jira/secure/attachment/12431424/MATH-296.2.patch,18/Sep/09 20:31;luc;math-296-test.patch;https://issues.apache.org/jira/secure/attachment/12420070/math-296-test.patch,25/Sep/09 16:56;jkff;math-296.patch;https://issues.apache.org/jira/secure/attachment/12420572/math-296.patch,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-09-18 14:52:52.174,,,false,,,,,,,,,,,,,,34191,,,Tue Jan 26 21:54:45 UTC 2010,,,,,,0|i0rvbz:,160720,,,,,,,,"18/Sep/09 14:52;luc;What are your settings for bandwidth and robustness iterations ?
I have tried your examples and the first one doesn't lead to NaN with the default settings for me.","18/Sep/09 15:54;romeop;My settings are 0.3 and 4 (these are used as defaults in R). 
If I use the standard values I still get:
|3.100000000000003|6.099999999999999|3.0999999999999996|NaN|NaN|NaN|NaN|NaN|7.1|7.85|

Here is my test code for a quick look:

public void testLoessInterpolator() throws MathException{
		dataX = new double[10];
		dataX[0] = 1.5;
		dataX[1] = 3.0;
		dataX[2] = 6;
		dataX[3] = 8;
		dataX[4] = 12;		
		dataX[5] = 13;
		dataX[6] = 22;
		dataX[7] = 24;
		dataX[8] = 28;
		dataX[9] = 31;
		
		
		dataY = new double[10];
		dataY[0] = 3.1;
		dataY[1] = 6.1;
		dataY[2] = 3.1;
		dataY[3] = 2.1;
		dataY[4] = 1.4;		
		dataY[5] = 5.1;
		dataY[6] = 5.1;
		dataY[7] = 6.1;
		dataY[8] = 7.1;
		dataY[9] = 7.2;
		
		LoessInterpolator li = new LoessInterpolator();
		double[] ly = li.smooth(dataX, dataY);
		
		for (int i = 0; i < ly.length; i++) {
			System.out.println(ly[i]);
		}		
	}

Is there any obvious error which I do not see? Is it possible (although highely unlikely) that some java floating point operations are broken on Vista64 (which I am using)?
Can you show your results?","18/Sep/09 18:45;luc;OK. I reproduce exactly your results on a Linux computer with openJDK 1.6 on an AMD64 processor.
I'll have a look at this issue.","18/Sep/09 20:31;luc;The math-296-test.patch is a set of two test cases that reproduce the problem.

Running the first test shows that at the end of iteration 0, the weights for points at indices 4 and 5 are set to 0. At iteration 1, when computing point i=4, the bandwidth is such that ileft=3 and iright=4. For k=3, we get w=0 because dist=4 and denom=1/4. For k=4 and k=5, we get w=0 because robustness[k]=0. So all w are 0, sumWeights=0 and since it is used in some divisions, NaN appears.

I think their should be some protection against this case somewhere. I'll ask the author of the original contribution about this.","22/Sep/09 07:49;jkff;Thanks for reporting the problem, Romeo. Actually, I did not compare the results of my implementation with those of R's: I purely implemented the algorithm described in the paper, and checked that the results are sensible. Unfortunately I will most probably not have time to debug the issue until weekend, but I can provide explanations to you about the code in case the issue is so critical for you that you urge to fix it yourself :)","24/Sep/09 18:22;romeop;Eugene, that is great news. I already tried to fix it but did not get very far. 
First of all, please ignore the sample values that are supposed to be from loessFit in the example above, they are not. I have reviewed my code and I can not reproduce those values. Sorry for that. When I try the examples again I get hardly any changes except for the NaN values (perhaps due to the fact that the values are not very close together?). The testcases should be deleted/changed to reflect this  I have compiled a more appropriate testcase:

|xval|0.1|0.2|0.3|0.4|0.5|0.6|0.7|0.8|0.9|1.0|1.1|1.2|1.3|1.4|1.5|1.6|1.7|1.8|1.9|2.0|
|yval|0.47|0.48|0.55|0.56|-0.08|-0.04|-0.07|-0.07|-0.56|-0.46|-0.56|-0.52|-3.03|-3.08|-3.09|-3.04|3.54|3.46|3.36|3.35|
|result|0.46076343001050907|0.49997135715509317|0.5429801876574236|0.3191000440696382|0.17824135321363765|-0.03999999999999973|-0.0699999999999999|-0.21301261447145203|-0.3260295715150817|-0.45999999999999996|-0.560000000000002|-1.4961752602921834|-2.1317826106042093|-3.0799999999999965|-3.089999999999987|-0.6229499113916326|0.9970283360414918|3.450011252797152|3.3907474604481567|3.336774229999947|
|loessFit|0.4609008580907899|0.4985267031275769|0.5408291873645029|0.308077939113221|0.1754074466314782|-0.0419929152622151|-0.07205683451999362|-0.19661654144034335|-0.31073415047540565|-0.44615854234427305|-0.5567051673879394|-1.4972970376332615|-2.1330233520442317|-3.08|-3.0900000000000043|-0.6208045742447532|0.9823842010251099|3.449395357814414|3.389793487936696|3.3359749840089385|
|weight(see below)|1.0|1.0|1.0|1.0|1.0|1.0|1.0|1.0|1.0|1.0|1.0|1.0|0.0|0.0|1.0|1.0|0.0|0.0|1.0|1.0|
|loessFit with weights|0.4780117093745368|0.4924876444111407|0.48398021996388496|0.3201446090891406|0.17880078999161195|-0.003305611690841502|-0.08829472923805592|-0.20902549447499932|-0.3267558674057103|-0.45542226638134214|-0.518369970805548|-0.5365908808969353|-1.492628917733731|-2.1151751796193703|-3.09|-3.0400000000000005|-2.9899999999999998|0.15500000000000005|1.7524999999999986|3.3499999999999996|


The results do not differ very much and there are no NaN, if the values are close together. Can this be explained by a slightly different algorithm in R?

For easy copy and paste:
double[] yval={0.47, 0.48, 0.55, 0.56,-0.08,-0.04,-0.07,-0.07,-0.56,-0.46,-0.56,-0.52,-3.03,-3.08,-3.09,-3.04, 3.54, 3.46, 3.36, 3.35};
double[] xval={0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0};
double[] weights = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1};
LoessInterpolator li = new LoessInterpolator(0.3,4);
result = li.smooth(xval, yval);


Another related thing. A very common use of the R-function is with weights (that is how we use it as well): 
*loessFit(yValues, xValues, weights)*
where weights are the values 0 or 1, I dont know if some are using weights between 0 and 1 
If a spot has weight 0 it still gets loessfitted but does not affect his neighbours. This way you can specifiy more relevant points and have the complete data set interpolate around these.

In your implementation you use robustnessWeights and set them initially to 1. Do I understand this correctly that these are actually the same weights as the ones mentioned above?
Would it be possible to add a function with a third parameter and have the weights provided by the user? I tried just passing them trough as a parameter instead of initializing with 1 but it did not do the trick.","25/Sep/09 16:56;jkff;Romeo,
Here is a patch that implements the weighted loess fit (the results agree reasonably with R's implementation).
Also, the patch fixes some numeric instability by introducing an 'accuracy' parameter.

Luc,
Could you please review the patch and probably apply it?","25/Sep/09 18:35;luc;solved in subversion repository as of r818942.
patch applied with slight changes
thanks for the report and thanks for the patch","27/Sep/09 14:12;romeop;Eugene, Luc, thanks. Great news. I have been testing this with some bigger real-life datasets (~800 points) and the results seem to be very close to R).
There is one more problem though but I think I have already found the fix. You can still get NaN-results. 
The reason is that sumWeights can become 0 and there are a couple of divisions with sumWeights. This multiplies to the degree that the complete result set is an array of NaN.
I think (but am not sure) that this happens if you have a long series of 0-weights which is bigger then the amount of points you get through the bandwidth.

I have just added the following lines after these sumWeights divisions:
after: *double meanXSquared = sumXSquared / sumWeights;*
                
                if(sumWeights==0){
                	meanX=0;
                	meanY=0;
                	meanXY=0;
                	meanXSquared=0;
                }
That did the trick for me and the results seem accurate. I will test more during the next week with even more datasets. I hope I do not make any fundamental thinking error.

If you think this is correct please add it to the code.","30/Sep/09 14:25;jkff;Romeo, I am not sure about whether this behavior should be considered correct. 
It essentially means that there is no data inside an approximation window at all, so why should we make the data appear to be zero? 
How about a variant ""express the approximation window in terms of the number of points with non-zero weights""?
","30/Sep/09 22:13;romeop;Sorry, maybe I was a little unclear in my explanation. 
Actually there is data, there are just no weights in the approximation window. Lets say we have 300 point and the first 100 are weighted 0, the others are weighted 1, they are still valid data points. Being weighted 0 does not mean the data point can be deleted, it just means it does not affect his neighbours in terms of normalization (that is how the concept of normalization weights was explained to me)
What happens currently is the first ones become NaN and their neighbours become NaN to the point that *all* points become NaN. This shouldn't happen.
But do not misunderstand me, I am just assuming that the reason is because of the bandwidth as my dataset has this structure, I am not really sure about it. 
I have done some debugging on the code but there are really a lot of loops so I could not pinpoint when and why exactly this is starting to happen as it is not right from the beginning.","01/Oct/09 15:00;jkff;Romeo, have I understood you correctly that you'd like the data points with zero weights to not influence the approximation *coefficients* at all, but to still have computed approximation *values* for them?","07/Oct/09 16:58;romeop;Yes. I think that is correct. No influence on the approximation coefficients but still getting normalized (getting approximation values). 

Here is an except from the userguide for the R-limma package, which uses the loessFit function for some operations:

bq.Any spot quality weights found in RG will be used in the normalization by default. This means for example that spots with zero weight (flagged out) will not influence the normalization of other spots. The use of spot quality weights will not however result in any spots being removed from the data object. Even spots with zero weight will be normalized and will appear in the output object, such spots will simply not have any influence on the other spots.

However I am not sure how to handle the bandwidth. Example: if you have a bandwidth of 0.3, how do you compute the relevant points?
1. Find the 30% in the complete set and the use only the weighted ones inside this set
2. Look at all weighted ones and use 30% of them

To me the first one sounds like the logical one but I am not sure.","20/Oct/09 17:45;jkff;Sorry for the delay again: I just forgot about the issue. Don't hesitate to ping me, I am not a very organized person.

Adding zero-weighted points *must not* change the result in non-zero-weighted points and the approximation coefficients.
Thus, zero-weighted points must not participate in computing the bandwidth, and generally in computing anything at all: that is the *second* approach.

However, seems like R takes the *first* one, because I've implemented the second one and results disagreed with R on the testcase above.

Are you OK with having a more logical implementation that disagrees with R? :)",23/Dec/09 01:19;psteitz;Eugene's reasoning sounds correct to me.  I am inclined to say we document carefully and resolve this issue.  Any objections?,"23/Dec/09 19:21;luc;I agree, but would like to know what R produces when a large (read larger than bandwidth) chunk of zero weights occurs.","25/Jan/10 18:07;luc;Coming back to this issue.
Eugene, as you said you implemented the second approach and people agree with your reasoning,
could you provide a patch using this second approach so we can apply it ?",26/Jan/10 14:27;jkff;Here is the patch.,"26/Jan/10 21:54;luc;second patch applied as of r903440
thanks for the patch",,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomDataImpl.nextPoisson fails for means in range 6.0 - 19.99,MATH-295,12435450,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,mcfall,mcfall,11/Sep/09 17:40,14/Apr/10 00:18,07/Apr/19 20:38,11/Sep/09 17:44,1.2,2.0,,,,,,,,,0,,,,,,,,"RandomDataImpl.nextPoission(double mean) fails frequently (but not every time) from calls with mean >= 6.0 and < 20.0
Below 6.0 and above 20 it seems fine, as far as I can tell by testing values at random.

When it fails, the exception is as follows  - this from calling nextPoisson(6.0)

org.apache.commons.math.MathRuntimeException$4: must have n >= 0 for n!, got n = -2
	at org.apache.commons.math.MathRuntimeException.createIllegalArgumentException(MathRuntimeException.java:282)
	at org.apache.commons.math.util.MathUtils.factorialLog(MathUtils.java:561)
	at org.apache.commons.math.random.RandomDataImpl.nextPoisson(RandomDataImpl.java:434)

ie it's calling MathUtils.factorialLog with a negative argument.

",Java 1.6 on Mac 0S X,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150484,,,Fri Sep 11 17:44:55 UTC 2009,,,,,,0|i0rvc7:,160721,,,,,,,,"11/Sep/09 17:43;mcfall;Sorry, duplicate of Math 294 (got a 'server down for maintenance' error message when raising 294 but obviously it did get stored)",11/Sep/09 17:44;mcfall;Duplicate of Math 294,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomDataImpl.nextPoisson fails for means in range 6.0 - 19.99,MATH-294,12435449,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mcfall,mcfall,11/Sep/09 17:34,14/Apr/10 00:20,07/Apr/19 20:38,12/Oct/09 02:08,1.2,2.0,,,,,,2.1,,,0,,,,,,,,"math.random.RandomDataImpl.nextPoisson(double mean) fails frequently (but not always) for values of mean between 6.0 and 19.99 inclusive. For values below 6.0 (where I see there is a branch in the logic) and above 20.0 it seems to be okay (though I've only randomly sampled the space and run a million trials for the values I've tried)

When it fails, the exception is as follows (this for a mean of 6.0)

org.apache.commons.math.MathRuntimeException$4: must have n >= 0 for n!, got n = -2
	at org.apache.commons.math.MathRuntimeException.createIllegalArgumentException(MathRuntimeException.java:282)
	at org.apache.commons.math.util.MathUtils.factorialLog(MathUtils.java:561)
	at org.apache.commons.math.random.RandomDataImpl.nextPoisson(RandomDataImpl.java:434) 

ie MathUtils.factorialLog is being called with a negative input

To reproduce:

    JDKRandomGenerator random = new JDKRandomGenerator();
    random.setSeed(123456);
    RandomData randomData = new RandomDataImpl(random);

    for (int i=0; i< 1000000; i++){
        randomData.nextPoisson(6.0);
    }
",Java 1.6 on mac osX,,,,,,,,,,,,,,,,,,,,13/Sep/09 14:45;luc;math-294.patch;https://issues.apache.org/jira/secure/attachment/12419421/math-294.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-09-13 14:45:08.761,,,false,,,,,,,,,,,,,,34180,,,Mon Oct 12 02:08:12 UTC 2009,,,,,,0|i0rvcf:,160722,,,,,,,,"13/Sep/09 14:45;luc;The failure seems to be related with a wrong detection of out of bound cases.
In the example given the failure occurs at iteration 125 on my computer (Open JDK 1.6, Linux, 64bits). At the beginning of the loop, the u = nextUniform(0.0, c) random drawing leads to a value u smaller than c1. So the first branch of the if is taken and another random drawing is done : z = nextGaussian(0.0, 1.0) which leads to a value for x far below mu (x = -8, mu = 2).

This is detected as w is set to positive infinity, but in fact this is not sufficient. The ""accept"" boolean is still computed despite it will always be false.

The attached patch is an attempt to compute ""accept"" only in some cases and to force it to ""false"" in other cases without computation.

I did *not* check this in subversion because I would like some other people to have a look at it. I am not sure it does work properly because when I compute for example the mean of 200, 100000, 1000000 or 10000000 calls to nextPoisson(6.0), I always get values between 14.7 and 15.2. I have seen the variance of a Poisson distribution is the same as its mean and so could expect a large value, but this still looked strange to me.","15/Sep/09 02:30;psteitz;Lets hold on the patch for now.  We need to understand why the rejection algorithm is failing.  Based on a quick review of the reference in the javadoc (p. 511, X.3), it looks like the following line in the u < c_1 case should not be subtracting 1.0 at the end.

y = -Math.abs(z) * Math.sqrt(mu) - 1.0;

I have not tested the effect dropping the -1 here as I am still working through the algorithm.","28/Sep/09 10:45;psteitz;In r819492, I committed a goodness of fit test (testNextPoissionConistency) for the values generated by nextPoisson.  Currently, it tests only values up to 5.  When this issue is resolved, the upper bound in the test needs to be increased.

I have not been able to rectify the current code so that it succeeds for all values and passes the test.  The implementation does not match the reference in the javadoc and the reference appears to contain errors.  If someone does not beat me to it or supply a patch that passes the test, I will find and implement a different version of the rejection algorithm for means > 6.",12/Oct/09 02:08;psteitz;Fixed in r824214,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Matrix's ""OutOfBoundException"" in SimplexSolver",MATH-293,12435216,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ciaccia,ciaccia,09/Sep/09 14:07,14/Apr/10 00:21,07/Apr/19 20:38,10/Sep/09 08:23,2.0,,,,,,,2.1,,,0,,,,,,,,"Hi all,
This bug is somehow related to incident MATH-286, but not necessarily...

Let's say I have an LP and I solve it using SimplexSolver. Then I create a second LP similar to the first one, but with ""stronger"" constraints. The second LP has the following properties:
* the only point in the feasible region for the second LP is the solution returned for the first LP
* the solution returned for the first LP is also the (only possible) solution to the second LP

This shows the problem:

{code:borderStyle=solid}
LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.4, 0.6}, 0 );
Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 30.0));
constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 30.0));
constraints.add(new LinearConstraint(new double[] { 0.8, 0.2, 0.0, 0.0, 0.0, 0.0 }, Relationship.GEQ, 10.0));
constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.7, 0.3, 0.0, 0.0 }, Relationship.GEQ, 10.0));
constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.0, 0.0, 0.4, 0.6 }, Relationship.GEQ, 10.0));

RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);

double valA = 0.8 * solution.getPoint()[0] + 0.2 * solution.getPoint()[1];
double valB = 0.7 * solution.getPoint()[2] + 0.3 * solution.getPoint()[3];
double valC = 0.4 * solution.getPoint()[4] + 0.6 * solution.getPoint()[5];

f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.4, 0.6}, 0 );
constraints = new ArrayList<LinearConstraint>();
constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 30.0));
constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 30.0));
constraints.add(new LinearConstraint(new double[] { 0.8, 0.2, 0.0, 0.0, 0.0, 0.0 }, Relationship.GEQ, valA));
constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.7, 0.3, 0.0, 0.0 }, Relationship.GEQ, valB));
constraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.0, 0.0, 0.4, 0.6 }, Relationship.GEQ, valC));

solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);
{code} 

Instead of returning the solution, SimplexSolver throws an Exception:

{noformat} Exception in thread ""main"" org.apache.commons.math.linear.MatrixIndexException: no entry at indices (0, 7) in a 6x7 matrix
	at org.apache.commons.math.linear.Array2DRowRealMatrix.getEntry(Array2DRowRealMatrix.java:356)
	at org.apache.commons.math.optimization.linear.SimplexTableau.getEntry(SimplexTableau.java:408)
	at org.apache.commons.math.optimization.linear.SimplexTableau.getBasicRow(SimplexTableau.java:258)
	at org.apache.commons.math.optimization.linear.SimplexTableau.getSolution(SimplexTableau.java:336)
	at org.apache.commons.math.optimization.linear.SimplexSolver.doOptimize(SimplexSolver.java:182)
	at org.apache.commons.math.optimization.linear.AbstractLinearOptimizer.optimize(AbstractLinearOptimizer.java:106){noformat} 

I was too optimistic with the bug MATH-286 ;-)",java 1.6 on Windows XP 32-Bit,,,,,,,,,,,,,,,,,,,,10/Sep/09 07:28;bmccann;SimplexSolverTest.patch;https://issues.apache.org/jira/secure/attachment/12419148/SimplexSolverTest.patch,10/Sep/09 07:28;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12419147/SimplexTableau.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-09-09 18:39:26.801,,,false,,,,,,,,,,,,,,34198,,,Thu Sep 10 08:23:11 UTC 2009,,,,,,0|i0rvcn:,160723,,,,,,,,"09/Sep/09 18:39;bmccann;Here's the standard LP description of the problem:
max .8 x0 + .2 x1 + .7 x2 + .3 x3 + .4 x4 + .6 x5
s.t.
x0 + x2 + x4 = 30
x1 + x3 + x5 = 30
.8 x0 + .2 x1 >= 10
.7 x2 + .3 x3 >= 10
.4 x4 + .6 x5 >= 10","09/Sep/09 19:00;ciaccia;Hi Ben,
Just to be clear, the first LP is solved correctly with the following solution:

Value: 40.57142857142857

x0: 15.714285714285714
x1: 0.0
x2: 14.285714285714286
x3: 0.0
x4: 0.0
x5: 30.0

Then I create new constraints that are satisfied by the solution here above:
c3: 0.8 * x0 + 0.2 * x1 >= 0.8 * 15.714285714285714 ( = 12.571428571428571)
c4: 0.7 * x2 + 0.3 * x3 >= 0.7 * 14.285714285714286 ( = 10.0)
c5: 0.4 * x4 + 0.6 * x5 >= 0.6 * 30.0 ( = 18.0)

Note that by its construction, the solution above satisfies the constraints c3, c4, and c5.

If I try to solve the new LP, a ""OutOfBoundException"" is thrown...","10/Sep/09 07:28;bmccann;Thanks for the bug report Andrea.  I was expecting this problem when I submitted the patch for MATH-286, but didn't have a good test case to work against and verify the fix.  This is largely what I was thinking of when I mentioned we should test some more, but didn't think you'd beat me to it in finding a good example to work off of :o)  When I changed the columns being dropped in the patch for MATH-286 it meant there wasn't always a way to always tell which variable each column of the tableau represented.  This patch creates a mapping between column index and variable label so that when we can read the final solution out of the tableau.  I feel much better about this now.  Thanks!","10/Sep/09 08:23;luc;patch applied in subversion repository as of r813301
thanks to Andrea and Ben",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestUtils.assertRelativelyEquals() generates misleading error on failure,MATH-292,12434957,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,06/Sep/09 22:04,14/Apr/10 00:25,07/Apr/19 20:38,09/Sep/09 10:12,2.0,,,,,,,2.1,,,0,,,,,,,,"TestUtils.assertRelativelyEquals() generates misleading error on failure.

For example:

TestUtils.assertRelativelyEquals(1.0, 0.10427661385154971, 1.0e-9)

generates the error message:

junit.framework.AssertionFailedError: expected:<0.0> but was:<0.8957233861484503>

which is not very helpful.",,,,,,,,,,,,,,,,,,,,,07/Sep/09 13:20;sebb@apache.org;MATH-292.patch;https://issues.apache.org/jira/secure/attachment/12418802/MATH-292.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-09-07 18:03:07.151,,,false,,,,,,,,,,,,,,150483,,,Wed Sep 09 10:12:01 UTC 2009,,,,,,0|i0rvcv:,160724,,,,,,,,07/Sep/09 13:20;sebb@apache.org;Possible fix which converts the relative error to an absolute error.,"07/Sep/09 18:03;luc;The patch seems good to me.
+1 to commit it","09/Sep/09 10:12;sebb@apache.org;URL: http://svn.apache.org/viewvc?rev=812871&view=rev
Log:
MATH-292 TestUtils.assertRelativelyEquals() generates misleading error on failure",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in SimplexTableau.initialize,MATH-290,12433932,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ciaccia,ciaccia,25/Aug/09 10:53,14/Apr/10 00:27,07/Apr/19 20:38,27/Aug/09 08:08,2.0,,,,,,,2.1,,,0,,,,,,,,"SimplexTableau throws a NullPointerException when no solution can be found instead of a NoFeasibleSolutionException

Here is the code that causes the NullPointerException:

LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 1, 5 }, 0 );
Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
constraints.add(new LinearConstraint(new double[] { 2, 0 }, Relationship.GEQ, -1.0));

RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MINIMIZE, true);

Note: Tested both with Apache Commons Math 2.0 release and SVN trunk",Java 1.6.0_13 on Windows XP 32-bit ,,,,,,,,,,,,,,,,,,,,25/Aug/09 22:28;bmccann;SimplexSolverTest.patch;https://issues.apache.org/jira/secure/attachment/12417671/SimplexSolverTest.patch,25/Aug/09 22:28;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12417670/SimplexTableau.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-08-25 20:43:17.23,,,false,,,,,,,,,,,,,,34153,,,Thu Aug 27 08:08:17 UTC 2009,,,,,,0|i0rvdb:,160726,,,,,,,,"25/Aug/09 20:43;bmccann;Thanks for the bug report.  I'm glad to be finding these bugs (hopefully the last of them :o)  That was the point in open sourcing this, so looks like that was a good decision!
Problem here was that when the constraint's right hand side is negative, we need to flip the constraint around so that it is positive.  We were doing that too late during the initialization and it should have been done at the very beginning, so that we only deal with the normalized version instead of both versions.","25/Aug/09 22:28;bmccann;The original patch is sufficient, but I'm attaching a new one that also cleans up some code formatting (weird spacing in SimplexTableau) and also provides a little bit better test.
Andrea, the correct solution for the example you provided is 0.  You restricted the variables to be non-negative and are trying to minimize the objective function, so the best you can do is make both variables 0 for an objective function value of 0.","26/Aug/09 06:35;ciaccia;Hi Ben, what you say in your last comment is not true. There is no feasible solution to my problem...
Maybe I should have expressed it in ""human"" readable format:

min: 1 x + 5 y;
r1: 2 x + 0 y <= -1;
x >= 0;
y >= 0;

x = 0 and y = 0 is not a valid solution since 2 * 0 + 0 * 0 (=0) is not <= -1...

The ""trick"" is that there will be never non-negative variables (in this case [x, y]) that multiplied with non-negative coefficients (here [2, 0]) will produce a negative result.

Do you agree?",26/Aug/09 15:49;bmccann;You originally wrote >= in the constraint and now you're writing <=.  Perhaps that's where the confusion is?  The answer is 0 when you use <=.,"26/Aug/09 15:56;ciaccia;Ops... I found the bug with the <= but then I transcribed it wrongly... Sorry.

Another thing I found that is not related to this bug, are the comments in the SimplexTableau. JavaDoc for divideRow was copied from substractRow without being changed ;-)","27/Aug/09 08:08;luc;fixed in subversion repository as of r808313
latest version of patch applied and two tests used (one with <= as the constraint and one with >= as the constraint)
thanks to Andrea for the report
thanks to Ben for the patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver not working as expected 2,MATH-288,12433883,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,kefa,kefa,24/Aug/09 22:31,14/Apr/10 00:30,07/Apr/19 20:38,25/Aug/09 18:10,2.1,,,,,,,2.1,,,0,,,,,,,,"SimplexSolver didn't find the optimal solution.

Program for Lpsolve:
=====================
/* Objective function */
max: 7 a 3 b;

/* Constraints */
R1: +3 a -5 c <= 0;
R2: +2 a -5 d <= 0;
R3: +2 b -5 c <= 0;
R4: +3 b -5 d <= 0;
R5: +3 a +2 b <= 5;
R6: +2 a +3 b <= 5;

/* Variable bounds */
a <= 1;
b <= 1;
=====================
Results(correct): a = 1, b = 1, value = 10


Program for SimplexSolve:
=====================
LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[]{7, 3, 0, 0}, 0);
Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>();
podmienky.add(new LinearConstraint(new double[]{1, 0, 0, 0}, Relationship.LEQ, 1));
podmienky.add(new LinearConstraint(new double[]{0, 1, 0, 0}, Relationship.LEQ, 1));
podmienky.add(new LinearConstraint(new double[]{3, 0, -5, 0}, Relationship.LEQ, 0));
podmienky.add(new LinearConstraint(new double[]{2, 0, 0, -5}, Relationship.LEQ, 0));
podmienky.add(new LinearConstraint(new double[]{0, 2, -5, 0}, Relationship.LEQ, 0));
podmienky.add(new LinearConstraint(new double[]{0, 3, 0, -5}, Relationship.LEQ, 0));
podmienky.add(new LinearConstraint(new double[]{3, 2, 0, 0}, Relationship.LEQ, 5));
podmienky.add(new LinearConstraint(new double[]{2, 3, 0, 0}, Relationship.LEQ, 5));
SimplexSolver solver = new SimplexSolver();
RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true);
=====================
Results(incorrect): a = 1, b = 0.5, value = 8.5

P.S. I used the latest software from the repository (including MATH-286 fix).",Java 1.6.0_13 on Windows XP 32-bit ,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,25/Aug/09 02:17;bmccann;SimplexSolver.patch;https://issues.apache.org/jira/secure/attachment/12417557/SimplexSolver.patch,25/Aug/09 02:17;bmccann;SimplexSolverTest.patch;https://issues.apache.org/jira/secure/attachment/12417558/SimplexSolverTest.patch,25/Aug/09 02:17;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12417559/SimplexTableau.patch,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-08-25 01:06:07.346,,,false,,,,,,,,,,,,,,34178,,,Tue Aug 25 18:10:08 UTC 2009,,,,,,0|i0rvdr:,160728,,,,,,,,"25/Aug/09 01:06;bmccann;Thanks for the bug report.  I've confirmed this is an issue.

Here's a slightly smaller version of the problem that causes the same bug, which might be easier for debugging:

MAX 7 a + 3 b
s.t.
3 a -5 c <= 0
2 a -5 d <= 0
3 b -5 d <= 0
a <= 1
b <= 1

        LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 7, 3, 0, 0 }, 0 );
        Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
        constraints.add(new LinearConstraint(new double[] { 3, 0, -5, 0 }, Relationship.LEQ, 0.0));
        constraints.add(new LinearConstraint(new double[] { 2, 0, 0, -5 }, Relationship.LEQ, 0.0));
        constraints.add(new LinearConstraint(new double[] { 0, 3, 0, -5 }, Relationship.LEQ, 0.0));
        constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0 }, Relationship.LEQ, 1.0));
        constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 0 }, Relationship.LEQ, 1.0));
    
        SimplexSolver solver = new SimplexSolver();
        RealPointValuePair solution = solver.optimize(f, constraints, GoalType.MAXIMIZE, true);
        assertEquals(10.0, solution.getValue(), .0000001);
","25/Aug/09 02:17;bmccann;Patch attached.  It was a 1 character bug.  I was saying to only do the minimum ratio test if the entry is >= 0, but it should have been > 0 (dividing by 0 is never good :o)
Thanks again for the bug report.","25/Aug/09 18:10;luc;resolved in subversion repository as of r807738
patch applied (except for debug print function)
thanks for the repoart and thanks for the patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimplexSolver not working as expected?,MATH-286,12433578,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ciaccia,ciaccia,20/Aug/09 15:04,14/Apr/10 00:30,07/Apr/19 20:38,10/Sep/09 08:22,2.0,,,,,,,2.1,,,0,,,,,,,,"I guess (but I could be wrong) that SimplexSolver does not always return the optimal solution, nor satisfies all the constraints...

Consider this LP:

max: 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3 + 0.6 x4 + 0.4 x5;
r1: x0 + x2 + x4 = 23.0;
r2: x1 + x3 + x5 = 23.0;
r3: x0 >= 10.0;
r4: x2 >= 8.0;
r5: x4 >= 5.0;

LPSolve returns 25.8, with x0 = 10.0, x1 = 0.0, x2 = 8.0, x3 = 0.0, x4 = 5.0, x5 = 23.0;

The same LP expressed in Apache commons math is:

LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.6, 0.4 }, 0 );
Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 23.0));
constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 23.0));
constraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0 }, Relationship.GEQ, 10.0));
constraints.add(new LinearConstraint(new double[] { 0, 0, 1, 0, 0, 0 }, Relationship.GEQ, 8.0));
constraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0 }, Relationship.GEQ, 5.0));

RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);

that returns 22.20, with x0 = 15.0, x1 = 23.0, x2 = 8.0, x3 = 0.0, x4 = 0.0, x5 = 0.0;

Is it possible SimplexSolver is buggy that way? The returned value is 22.20 instead of 25.8, and the last constraint (x4 >= 5.0) is not satisfied...

Am I using the interface wrongly?",Java 1.6.0_13 on  Windows XP 32-bit,,,,,,,,,,,,,,,,,,,,08/Sep/09 00:16;bmccann;SimplexSolver.patch;https://issues.apache.org/jira/secure/attachment/12418860/SimplexSolver.patch,09/Sep/09 12:08;ciaccia;SimplexSolverTest-Andrea.patch;https://issues.apache.org/jira/secure/attachment/12419049/SimplexSolverTest-Andrea.patch,08/Sep/09 00:16;bmccann;SimplexSolverTest.patch;https://issues.apache.org/jira/secure/attachment/12418861/SimplexSolverTest.patch,21/Aug/09 22:11;bmccann;SimplexSolverTest.patch;https://issues.apache.org/jira/secure/attachment/12417328/SimplexSolverTest.patch,08/Sep/09 00:16;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12418862/SimplexTableau.patch,21/Aug/09 22:11;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12417327/SimplexTableau.patch,09/Sep/09 01:54;bmccann;patch.zip;https://issues.apache.org/jira/secure/attachment/12419011/patch.zip,31/Aug/09 17:24;bmccann;simplex.txt;https://issues.apache.org/jira/secure/attachment/12418165/simplex.txt,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,2009-08-21 20:25:41.58,,,false,,,,,,,,,,,,,,34144,,,Thu Sep 10 08:22:11 UTC 2009,,,,,,0|i0rvdz:,160729,,,,,,,,21/Aug/09 20:25;bmccann;I have confirmed this is an issue.,"21/Aug/09 20:38;bmccann;Here's a smaller version of the problem that also fails:

max 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3
s.t. 
x0 + x2 = 23.0
x1 + x3 = 23.0

    @Test
    public void testMath268() throws OptimizationException {
      LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3 }, 0 );
      Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
      constraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0 }, Relationship.EQ, 23.0));
      constraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1 }, Relationship.EQ, 23.0));
      
      SimplexSolver solver = new SimplexSolver();
      RealPointValuePair solution = solver.optimize(f, constraints, GoalType.MINIMIZE, true);
      assertEquals(25.3, solution.getValue(), .0000001);
    }

","21/Aug/09 20:50;bmccann;And a yet smaller version that is failing:

max 0.2 x1 + 0.3 x3
s.t. 
x1 + x3 = 23.0

    @Test
    public void testMath268() throws OptimizationException {
      LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.2, 0.3 }, 0 );
      Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
      constraints.add(new LinearConstraint(new double[] { 1, 1 }, Relationship.EQ, 23.0));

      SimplexSolver solver = new SimplexSolver();
      RealPointValuePair solution = solver.optimize(f, constraints, GoalType.MINIMIZE, true);
      assertEquals(6.9, solution.getValue(), .0000001);
    }","21/Aug/09 21:06;ciaccia;Hi Ben,
Your test here above is wrong, the GoalType should be MAXIMIZE and not MINIMIZE...
The MINIMIZE returns 4.6 which is the expected value.","21/Aug/09 21:32;bmccann;Whoops, right you are.  Thanks for catching that.
This should be a better example for debugging:

max 0.2 x1 + 0.3 x3
s.t.
x1 + x3 = 23.0

    @Test
    public void testMath268() throws OptimizationException {
      LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.2, 0.3 }, 0 );
      Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
      constraints.add(new LinearConstraint(new double[] { 1, 1 }, Relationship.EQ, 23.0));

      RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);
      assertEquals(6.9, solution.getValue(), .0000001);
    }

","21/Aug/09 21:38;bmccann;The solution is being calculated correctly, but the wrong thing is being returned.  The bug is in SimplexTableau.getSolution()","21/Aug/09 22:11;bmccann;Here's the fix.
Note that the test case I posted in the comment was named dyslexically.  It should be math286 not math268.","21/Aug/09 23:09;luc;Fixed in subversion repository as of r806753.
Patch applied.
Thanks to Andrea for the report and thanks to Ben for the fix","28/Aug/09 14:12;ciaccia;Hi all,
Maybe I'm doing something wrong extracting the latest version from the SVN repository, but the original issue is still not working on my apache commons math working copy :-(

Here is what I did:
-I extracted the latest version from the SVN repository
-I executed the original problematic LP for this bug and it still fails. Anyway, the simplified version (but maybe incompatible) succeeds...

Could please anyone look at that?
Thanks
Andrea","31/Aug/09 16:41;bmccann;Yes, there is still an error with this example. The bug for the smaller example was fixed, but there's something still affecting the larger one. I haven't been able to figure it out yet, but here's another smaller example that is also failing:

    @Test
    public void testMath286Reopened() throws OptimizationException {
        LinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.7 }, 0 );
        Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();
        constraints.add(new LinearConstraint(new double[] { 1, 1 }, Relationship.LEQ, 18.0));
        constraints.add(new LinearConstraint(new double[] { 1, 0 }, Relationship.GEQ, 10.0));
        constraints.add(new LinearConstraint(new double[] { 0, 1 }, Relationship.GEQ, 8.0));

        SimplexSolver solver = new SimplexSolver();
        RealPointValuePair solution = solver.optimize(f, constraints, GoalType.MAXIMIZE, true);
        assertEquals(13.6, solution.getValue(), .0000001);
    } ",31/Aug/09 17:24;bmccann;Here are the corresponding Simplex tableaus.  Let me know if you spot the error.,"31/Aug/09 19:11;ciaccia;Hi all,
What I wrote before doesn't really make sense... 

Look at this LP simplex solver:

http://www.phpsimplex.com/simplex/page2.php?objetivo=max&x1=0.8&x2=0.7&restricciones=3&variables=2&l=en&x11=1&x12=1&desigualdad1=-1&y1=18&x21=1&x22=0&desigualdad2=1&y2=10&x31=0&x32=1&desigualdad3=1&y3=8&Submit=Continue

why after the first phase is over the coefficients for x1 and x2 are set to 0.8 and 0.7 while here they are 0? Couldn't be the SimplexTableau's discardArtificialVariables doesn't do everything correct?

Just guessing...","01/Sep/09 05:09;bmccann;If I remember correctly phpsimplex formed the tableau a bit differently.  Instead of creating the tableau from the beginning with two objective functions, they only use one at the beginning, and then insert the phase 2 objective function later.  As a result, the tableaus aren't directly comparable.
I emailed Professor Chinneck at Carleton University for some help because I retaught myself a lot of this using a draft of a book he's working on.  He suggested looking into ways to deal with degenerate solutions, so I'll see if I can read up on that a bit:
""You've hit a degeneracy (which means that there are multiple solutions with the same value of objective function). The problem came when, in your second tableau, you pivoted on the third row (corresponding to s1) instead of the last row (corresponding to a2). Understandable since the minimum ratio test came out tied.""
","01/Sep/09 21:39;bmccann;phpsimplex said that when there's a tie, they make the artificial variable leave the basis, which we're not doing and would solve the problem in this case:
http://www.phpsimplex.com/en/simplex_method_theory.htm#anomalous
i'll try putting together a patch, likely this weekend, to do that","08/Sep/09 00:16;bmccann;Happy Labor Day!  To celebrate, here's a new set of patches that adds the ability to deal with degeneracy in the SimplexTableau.
The test includes four tests: the two bugs in MATH-286, the bug from MATH-288, and the bug from MATH-290.  I think they should all go in to prevent a regression in the future.  They all run very quickly and there are a lot of edge cases in the Simplex algorithm, so I'd prefer to be safe.","08/Sep/09 08:42;luc;resolved in subversion repository as of r812390
Benjamin's patches from 2009-09-07 have been committed with very slight changes
(an == test between Integer instances replaced by a call to equals)
thanks for the patches","08/Sep/09 08:51;ciaccia;Hi all,
I'm still not convinced the original problem is correct... Could you please try?

I would add the original LP with 6 variables and 5 restrictions to the test cases, since I'm not completely sure the ""smaller examples"" address exactly the same problem I posted weeks ago.","08/Sep/09 18:24;bmccann;Wow, sorry, I'll take another look and make sure to test against the original problem this time as well.","08/Sep/09 22:35;bmccann;The problem still exists when we are dropping columns moving between phase 1 and phase 2.  I was pointed towards this book for possible solutions (p80-82):
http://www.amazon.com/Linear-Programming-Introduction-Operations-Engineering/dp/0387948333#reader","09/Sep/09 01:54;bmccann;Here's a patch that correctly solves the problem Andrea posted (sorry for not completely fixing the problem with my earlier patches.)  It's mostly refactoring to make the solution easier, which was to drop positive cost non-artificial variables and nonbasic artificial variables at the end of phase 1 instead of all artificial variables.
There are some practical problems with degeneracy and redundant constraints that were not obvious to me from the theory when I first started working on this.  We should really test this a bit more with constraints outnumbering variables and redundant constraints to really be comfortable that all the issues are addressed, but we're certainly pretty close now if not there yet.
Andrea, thanks for the reports and validating fixes (or lack there of).  Luc, thanks for quickly getting these into SVN and letting me know about the problems.","09/Sep/09 08:55;luc;The patch from 2009-09-08 has been applied in subversion repository as of r812831.

After the patch, an existing test (SimplexTableauTest.testdiscardArtificialVariables) failed, so I had to update the expected matrix to prevent this failure. *This should be validated* as I really don't know if the failure were expected after the change or if something wrong occurred. Ben, could you have a look at this ?

This time, I'm not marking the issue as resolved, Andrea I'll wait until you consider it solved by yourself.

Thanks to everybody for their time on this issue, I'm sure we are gradually improving things.","09/Sep/09 12:08;ciaccia;Hi Luc, hi Ben,
This time I can confirm Ben's patch fixes the original problem. :-)

If I find something else related to this bug (or not) I will let you know, but I'm pretty sure this time the SimplexSolver is much more robust it was a couple of weeks ago!

I ""improved"" the JUnit test case for this bug, I attach the patch to this message. There are 5 new assertions that check all the constraints are really satisfied.

Thanks again for all the commitment
Andrea

PS: I don't know if this is somehow related, but I found another bug MATH-293... Ben, please look at that as well.","09/Sep/09 12:25;luc;I applied Andrea's patch in subversion as of r812921, thanks for providing it.
I'll wait for Ben's acknowledgement about the changes I've made in SimplexTableauTest.testdiscardArtificialVariables before marking the issue as resolved.","09/Sep/09 16:38;bmccann;Thanks Luc.  You can close this issue.  A note, we might want to rename testdiscardArtificialVariables to testDropPhase1Objective to match the updated method name.  I overlooked this earlier.  I look at MATH-293.",10/Sep/09 08:22;luc;fixed now (we hope),,,,,,,,,,,,,,,,,,,,,
MultiDirectional optimzation loops forver if started at the correct solution,MATH-283,12432884,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Blocker,Fixed,luc,michael.nischt,michael.nischt,12/Aug/09 14:51,14/Apr/10 00:32,07/Apr/19 20:38,14/Aug/09 19:25,2.0,,,,,,,2.1,,,0,,,,,,,,"MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution.

see the attached test case (testMultiDirectionalCorrectStart) as an example.",none specific for the issue (it's a programming bug),86400,86400,,0%,86400,86400,,,,,,,,,,,,,,12/Aug/09 15:02;michael.nischt;MultiDirectionalCorrectStartTest.java;https://issues.apache.org/jira/secure/attachment/12416328/MultiDirectionalCorrectStartTest.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-08-14 19:25:15.696,,,false,,,,,,,,,,,,,,34193,,,Fri Aug 14 19:25:15 UTC 2009,,,,,,0|i0rven:,160732,,,,,,,,12/Aug/09 15:02;michael.nischt;the failing unit test,"14/Aug/09 19:25;luc;fixed in subversion repository as of r804328
thanks for the report and the fix suggestion",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChiSquaredDistributionImpl.cumulativeProbability > 1,MATH-282,12432549,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,akiezun,akiezun,07/Aug/09 18:24,13/Apr/10 10:26,07/Apr/19 20:38,08/Mar/10 23:00,1.0,1.1,1.2,2.0,,,,2.1,,,0,,,,,,,,"Calling 
new ChiSquaredDistributionImpl(1.0).cumulativeProbability(66.41528551683048)

returns 1.000000000000004, which is bogus (should never be > 1)",called from Scala code,,,,,,,,,,,,,,,,,,,,01/Mar/10 11:50;psteitz;distributions.patch;https://issues.apache.org/jira/secure/attachment/12437474/distributions.patch,08/Aug/09 16:05;luc;math-282.patch;https://issues.apache.org/jira/secure/attachment/12415937/math-282.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-08-08 16:05:08.945,,,false,,,,,,,,,,,,,,34161,,,Mon Mar 08 23:00:26 UTC 2010,,,,,,0|i0rvev:,160733,,,,,,,,"08/Aug/09 16:05;luc;The problem seems to be related to numerical accuracy.
The computation here involves lots of iterations in the Gamma.regularizedGammaP method (115 iterations on my computer) and the final expression involves several transcendant functions (exp, log and logGamma).

The proposed patch simply filters out the values exceeding 1.0 in the gamma distribution implementation which is used by Chi squared distribution.

I am not sure this is the best fix to this issue, so I won't commit it myself. I prefer someone else could verify it.","09/Aug/09 18:07;psteitz;I am working on getting a bound in terms of the arguments for the Gamma distribution similar to what we do for the Normal distribution, so we can return 1 when we know that the tail probability is going to be bounded above by a sufficiently small number - say, 10E-20.   ","10/Aug/09 01:20;psteitz;Unfortunately, from R and [1], it looks like the value in the issue report should be ~ 1 - 10^-13, and I don't like the idea of top-coding with such a tight bound. Need to look at the regularized gamma approximation.

[1] ""The Moment Bound Is Tighter Than Chernoff's Bound for Positive Tail Probabilities"", Thomas K Philips, Randolph Nelson,  American Statistician 1995 vol. 49 (May, 1995) pp. 175-178","11/Oct/09 21:53;psteitz;The problem in Gamma.regularizedGammaP reported here is also causing incorrect results from PoissonDistributionImpl#cumulativeProbability.  The (currently disabled) test case, testCumulativeProbabilitySpecial() in PoissonDistributionTest illustrates the problem.  For some (not all) large lambda and some (not all) x in (0.9 * lambda, lambda), NaN or zero cumulative probabilities are returned.","22/Feb/10 12:04;psteitz;I have narrowed this down to two issues in Gamma

*  choice of when to use the series expansion vs continued fraction in computing the regularized gamma functions - changing from (a >= 1.0 && x > a) to (x > a + 1) as criteria for when to use continued fraction reduces incidence of NaN values returned and improves accuracy for some arguments.
*  handling the case when the continued fraction diverges.  I am still working on convincing myself that divergence is expected in failing test cases, in which case, logic can be changed to catch the continued fraction divergence and fall back to the series approximation in that case.  This will require refactoring the regularizedGammaP and Q functions to encapsulate the series / fraction computation instead of calling one another based on argument test.
","01/Mar/10 11:50;psteitz;The attached patch resolves this issue as well as MATH-301 and the problems described above with the Poisson distribution.  The patch bundles quite a few changes, some of which are not strictly necessary to resolve these issues.  The following is a summary.

* BrentSolver has been changed to expose its configured absolute accuracy.   This solver is used by the default inverse cum implementation in AbstractContinuousDistribution and the hard-coded setting (1E-6) was limiting accuracy in inverse cumulative probability estimates.  AbstractContinuousDistribution was changed to allow distributions to set this value and NormalDistributionImpl was changed to set it to 1E-9 by default and allow users to configure it via a constructor argument.   If all are happy with this change, I will similarly change other distributions to override the new getSolverAbsoluteAccuracy method and add constructors to configure the value.

* AbstractContinuousDistribution and AbstractIntegerDistribution inverseCumulativeProbability methods have been modified to check for NaN values returned by cumulativeProbability and throw MathExceptions when this happens.

* The criteria for choosing between the Lanczos series and continued fraction expansion when computing regularized gamma functions has been changed to (x >= a + 1).  When using the series approximation (regularizedGammaP), divergence to infinity is checked and when this happens, 1 is returned. 

* When scaling continued fractions to (try to) avoid divergence to infinity, the larger of a and b is used as a scale factor and the attempt to scale is repeated up to 5 times, using successive powers of the scale factor.  

* The maximum number of iterations used in estimating cumulative probabilities for PoissonDistributionImpl has been decreased from Integer.MAX_VALUE to 10000000 and made configurable.

Review and comment much appreciated.  One thing that I would like improve is to get decent top-coding in place in terms of the arguments to the regularized gamma functions.  The Poisson inverse cum tests take a very long time now because for very large values of x, the continued fractions are taking a long time to converge.  This is needless computation, as the value returned is 1.  We should be able to analytically determine bounds here.","01/Mar/10 19:25;luc;This looks fine to me (congrats for the error messages translations).
I am a little puzzled by the MathException thrown in some methods to be caught in the same method and wrapped in a FunctionEvaluationException. Could the MathException be a FunctionEvaluationException from the start (even if other MathException can be thrown and need to be wrapped by themselves) ?","01/Mar/10 20:18;psteitz;Thanks, Luc! 

Here is the perhaps strange logic explaining the odd exception nesting that you pointed out.  When a cumulativeProbability function returns NaN in the context of estimating inverse cum, the immediate exception is really a bad-value-returned exception, not a FunctionEvaluationException - there is no exception encountered evaluating the function, it just returns a bad value - so I code it as MathException.  The exception that the caller gets is FunctionEvaluationException, because there is in fact an error evaluating the inverse cum.  Wrapped inside is the MathException with the message indicating that NaN was returned by a cum activation.   

I guess it comes down to how we view FunctionEvaluationException and in particular is it appropriate to throw when NaN is returned by a method that logically should not return NaNs.  Thinking some more about this, I think so, so I will change the patch to throw FunctionEvaluationException in the first incidence.

Thanks again for looking at this carefully.  I am glad I got the translations right - the one I was worried about was as much English as French - ""diverge to NaN"" makes me cringe a little ;)",08/Mar/10 23:00;psteitz;Fixed in r920558,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RealMatrix.subtract javadoc typo,MATH-281,12431966,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,ashuang,ashuang,01/Aug/09 17:45,07/Aug/09 09:19,07/Apr/19 20:38,01/Aug/09 19:03,,,,,,,,2.0,,,0,,,,,,,,"I think the javadoc for RealMatrix.subtract is wrong.  It says ""@return this + m"", which perhaps should be ""@return this - m""

Index: src/main/java/org/apache/commons/math/linear/RealMatrix.java
===================================================================
--- src/main/java/org/apache/commons/math/linear/RealMatrix.java	(revision 799902)
+++ src/main/java/org/apache/commons/math/linear/RealMatrix.java	(working copy)
@@ -60,7 +60,7 @@
      * Compute this minus m.
      *
      * @param m    matrix to be subtracted
-     * @return     this + m
+     * @return     this - m
      * @throws  IllegalArgumentException if m is not the same size as this
      */
     RealMatrix subtract(RealMatrix m) throws IllegalArgumentException;
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-08-01 19:03:00.963,,,false,,,,,,,,,,,,,,150479,,,Fri Aug 07 09:19:02 UTC 2009,,,,,,0|i0rvf3:,160734,,,,,,,,"01/Aug/09 19:03;luc;fixed in subversion repository as of r799906
thanks for the report",07/Aug/09 09:19;luc;closing resolved issue for 2.0 release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bug in inverseCumulativeProbability() for Normal Distribution,MATH-280,12429647,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,murkle,murkle,06/Jul/09 21:26,07/Aug/09 09:18,07/Apr/19 20:38,07/Jul/09 09:21,1.2,,,,,,,2.0,,,0,,,,,,,,"
 * @version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $
 */
public class NormalDistributionImpl extends AbstractContinuousDistribution 


 * @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $
 */
public abstract class AbstractContinuousDistribution


This code:

        	DistributionFactory factory = app.getDistributionFactory();
        	NormalDistribution normal = factory.createNormalDistribution(0,1);
        	double result = normal.inverseCumulativeProbability(0.9772498680518209);

gives the exception below. It should return (approx) 2.0000...

normal.inverseCumulativeProbability(0.977249868051820); works fine

These also give errors:
0.9986501019683698 (should return 3.0000...)
0.9999683287581673 (should return 4.0000...)

org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0
	at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103)
	at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145)


",Java 1.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-07-07 09:21:22.141,,,false,,,,,,,,,,,,,,34166,,,Fri Aug 07 09:18:40 UTC 2009,,,,,,0|i0rvfb:,160735,,,,,,,,"07/Jul/09 09:21;luc;fixed in subversion repository as of r791766
thanks for the report",07/Aug/09 09:18;luc;closing resolved issue for 2.0 release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultipleLinearRegression - test for minimum number of samples,MATH-279,12428442,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,mbjorkegren,mbjorkegren,20/Jun/09 07:13,07/Aug/09 09:18,07/Apr/19 20:38,05/Jul/09 13:31,2.0,,,,,,,2.0,,,0,,,,,,,,"It's currently possible to pass in so few rows (samples) that there isn't a row for each column (predictor).  Does this look like the right thing to do?

{code}
Index: AbstractMultipleLinearRegression.java
===================================================================
--- AbstractMultipleLinearRegression.java       (revision 786758)
+++ AbstractMultipleLinearRegression.java       (working copy)
@@ -91,6 +91,9 @@
                   ""dimension mismatch {0} != {1}"",
                   (x == null) ? 0 : x.length,
                   (y == null) ? 0 : y.length);
+        } else if (x[0].length > x.length){
+            throw MathRuntimeException.createIllegalArgumentException(
+                    ""not enough data ("" + x.length + "" rows) for this many predictors ("" + x[0].length + "" predictors)"");
         }
     }
 {code}

",,600,600,,0%,600,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-07-05 13:31:29.287,,,false,,,,,,,,,,,,,,150478,,,Fri Aug 07 09:18:13 UTC 2009,,,,,,0|i0rvfj:,160736,,,,,,,,"05/Jul/09 13:31;luc;fixed in subversion repository as of r791244
patch was only slightly modified to avoid a NPE
thanks for the patch",07/Aug/09 09:18;luc;closing resolved issue for 2.0 release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MANIFEST.MF contains incorrect ""Import-Package"" OSGi header (makes it unusable for OSGi)",MATH-275,12427239,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Won't Fix,,paulfield,paulfield,05/Jun/09 18:36,14/Jun/09 18:51,07/Apr/19 20:38,14/Jun/09 18:51,1.2,,,,,,,,,,0,,,,,,,,"The MANIFEST.MF file contains this OSGi header:

Import-Package: org.apache.commons.discovery.tools,
 org.apache.commons.math;version=1.2,
 org.apache.commons.math.analysis;version=1.2,
 org.apache.commons.math.complex;version=1.2,
...

This header defines what the bundle requires from the OSGi runtime and so it shouldn't include the packages defined in the bundle (i.e. all the org.apache.commons packages). I suspect it shouldn't include the org.apache.commons.discovery.tools package either as I can't see that math has a dependency on that (I could be wrong :-) ).

The presence of this header causes the Math library to be unusable as an OSGi bundle in Eclipse RCP development (and I suspect in any OSGI container).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-06-07 22:11:57.344,,,false,,,,,,,,,,,,,,150474,,,Sun Jun 14 18:51:25 UTC 2009,,,,,,0|i0rvgf:,160740,,,,,,,,"07/Jun/09 22:11;niallp;Commons uses the maven-bundle-plugin which is a wrapper for the Bnd tool and this is the default behaviour. This issue has been raised a few times before, I'm not an OSGi expert but we took advice from the Apache Felix project who maintain the maven plugin - in the words of Peter Kriens who is one of the leading OSGi experts and author of the Bnd tool:

""Bnd automatically imports all exports to allow substitutability. If  you do not do this, you create all kinds of standalone class spaces  and things will not work together. It is generally bad practice to  only export a package.""

http://commons.markmail.org/message/lgmj7srrxhld42tp
","07/Jun/09 22:20;niallp;Also Martin Oberhuber's response indicated the usage patterns with eclipse is different from other OSGi containers

http://commons.markmail.org/message/44llxa5oa4rtefa7

Seems like we can't satisfy all camps, so we chose to follow the advice from Apache Felix. I would only be in favour of changing what we do here in commons if the experts from Apache Felix changed their advice, so if you want to discuss this further it might be better to do it on the Felix mailing list. If you can persuade them to change their advice then I'm sure we would follow it.",14/Jun/09 18:51;psteitz;Agree with Niall's analysis.  We can reopen and change if and when guidance from Felix changes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testing for symmetric positive definite matrix in CholeskyDecomposition,MATH-274,12427143,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,rossetti,rossetti,04/Jun/09 19:24,07/Aug/09 09:16,07/Apr/19 20:38,04/Jun/09 20:11,2.0,,,,,,,2.0,,,0,,,,,,,,"I used this matrix:

        double[][] cv = {
            {0.40434286, 0.09376327, 0.30328980, 0.04909388},
            {0.09376327, 0.10400408, 0.07137959, 0.04762857},
            {0.30328980, 0.07137959, 0.30458776, 0.04882449},
            {0.04909388, 0.04762857, 0.04882449, 0.07543265}
        };

And it works fine, because it is symmetric positive definite

I tried this matrix:

        double[][] cv = {
            {0.40434286, -0.09376327, 0.30328980, 0.04909388},
            {-0.09376327, 0.10400408, 0.07137959, 0.04762857},
            {0.30328980, 0.07137959, 0.30458776, 0.04882449},
            {0.04909388, 0.04762857, 0.04882449, 0.07543265}
        };

And it should throw an exception but it does not.  I tested the matrix in R and R's cholesky decomposition method returns that the matrix is not symmetric positive definite.

Obviously your code is not catching this appropriately.

By the way (in my opinion) the use of exceptions to check these conditions is not the best design or use for exceptions.  If you are going to force the use to try and catch these exceptions at least provide methods  to test the conditions prior to the possibility of the exception.  

","Mac OS X, NetBeans",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-06-04 20:11:26.076,,,false,,,,,,,,,,,,,,34201,,,Fri Aug 07 09:16:16 UTC 2009,,,,,,0|i0rvgn:,160741,,,,,,,,"04/Jun/09 20:11;luc;fixed in subversion repository as of r 781845
Concerning the exception, it is not possible to check the matrix without trying to decompose it, so providing an external check would be a waste as it would already do almost everything. In fact, it was exactly the reason for the bug: the check was done too early on the raw matrix, not on the matrix after some changes have been made to its elements.
thanks for the report","04/Jun/09 20:57;rossetti;Luc,

Thanks for fixing the error.

You are missing my point.  You should provide the user the opportunity  
to check the condition, rather than having to rely on exceptions.  The  
client can make the decision whether it is a waste or not.

And, we will agree to disagree on how the library is using exceptions.

Regards,
Manuel



-----------------------------------------------------
Manuel D. Rossetti, Ph.D., P.E.
Associate Professor of Industrial Engineering
University of Arkansas
Department of Industrial Engineering
4207 Bell Engineering Center
Fayetteville, AR 72701
Phone: (479) 575-6756
Fax: (479) 575-8431
email: rossetti@uark.edu
www: www.uark.edu/~rossetti



",07/Aug/09 09:16;luc;closing resolved issue for 2.0 release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Basic variable is not found correctly in simplex tableau,MATH-273,12426998,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,bmccann,bmccann,03/Jun/09 05:11,07/Aug/09 09:15,07/Apr/19 20:38,03/Jun/09 09:07,2.0,,,,,,,2.0,,,0,,,,,,,,"The last patch to SimplexTableau caused an automated test suite I'm running at work to go down a new code path and uncover what is hopefully the last bug remaining in the Simplex code.
SimplexTableau was assuming an entry in the tableau had to be nonzero to indicate a basic variable, which is incorrect - the entry should have a value equal to 1.",,,,,,,,,,,,,,,,,,,,,03/Jun/09 05:12;bmccann;SimplexSolverTest.patch;https://issues.apache.org/jira/secure/attachment/12409728/SimplexSolverTest.patch,03/Jun/09 05:12;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12409727/SimplexTableau.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-06-03 09:07:06.044,,,false,,,,,,,,,,,,,,34190,,,Fri Aug 07 09:15:43 UTC 2009,,,,,,0|i0rvgv:,160742,,,,,,,,03/Jun/09 05:12;bmccann;Here's the patch.,"03/Jun/09 09:07;luc;fixed in subversion repository as of r781304
patch applied
thanks",07/Aug/09 09:15;luc;closing resolved issue for 2.0 release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simplex Solver arrives at incorrect solution,MATH-272,12426720,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,bmccann,bmccann,30/May/09 01:01,07/Aug/09 09:15,07/Apr/19 20:38,02/Jun/09 19:38,2.0,,,,,,,2.0,,,0,,,,,,,,I have reduced the problem reported to me down to a minimal test case which I will attach.,,,,,,,,,,,,,,,,,,,,,30/May/09 01:02;bmccann;SimplexSolvetTest.txt;https://issues.apache.org/jira/secure/attachment/12409441/SimplexSolvetTest.txt,30/May/09 03:12;bmccann;SimplexTableau.patch;https://issues.apache.org/jira/secure/attachment/12409449/SimplexTableau.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-06-02 19:38:46.461,,,false,,,,,,,,,,,,,,34175,,,Fri Aug 07 09:15:24 UTC 2009,,,,,,0|i0rvh3:,160743,,,,,,,,30/May/09 01:02;bmccann;Test case to be added to SimplexSolverTest.  It is currently failing and should be fixed.,"30/May/09 03:12;bmccann;Previously, there was a bug where we could set one of a number of variables equal to some value.  We were setting all the variables instead of choosing one.  When I patched that bug, I did it incorrectly.  This is a correct implementation, which causes the old bug and the attached test to both pass.","02/Jun/09 19:38;luc;fixed in subversion repository as of r781135
patch applied
thanks","02/Jun/09 19:53;bmccann;Thanks!



",07/Aug/09 09:15;luc;closing resolved issue for 2.0 release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need a digamma function,MATH-267,12426209,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tdunning,tdunning,23/May/09 23:40,07/Aug/09 09:15,07/Apr/19 20:38,25/May/09 13:22,,,,,,,,2.0,,,0,,,,,,,,"
A mahout summer of code student needed a digamma function with Apache permissions so I wrote one.  It includes test cases, correct copyright and seems to work well.  I don't know where in commons math this would go so I figured I would post this and ask for advice.
",,,,,,,,,,,,,,,,,,,,,24/May/09 04:37;tdunning;Adds_digamma_function.patch;https://issues.apache.org/jira/secure/attachment/12408900/Adds_digamma_function.patch,"24/May/09 06:39;tdunning;Adds_trigamma_function,_improves_comments.patch;https://issues.apache.org/jira/secure/attachment/12408901/Adds_trigamma_function%2C_improves_comments.patch",23/May/09 23:41;tdunning;digamma.tgz;https://issues.apache.org/jira/secure/attachment/12408892/digamma.tgz,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,2009-05-24 02:45:14.682,,,false,,,,,,,,,,,,,,150469,,,Fri Aug 07 09:15:01 UTC 2009,,,,,,0|i0rvi7:,160748,,,,,,,,23/May/09 23:41;tdunning;Source code for implementation.,"24/May/09 02:45;psteitz;Thanks!  Have a look at the o.a.c.m.special package,  There is a Gamma class there that could use this,",24/May/09 04:37;tdunning;Here is a patch that integrates the digamma function into the normal commons math structure.,"24/May/09 04:40;tdunning;
I have left this open rather than resolve it because I don't see a ""patch available"" state such as I am used to on Hadoop and related projects.

The patch is available and should be ready for committing (subject to committers approving, of course).",24/May/09 06:05;psteitz;Patch applied in r778091 with only minor changes. thanks!,"24/May/09 06:37;tdunning;The previous patch lost some comments from my original.

Also, I added a trigramma function.  ","24/May/09 06:39;tdunning;I improved the javadoc and added a trigamma function with associated tests.

This patch is relative to current trunk which already contains the digamma function that Phil just committed. 
","25/May/09 13:22;psteitz;Second patch applied.  I made a few small changes to the javadoc to match the style of [math].

Thanks!",07/Aug/09 09:15;luc;closing resolved issue for 2.0 release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent API in Frequency,MATH-260,12422791,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,sebb@apache.org,sebb@apache.org,14/Apr/09 13:38,14/Apr/10 00:39,07/Apr/19 20:38,30/Dec/09 20:12,2.0,,,,,,,2.1,,,0,,,,,,,,"The overloaded Frequency methods are not consistent in the parameter types that they handle.

addValue() has an Integer version which converts the parameter to a Long, and then calls addValue(Object).

The various getxxx() methods all handle Integer parameters as an Object.

Seems to me that it would be better to treat Integer consistently.

But perhaps there is a good reason for having an addValue(Integer) method but no getxxx(Integer) methods?
If so, then it would be helpful to document this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-15 01:14:36.248,,,false,,,,,,,,,,,,,,150462,,,Wed Dec 30 20:12:52 UTC 2009,,,,,,0|i0rvjr:,160755,,,,,,,,"15/Apr/09 01:14;psteitz;the getXxx methods handle Integer arguments by converting them to longs.  See, eg. the test case, testIntegerValues()","15/Apr/09 10:23;sebb@apache.org;Yes, I know that the getXXX(Object) methods handle Integer specially; but so does the addValue(Object) method - it has to, in case an Integer is passed as an Object.

But why is there an addValue(Integer) method? What does it achieve?
There are no corresponding getXXX(Integer) methods.","27/Dec/09 17:56;luc;Note: the addValue(Object) has been deprecated as of 2.0 and replaced by addValue(Comparable).
I think we could also deprecate or even remove addValue(Integer) since addValue(Comparable) is sufficient for all purposes. Removing the method would probably not harm any users if they can recompile their code (but it would harm if they cannot recompile, of course).","30/Dec/09 20:12;psteitz;addValue(Integer) has been deprecated, marked for removal in 3.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bugs in Frequency API,MATH-259,12422113,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,06/Apr/09 13:16,07/Aug/09 09:12,07/Apr/19 20:38,25/Apr/09 17:52,,,,,,,,,,,0,,,,,,,,"I think the existing Frequency API has some bugs in it.

The addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException.
In fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects.
This could be fixed by checking that the object is Comparable.

Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable.

The getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:

{code}
        final Object OBJ = new Object();
        f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below
        System.out.println(f.getCount(OBJ)); // 0
        System.out.println(f.getPct(OBJ)); // 0.0
{code}

Rather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object.
Also, it should make it easier to implement generics.

However, this would cause compilation failures for some programs that pass Object rather than Comparable to the class.
These would need recoding, but I think they would continue to run OK against the new API.

It would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object.
But is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.",,,,,,,,,,,,,,,,,MATH-261,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-15 01:20:49.95,,,false,,,,,,,,,,,,,,34203,,,Fri Aug 07 09:12:49 UTC 2009,,,,,,0|i0rvjz:,160756,,,,,,,,"15/Apr/09 01:20;psteitz;I am OK with adding a check and throwing illegalArgumentExeption if an object that does not implement Comparable is supplied to these methods (as indicated in the javadoc), but not keen on introducing the compatibility issue.","17/Apr/09 13:55;sebb@apache.org;See:

URL: http://svn.apache.org/viewvc?rev=765996&view=rev
Log:
MATH-259 - check for Comparable when adding values

I overlooked the change of Exception, so there's also:

URL: http://svn.apache.org/viewvc?rev=766003&view=rev
Log:
MATH-259 - throw IllegalArgument rather than ClassCast to better retain original behaviour

==

I added a new method

   public void addValue(Comparable<?> v)

which is called from

   public void addValue(Object v)

which I took the liberty of deprecating, so the compiler will warn users about non-Comparable objects.
Hope that's OK.

Note that it's still possible for mutually non-Comparable values to be added, because the code does not check comparisons both ways, it relies on HashMap to do so.

I.e. if B.compareTo(A) is OK, but A.compareTo(B) does not exist, then it is possible to add A, then B without any complaints.
This later causes a ClassCastException in some of the getXXX() methods.
However this is not a valid Comparable implementation, as they are supposed to be symmetric.",25/Apr/09 17:52;sebb@apache.org;API now tidied up as far as possible whilst still being compatible.,07/Aug/09 09:12;luc;closing resolved issue for 2.0 release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Frequency class Javadoc is ambiguous; need more tests,MATH-258,12422095,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,06/Apr/09 08:55,23/Apr/09 02:26,07/Apr/19 20:38,12/Apr/09 19:33,,,,,,,,2.0,,,0,,,,,,,,"[Extracted from mailing list so it does not get lost]

The Javadoc is ambiguous - can one add both int and char to the same instance of Frequency?

Depending on how one reads the class Javadoc, this should be allowed,
as one can compare ""int"" and ""char"" in the same way that one can
compare ""int"" and ""long"" - i.e. both are ""comparable"". Now ""int"" and
""char"" are not Comparable, but then neither are ""int"" and ""long"".

I think the Javadoc needs to make the intention clear, and then the
tests can be enhanced to enforce the documented behaviour.

At present, there's no check to show what happens when int and char are mixed.

There are other tests that should be added, to show what is expected when mixing different Comparable classes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-12 19:33:58.047,,,false,,,,,,,,,,,,,,150461,,,Sun Apr 12 19:33:58 UTC 2009,,,,,,0|i0rvk7:,160757,,,,,,,,12/Apr/09 19:33;psteitz;Fixed in r764316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FastFourierTransformer.MultiDimensionalComplexMatrix.get() not consistent with FastFourierTransformer.MultiDimensionalComplexMatrix.set(),MATH-257,12422072,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,sebb@apache.org,sebb@apache.org,05/Apr/09 17:35,14/Mar/10 01:42,07/Apr/19 20:38,05/Apr/09 18:43,,,,,,,,2.0,,,0,,,,,,,,"FastFourierTransformer.MultiDimensionalComplexMatrix.get() is not consistent with FastFourierTransformer.MultiDimensionalComplexMatrix.set().

The set() method does not allow a null parameter to get past the first check.

The get() method allows a null parameter provided dimensionSize.length <= 1.

This seems wrong, both because it is inconsistent and because it may allow an NPE later if dimensionSize.length == 1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-05 18:43:21.969,,,false,,,,,,,,,,,,,,150460,,,Sun Apr 05 18:43:21 UTC 2009,,,,,,0|i0rvkf:,160758,,,,,,,,"05/Apr/09 18:43;luc;fixed in subversion repository as of r762131.
I took the opportunity to also simplify the initialization of the dimensionSize array, using a two passes algorithm.
The inconsistency by itself was removed by having bot set and get be no-op for null vectors",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MessagesResources_fr has package-protected mutable static array; should be made private,MATH-255,12422055,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,sebb@apache.org,sebb@apache.org,04/Apr/09 21:28,23/Apr/09 02:26,07/Apr/19 20:38,05/Apr/09 14:37,,,,,,,,2.0,,,0,,,,,,,,"MessagesResources_fr has package-protected mutable static array; this should be made private.

It does not appear to be accessed outside the class, and anyway the getContents() method provides safe access to a clone of the array.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-05 14:37:08.184,,,false,,,,,,,,,,,,,,150458,,,Sun Apr 05 14:37:08 UTC 2009,,,,,,0|i0rvkv:,160760,,,,,,,,"05/Apr/09 14:37;luc;fixed in subversion repository as of r762096
thanks for the report",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnivariateRealSolverUtils.factory is a mutable static field which is not initialised safely.,MATH-254,12422054,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,sebb@apache.org,sebb@apache.org,04/Apr/09 21:24,23/Apr/09 02:26,07/Apr/19 20:38,05/Apr/09 15:18,,,,,,,,2.0,,,0,,,,,,,,"UnivariateRealSolverUtils.factory is a mutable static field which is not initialised safely.

Suggest using IODH (init on demand holder) idiom for this, else use synchronized getInstance().",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-05 15:18:18.775,,,false,,,,,,,,,,,,,,150457,,,Sun Apr 05 15:18:18 UTC 2009,,,,,,0|i0rvl3:,160761,,,,,,,,"05/Apr/09 15:18;luc;fixed in subversion repository as of r762102
thanks for the report and the suggestion. IODH is a pattern I really appreciate, it is neat and elegant.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MathRuntimeException and MathException are both thread-hostile.,MATH-253,12422053,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,sebb@apache.org,sebb@apache.org,04/Apr/09 21:22,23/Apr/09 02:26,07/Apr/19 20:38,05/Apr/09 15:37,,,,,,,,2.0,,,0,,,,,,,,"MathRuntimeException and MathException are both thread-hostile.

They have a mutable static field cachedResources which is used in performing translations.

If two threads use different locales - which is highly likely in the case of non-US locales, as some methods use Locale.US - then they may get corrupt or incorrect output.

If a cache is really desired, it could either be ThreadLocal, or volatile, but it that case the translate method needs to fetch the value once (and update it once if necessary).

As it stands, the method can check the resource, find it is OK, then fetch it again to use it, by which time it might have changed. Further, one thread may set the variable, and another thread may see a partially constructed Resources object (it's not final).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-04-05 15:37:33.354,,,false,,,,,,,,,,,,,,150456,,,Sun Apr 05 15:37:33 UTC 2009,,,,,,0|i0rvlb:,160762,,,,,,,,"05/Apr/09 15:37;luc;fixed in subversion repository as of r762107.
The cache has been removed completely, so the bundle is retrieved and used at each request.
Since this is only used when errors are triggered, there should not be any noticeable drop in performance.
thanks fo the report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fraction.comparTo returns 0 for some differente fractions,MATH-252,12421502,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,29/Mar/09 16:50,23/Apr/09 02:26,07/Apr/19 20:38,29/Mar/09 16:52,1.2,,,,,,,2.0,,,0,,,,,,,,"If two different fractions evaluate to the same double due to limited precision,
the compareTo methode returns 0 as if they were identical.

{code}
// value is roughly PI - 3.07e-18
Fraction pi1 = new Fraction(1068966896, 340262731);

// value is roughly PI + 1.936e-17
Fraction pi2 = new Fraction( 411557987, 131002976);

System.out.println(pi1.doubleValue() - pi2.doubleValue()); // exactly 0.0 due to limited IEEE754 precision
System.out.println(pi1.compareTo(pi2)); // display 0 instead of a negative value
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,34164,,,Sun Mar 29 16:52:34 UTC 2009,,,,,,0|i0rvlj:,160763,,,,,,,,29/Mar/09 16:52;luc;fixed in subversion as of r759725,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Website SVN instructions refer to nonexistent branch,MATH-244,12413112,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,jparker,jparker,23/Jan/09 17:28,23/Apr/09 02:26,07/Apr/19 20:38,23/Jan/09 19:43,,,,,,,,,,,0,,,,,,,,"On the Source Repository page on the Math website:
http://commons.apache.org/math/source-repository.html

The SVN checkout instructions refer to the MATH_2_0 branch, which doesn't exist. I substituted the trunk instead.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-23 19:43:43.872,,,false,,,,,,,,,,,,,,150448,,,Fri Jan 23 19:43:43 UTC 2009,,,,,,0|i0rvnb:,160771,,,,,,,,"23/Jan/09 19:43;luc;Fixed in subversion as of r737149.
The site has been regenerated, it should be put online in a few hours.
Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MathUtils.gcd(Integer.MIN_VALUE, 0) should throw an Exception instead of returning Integer.MIN_VALUE",MATH-243,12412941,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,chsemrau,chsemrau,21/Jan/09 21:15,23/Apr/09 02:26,07/Apr/19 20:38,21/Feb/09 13:55,,,,,,,,,,,0,,,,,,,,"The gcd method should throw an Exception for gcd(Integer.MIN_VALUE, 0), like for gcd(Integer.MIN_VALUE, Integer.MIN_VALUE). The method should only return nonnegative results.",,,,,,,,,,,,,,,,,,,,,21/Jan/09 21:50;chsemrau;gcdPatch.txt;https://issues.apache.org/jira/secure/attachment/12398432/gcdPatch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2009-02-21 13:55:19.546,,,false,,,,,,,,,,,,,,34210,,,Sat Feb 21 13:55:19 UTC 2009,,,,,,0|i0rvnj:,160772,,,,,,,,"21/Jan/09 21:50;chsemrau;Attached is a patch for gcd, and also for lcm, which failed for some special cases: lcm failed for (0,0) and for (Integer.MIN_VALUE, power of 2).

I also added Javadoc for special cases.","21/Feb/09 13:55;luc;Fixed in trunk as of r746511.
Thanks for the patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MathUtils.binomialCoefficient(n,k) fails for large results",MATH-241,12412642,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,chsemrau,chsemrau,16/Jan/09 23:34,23/Apr/09 02:26,07/Apr/19 20:38,19/Jan/09 23:53,2.0,,,,,,,2.0,,,0,,,,,,,,"Probably due to rounding errors, MathUtils.binomialCoefficient(n,k) fails for results near Long.MAX_VALUE.

The existence of failures can be demonstrated by testing the recursive property:

{noformat}
         assertEquals(MathUtils.binomialCoefficient(65,32) + MathUtils.binomialCoefficient(65,33),
                 MathUtils.binomialCoefficient(66,33));
{noformat}

Or by directly using the (externally calculated and hopefully correct) expected value:

{noformat}
         assertEquals(7219428434016265740L, MathUtils.binomialCoefficient(66,33));
{noformat}

I suggest a nonrecursive test implementation along the lines of

{code:title=MathUtilsTest.java|borderStyle=solid}
    /**
     * Exact implementation using BigInteger and the explicit formula
     * (n, k) == ((k-1)*...*n) / (1*...*(n-k))
     */
	public static long binomialCoefficient(int n, int k) {
		if (k == 0 || k == n)
			return 1;
		BigInteger result = BigInteger.ONE;
		for (int i = k + 1; i <= n; i++) {
			result = result.multiply(BigInteger.valueOf(i));
		}
		for (int i = 1; i <= n - k; i++) {
			result = result.divide(BigInteger.valueOf(i));
		}
		if (result.compareTo(BigInteger.valueOf(Long.MAX_VALUE)) > 0) {
			throw new ArithmeticException(
                                ""Binomial coefficient overflow: "" + n + "", "" + k);
		}
		return result.longValue();
	}
{code} 

Which would allow you to test the expected values directly:

{noformat}
         assertEquals(binomialCoefficient(66,33), MathUtils.binomialCoefficient(66,33));
{noformat}
",,,,,,,,,,,,,,,,,,,,,19/Jan/09 18:40;psteitz;binomialPatch.txt;https://issues.apache.org/jira/secure/attachment/12398252/binomialPatch.txt,19/Jan/09 22:05;chsemrau;binomialPatch_cs.txt;https://issues.apache.org/jira/secure/attachment/12398265/binomialPatch_cs.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2009-01-19 18:40:57.164,,,false,,,,,,,,,,,,,,34152,,,Mon Jan 19 23:53:48 UTC 2009,,,,,,0|i0rvnz:,160774,,,,,,,,"19/Jan/09 18:40;psteitz;First, thanks for reporting this.  Due to log/exp rounding and double/long conversion, the current code returns bad values for many long-representable values, starting as low as n = 48.  The returned value can be off by as much as 200,000.  The error in binomial(66, 29) is 214,880.  All b(n,k) for n < 48 are exact.

Attached is a patch that ensures accuracy up to n = 200 (specified as a constant) and allows the user to force exact computation for values beyond this if desired.  For n <= 200, the implementation works like an unwound recursive implementation.   I also improved the accuracy of the double-valued and log versions.   The latter perform better than the current implementations, but the long-valued version is approximately 8x slower than the current version.  I did not benchmark the BigInteger version, but suspect that would be slower still.  The most accurate (for n <= 200) non-recursive formula that I could find is the one that I implemented in the double version.

I also investigated overflow behavior and added tests to confirm correctness.  As stated in the API doc, overflows start at n = 67.  For n = 200,  values of k less than 14 or greater than 186 can still be computed without overflow; but all others throw ArithmeticException.

I would appreciate feedback on the patch and any better ideas on how to fix the problem.",19/Jan/09 22:05;chsemrau;Attached is my version of a new binomialCoefficient function.,"19/Jan/09 22:16;chsemrau;I think the recursive computation of Pascal's triangle (even with caching or dynamic programming) is unnecessarily complicated except to ensure correct values.

The attached patch ensures accuracy for all values that can be represented as a long integer, with a running time proportional to k*log(k) (assuming gcd(i,j) takes log(j) steps). It should be faster than the current version for n <= 61, but for n > 61 my version computes as much as k different gcd values, which might be slower.

I did not modify the double and log version, but your patch can be applied to these.","19/Jan/09 23:53;psteitz;Applied second patch along with changes to double, log versions from first patch in r735879.  Many thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MathUtils.factorial(n) fails for n >= 17,MATH-240,12412638,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,psteitz,chsemrau,chsemrau,16/Jan/09 22:43,23/Apr/09 02:26,07/Apr/19 20:38,19/Jan/09 19:43,2.0,,,,,,,2.0,,,0,,,,,,,,"The result of MathUtils.factorial( n ) for n = 17, 18, 19 is wrong, probably because of rounding errors in the double calculations.

Replace the first line of MathUtilsTest.testFactorial() by

        for (int i = 1; i <= 20; i++) {

to check all valid arguments for the long result and see the failure.

I suggest implementing a simple loop to multiply the long result - or even using a precomputed long[] - instead of adding logarithms.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-19 19:43:52.673,,,false,,,,,,,,,,,,,,34155,,,Mon Jan 19 19:43:52 UTC 2009,,,,,,0|i0rvo7:,160775,,,,,,,,19/Jan/09 19:43;psteitz;Thanks for reporting this.  Fixed in r735781.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MathUtils.gcd(u, v) fails when u and v both contain a high power of 2",MATH-238,12412633,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,chsemrau,chsemrau,16/Jan/09 22:24,23/Apr/09 02:26,07/Apr/19 20:38,16/Jan/09 23:07,2.0,,,,,,,,,,0,,,,,,,,"The test at the beginning of MathUtils.gcd(u, v) for arguments equal to zero fails when u and v contain high enough powers of 2 so that their product overflows to zero.

        assertEquals(3 * (1<<15), MathUtils.gcd(3 * (1<<20), 9 * (1<<15)));

Fix: Replace the test at the start of MathUtils.gcd()

        if (u * v == 0) {

by

        if (u == 0 || v == 0) {
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2009-01-16 23:07:17.149,,,false,,,,,,,,,,,,,,34222,,,Fri Jan 16 23:07:17 UTC 2009,,,,,,0|i0rvon:,160777,,,,,,,,"16/Jan/09 23:07;luc;fixed in trunk as of r73517

thanks for the report",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket,MATH-227,12405126,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,brentworden,joerx,joerx,25/Sep/08 08:23,23/Jul/12 23:24,07/Apr/19 20:38,26/Sep/08 03:11,1.2,,,,,,,,,,0,,,,,,,,"We are using the FDistributionImpl from the commons.math project to do
some statistical calculations, namely receiving the upper and lower
boundaries of a confidence interval. Everything is working fine and the
results are matching our reference calculations.

However, the FDistribution behaves strange if a
denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95.
This results in an IllegalArgumentsException, stating:
        
Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity
upperBound=1.7976931348623157E308
        
coming from
org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket
        
The problem is the 'initial' parameter to that function, wich is
POSITIVE_INFINITY and therefore not within the boundaries. I already
pinned down the problem to the FDistributions getInitialDomain()-method,
wich goes like:

        return getDenominatorDegreesOfFreedom() /
                    (getDenominatorDegreesOfFreedom() - 2.0);
        
Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead
to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this
operation is then directly passed into the
UnivariateRealSolverUtils.bracket() - method as second argument.","Java 1.5.0_15, Linux",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-09-26 03:11:58.6,,,false,,,,,,,,,,,,,,34147,,,Fri Sep 26 03:11:58 UTC 2008,,,,,,0|i0rvqv:,160787,,,,,,,,26/Sep/08 03:11;brentworden;SVN 699157.  fixed in trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CorrelatedRandomVectorGenerator generates invariant samples,MATH-226,12404467,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ssiegel95,ssiegel95,16/Sep/08 20:08,23/Apr/09 02:26,07/Apr/19 20:38,16/Sep/08 21:05,1.2,,,,,,,,,,0,,,,,,,,"For the following code sample, the output is:
1.0,3.9432161557722925,16.66859790068678,1.0743673824292688,
1.0,-1.4103098147521094,-2.670854139636077,1.8368602953644368,
1.0,0.230029048125738,12.67864233710285,0.1124537698401884,

Why is the first column of each row fixed at 1.0?

Here is the code that generated this:

import org.apache.commons.math.linear.RealMatrix;
import org.apache.commons.math.linear.RealMatrixImpl;
import org.apache.commons.math.random.CorrelatedRandomVectorGenerator;
import org.apache.commons.math.random.GaussianRandomGenerator;
import org.apache.commons.math.random.JDKRandomGenerator;

public class TestMath {

	public static void sampler(double[] mean, double[][] cov, double[][] s) {
		RealMatrix covRM = new RealMatrixImpl(cov);
		try {
			CorrelatedRandomVectorGenerator sg = new CorrelatedRandomVectorGenerator(
					mean, covRM, 0.00001, new GaussianRandomGenerator(
							new JDKRandomGenerator()));

			for (int i = 0; i < s.length; i++) {
				s[i] = sg.nextVector();
			}
		} catch (Exception e) {
			e.printStackTrace();
			System.exit(-1);
		}

	}

	static void print(double[][] s) {
		for (int r = 0; r < s.length; r++) {
			for (int c = 0; c < s[r].length; c++)
				System.out.print(s[r][c] + "","");
			System.out.println();

		}

	}

	public static void main(String[] args) {
		double[] mean = { 1, 1, 10, 1 };
		double[][] cov = { { 1, 3, 2, 6 }, { 3, 13, 16, 2 }, { 2, 16, 38, -1 },
				{ 6, 2, -1, 197 } };

		
		double[][] s = new double[3][4];

		TestMath.sampler(mean, cov, s);
		TestMath.print(s);

		
	}
}
",WIN32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-09-16 21:05:59.41,,,false,,,,,,,,,,,,,,34148,,,Tue Sep 16 22:28:11 UTC 2008,,,,,,0|i0rvr3:,160788,,,,,,,,"16/Sep/08 21:05;luc;There was an error in the Choleski decomposition with rows reordering. The wrong permutation was used
(swap[i] instead of index[i] in the root matrix construction at the end of the decompose method).

The problem is now fixed in subversion repository, in branch MATH_2_0 as of r696054, and a new test has been added using your data.

Thanks for reporting the problem.","16/Sep/08 22:28;ssiegel95;Thanks for the very speedy reply and thanks for commons-math, it has proven to be very helpful.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Result of multiplying and equals for complex numbers is wrong,MATH-221,12403362,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,news.dieter,news.dieter,29/Aug/08 13:31,23/Apr/09 02:26,07/Apr/19 20:38,29/Aug/08 15:52,1.2,,,,,,,,,,0,,,,,,,,"Hi.

The bug relates on complex numbers.
The methods ""multiply"" and ""equals"" of the class Complex are involved.

mathematic background:  (0,i) * (-1,0i) = (0,-i).

little java program + output that shows the bug:
-----------------------------------------------------------------------
{code}
import org.apache.commons.math.complex.*;
public class TestProg {
        public static void main(String[] args) {

                ComplexFormat f = new ComplexFormat();
                Complex c1 = new Complex(0,1);
                Complex c2 = new Complex(-1,0);

                Complex res = c1.multiply(c2);
                Complex comp = new Complex(0,-1);

                System.out.println(""res:  ""+f.format(res));
                System.out.println(""comp: ""+f.format(comp));

                System.out.println(""res=comp: ""+res.equals(comp));
        }
}
{code}
-----------------------------------------------------------------------

res:  -0 - 1i
comp: 0 - 1i
res=comp: false

-----------------------------------------------------------------------

I think the ""equals"" should return ""true"".
The problem could either be the ""multiply"" method that gives (-0,-1i) instead of (0,-1i),
or if you think thats right, the equals method has to be modified.

Good Luck
Dieter","OS: Debian lenny
IDE: Eclipse; Version: 3.4.0; Build id: I20080617-2000
java.runtime.version=1.6.0_04-b12
java.vendor.url=http://java.sun.com/
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-08-29 15:52:38.278,,,false,,,,,,,,,,,,,,34196,,,Fri Aug 29 15:52:38 UTC 2008,,,,,,0|i0rvs7:,160793,,,,,,,,"29/Aug/08 15:52;luc;According to IEEE-754 (the standard that specifies double number representation), 0 is a signed value, so there are two different representations: +0 and -0, and there is a specific rule that say these two representations should always compare as equal.

The fact the multiplication returns a real part as -0 is therefore correct behavior. The fact it is not equal to another value where +0 appears was an error in out implementation of the equal method.

The problem has been fixed in the subversion repository (in branch 2.0) as of r690308.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fixed step Runge-Kutta integrators slightly change step size,MATH-214,12400013,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,luc,luc,luc,10/Jul/08 12:34,23/Apr/09 02:26,07/Apr/19 20:38,10/Jul/08 12:43,1.2,,,,,,,2.0,,,0,,,,,,,,"When a fixed step Runge-Kutta integrator is used, it may slightly change the step size for all integration range.
This is due to a step recomputation feature which ensures the last step ends exactly at the end of the range.

This feature should be removed, since it is unnatural and does not obey users choices.

It should be replaced by the same kind of solution already adopted for discrete events: truncating only the last step.",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,34135,,,Thu Jul 10 12:43:08 UTC 2008,,,,,,0|i0rvtr:,160800,,,,,,,,10/Jul/08 12:43;luc;fixed as of r675552,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
public method FirstOrderIntegrator.getSwitchingFunctions() return type references a package protected class,MATH-210,12398680,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,luc,luc,luc,20/Jun/08 10:46,23/Apr/09 02:26,07/Apr/19 20:38,08/Jul/08 20:14,1.2,,,,,,,2.0,,,0,,,,,,,,"getSwitchingFunctions() returns a collection of SwitchState, not a collection of SwitchingFunction.
This is counter-intuitive and not useful since the class is package protected so cannot be used outside.
This is also not useful since one cannot retrieve the underlying switching function.",all,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150424,,,Tue Jul 08 20:14:33 UTC 2008,,,,,,0|i0rvun:,160804,,,,,,,,08/Jul/08 20:14;luc;fixed as of r673752,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RealMatrixImpl#operate gets result vector dimensions wrong,MATH-209,12398389,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,murphy,murphy,17/Jun/08 03:02,23/Apr/09 02:26,07/Apr/19 20:38,17/Jun/08 19:10,1.2,,,,,,,,,,0,,,,,,,,"{{org.apache.commons.math.linear.RealMatrixImpl#operate}} tries to create a result vector that always has the same length as the input vector. This can result in runtime exceptions if the matrix is non-square and it always yields incorrect results if the matrix is non-square. The correct behaviour would of course be to create a vector with the same length as the row dimension of the matrix.

Thus line 640 in RealMatrixImpl.java should read
  {{double[] out = new double[nRows];}}
instead of
  {{double[] out = new double[v.length];}}
",,120,120,,0%,120,120,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-06-17 19:10:52.992,,,false,,,,,,,,,,,,,,34209,,,Tue Jun 17 19:10:52 UTC 2008,,,,,,0|i0rvuv:,160805,,,,,,,,"17/Jun/08 19:10;luc;Sorry for this dumb mistake.

Fixed in 2.0 branch as of r668798.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implementation of GeneticAlgorithm.nextGeneration() is wrong,MATH-207,12396580,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,psteitz,david.stefka,david.stefka,21/May/08 21:50,07/Aug/09 09:20,07/Apr/19 20:38,30/Jun/09 00:44,2.0,,,,,,,2.0,,,0,,,,,,,,"The implementation of GeneticAlgorithm.nextGeneration() is wrong, since the only way how a Chromosome can get into the new generation is by mutation. 

Enclosed, I am sending a patch for this.",,600,600,,0%,600,600,,,,,,,,,,,,,,29/Sep/08 11:05;david.stefka@gmail.com;ASF.LICENSE.NOT.GRANTED--genetics_impl.zip;https://issues.apache.org/jira/secure/attachment/12391126/ASF.LICENSE.NOT.GRANTED--genetics_impl.zip,26/Mar/09 13:50;david.stefka@gmail.com;geneticAlgorithms.zip;https://issues.apache.org/jira/secure/attachment/12403714/geneticAlgorithms.zip,01/Mar/09 19:41;bmccann;geneticalgorithm.patch;https://issues.apache.org/jira/secure/attachment/12401205/geneticalgorithm.patch,21/May/08 21:51;david.stefka;patch;https://issues.apache.org/jira/secure/attachment/12382518/patch,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,2008-09-27 18:09:43.157,,,false,,,,,,,,,,,,,,150422,,,Fri Aug 07 09:20:08 UTC 2009,,,,,,0|i0rvvb:,160807,,,,,,,,21/May/08 21:51;david.stefka;Patch for the bug,"27/Sep/08 18:09;psteitz;Patch applied to MATH_2_0 branch in r699704.

Leaving open because I think we need to either make nextGeneration public or protected or make it configurable.  Interested in comments on this.","29/Sep/08 11:05;david.stefka@gmail.com;Dear Phil,

if you are interested in the topic of genetic algorithms, I am sending a 
simple implementation of GA using the commons-math framework. If you find any 
of the ideas in it useful, you may use it.

Best regards,
David Stefka


-- 
David Stefka
david.stefka@gmail.com (personal mail)

","27/Feb/09 02:07;chengas123;I think that making nextGeneration public would be reasonable.
A few other thoughts about the API:
All the setters should be changed to constructor arguments.  If you do not call each of the setters then you are in an illegal state.  You avoid the problem of having some call evolve before the mutation policy and crossover policy are set if you just make them required in the constructor.
For crossOverRate and mutationRate we should specify that the input is expected to be between 0 and 1 and throw an exception when given invalid input.  It's currently not clear in the javadocs what is expected.",01/Mar/09 19:41;bmccann;Patch to improve the API.,09/Mar/09 18:50;bmccann;Any thoughts on the patch submitted to clean up the API?,"18/Mar/09 12:50;david.stefka@gmail.com;I think the ideas to clean up the API are okay. However, I don't know whether the patch is against the latest version. Things are starting to be a bit messy... :)","19/Mar/09 02:00;psteitz;Sorry for delay in reviewing these patches.  I like both the implementation (David's first patch) and the API improvements and will apply them both if I can get answers to the following.

1) David - pls confirm that you can grant the code in your patch.  It is a borderline case whether or not we need a code grant.  Given that it just implements the commons-math ga framework, I think it is OK to commit without a grant; but since you did not include ASL license headers in the patch, I need you to confirm that you own the code and can freely contribute it under the terms of the Apache Contributor License Agreement.  If you don't mind, it would be good to file a CLA.

2) David's patch looks like it has some JDK 1.6 dependencies.  Math 2.0 targets 1.5+, so these need to be removed.  I can do this; but a revised patch with these removed and ASL headers would be appreciated.

3) Ben - why expose fields as protected?  

Thanks for the patches!","19/Mar/09 15:46;david.stefka@gmail.com;ad 1) what do you mean by ""file a CLA""? Should I include the license header (e.g. in GeneticAlgorithm.java) to every file in the framework? I do own the code and I agree with the Apache license (http://www.apache.org/licenses/LICENSE-2.0)
ad 2) okay, i will try to make it Java 1.5+ compatible

BTW, I am currently working on a better way of representing permutations, so do not include the original patch now. I will post here a revised version of the implementation in a week or so.","19/Mar/09 20:18;psteitz;Thanks, David!

All files should include headers like what you see in GenericAlgorithm.java or any other apache java source file.  As long as you own the code and are willing to attach those headers, we should be OK.   A Contributors Licence Agreement (CLA) is a good thing to have on file and will be required in any case should you become a committer.  Have a look at the ""Contributor License Agreements"" section at http://www.apache.org/licenses/ to find the form.  The form itself includes info on how to submit it.

Thanks again for your conttribution.
","26/Mar/09 13:48;david.stefka@gmail.com;Hi again,

I have finished work on better representation of permutations in GA, so I am sending the implementation in a .zip file. The code includes:

 * new implementation of RandomKey -- chromosome representing permutations; the old PermutationChromosome implementation has been removed
 * some minor improvements (chromosome's representation is held in an unmodifiable list instead of an array, better implementation of generic issues, renaming of several classes, etc.)
 * GeneticAlgorithm has a static Random instance, which is used for all random numbers in the implementation. This is useful for debugging, because if the Random is initialized to a particular seed, the behavior can be reproduced. If a similar mechanism is already somewhere in the
 * the code should be java 1.5 compatible
 * all the files should include the Apache license headers

I have also filled and signed the CLA and sent it to secretary@apache.org",26/Mar/09 13:50;david.stefka@gmail.com;New implementation of basic GA algorithms,"04/May/09 20:08;chengas123;Sorry for my delayed response.  JIRA didn't email me on any of these updates for whatever reason.
There's no real need to make the fields protected in my patch.  Private would be fine.  I've just gotten in the habit of frequently using protected to allow easier subclassing and unit testing, but those fields all have public getters, so no harm in making them private.","14/Jun/09 19:08;psteitz;Committed a slightly modified version of David's last patch in r784604.

Other than minor javadoc/formatting changes to make checkstyle happy, I also made the shared source of randomness pluggable.

I am not 100% happy with the static randomGenerator attached to GeneticAlgorithm, though I understand and support the need to ensure reproducibility for some applications.  Comments / suggestions for better ways to do this welcome.

Leaving open as we need to update the user guide to complete this.",30/Jun/09 00:44;psteitz;User guide updated in r789511,07/Aug/09 09:20;luc;closing resolved issue for 2.0 release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ComplexFormat.parse doesn't parse a double e.g. 0.0,MATH-206,12396454,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,fsala,fsala,20/May/08 22:08,23/Apr/09 02:26,07/Apr/19 20:38,20/May/08 22:15,1.2,,,,,,,,,,0,,,,,,,,"When running:

public static void main(String[] args) {
        try {
            String s = ""0.0"";
            ComplexFormat cf = new ComplexFormat();
            Complex c = cf.parse(s);
            System.out.println(""c = "" + c);
        } catch (ParseException ex) {
            ex.printStackTrace();
        }
    }

i get the following error:

java.text.ParseException: Unparseable complex number: ""0.0""
        at org.apache.commons.math.complex.ComplexFormat.parse(ComplexFormat.java:307)
        at complexformattest.Main.main(Main.java:26)

With integers it works correctly but support for doubles is even more important ;] (from my point of view). I downloaded the ""Latest release""

Hope u fix it quick. I have a proposal but it affects much the code. Maybe You have some quick hint?",Windows Vista + JDK 6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-05-20 23:11:55.577,,,false,,,,,,,,,,,,,,34158,,,Wed May 21 11:43:30 UTC 2008,,,,,,0|i0rvvj:,160808,,,,,,,,"20/May/08 22:15;fsala;It works with ""0,0"".

But anyway doubles in java uses a dot not a comma. ","20/May/08 23:11;sebb@apache.org;Surely the decimal ""point"" depends on the Locale - it may be ""."" or "","" (perhaps there are others).","21/May/08 11:43;luc;Using a no-argument constructor for ComplexFormat leads to use a default format which itself relies on the default locale as returned by Locale.getDefault(). This is a normal behavior and is the same behavior you get from parsing a single double value with functions like Double.parseDouble().

Parsing non-localized strings even in localized environments can be done by supplying the format to use to ComplexFormat, for example like this:

  ComplexFormat cf = new ComplexFormat(NumberFormat.getInstance(Locale.US));",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mathematical error in comment for FastCosineTransformer,MATH-205,12396029,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Trivial,Fixed,,rwerp,rwerp,14/May/08 15:27,23/Apr/09 02:26,07/Apr/19 20:38,15/May/08 19:56,1.2,,,,,,,,,,0,,,,,,,,"The formula in the comments for transform() method of FastCosineTransformer has the form:

F_n = (1/2) [f_0 + (-1)^n f_N] + \Sum_{k=0}^{N-1} f_k \cos(\pi nk/N)

This is incorrect and should be

F_n = (1/2) [f_0 + (-1)^n f_N] + \Sum_{k=1}^{N-1} f_k \cos(\pi nk/N)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-05-15 19:56:46.389,,,false,,,,,,,,,,,,,,150421,,,Thu May 15 19:56:46 UTC 2008,,,,,,0|i0rvvr:,160809,,,,,,,,15/May/08 19:56;luc;fixed in trunk as of r656814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BrentSolver throws IllegalArgumentException ,MATH-204,12395456,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,brentworden,mickeydog,mickeydog,06/May/08 20:33,23/Jul/12 23:17,07/Apr/19 20:38,07/May/08 13:37,1.2,,,,,,,2.0,,,0,,,,,,,,"I am getting this exception:

java.lang.IllegalArgumentException: Function values at endpoints do not have different signs.  Endpoints: [-100000.0,1.7976931348623157E308]  Values: [0.0,-101945.04630982173]
at org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:99)
at org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:62)

The exception should not be thrown with values  [0.0,-101945.04630982173] because 0.0 is positive.
According to Brent Worden, the algorithm should stop and return 0 as the root instead of throwing an exception.

The problem comes from this method:
    public double solve(double min, double max) throws MaxIterationsExceededException, 
        FunctionEvaluationException {
        
        clearResult();
        verifyInterval(min, max);
        
        double yMin = f.value(min);
        double yMax = f.value(max);
        
        // Verify bracketing
        if (yMin * yMax >= 0) {
            throw new IllegalArgumentException
            (""Function values at endpoints do not have different signs."" +
                    ""  Endpoints: ["" + min + "","" + max + ""]"" + 
                    ""  Values: ["" + yMin + "","" + yMax + ""]"");       
        }

        // solve using only the first endpoint as initial guess
        return solve(min, yMin, max, yMax, min, yMin);

    }

One way to fix it would be to add this code after the assignment of yMin and yMax:
        if (yMin ==0 || yMax == 0) {
        	return 0;
       	}
",Win XP,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-05-07 13:37:07.848,,,false,,,,,,,,,,,,,,34146,,,Wed May 07 13:37:07 UTC 2008,,,,,,0|i0rvvz:,160810,,,,,,,,07/May/08 13:37;brentworden;SVN 654100.  added root checks for the endpoints.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T-test p-value precision hampered by machine epsilon,MATH-201,12393132,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,brentworden,pwyngaard,pwyngaard,04/Apr/08 16:31,23/Jul/12 23:17,07/Apr/19 20:38,06/Apr/08 01:24,1.2,,,,,,,2.0,,,0,,,,,,,,"The smallest p-value returned by TTestImpl.tTest() is the machine epsilon, which is 2.220446E-16 with IEEE754 64-bit double precision floats.

We found this bug porting some analysis software from R to java, and noticed that the p-values did not match up.  We believe we've identified why this is happening in commons-math-1.2, and a possible solution.

Please be gentle, as I am not a statistics expert!

The following method in org.apache.commons.math.stat.inference.TTestImpl currently implements the following method to calculate the p-value for a 2-sided, 2-sample t-test:

protected double tTest(double m1, double m2, double v1, double v2,  double n1, double n2)

and it returns:

        1.0 - distribution.cumulativeProbability(-t, t);

at line 1034 in version 1.2.

double cumulativeProbability(double x0, double x1) is implemented by org.apache.commons.math.distribution.AbstractDisstribution, and returns:

        return cumulativeProbability(x1) - cumulativeProbability(x0);

So in essence, the p-value returned by TTestImpl.tTest() is:

1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t))

For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0.  When cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because:

1.0 - 1.0 + 0.0 = 0.0

An alternative calculation for the p-value of a 2-sided, 2-sample t-test is:

p = 2.0 * cumulativeProbability(-t)

This calculation does not suffer from the machine epsilon problem, and we are now getting p-values much smaller than the 2.2E-16 limit we were seeing previously.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-04-06 01:24:56.491,,,false,,,,,,,,,,,,,,150418,,,Sun Apr 06 01:24:56 UTC 2008,,,,,,0|i0rvwn:,160813,,,,,,,,"06/Apr/08 01:24;brentworden;SVN 645193.

Changes applied.  Thank you for reporting this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractEstimator: getCovariances() and guessParametersErrors() crash when having bound parameters,MATH-200,12392292,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,luc,ppv666,ppv666,25/Mar/08 22:22,23/Apr/09 02:26,07/Apr/19 20:38,28/Mar/08 20:11,1.2,,,,,,,,,,0,,,,,,,,"the two methods getCovariances() and guessParametersErrors() from org.apache.commons.math.estimation.AbstractEstimator crash with ArrayOutOfBounds exception when some of the parameters are bound. The reason is that the Jacobian is calculated only for the unbound parameters. in the code you loop through all parameters.

line #166: final int cols = problem.*getAllParameters()*.length;
should be replaced by:  final int cols = problem.*getUnboundParameters()*.length;
(similar changes could be done in guessParametersErrors())

the dissadvantage of the above bug fix is that what is returned to the user is an array with smaller size than the number of all parameters. Alternatively, you can have some logic in the code which writes zeros for the elements of the covariance matrix corresponding to the bound parameters",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-03-25 23:33:11.554,,,false,,,,,,,,,,,,,,34183,,,Fri Mar 28 20:11:06 UTC 2008,,,,,,0|i0rvwv:,160814,,,,,,,,"25/Mar/08 23:33;luc;Since the contract on the two methods is related to the complete parameters array, only the more elaborate fix with specific logic for bound parameters is feasible.

I agree this is a critical error. I will try to provide a fix in the next few days.

Thanks for reporting this.","26/Mar/08 07:37;ppv666;Thanks a lot for your quick response!
Best wishes,
Plamen


","28/Mar/08 20:11;luc;I finally changed my mind and limited the results of both methods to unbound parameters only.
This is more compliant with what user can expect when they bind parameters (a problem with bound parameters should behave exactly as if these parameters were simply constants).
The previous javadoc was not clear about this and the methods did not work at all in this case, so such a change seems wiser to me.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exception in LevenbergMarquardtEstimator,MATH-199,12392015,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mickeydog,mickeydog,20/Mar/08 22:56,23/Apr/09 02:26,07/Apr/19 20:38,23/Mar/08 13:40,1.2,,,,,,,,,,0,,,,,,,,"I get this exception:

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1
       at org.apache.commons.math.estimation.LevenbergMarquardtEstimator.qrDecomposition(LevenbergMarquardtEstimator.java:772)
       at org.apache.commons.math.estimation.LevenbergMarquardtEstimator.estimate(LevenbergMarquardtEstimator.java:232)
       at quadraticFitterProblem.QuadraticFitterProblem.<init>(QuadraticFitterProblem.java:27)
       at quadraticFitterProblem.QuadraticFitterProblem.main(QuadraticFitterProblem.java:40)
on the code below.

The exception does not occur all the weights in the quadraticFitter are 0.0;


---------------------------------------------------------------------------------------------

package quadraticFitterProblem;

import org.apache.commons.math.estimation.EstimationException;
import org.apache.commons.math.estimation.LevenbergMarquardtEstimator;
//import org.apache.commons.math.estimation.WeightedMeasurement;

import com.strategicanalytics.dtd.data.smoothers.QuadraticFitter;

public class QuadraticFitterProblem {

       private QuadraticFitter quadraticFitter;

       public QuadraticFitterProblem() {
         // create the uninitialized fitting problem
         quadraticFitter = new QuadraticFitter();

         quadraticFitter.addPoint (0,  -3.182591015485607, 0.0);
         quadraticFitter.addPoint (1,  -2.5581184967730577, 4.4E-323);
         quadraticFitter.addPoint (2,  -2.1488478161387325, 1.0);
         quadraticFitter.addPoint (3,  -1.9122489313410047, 4.4E-323);
         quadraticFitter.addPoint (4,  1.7785661310051026, 0.0);

         try {
           // solve the problem, using a Levenberg-Marquardt algorithm with
default settings
           LevenbergMarquardtEstimator estimator = new LevenbergMarquardtEstimator();
           //WeightedMeasurement[] wm = quadraticFitter.getMeasurements();
           estimator.estimate(quadraticFitter);

         } catch (EstimationException ee) {
               System.err.println(ee.getMessage());
         }
       }

       /**
        * @param args
        *
        */
       public static void main(String[] args) {

                       new QuadraticFitterProblem();
                       System.out.println (""Done."");
       }

}

----------------------------------------------------------------------------------------------
import org.apache.commons.math.estimation.EstimatedParameter;
//import org.apache.commons.math.estimation.EstimationException;
//import org.apache.commons.math.estimation.LevenbergMarquardtEstimator;
import org.apache.commons.math.estimation.SimpleEstimationProblem;
import org.apache.commons.math.estimation.WeightedMeasurement;

public class QuadraticFitter extends SimpleEstimationProblem {

       // y = a x<sup>2</sup> + b x + c
   private EstimatedParameter a;
   private EstimatedParameter b;
   private EstimatedParameter c;

   /**
    * constructor
    *
    *Fitter for a quadratic model to a sample of 2D points.
    * <p>The model is y(x) = a x<sup>2</sup> + b x + c
    * its three parameters of the model are a, b and c.</p>
    */
   public QuadraticFitter() {

       // three parameters of the model
       a = new EstimatedParameter(""a"", 0.0);
       b = new EstimatedParameter(""b"", 0.0);
       c = new EstimatedParameter(""c"", 0.0);

       // provide the parameters to the base class which
       // implements the getAllParameters and getUnboundParameters methods
       addParameter(a);
       addParameter(b);
       addParameter(c);
   }

   /**
    * Add a sample point
    *
    * @param x abscissa
    * @param y ordinate
    * @param w weight
    */
   public void addPoint(double x, double y, double w) {
       addMeasurement(new LocalMeasurement(x, y, w));
   }

   /**
    * Get the value of the quadratic coefficient.
    *
    * @return the value of a for the quadratic model
    * y = a x<sup>2</sup> + b x + c
    */
   public double getA() {
       return a.getEstimate();
   }

   /**
    * Get the value of the linear coefficient.
    *
    * @return the value of b for the quadratic model
    * y = a x<sup>2</sup> + b x + c
    */
   public double getB() {
       return b.getEstimate();
   }

   /**
    * Get the value of the constant coefficient.
    *
    * @return the value of ac for the quadratic model
    * y = a x<sup>2</sup> + b x + c
    */
   public double getC() {
       return c.getEstimate();
   }

   /**
    * Get the theoretical value of the model for some x.
    * <p>The theoretical value is the value computed using
    * the current state of the problem parameters.</p>
    *
    * Note the use of Hörner's method (synthetic division) for
evaluating polynomials,
    * (more efficient)
    *
    * @param x explanatory variable
    * @return the theoretical value y = a x<sup>2</sup> + b x + c
    */
   public double theoreticalValue(double x) {
       //System.out.println (""x = "" + x + ""  a.getEstimate() = "" +
a.getEstimate() + ""  b.getEstimate() = "" + b.getEstimate() + ""
c.getEstimate() = "" + c.getEstimate());
       return ( (a.getEstimate() * x + b.getEstimate() ) * x +
c.getEstimate());
   }

   /**
    * Get the partial derivative of the theoretical value
    * of the model for some x.
    * <p>The derivative is computed using
    * the current state of the problem parameters.</p>
    *
    * @param x explanatory variable
    * @param parameter estimated parameter (either a, b, or c)
    * @return the partial derivative dy/dp
    */
   private double partial(double x, EstimatedParameter parameter) {
       // since we know the only parameters are a, b and c in this
       // class we simply use ""=="" for efficiency
       if (parameter == a) {
           return x * x;
       } else if (parameter == b) {
           return x;
       } else {
           return 1.0;
       }

   }


   /** Internal measurements class.
    * <p>The measurement is the y value for a fixed specified x.</p>
    */
   private class LocalMeasurement extends WeightedMeasurement {

       static final long serialVersionUID = 1;

       private final double x;

       // constructor
       public LocalMeasurement(double x, double y, double w) {
           super(w, y);
           this.x = x;
       }

       public double getTheoreticalValue() {
           // the value is provided by the model for the local x
           return theoreticalValue(x);
       }

       public double getPartial(EstimatedParameter parameter) {
           // the value is provided by the model for the local x
           return partial(x, parameter);
       }

   }

 }","Windows XP
Java 6
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-03-23 13:40:12.946,,,false,,,,,,,,,,,,,,34160,,,Mon Mar 24 16:29:26 UTC 2008,,,,,,0|i0rvx3:,160815,,,,,,,,"23/Mar/08 13:40;luc;Fixed in svn as of r640205

The problem was due to an overflow in Q.R decomposition. One of  the transformed columns had both infinite and NaN elements, so the test of the norm was never met and a column index was never set.

The fix consist in detecting non-numeric norms and throwing an EstimationException stating the Q.R decomposition could not be performed.","24/Mar/08 13:56;mickeydog;Is it accurate to say that the problem is caused by the 4.4E-323 in the error term?
(ie, the 4.4E-323 causes one of the transformed columns to have values of Infinity or NaN?

If I eliminate those error terms, e.g.:
	  quadraticFitter.addPoint (0,  -3.182591015485607, 0.0);
	  quadraticFitter.addPoint (1,  -2.5581184967730577, 0.5);
	  quadraticFitter.addPoint (2,  -2.1488478161387325, 1.0);
	  quadraticFitter.addPoint (3,  -1.9122489313410047, 0.5);
	  quadraticFitter.addPoint (4,  1.7785661310051026, 0.0);
then, indeed, the error does not occur.

I have a concern that, while it is true that  4.4E-323 is a number of extremely small magnitude, it is a vaild double, but using it causes an exception.
:Perhaps I am not seeing this correctly.

","24/Mar/08 15:09;luc;Yes, this is this small value that triggers the error. It is a valid number but during the Q.R decomposition, this number is used in several operations. One of these operations lead to an overflow.

At line 785 of LevenbergMarquardt.java, we compute :  double betak = 1.0 / (ak2 - akk * alpha)

With the very small values of the example, the value for betak overflows while processing the second column (first column is computed without problem). The denominator ak2 - akk * alpha has a value of 1.43e-322 which can be handled by double. It's inverse is 6.9e321, far larger than the Double.MAX_VALUE which is about 1.8e+308. The result of the computation is that betak is set to positive infinity. The rest of the computation behaves badly with such a value. Some of the elements of third column are set to infinity, others are set to NaN.

The fact is that despite many representable double number have a representable inverse in IEE754, it is not true for all.","24/Mar/08 15:57;mickeydog;That being the case, could the commons math library define a constant such that its inverse it  equal to Double.MAX_VALUE?
I could then use this as a limit.
","24/Mar/08 16:29;luc;Numbers that behave well are ""normal numbers"" as defined by IEEE754. Numbers to avoid in your case are ""subnormal numbers"". You can use Double.MIN_NORMAL if you use Java 6, or simply 4 / Double.MAX_VALUE (which is almost but not exactly the same value) if you use an earlier version of Java.

Beware that this may not be enough because you can test only the numbers you put in the method, not the intermediate values computed within it. For example the ak2, akk and alpha values are not available in the statement above, and betak is available only on output.

I'm not sure such constants have a place in [math], but this is only a personal opinion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"java.lang.StringIndexOutOfBoundsException in ComplexFormat.parse(String source, ParsePosition pos)",MATH-198,12391806,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,fsala,fsala,18/Mar/08 23:06,23/Apr/09 02:26,07/Apr/19 20:38,23/Mar/08 12:23,1.2,,,,,,,,,,0,,,,,,,,"The parse(String source, ParsePosition pos) method in the ComplexFormat class does not check whether the imaginary character is set or not which produces StringIndexOutOfBoundsException in the substring method :

(line 375 of ComplexFormat)
...
        // parse imaginary character
        int n = getImaginaryCharacter().length();
        
        startIndex = pos.getIndex();
        int endIndex = startIndex + n;
        if (source.substring(startIndex, endIndex).compareTo(
            getImaginaryCharacter()) != 0) {
...
I encoutered this exception typing in a JTextFied with ComplexFormat set to look up an AbstractFormatter.
If only the user types the imaginary part of the complex number first, he gets this exception.

Solution: Before setting to n length of the imaginary character, check if the source contains it. My proposal:
...
        int n = 0;
        if (source.contains(getImaginaryCharacter()))
        n = getImaginaryCharacter().length();
...		 

F.S.","Ubuntu 7.10, JDK 6.0, PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-03-23 12:23:59.055,,,false,,,,,,,,,,,,,,34202,,,Sun Mar 23 12:23:59 UTC 2008,,,,,,0|i0rvxb:,160816,,,,,,,,"23/Mar/08 12:23;luc;Fixed in svn as of r 640191.

The adopted fixed is different from the proposed one, it simply checks the length of the string with respect to the expected position of the imaginary character",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RandomDataImpl.nextPoisson() is extreme slow for large lambdas,MATH-197,12391611,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,brentworden,zubow,zubow,17/Mar/08 11:55,07/Aug/09 09:19,07/Apr/19 20:38,21/Jun/09 15:10,1.0,1.1,1.2,,,,,2.0,,,0,,,,,,,,"The RandomDataImpl.nextPoisson() is extreme slow for large lambdas:

E.g. drawing 100 random numbers with lambda = 1000 takes around 10s on my dual core with 2.2GHz.
With lambda smaller than 500 everything is fine. Any ideas?

    RandomDataImpl r = new RandomDataImpl();
    r.reSeed(101);

    int d = 100;
    long poissonLambda = 1000;

    long st = System.currentTimeMillis();
    for (int row = 0; row < d; row++) {
      long nxtRnd = r.nextPoisson(poissonLambda);
    }
    System.out.println(""delta "" + (System.currentTimeMillis() - st));
","jdk1.6.0_04, windows xp",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-03-23 11:11:48.44,,,false,,,,,,,,,,,,,,34137,,,Fri Aug 07 09:19:48 UTC 2009,,,,,,0|i0rvxj:,160817,,,,,,,,"23/Mar/08 11:11;luc;The problems comes from the way the distribution is computed. A Poisson process is simulated and lots of random numbers are drawn, until there product reach some threshold.
If lambda is greater or equal to 746, the value of Math.exp(-lambda) cannot be represented anymore in a double and it is replaced by 0 (exactly 0). The previous value, for lambda = 745 is 4.9e-324, which is ... small.
In this case, the threshold is never reached and the loop is exited only because of an exceeded count.
This means that in addition to being long to obtain, the result is false (it will be exactly 1000 * lambda).

So the current algorithm cannot handle large lambda values.

Does anybody have a reference for another algorithm that could be used for large lambda values ?","24/Mar/08 15:55;brentworden;I've been looking at  Devroye's 'Non-Uniform Random Variate Generation' book.  The book is freely available at http://cg.scs.carleton.ca/~luc/rnbookindex.html.

Chapter 10 defines some rejection methods for Poisson generation which, the author claims, are uniformily fast for all lambda.

I hope to soon implement one or more of the rejection methods and test its performance.","21/Jun/09 12:04;jkff;I have had a look at the source and it looks like the method is already implemented, so the bug should probably be closed?",21/Jun/09 15:10;psteitz;Fix was committed in r762194 by brentworden,07/Aug/09 09:19;luc;closing resolved issue for 2.0 release,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SummaryStatistics computes sum of logs but does not provide access to it,MATH-191,12388270,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,psteitz,psteitz,09/Feb/08 16:23,23/Apr/09 02:26,07/Apr/19 20:38,09/Feb/08 23:43,1.0,1.1,,,,,,1.2,,,0,,,,,,,,"A SumOfLogs instances is maintained and incremented by SummaryStatistics, but there is no getSumOfLogs method to report the value of this statistic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,150414,,,Sat Feb 09 23:43:24 UTC 2008,,,,,,0|i0rvyv:,160823,,,,,,,,09/Feb/08 23:43;psteitz;Fixed in r 620221.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect SVN properties for some files,MATH-190,12387809,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,sebb@apache.org,sebb@apache.org,04/Feb/08 19:00,23/Apr/09 02:26,07/Apr/19 20:38,04/Feb/08 20:30,,,,,,,,1.2,,,0,,,,,,,,"There seem to be a few incorrect SVN properties in SVN trunk; following is a list of SVN commands that can be used to fix them:

These should be done on whatever OS was used to create the files, probably Unix, otherwise there may be problems:

svn ps svn:eol-style native src/java/org/apache/commons/math/random/UncorrelatedRandomVectorGenerator.java
svn ps svn:eol-style native src/java/org/apache/commons/math/stat/inference/OneWayAnova.java
svn ps svn:eol-style native src/java/org/apache/commons/math/stat/inference/OneWayAnovaImpl.java
svn ps svn:eol-style native src/test/org/apache/commons/math/stat/inference/OneWayAnovaTest.java

These files should not be executable ;-) - can be done on any OS:

svn pd svn:executable src/java/org/apache/commons/math/transform/package.html
svn pd svn:executable src/mantissa/src/org/spaceroots/mantissa/algebra/Chebyshev.java
svn pd svn:executable src/mantissa/src/org/spaceroots/mantissa/algebra/Hermite.java
svn pd svn:executable src/mantissa/src/org/spaceroots/mantissa/algebra/Legendre.java
svn pd svn:executable src/mantissa/src/org/spaceroots/mantissa/algebra/Polynomial.java
svn pd svn:executable src/mantissa/src/org/spaceroots/mantissa/algebra/PolynomialFraction.java
svn pd svn:executable src/mantissa/tests-src/org/spaceroots/mantissa/algebra/ChebyshevTest.java
svn pd svn:executable src/mantissa/tests-src/org/spaceroots/mantissa/algebra/HermiteTest.java
svn pd svn:executable src/mantissa/tests-src/org/spaceroots/mantissa/algebra/LegendreTest.java
svn pd svn:executable src/mantissa/tests-src/org/spaceroots/mantissa/algebra/PolynomialDoubleTest.java
svn pd svn:executable src/mantissa/tests-src/org/spaceroots/mantissa/algebra/PolynomialFractionTest.java
svn pd svn:executable src/mantissa/tests-src/org/spaceroots/mantissa/algebra/PolynomialRationalTest.java
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-02-04 20:30:58.257,,,false,,,,,,,,,,,,,,150413,,,Mon Feb 04 20:30:58 UTC 2008,,,,,,0|i0rvz3:,160824,,,,,,,,"04/Feb/08 20:30;luc;fixed
thanks for both the issue and the commands ;-)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MatrixUtils - are these supposed to have a public or package scope constructor?,MATH-187,12387756,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,sebb@apache.org,sebb@apache.org,04/Feb/08 01:40,23/Apr/09 02:26,07/Apr/19 20:38,10/Feb/08 21:11,,,,,,,,,,,0,,,,,,,,"Package or public?

{code}
    /**
     * Default constructor.  Package scope to prevent unwanted instantiation. 
     */
    public MatrixUtils() {
        super();
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-02-04 04:47:56.759,,,false,,,,,,,,,,,,,,150410,,,Sun Feb 10 21:11:01 UTC 2008,,,,,,0|i0rvzr:,160827,,,,,,,,"04/Feb/08 04:47;psteitz;The comment is wrong.  The constructor is public and needs to stay that way, though at this point we could deprecate it.","10/Feb/08 21:11;psteitz;Fixed in r 620330.
Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test results depend on java version,MATH-186,12387752,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,luc,luc,03/Feb/08 22:31,23/Apr/09 02:26,07/Apr/19 20:38,03/Feb/08 22:47,1.2,,,,,,,1.2,,,0,,,,,,,,"Running the tests with maven and changing the java version used by changing JAVA_HOME in the ~/.mavenrc file, I get different results.

With the Eclipse compiler set to 1.3 compatibility and with blackdown jvm (1.4), the tests succed.
With the Sun jvm (1.6), SummaryStatisticsAbstractTest.testEqualsAndHashCode (which is used both by SummaryStatisticsTest and SynchronizedSummaryStatisticsTest) fails.

The error is related to geometric mean computation, which lead to slightly different results depending on the order of added elements. One instance returns 2.213363839400643 and the other returns 2.2133638394006434. Both results are consistent with IEEE754 arithmetic (they differ in the last two bits).

Using Sun 1.6.0_03 jvm, the different values induce a test failure when SummaryStatistics.equals() method is called (it checks for exact equality). If this part of the test is commented out, another failure occurs when the SummaryStatistics.hashcode() method is called.

Changing the equals method would be possible, but would be a change of semantics and would imply choosing some threshold which would never suit everybody needs. Changing the hashcode method simply does not seem realistic to me. So I would like to keep these methods as they are now. So the main conclusion would be that the test is too sensitive to jvm implementation (which are consistent with IEEE754 arithmetic in this case).

I don't know what to do about this issue.","GNU/Linux (ubuntu Gutsy Gibbon),  java 1.3: eclipse compiler, java 1.4: Blackdown-1.4.2-02, java 1.6:  SUN 1.6.0_03-b05,  AMD athlon XP2000+",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-02-03 22:44:06.256,,,false,,,,,,,,,,,,,,150409,,,Sun Feb 03 22:54:45 UTC 2008,,,,,,0|i0rvzz:,160828,,,,,,,,"03/Feb/08 22:44;psteitz;I think the test should be modified to add data in the same order in each case, which will ensure equals semantics works.  The assumption that arithmetic operations are commutative is not true and the test in its current form depends on that.","03/Feb/08 22:47;luc;Changed slightly the test to be less sensitive to JVM issues.
The change was simply to add the points in a different order. So instead of having one instance handling values 2, 1, 3, 4 and the other handling values 4, 2, 3, 1, the second instance now handles 2, 3, 1, 4.
It works, but it is definitely not a clean solution. The problem may happen again with other jvm implementations.","03/Feb/08 22:53;luc;Sorry Phil, I did not see your proposal before sending my last comment.
You are right, I will commit a new fix for this using exactly the same order (2, 1, 3, 4) for both instances. This is a real solution, not a dirty trick like mine.
Thanks","03/Feb/08 22:54;psteitz;For this to happen with other JVM impls with the change just made, the jvm would have to produce different results for the exact same computations, which would be odd.  Equals is strictly defined for this class, which I think is OK and the test is really supposed to just be testing equals and hashcode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
using an empty file to ValueServer in REPLAY_MODE triggers a NULL_POINTER_EXCEPTION,MATH-185,12387712,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,luc,luc,02/Feb/08 17:57,23/Apr/09 02:26,07/Apr/19 20:38,02/Feb/08 18:02,1.2,,,,,,,1.2,,,0,,,,,,,,"when the URL provided to ValueServer.setValuesFileURL() contains no data,
subsequent calls to ValueServer.getNext() in replay mode triggers an exception at the
end of the private method getNextReplay.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,34134,,,Sat Feb 02 18:02:08 UTC 2008,,,,,,0|i0rw07:,160829,,,,,,,,02/Feb/08 18:02;luc;fixed as of r617850,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cumulativeProbability((double)n, (double)n) returns 0 for integer distributions",MATH-184,12387678,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,yegor,yegor,01/Feb/08 22:17,23/Apr/09 02:26,07/Apr/19 20:38,11/Feb/08 01:05,1.1,1.2,,,,,,1.2,,,0,,,,,,,,"cumulativeProbability((double)n, (double)n) returns 0 for
discrete/integer distributions

I suppose AbstractIntegerDistribution.cumulativeProbability(double,
double) should be overridden to call its (int, int) version instead of
using default one from AbstractDistribution",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2008-02-11 01:05:28.653,,,false,,,,,,,,,,,,,,150408,,,Mon Feb 11 01:05:28 UTC 2008,,,,,,0|i0rw0f:,160830,,,,,,,,"11/Feb/08 01:05;psteitz;Fixed in r 620368.
Thanks for reporting this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Findbugs Report,MATH-183,12387636,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,01/Feb/08 14:25,23/Apr/09 02:26,07/Apr/19 20:38,03/Feb/08 21:41,,,,,,,,1.2,,,0,,,,,,,,"The attachment (to follow) is a summary of a Findbugs run.

It's possible that the use of == for comparing floats is intended; if so perhaps it should be commented.

I think all the other bug reports are valid, though of course the ones relating to the exposure of internal implementation may be ignored",,,,,,,,,,,,,,,,,,,,,01/Feb/08 14:27;sebb@apache.org;MathFindbugs.csv;https://issues.apache.org/jira/secure/attachment/12374555/MathFindbugs.csv,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2008-02-01 23:43:01.445,,,false,,,,,,,,,,,,,,150407,,,Sun Feb 03 21:41:36 UTC 2008,,,,,,0|i0rw0n:,160831,,,,,,,,01/Feb/08 14:27;sebb@apache.org;CSV file summarising Findbugs report,"01/Feb/08 23:43;luc;The first two warnings about the non final TOO_SMALL fields have been discussed on the list (see http://markmail.org/message/tnxlul6qfjzs3mr3). Phil proposed to comment this in the release notes.

The next four floating point equality warnings and the last four internal representation exposed are intentional. They are identified in a commented findbugs exclusion filter in the subversion tree (file findbugs-exclude-filter.xml added about three weeks ago). This filter is active when findbugs report is created from maven2 (see plugin configuration in pom.xml).

The null pointer possible dereference was also discussed here (see http://markmail.org/message/33jpeig4qjmvtvy7). I forgot to check after Phil answered. I'll do it.

I'll fix the static inner class in Frequency if possible without breaking compatibility.

I understand the concerns about synchronisation, but would prefer someone else check them also.

As a summary, only the first two warnings should remain at the end.
Thanks for the report.
","01/Feb/08 23:56;sebb@apache.org;OK.

However, I suggest that the floating point equality tests should be commented in the actual code, as it's an unusual idiom.

As to synchronisation, it depends partly on whether the Math classes are intended to be thread-safe or not.","03/Feb/08 19:15;luc;I have looked again at the synchronisation issues.
The one in ResizableDoubleArray was straightforward and fixed.
The ones in SummaryStatistics are more difficult. They are due to the various setters and getters for implementations (set*Impl and get*Impl) which are synchronized, whereas other methods using these implementations are not synchronized.  In fact, there are more issues than the two appearing in the report: if we fix these two, a bunch of other problems appear, which should also be fixed. At the end, almost all methods end up being synchronized. This seems not to be the objective of this class but rather the objective of the SynchronizedSummaryStatistics.
So I would suggest moving the synchronization on the setters/getters from the SummaryStatistics class to the SynchronizedSummaryStatistics. derived class.
What do you think ?","03/Feb/08 19:44;sebb@apache.org;Yes, making SynchronizedSummaryStatistics fully sync, and removing synch from SummaryStatistics seems like a good idea, as the latter class is neither one thing nor the other.",03/Feb/08 19:55;psteitz;I agree that moving synch to the synch classes is better.,03/Feb/08 21:41;luc;synchronized setters/getters have been moved to SynchronizedSummaryStatistics class as of r618097,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
converting some double numbers to Fraction can lead to integer overflows,MATH-182,12387627,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,luc,luc,luc,01/Feb/08 11:59,23/Apr/09 02:26,07/Apr/19 20:38,01/Feb/08 12:03,1.2,,,,,,,1.2,,,0,,,,,,,,"converting 0.75000000001455192 leads to an overflow at the third iteration
 converting 1.0e10 leads to an overflow before the loop (while computiong a0)","GNU/Linux, Sun Java 1.6",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,34223,,,Fri Feb 01 12:03:18 UTC 2008,,,,,,0|i0rw0v:,160832,,,,,,,,01/Feb/08 12:03;luc;fixed as of r617482,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"chiSquare(double[] expected, long[] observed) is returning incorrect test statistic",MATH-175,12383845,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,carlanderson,carlanderson,04/Dec/07 02:31,23/Apr/09 02:26,07/Apr/19 20:38,09/Jan/08 05:17,1.1,,,,,,,1.2,,,0,,,,,,,,"ChiSquareTestImpl is returning incorrect chi-squared value. An implicit assumption of public double chiSquare(double[] expected, long[] observed) is that the sum of expected and observed are equal. That is, in the code:
for (int i = 0; i < observed.length; i++) {
            dev = ((double) observed[i] - expected[i]);
            sumSq += dev * dev / expected[i];
        }
this calculation is only correct if sum(observed)==sum(expected). When they are not equal then one must rescale the expected value by sum(observed) / sum(expected) so that they are.
Ironically, it is an example in the unit test ChiSquareTestTest that highlights the error:

long[] observed1 = { 500, 623, 72, 70, 31 };
        double[] expected1 = { 485, 541, 82, 61, 37 };
        assertEquals( ""chi-square test statistic"", 16.4131070362, testStatistic.chiSquare(expected1, observed1), 1E-10);
        assertEquals(""chi-square p-value"", 0.002512096, testStatistic.chiSquareTest(expected1, observed1), 1E-9);

16.413 is not correct because the expected values do not make sense, they should be: 521.19403 581.37313  88.11940  65.55224  39.76119 so that the sum of expected equals 1296 which is the sum of observed.

Here is some R code (r-project.org) which proves it:
> o1
[1] 500 623  72  70  31
> e1
[1] 485 541  82  61  37
> chisq.test(o1,p=e1,rescale.p=TRUE)

        Chi-squared test for given probabilities

data:  o1 
X-squared = 9.0233, df = 4, p-value = 0.06052

> chisq.test(o1,p=e1,rescale.p=TRUE)$observed
[1] 500 623  72  70  31
> chisq.test(o1,p=e1,rescale.p=TRUE)$expected
[1] 521.19403 581.37313  88.11940  65.55224  39.76119





 ",windows xp,,,,,,,,,,,,,,,,,,,,04/Dec/07 02:33;carlanderson;chi.xls;https://issues.apache.org/jira/secure/attachment/12370901/chi.xls,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-12-05 03:58:31.463,,,false,,,,,,,,,,,,,,34211,,,Wed Jan 09 05:17:45 UTC 2008,,,,,,0|i0rw2f:,160839,,,,,,,,"04/Dec/07 02:33;carlanderson;here is the calculations currently in the code, and below the correct calculations","05/Dec/07 03:58;psteitz;Thank you for reporting this.  I agree that if the count sums are unequal, the test is not meaningful (at least I can't see a meaningful interpretation).  So the question is, do we throw IllegalArgumentException in this case or assume expected should be rescaled? ","05/Dec/07 17:43;carlanderson;Hi Phil,

I coded a rescaling, as below, but I have to admit that I spent a long
time puzzling over why results from Java differed from those with R
because neither threw an exception or any warning that the argument sums
differed. It just didn't occur to me at first that this was an issue.

Carl


package com.archimedesmodel.automation.stats;

import org.apache.commons.math.stat.inference.ChiSquareTestImpl;

public class ArchiChiSquared extends ChiSquareTestImpl {

	public double chiSquare(double[] expected, long[] observed)
			throws IllegalArgumentException {
		double sumSq = 0.0d;
		double dev = 0.0d;
		if ((expected.length < 2) || (expected.length !=
observed.length)) {
			throw new IllegalArgumentException(
					""observed, expected array
lengths incorrect"");
		}

		double sumObs = 0;
		for (int i = 0; i < observed.length; i++) {
			sumObs += observed[i];
			if (observed[i] < 0) {
				throw new IllegalArgumentException(
						""observed counts must be
non-negative"");
			}
		}

		double sumExp = 0;
		for (int i = 0; i < expected.length; i++) {
			sumExp += expected[i];
			if (expected[i] <= 0) {
				throw new IllegalArgumentException(
						""expected counts must be
postive"");
			}
		}

		double ratio = 1.0;
		if (Double.compare(sumObs, sumExp) != 0) {
			//log some warning?
			ratio = sumObs / sumExp;
		}

		for (int i = 0; i < observed.length; i++) {
			dev = ((double) observed[i] - ratio *
expected[i]);
			sumSq += dev * dev / (ratio * expected[i]);
		}
		return sumSq;
	}

}







",09/Jan/08 05:17;psteitz;Automatic rescaling fix committed in r610274. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mean.evaluate() should use a two-pass algorithm,MATH-174,12383747,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,psteitz,psteitz,02/Dec/07 19:15,23/Apr/09 02:26,07/Apr/19 20:38,08/Dec/07 03:02,1.0,1.1,,,,,,1.2,,,0,,,,,,,,"Since it has access to the full array of stored data, Mean.evaluate(double[]) can improve its accuracy by executing a two-pass algorithm, first computing an initial estimate using the definitional forumla and then correcting that value by the mean deviation against that value.  The attached patch makes the correction and includes an algorithm reference.  It also re-activates and increases sensitivity in some of the certified data tests.  The Michelson data test fails with the current implementation, showing a difference in the 13th significant digit.

This change will improve accuracy in DescriptiveStatistics.getMean and also in Variance.evaluate() and DescriptiveStatistics.getVariance().  There is a cost associated with the change - as it roughly doubles the arithmetic operations required to compute the mean.  Since it only applies to the ""stored array"" implementation and the implementation will be pluggable in 1.2,  my preference is to move ahead with this change.",,,,,,,,,,,,,,,,,,,,,02/Dec/07 19:17;psteitz;mean.patch;https://issues.apache.org/jira/secure/attachment/12370797/mean.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-12-02 19:47:40.444,,,false,,,,,,,,,,,,,,150401,,,Sat Dec 08 03:02:34 UTC 2007,,,,,,0|i0rw2n:,160840,,,,,,,,"02/Dec/07 19:47;luc;I agree with you, go ahead with this change.",08/Dec/07 03:02;psteitz;Fixed in r602306.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConvergenceException in normal CDF,MATH-167,12372594,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,uttumuttu,uttumuttu,28/Jun/07 14:50,23/Apr/09 02:26,07/Apr/19 20:38,19/Nov/07 01:18,,,,,,,,1.2,,,0,,,,,,,,"NormalDistributionImpl::cumulativeProbability(double x) throws ConvergenceException
if x deviates too much from the mean. For example, when x=+/-100, mean=0, sd=1.
Of course the value of the CDF is hard to evaluate in these cases,
but effectively it should be either zero or one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-07-07 15:35:10.686,,,false,,,,,,,,,,,,,,34194,,,Mon Nov 19 01:18:28 UTC 2007,,,,,,0|i0rw3z:,160846,,,,,,,,"07/Jul/07 15:35;psteitz;Thanks for reporting this.  I see three alternatives to address - appreciate comments.
1) Determine tail resolution possible with current impl (hopefully not different on different JDKs, platforms) and ""top code"", checking arguments and returning 0 or 1, resp if argument is too far in SD units from the mean.  To find the cut points, empirically determine where convergence starts to fail.  Document the cut points in javadoc for Impl.
2) Catch ConvergenceException and return 0 or 1, resp if argument is far from the mean; rethrow otherwise (though this should never happen).
3) Resolve as WONTFIX and leave it to client to catch and handle ConvergenceException, examining argument.  Document algorithm more fully and warn that ConvergenceException will be thrown if tail probability cannot be accurately estimated or distinguished from 0.
My first thought was 2 and I guess I still favor that, since 3) is inconvenient for users and 1) may not be stable unless cut points are conservative.
Note that this same problem may apply to tail probablilities of other continuous distributions and we should check and address all of these before resolving this issue.


","22/Jul/07 02:33;psteitz;Fixed for nomal distribution in r558450.
Leaving open because we should look at other distributions before closing.  ","19/Nov/07 01:18;psteitz;While the t, F and Gamma distributions all use convergents (which may fail to converge for tail probabilities), there is no obvious way (to me at least) to test and set uniform bounds as we did for the Normal distribution in the fix applied in r558450.  Since this issue was reported against the Normal distribution, I am resolving this issue as fixed.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Special functions not very accurate,MATH-166,12370828,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,ltheussl,ltheussl,04/Jun/07 09:34,23/Apr/09 02:26,07/Apr/19 20:38,20/Jun/07 22:24,1.1,,,,,,,1.2,,,0,,,,,,,,"The Gamma and Beta functions return values in double precision but the default epsilon is set to 10e-9. I think that the default should be set to the highest possible accuracy, as this is what I'd expect to be returned by a double precision routine. Note that the erf function already uses a call to Gamma.regularizedGammaP with an epsilon of 1.0e-15.",,,,,,,,,,,,,,,,,,,,,04/Jun/07 09:36;ltheussl;MATH-166.patch;https://issues.apache.org/jira/secure/attachment/12358839/MATH-166.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2007-06-11 05:15:23.526,,,false,,,,,,,,,,,,,,34174,,,Wed Jun 20 22:24:30 UTC 2007,,,,,,0|i0rw47:,160847,,,,,,,,04/Jun/07 09:36;ltheussl;Patch to increase the default accuracy and corresponding tests.,11/Jun/07 05:15;psteitz;I see no reason not to apply this patch for 1.2.,20/Jun/07 22:24;psteitz;Patch applied.  Thanks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The evaluate method and the getResult method of class Variance give different results,MATH-163,12366130,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Won't Fix,,nsmeets,nsmeets,29/Mar/07 14:25,23/Apr/09 02:26,07/Apr/19 20:38,11/Jun/07 04:47,1.1,,,,,,,1.2,,,0,,,,,,,,"Consider the following test code:

  // construct an array of input values, containing infinity  
  double[] values = new double[] {1.0, 2.0, Double.POSITIVE_INFINITY};
  // find the variance using Variance.evaluate(double[])
  Variance var1 = new Variance();
  double value1 = var1.evaluate(values);
  // find the variance using Variance.getResult()
  Variance var2 = new Variance();
  var2.incrementAll(values);
  double value2 = var2.getResult();
  // print out the results
  System.out.println(value1);
  System.out.println(value2);

This code prints out:

NaN
Infinity

So, we get two different variances, depending on the method we use. 
(The same is true when we use Double.NEGATIVE_INFINITY as input value instead of Double.POSITIVE_INFINITY.)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,2007-03-29 15:18:50.243,,,false,,,,,,,,,,,,,,34199,,,Mon Jun 11 04:47:32 UTC 2007,,,,,,0|i0rw4v:,160850,,,,,,,,"29/Mar/07 15:18;rwinston@eircom.net;I guess the correct answer is Inf - although I can see that some other packages return NaN. For instance in R:

> x=c(1.0,2.0,Inf)
> var(x)
[1] NaN
","01/Apr/07 20:20;psteitz;Thanks for reporting this.  I agree with Rory that the spirit of IEEE754 (which says examine limit as x -> INF when evaluating expressions involving INF) implies the result of this computation should be positive infinity in this particular case, as the getResult() method gives.  For reasons described below, it may be difficult, however, to correctly handle all INF cases without impacting performance, so I am leaning toward WONTFIX at this point; though open to suggestions / patches.

The reason that the results of the two methods are different is they use different computing formulas.  The getResult method is meant to be used when the data is not persisted - i.e., after repeatedly calling increment, supplying values in a stream (and updating sums), but not storing the whole set of values.  It therefore uses a ""one pass"" algorithm (""West's algorithm"", referenced in javadoc) to compute the variance.  The evaluate method exploits the fact that it has the full array of values supplied and uses a two-pass method (""corrected two-pass algorithm"" from Chan, Golub, Levesque, Algorithms for Computing the Sample Variance, American Statistician, August 1983).  These methods may give different results in some examples, with the second more accurate.  The javadoc should be improved to make this clearer and to recommend that evaluate  should be preferred over incrementAll-getResult when the full array of values is available.  That I will do.",11/Jun/07 04:47;psteitz;Javadoc clarification has been made. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RandomDataImpl nextInt(int, int) nextLong(long, long)",MATH-153,12346925,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,remi_arntzen,remi_arntzen,31/Jul/06 03:48,23/Jul/12 23:24,07/Apr/19 20:38,05/Apr/07 15:20,1.1,,,,,,,1.2,,,0,,,,,,,,"RandomDataImpl.nextInt(Integer.MIN_VALUE, Integer.MAX_VALUE) suffers from overflow errors.

change
return lower + (int) (rand.nextDouble() * (upper - lower + 1));
to
return (int) (lower + (long) (rand.nextDouble()*((long) upper - lower + 1)));

additional checks must be made for the nextlong(long, long) method.
At first I thought about using MathUtils.subAndCheck(long, long) but there is only an int version avalible, and the problem is that subAndCheck internaly uses long values to check for overflow just as my previous channge proposes.  The only thing I can think of is using a BigInteger to check for the number of bits required to express the difference.",,,,,,,,,,,,,,,,,,,,,09/Aug/06 05:58;remi_arntzen;Test.diff;https://issues.apache.org/jira/secure/attachment/12338445/Test.diff,08/Aug/06 21:47;remi_arntzen;diff.txt;https://issues.apache.org/jira/secure/attachment/12338418/diff.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,2006-08-06 15:01:15.0,,,false,,,,,,,,,,,,,,34179,,,Thu Apr 05 15:20:45 UTC 2007,,,,,,0|i0rw73:,160860,,,,,,,,"31/Jul/06 04:16;remi_arntzen;once Math-154 (MathUtils addAndCheck and subAndCheck for long values)
is commited nextLong(long, long) could become

public long nextLong(long lower, long upper) {
    if (lower >= upper) {
        throw new IllegalArgumentException(
                ""upper bound must be > lower bound"");
    }
    RandomGenerator rand = getRan();
    boolean overFlow = false;
    try {
        MathUtils.subAndCheck(upper, lower);
    } catch (ArithmeticException thrown) {
        overFlow = true;
    }
    
    long returnValue;
    
    if (upper - lower == Long.MAX_VALUE) {
        //upper - lower + 1 = Long.MAX_VALUE + 1 = *overflow error*;
        overFlow = true;
    }
    
    if (!overFlow) {
        returnValue = lower + (long) (rand.nextDouble() * (upper -
                                                           lower + 1)
                                                          );
    } else {
        returnValue = rand.nextLong();
        while (returnValue > upper || returnValue < lower) {
            returnValue = rand.nextLong();
        }
    }
    return returnValue;
}","06/Aug/06 15:01;psteitz;Agree this is a bug and should be fixed.   Thanks for reporting this issue.

Using the added method in MATH-154 to detect overflow in the long case is good.

There should be a way to avoid the while loop at the end of the long impl.

We also need test cases to close this.
","07/Aug/06 13:19;brentworden;I think we can avoid the overflow conditions simply by distributing the multiplication of the random value.  With this, the method body would become:

double r = rand.nextDouble();
return (int)((r * upper) + ((1.0 - r) * lower) + r);
",05/Apr/07 15:20;brentworden;SVN 525842: Corrected nextInt and nextLong to handle wide value ranges.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MathUtils.round incorrect result,MATH-151,12343947,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,buzaz,buzaz,05/Jun/06 22:22,23/Apr/09 02:26,07/Apr/19 20:38,04/Jul/06 15:19,1.1,,,,,,,,,,0,,,,,,,,"MathUtils.round(39.245, 2) results 39.24, however it should be 39.25, with default rounding mode BigDecimal.ROUND_HALF_UP.

I found that internally MathUtils.round multiplies the given number by 10^scale.
 39.245 * 100.0 results 3924.49999...5 , and after this the calculation is not correct any more.","Win2K, Sun JDK1.5.0_05 b05",,,,,,,,,,,,,,,,,,,,29/Jun/06 03:12;luc;math-151.patch;https://issues.apache.org/jira/secure/attachment/12336090/math-151.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,2006-06-06 20:40:32.0,,,false,,,,,,,,,,,,,,150383,,,Tue Jul 04 15:19:27 UTC 2006,,,,,,0|i0rw7j:,160862,,,,,,,,06/Jun/06 20:40;psteitz;Thanks for reporting this bug.  Could be we want to reconsider the changes applied to address http://issues.apache.org/jira/browse/MATH-32.  Patches welcome!,"07/Jun/06 04:11;luc;I think this behaviour is normal.
Despite 39.245 has a finite representation in decimal, this is not true in base 2, it has a infinite number of digits. For the sake of simplicity, here is the representation in base 16 (much more readable than base 2) :

  0x27.3EB851EB851EB851EB851...

Note that the pattern EB851 repeats itself ad infinitum, in base 2 it is a 20 bits pattern.
If this number could be represented in a virtual infinite register, multiplying it by 100.0 (base 10) would be
multiplying it by 0x64 (base 16) and the result would be 0xF54.8 which is 3924.5 as could be rounded as expected.

However, primitive doubles are represented in Java using the IEEE754 norm, where the mantissa is 53 bits long, counting an implicit first bit. This implies that in the previous infinite number only the following is represented in IEE754:

  0x27.3EB851EB851

hence, when this number is multiplied by 100, we get the result:

  0xF54.7FFFFFFFFA4 or 3924.49999999999477040546480566263198...

So the error is exactly 0x0.0000000005C or 92/16^11 which is approximately 5.229e-12.

The problem comes from the fact the original number cannot be represented exactly (it is not what is sometimes called a ""floating-point number"" or ""normal number""), and in this case the first neglected hexadecimal digit is large (E) which explains the large error (7 wrong bits after the multiplication).

I would say this is not an issue with commons-math but with IEE754, we cannot do anything about it.

Note that the same would occur if we could add one pattern by adding 20 bits. In this case, the final number would be 3924.4999999999999999950126700065666 and the error appoximately 4.987e-18 (92/16^16), but still lesser than 3924.5","11/Jun/06 07:26;psteitz;Thanks for digging into this, Luc.

I agree that the source of the problem here is in the decimal - binary conversion; but I don't want to close this as ""invalid"" or ""wontfix"" until we have exhausted all practical means to address the problem from the user standpoint.  In other words, I would prefer to find a workaround if that is possible.  

One possibility is to revert to the implementation in R226479 or earlier.  See http://svn.apache.org/repos/asf/jakarta/commons/proper/math/trunk/src/java/org/apache/commons/math/util/MathUtils.java?revision=226479 
where we used BigDecimal rounding.  We changed this partly for efficiency, partly to address http://issues.apache.org/jira/browse/MATH-32, which we would have to address in some other way if we go that route.

Ideas or patches welcome!","11/Jun/06 16:04;luc;The point is neither on the rounding method not on the scale factor, it is rather on the initial number itself.
39.245 has no exact representation in IEE754. it lies between two representable numbers (I forgot 4 bits in my previous post):

      0x139f5c28f5c28f  * 2^-47  < 39.245  <  0x139f5c28f5c290 * 2^-47
 or 0x27.3eb851eb851e  <  39.245  <  27.3eb851eb8520
 or 39.2449999999999974420461512636...  <  39.245  <  39.2450000000000045474735088646...

When we talk about 39.245, we refer to the decimal representation that was parsed either by some data input function using something like Double.parseDouble(String) or we refer to a litteral value in the Java code, which in fact is also parsed at compilation time, probably also by double.parseDouble(String) or a similar function. The virtual machine doesn't see the 39.245 real number we want, it sees either 0x27.3EB851EB851E  or x27.3EB851EB8520 depending on the parsing behaviour. This is not a Java-related problem, it is also true for languages like C, C++, fortran, whatever.

In the case discussed here, the value was the low one (i.e. 0x27.3EB851EB851E, or 39.2449999999999974420461...). The parsing method did a good job here in my opinion as this number is the closest one to the real number we wanted (the error is about 2.55e-15 and it would have been 4.54e-15 if the other alternative were chosen).

Going back to the initial problem, and assuming we now start from 39.2449999999999974420461..., we want to round this number 2 digits after the decimal point. MathUtils.round answer is 39.24 (really 39.2400000000000019895...), which is ""only"" 0.0049999999999954525264... away from out number. Answering 39.25 (which CAN be represented exactly in IEEE754) would be ""0.005000000000002557953848..."" away. The initial number is not exactly at a 0.5 * 10^-n boundary, so switching between ROUND_HALF_DOWN, ROUND_HALF_UP or ROUND_HALF_EVEN does not change anything (I have checked this).
","11/Jun/06 23:57;psteitz;I understand and agree with your analysis of the IEEE754 representation, but I would still like to see if there is anyting clever that we can do to work around the problem.   Could be this is hopeless, but I am bothered by the fact that the previous implementation actually handles this correctly.  Sorry I messed up the link in the comment above to the earlier BigDecimal-based impl.  That should have been to 
http://svn.apache.org/viewvc/jakarta/commons/proper/math/trunk/src/java/org/apache/commons/math/util/MathUtils.java?revision=239294&view=markup

In any case, the impl there, modified to handle the special values included in later tests would be:
public static double round(double x, int scale, int roundingMethod) {
        try {
            return (new BigDecimal
                   (new Double(x).toString())
                   .setScale(scale, roundingMethod))
                   .doubleValue();
        } catch (NumberFormatException ex) {
            if (Double.isInfinite(x)) {
                return x;          
            } else {
                return Double.NaN;
            }
        }
    }

Before http://issues.apache.org/jira/browse/MATH-32, it was just
return (new BigDecimal(x).setScale(scale, roundingMethod))

The code above passes all tests, including even
double x = 39.0d;
x = x + 245d/1000d;
assertEquals(39.25,MathUtils.round(x, 2), 0.0);
","12/Jun/06 00:59;psteitz;Sorry, messed up the link yet again.  Best to just look at
http://svn.apache.org/viewvc/jakarta/commons/proper/math/trunk/src/java/org/apache/commons/math/util/MathUtils.java?view=log
or use a svn client to examine history. 

I also neglected to mention that part of the reason for the change to the new impl was performance.   I played with some microbenchmarks to confirm this some time ago, seeing about 5x-8x improvement in the direct impl.  It might be a good idea to do this testing rigorously.  

If we could characterize the inputs somehow that have ""bad"" IEEE754 representations, perhaps we could use the BigDecimal-based impl just for them.   I don't see any reliable way to do that, though.","15/Jun/06 09:31;luc;I was looking for a way to characterize those ""bad"" IEEE754 representations. Here is one proposal:
we could had an implementation of  the nextAfter method, either in MathUtils or in a new IeeeFunctions method in case we want also to add other interesting functions defined by the IEEE754 standard. The signature of this method is:

  static double nextAfter(double d, double direction)

It returns the next representable number after its first argument which lies on the same side as the second argument. Using this, we could compare the rounding of x and nextAfter(x, x+1) when the rounding mode is ROUND_HALF_UP. If the results is different, the IEEE754 representation of x is on some boundary. In fact, I think we could always return the rounding of nextAfter in this mode (after all if round(x) and round(nextAfter(x)) are the same, we could return either and if they are not the same, we want to return round(nextAfter, at least in some rounding methods). The test could be performed in the roundUnscaled method, and similar tests could be implemented in the other branches of the switch for other rounding methods.

This would be slower than the current implementation, but probably much faster than going back to rebuild a String and parsing it as a BigDecimal.

I have written an implementation of nextAfter. It is based on Double.doubleToLongBits, bits twiddling, and  Double.longBitsToDouble.
",20/Jun/06 14:03;psteitz;+1 to nextAfter as addition to MathUtils and for the approach outlined above for resolving this issue.  Thanks in advance for patches. ,"29/Jun/06 03:12;luc;This patch is an attempt to solve the issue.
The principle is to add a nexAfter method in MathUtils and to use it in order to very slightly shift the numbers (by one ulp) in the expected rounding direction in order to avoid some degenerate cases.
Note that if someone REALLY wants to round a number like 39.2449999999999974420461512636 and NOT 39.245, we will produce a wrong result. I didn't put any warning about this behaviour in the javadoc, but I think that if this patch is finally applied, the javadoc should be exlpicit.

I understand the user problem but do not really like answering like that, it seems more like an ad hoc trick to me. I'm not very proud of my first patch :-(","04/Jul/06 15:19;psteitz;Patch applied.  Thanks!

I understand that the patch is not really satisfying, but it makes the code better and resolves the reported problem.   A more elegant solution may be possible, but the patch improves the code and resolves the reported problem, so it should be applied.  If we find a better way to handle correct rounding of ""bad"" IEEE754 numbers, we can reopen or create a separate issue.

Thanks for the patch!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Comments Changes,MATH-112,12342480,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,zxg_32@yahoo.com,zxg_32@yahoo.com,17/Aug/05 03:59,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Index: TrapezoidIntegrator.java
===================================================================
--- TrapezoidIntegrator.java	(revision 233018)
+++ TrapezoidIntegrator.java	(working copy)
@@ -20,7 +20,7 @@
 
 /**
  * Implements the <a href=""http://mathworld.wolfram.com/TrapezoidalRule.html"">
- * Trapezoidal Rule</a> for integrating of real univariate functions. For
+ * Trapezoidal Rule</a> for integration of real univariate functions. For
  * reference, see <b>Introduction to Numerical Analysis</b>, ISBN 038795452X,
  * chapter 3.
  * <p>
@@ -39,7 +39,7 @@
     /**
      * Construct an integrator for the given function.
      * 
-     * @param f function to solve
+     * @param f function to integrate
      */
     public TrapezoidIntegrator(UnivariateRealFunction f) {
         super(f, 64);
Index: SimpsonIntegrator.java
===================================================================
--- SimpsonIntegrator.java	(revision 233018)
+++ SimpsonIntegrator.java	(working copy)
@@ -20,7 +20,7 @@
 
 /**
  * Implements the <a href=""http://mathworld.wolfram.com/SimpsonsRule.html"">
- * Simpson's Rule</a> for integrating of real univariate functions. For
+ * Simpson's Rule</a> for integration of real univariate functions. For
  * reference, see <b>Introduction to Numerical Analysis</b>, ISBN 038795452X,
  * chapter 3.
  * <p>
@@ -37,7 +37,7 @@
     /**
      * Construct an integrator for the given function.
      * 
-     * @param f function to solve
+     * @param f function to integrate
      */
     public SimpsonIntegrator(UnivariateRealFunction f) {
         super(f, 64);
Index: RombergIntegrator.java
===================================================================
--- RombergIntegrator.java	(revision 233018)
+++ RombergIntegrator.java	(working copy)
@@ -1,5 +1,5 @@
 /*
- * Copyright 2003-2005 The Apache Software Foundation.
+ * Copyright 2005 The Apache Software Foundation.
  *
  * Licensed under the Apache License, Version 2.0 (the ""License"");
  * you may not use this file except in compliance with the License.
@@ -20,7 +20,7 @@
 
 /**
  * Implements the <a href=""http://mathworld.wolfram.com/RombergIntegration.html"">
- * Romberg Algorithm</a> for integrating of real univariate functions. For
+ * Romberg Algorithm</a> for integration of real univariate functions. For
  * reference, see <b>Introduction to Numerical Analysis</b>, ISBN 038795452X,
  * chapter 3.
  * <p>
@@ -38,7 +38,7 @@
     /**
      * Construct an integrator for the given function.
      * 
-     * @param f function to solve
+     * @param f function to integrate
      */
     public RombergIntegrator(UnivariateRealFunction f) {
         super(f, 32);","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,36211.0,,,2005-08-25 07:26:51.0,,,false,,,,,,,,,,,,,,150344,,,Thu Aug 25 07:26:51 UTC 2005,,,,,,0|i0rwg7:,160901,,,,,,,,"17/Aug/05 04:02;zxg_32@yahoo.com;minor changes of comments
",25/Aug/05 07:26;j3322ptm@yahoo.de;Applied. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Add UnivariateRealIntegratorImpl.java,MATH-111,12342421,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,zxg_32@yahoo.com,zxg_32@yahoo.com,31/Jul/05 10:23,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Index: src/java/org/apache/commons/math/analysis/UnivariateRealIntegratorImpl.java
===================================================================
--- src/java/org/apache/commons/math/analysis/UnivariateRealIntegratorImpl.java
(revision 0)
+++ src/java/org/apache/commons/math/analysis/UnivariateRealIntegratorImpl.java
(revision 0)
@@ -0,0 +1,197 @@
+/*
+ * Copyright 2003-2005 The Apache Software Foundation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.commons.math.analysis;
+
+import java.io.Serializable;
+import org.apache.commons.math.FunctionEvaluationException;
+
+/**
+ * Provide a default implementation for several generic functions.
+ *  
+ * @version $Revision$ $Date: 2005-02-26 05:11:52 -0800 (Sat, 26 Feb 2005) $
+ */
+public abstract class UnivariateRealIntegratorImpl implements
UnivariateRealIntegrator,
+    Serializable {
+
+    /** serializable version identifier */
+    static final long serialVersionUID = -3365294665201465048L;
+
+    /** maximum relative error */
+    protected double relativeAccuracy;
+
+    /** maximum number of iterations */
+    protected int maximalIterationCount;
+
+    /** default maximum relative error */
+    protected double defaultRelativeAccuracy;
+
+    /** default maximum number of iterations */
+    protected int defaultMaximalIterationCount;
+
+    /** indicates whether an integral has been computed */
+    protected boolean resultComputed = false;
+
+    /** the last computed integral */
+    protected double result;
+
+    /** the last iteration count */
+    protected int iterationCount;
+
+    /** the integrand function */
+    protected UnivariateRealFunction f;
+
+    /**
+     * Construct an integrator with given iteration count and accuracy.
+     * 
+     * @param f the integrand function
+     * @param defaultMaximalIterationCount maximum number of iterations
+     * @param defaultRelativeAccuracy maximum relative error
+     * @throws IllegalArgumentException if f is null or the 
+     * defaultRelativeAccuracy is not valid
+     */
+    protected UnivariateRealIntegratorImpl(
+        UnivariateRealFunction f,
+        int defaultMaximalIterationCount,
+        double defaultRelativeAccuracy) {
+        
+        super();
+        
+        if (f == null) {
+            throw new IllegalArgumentException(""function can not be null."");
+        }
+        
+        this.f = f;
+        this.defaultRelativeAccuracy = defaultRelativeAccuracy;
+        this.relativeAccuracy = defaultRelativeAccuracy;
+        this.defaultMaximalIterationCount = defaultMaximalIterationCount;
+        this.maximalIterationCount = defaultMaximalIterationCount;
+    }
+
+    /**
+     * Access the last computed integral.
+     * 
+     * @return the last computed integral
+     * @throws IllegalStateException if no integral has been computed
+     */
+    public double getResult() {
+        if (resultComputed) {
+            return result;
+        } else {
+            throw new IllegalStateException(""No result available"");
+        }
+    }
+
+    /**
+     * Access the last iteration count.
+     * 
+     * @return the last iteration count
+     * @throws IllegalStateException if no integral has been computed
+     *  
+     */
+    public int getIterationCount() {
+        if (resultComputed) {
+            return iterationCount;
+        } else {
+            throw new IllegalStateException(""No result available"");
+        }
+    }
+
+    /**
+     * Convenience function for implementations.
+     * 
+     * @param result the result to set
+     * @param iterationCount the iteration count to set
+     */
+    protected final void setResult(double result, int iterationCount) {
+        this.result = result;
+        this.iterationCount = iterationCount;
+        this.resultComputed = true;
+    }
+
+    /**
+     * Convenience function for implementations.
+     */
+    protected final void clearResult() {
+        this.resultComputed = false;
+    }
+
+    /**
+     * Set the upper limit for the number of iterations.
+     * 
+     * @param count maximum number of iterations
+     */
+    public void setMaximalIterationCount(int count) {
+        maximalIterationCount = count;
+    }
+
+    /**
+     * Get the upper limit for the number of iterations.
+     * 
+     * @return the actual upper limit
+     */
+    public int getMaximalIterationCount() {
+        return maximalIterationCount;
+    }
+
+    /**
+     * Reset the upper limit for the number of iterations to the default.
+     */
+    public void resetMaximalIterationCount() {
+        maximalIterationCount = defaultMaximalIterationCount;
+    }
+
+    /**
+     * Set the relative accuracy.
+     * 
+     * @param accuracy the relative accuracy
+     * @throws IllegalArgumentException if the accuracy can't be achieved by
+     *  the integrator or is otherwise deemed unreasonable
+     */
+    public void setRelativeAccuracy(double accuracy) {
+        relativeAccuracy = accuracy;
+    }
+
+    /**
+     * Get the actual relative accuracy.
+     *
+     * @return the accuracy
+     */
+    public double getRelativeAccuracy() {
+        return relativeAccuracy;
+    }
+
+    /**
+     * Reset the relative accuracy to the default.
+     */
+    public void resetRelativeAccuracy() {
+        relativeAccuracy = defaultRelativeAccuracy;
+    }
+
+    /**
+     * Verifies that the endpoints specify an interval.
+     * 
+     * @param lower lower endpoint
+     * @param upper upper endpoint
+     * @throws IllegalArgumentException if not interval
+     */
+    protected void verifyInterval(double lower, double upper) {
+        if (lower >= upper) {
+            throw new IllegalArgumentException
+                (""Endpoints do not specify an interval: ["" + lower + 
+                        "","" + upper + ""]"");
+        }       
+    }
+}","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,31/Jul/05 10:30;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--UnivariateRealIntegratorImpl.java;https://issues.apache.org/jira/secure/attachment/12333653/ASF.LICENSE.NOT.GRANTED--UnivariateRealIntegratorImpl.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,35950.0,,,2005-08-01 05:20:22.0,,,false,,,,,,,,,,,,,,150343,,,Sun Aug 14 15:54:20 UTC 2005,,,,,,0|i0rwgf:,160902,,,,,,,,"31/Jul/05 10:30;zxg_32@yahoo.com;Created an attachment (id=15833)
New Source File UnivariateRealIntegratorImpl.java
","01/Aug/05 05:20;phil@steitz.com;These classes look good. Thanks!

Lets use *this* ticket to track these changes (combine with 35949).  Pls add new
files as attachments only.  Will commit when concrete implementation and test
cases have been added.","14/Aug/05 15:54;phil@steitz.com;

*** This bug has been marked as a duplicate of 36148 ***",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] TestStatistic interface, implementation, tests",MATH-110,12340711,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,16/May/03 04:30,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Includes only chi-square test statistic, used in testing homogeneity of random
sequences.  Used in test cases for RandomData to follow.

Illustrates proposed documentation style.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,16/May/03 04:34;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--MathTestSuitePatch.txt;https://issues.apache.org/jira/secure/attachment/12332275/ASF.LICENSE.NOT.GRANTED--MathTestSuitePatch.txt,16/May/03 04:31;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--TestStatistic.java;https://issues.apache.org/jira/secure/attachment/12332272/ASF.LICENSE.NOT.GRANTED--TestStatistic.java,16/May/03 04:31;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--TestStatisticImpl.java;https://issues.apache.org/jira/secure/attachment/12332273/ASF.LICENSE.NOT.GRANTED--TestStatisticImpl.java,16/May/03 04:33;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--TestStatisticTest.java;https://issues.apache.org/jira/secure/attachment/12332274/ASF.LICENSE.NOT.GRANTED--TestStatisticTest.java,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,19971.0,,,2003-05-16 05:03:12.0,,,false,,,,,,,,,,,,,,150342,,,Fri May 16 05:03:12 UTC 2003,,,,,,0|i0rwgn:,160903,,,,,,,,"16/May/03 04:31;phil@steitz.com;Created an attachment (id=6375)
TestStatistic Interface
","16/May/03 04:31;phil@steitz.com;Created an attachment (id=6376)
TestStatistic Implementation
","16/May/03 04:33;phil@steitz.com;Created an attachment (id=6377)
Test Statistic Test cases
","16/May/03 04:34;phil@steitz.com;Created an attachment (id=6378)
Patch to include test cases
","16/May/03 05:03;tobrien@discursive.com;All patches applied, exception the final patch to the test suite which
referenced a non-existant class RandomUtilTest.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong p-value in SimpleRegression (factor 2 difference),MATH-109,12341827,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,kim@kimvdlinde.com,kim@kimvdlinde.com,23/Oct/04 10:52,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"SimpleRegression gives back wrong P-value. (when test data set is compared to
STATISTICA software package output. Error is likely one-sided versus two-sided
testing issue:
OLD:
    public double getSignificance() throws MathException {
        return (
            1.0 - getTDistribution().cumulativeProbability(
                    Math.abs(getSlope()) / getSlopeStdErr()));
    }

FIXED:
    public double getSignificance() throws MathException {
        return 2d*(
            1.0 - getTDistribution().cumulativeProbability(
                    Math.abs(getSlope()) / getSlopeStdErr()));
    }","Operating System: other
Platform: All",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,31860.0,,,2004-10-24 02:56:37.0,,,false,,,,,,,,,,,,,,34138,,,Sun Oct 24 02:56:37 UTC 2004,,,,,,0|i0rwgv:,160904,,,,,,,,24/Oct/04 02:56;phil@steitz.com;This is a bug. Returned significance does not match javadoc.  Good catch.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Update UnivariateRealSolverImpl.java,MATH-108,12342486,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,zxg_32@yahoo.com,zxg_32@yahoo.com,18/Aug/05 02:51,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Index: UnivariateRealSolverImpl.java
===================================================================
--- UnivariateRealSolverImpl.java       (revision 233212)
+++ UnivariateRealSolverImpl.java       (working copy)
@@ -265,7 +265,8 @@
      */
     protected boolean isBracketing(double lower, double upper,
             UnivariateRealFunction f) throws FunctionEvaluationException {
-        return  (f.value(lower) * f.value(upper) < 0);
+        return ((f.value(lower) > 0 && f.value(upper) < 0) ||
+                (f.value(lower) < 0 && f.value(upper) > 0));
     }

     /**","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,18/Aug/05 02:57;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--UnivariateRealSolverImpl.java;https://issues.apache.org/jira/secure/attachment/12333725/ASF.LICENSE.NOT.GRANTED--UnivariateRealSolverImpl.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,36232.0,,,2005-08-18 05:54:32.0,,,false,,,,,,,,,,,,,,150341,,,Tue Aug 23 12:05:20 UTC 2005,,,,,,0|i0rwh3:,160905,,,,,,,,"18/Aug/05 02:56;zxg_32@yahoo.com;isBracketing() should use condition branching instead of multiplication because

1. saves one unnecessary floating-point multiplication
2. prevent underflow, for example
    double a = 4.5E-180;
    double b = -3.2E-180;
    double c = a * b;
   c would be -0.0, but in fact a and b still satisfy bracketing condition.
","18/Aug/05 02:57;zxg_32@yahoo.com;Created an attachment (id=16083)
UnivariateRealSolverImpl.java
","18/Aug/05 05:54;j3322ptm@yahoo.de;Well, commonly the cost of a single floating point operation can be safely
neglected compared to the cost of evaluating the function in question.","19/Aug/05 08:54;zxg_32@yahoo.com;Actually it should be like this. Maybe the compiler will optimize
to avoid repeated function evaluations.


Index: UnivariateRealSolverImpl.java
===================================================================
--- UnivariateRealSolverImpl.java       (revision 233396)
+++ UnivariateRealSolverImpl.java       (working copy)
@@ -265,7 +265,9 @@
      */
     protected boolean isBracketing(double lower, double upper,
             UnivariateRealFunction f) throws FunctionEvaluationException {
-        return  (f.value(lower) * f.value(upper) < 0);
+        double f1 = f.value(lower);
+        double f2 = f.value(upper);
+        return ((f1 > 0 && f2 < 0) || (f1 < 0 && f2 > 0));
     }

     /**
","19/Aug/05 08:57;zxg_32@yahoo.com;floating point multiplication is not big deal, underflow is not
frequent, but may happen in some cases.","23/Aug/05 05:59;phil@steitz.com;I am +1 for the last patch. Agree that function evals are what we should be
looking to minimize, but underflow looks like legitimate concern for current code.","23/Aug/05 12:05;brentworden;Applied.  SVN Revision 239305.  Applied to MATH_1_1 branch.

Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] wrong message in subAndCheck,MATH-107,12342632,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,elharo@metalab.unc.edu,elharo@metalab.unc.edu,14/Oct/05 23:57,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,1.1,,,,,,,,,,0,,,,,,,,"In MathUtils I found this code:

    public static int subAndCheck(int x, int y) {
        long s = (long)x - (long)y;
        if (s < Integer.MIN_VALUE || s > Integer.MAX_VALUE) {
            throw new ArithmeticException(""overflow: add"");
        }
        return (int)s;
    }

The message should be ""overflow: subtract""","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,37093.0,,,2005-10-16 07:34:22.0,,,false,,,,,,,,,,,,,,150340,,,Sun Oct 16 07:34:22 UTC 2005,,,,,,0|i0rwhb:,160906,,,,,,,,"15/Oct/05 00:02;elharo@metalab.unc.edu;Here's a test case for the bug. The fix is trivial.

    public void testSubAndCheckErrorMessage() {
        int big = Integer.MAX_VALUE;
        int bigNeg = Integer.MIN_VALUE;
        try {
            int res = MathUtils.subAndCheck(big, -1);
        } catch (ArithmeticException ex) {
            assertEquals(""overflow: subtract"", ex.getMessage());
        }
    }",16/Oct/05 07:34;phil@steitz.com;Patch applied.  Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] update task list,MATH-106,12340758,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,05/Jun/03 21:07,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached path the tasks.xml moves the following tasks into the completed bucket:

Binomial coefficients -- incorporate an ""exact"" implementation that is
limited to what can be stored in a long.  Also provided double-valued
implementation of binomial coefficients and their logs.

Add percentiles to stored Univariate implementations

It also includes some edits to the open tasks","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,05/Jun/03 21:08;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--taskListPatch.txt;https://issues.apache.org/jira/secure/attachment/12332310/ASF.LICENSE.NOT.GRANTED--taskListPatch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20507.0,,,2003-06-05 21:16:22.0,,,false,,,,,,,,,,,,,,150339,,,Thu Jun 05 21:16:22 UTC 2003,,,,,,0|i0rwhj:,160907,,,,,,,,"05/Jun/03 21:08;phil@steitz.com;Created an attachment (id=6646)
updates to task list
","05/Jun/03 21:16;mdiggory@gmail.com;I applied the patch and validated xml content. Ahh, progress! Maybe we should
consider ""classifying some of these tasks priorities now to get a grasp on
priority, otherwise the high/medium/low priority sections are just taking up space.

-Mark",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Add UnivariateRealIntegrator.java,MATH-105,12342420,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,zxg_32@yahoo.com,zxg_32@yahoo.com,31/Jul/05 10:22,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Index: src/java/org/apache/commons/math/analysis/UnivariateRealIntegrator.java
===================================================================
--- src/java/org/apache/commons/math/analysis/UnivariateRealIntegrator.java
(revision 0)
+++ src/java/org/apache/commons/math/analysis/UnivariateRealIntegrator.java
(revision 0)
@@ -0,0 +1,124 @@
+/*
+ * Copyright 2003-2005 The Apache Software Foundation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the ""License"");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.commons.math.analysis;
+
+import org.apache.commons.math.ConvergenceException;
+import org.apache.commons.math.FunctionEvaluationException;
+
+/**
+ * Interface for univariate real integration algorithms.
+ *  
+ * @version $Revision$ $Date: 2005-02-26 05:11:52 -0800 (Sat, 26 Feb 2005) $
+ */
+public interface UnivariateRealIntegrator {
+
+    /**
+     * Set the upper limit for the number of iterations.
+     * <p>
+     * Usually a high iteration count indicates convergence problem. However,
+     * the ""reasonable value"" varies widely for different cases.  Users are
+     * advised to use the default value.
+     * <p>
+     * A <code>ConvergenceException</code> will be thrown if this number
+     * is exceeded.
+     *  
+     * @param count maximum number of iterations
+     */
+    void setMaximalIterationCount(int count);
+
+    /**
+     * Get the upper limit for the number of iterations.
+     * 
+     * @return the actual upper limit
+     */
+    int getMaximalIterationCount();
+
+    /**
+     * Reset the upper limit for the number of iterations to the default.
+     * <p>
+     * The default value is supplied by the implementation.
+     * 
+     * @see #setMaximalIterationCount(int)
+     */
+    void resetMaximalIterationCount();
+
+    /**
+     * Set the relative accuracy.
+     * <p>
+     * This is used to stop iterations.
+     * 
+     * @param accuracy the relative accuracy
+     * @throws IllegalArgumentException if the accuracy can't be achieved
+     * or is otherwise deemed unreasonable
+     */
+    void setRelativeAccuracy(double accuracy);
+
+    /**
+     * Get the actual relative accuracy.
+     *
+     * @return the accuracy
+     */
+    double getRelativeAccuracy();
+
+    /**
+     * Reset the relative accuracy to the default.
+     * <p>
+     * The default value is provided by the implementation.
+     *
+     * @see #setRelativeAccuracy(double)
+     */
+    void resetRelativeAccuracy();
+
+    /**
+     * Integrate the function in the given interval.
+     * 
+     * @param min the lower bound for the interval
+     * @param max the upper bound for the interval
+     * @return the value of integral
+     * @throws ConvergenceException if the maximum iteration count is exceeded
+     * or the integrator detects convergence problems otherwise
+     * @throws FunctionEvaluationException if an error occurs evaluating the
+     * function
+     * @throws IllegalArgumentException if min > max or the endpoints do not
+     * satisfy the requirements specified by the integrator
+     */
+    double integrate(double min, double max) throws ConvergenceException, 
+        FunctionEvaluationException;
+
+    /**
+     * Get the result of the last run of the integrator.
+     * 
+     * @return the last result
+     * @throws IllegalStateException if there is no result available, either
+     * because no result was yet computed or the last attempt failed
+     */
+    double getResult();
+
+    /**
+     * Get the number of iterations in the last run of the integrator.
+     * <p>
+     * This is mainly meant for testing purposes. It may occasionally
+     * help track down performance problems: if the iteration count
+     * is notoriously high, check whether the function is evaluated
+     * properly, and whether another integrator is more amenable to the
+     * problem.
+     * 
+     * @return the last iteration count
+     * @throws IllegalStateException if there is no result available, either
+     * because no result was yet computed or the last attempt failed
+     */
+    int getIterationCount();
+}","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,31/Jul/05 10:27;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--UnivariateRealIntegrator.java;https://issues.apache.org/jira/secure/attachment/12333652/ASF.LICENSE.NOT.GRANTED--UnivariateRealIntegrator.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,35949.0,,,2005-08-14 15:54:48.0,,,false,,,,,,,,,,,,,,150338,,,Sun Aug 14 15:54:48 UTC 2005,,,,,,0|i0rwhr:,160908,,,,,,,,"31/Jul/05 10:27;zxg_32@yahoo.com;Created an attachment (id=15832)
New Source File
","14/Aug/05 15:54;phil@steitz.com;

*** This bug has been marked as a duplicate of 36148 ***",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Percentile calculation is not correct,MATH-104,12341318,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Won't Fix,,scott@duchin.com,scott@duchin.com,09/Mar/04 05:46,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"I took the microsoft excel example and plugged it in and got the wrong results:
Microsoft (1,3,2,4) 30% --> 1.9 (math returned 1.5)

Looking at the code I saw a few problems.
1) the pos = should be p * (n - 1) / 100.  (the code has a plus sign)
2) the current two conditionals after sorting should be replaced with one:
    if (intPos == length - 1) { return sorted[intPos]; }
3) the lower and upper should be retrieved from:
    lower = sorted[intPos] and upper = sorted[intPos + 1]
4) i believe passing in a zero percentile is okay so the top test should read
   || p < 0) ... (remove the = sign)","Operating System: Windows XP
Platform: PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,27524.0,,,2004-03-09 14:20:23.0,,,false,,,,,,,,,,,,,,150337,,,Sun Mar 14 04:05:40 UTC 2004,,,,,,0|i0rwhz:,160909,,,,,,,,"09/Mar/04 14:20;phil@steitz.com;Excel is using a different percentile interpolation definition than what [math]
currently uses. Unfortunately, the javadoc and test cases describing the [math]
algorithm have been inadvertently removed. [math] uses the ""index number"" method
with simple linear interpolation as described in the first example here:

http://www.math.bcit.ca/faculty/david_sabo/apples/math2441/section4/relstanding/RelStanding.htm

The code is working as designed.

The reference above also describes the method used by Excel, which is another
commonly used algorithm that gives different results.  R uses this method as well.

I am +1 to changing to the R / Excel method and will make the change (and add
complete javadoc) if there are no objections.","14/Mar/04 04:05;phil@steitz.com;After more research, I have changed my mind on this.  I added a reference to the
definition and estimation algorithm used.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ValueServer switching from URL to File is badly broken,MATH-103,12341193,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,william.barker@wilshire.com,william.barker@wilshire.com,08/Jan/04 14:59,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"On a Windows machine, where there are spaces in the install path, URL.getFile 
returns something like:
  /C:/Jakarta%20Development/commons-math/target/test-
classes/org/apache/commons/math/random/testData.txt

ValueServer then tries to construct a File out of this, which is of course an 
invalid path.  

IMHO it would be better to base the class on InputStream (instead of File), 
since in an app-server the concept of File isn't really defined.  Otherwise, it 
should be based on a URL, and use the openStream method to read the resource.","Operating System: Windows XP
Platform: PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,25972.0,,,2004-01-11 15:43:01.0,,,false,,,,,,,,,,,,,,150336,,,Mon Jan 12 04:25:59 UTC 2004,,,,,,0|i0rwi7:,160910,,,,,,,,"11/Jan/04 15:43;phil@steitz.com;Should be fixed now in CVS.  Dependency on URL -> string -> File has been
eliminated.  I am not able to test on Windows, so I am leaving this open.  Can
someone pls run the tests on Windows to confirm?  ",12/Jan/04 04:25;william.barker@wilshire.com;I can confirm that the junit tests now work on Windows.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Interpolation Source Files,MATH-102,12342465,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,zxg_32@yahoo.com,zxg_32@yahoo.com,14/Aug/05 11:14,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Add PolynomialFunctionNewtonForm.java
Add PolynomialFunctionNewtonFormTest.java
Add DividedDifferenceInterpolator.java
Add DividedDifferenceInterpolatorTest.java
Update NevilleInterpolator.java
Update NevilleInterpolatorTest.java","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,14/Aug/05 11:14;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--Interpolation.zip;https://issues.apache.org/jira/secure/attachment/12333710/ASF.LICENSE.NOT.GRANTED--Interpolation.zip,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,36178.0,,,2005-08-25 06:46:30.0,,,false,,,,,,,,,,,,,,150335,,,Thu Aug 25 06:46:30 UTC 2005,,,,,,0|i0rwif:,160911,,,,,,,,"14/Aug/05 11:14;zxg_32@yahoo.com;Created an attachment (id=16037)
Interpolation Source Files
","25/Aug/05 06:46;j3322ptm@yahoo.de;Files added (without change, revision pending)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] Remove mode, remove some methods from StatUtils",MATH-101,12340804,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,21/Jun/03 02:01,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch removes the following methods:

1. getMode() from StoreUnivariate, AbstractStoreUnivariate, and StatUtils.  As
defined in the javadoc, the mode has little meaning for continuously scaled data
(which is what will be analyzed in the Univariate implementations).  If users
want frequency distributions, they can use Freq(uency).   

2. Skewness, Kurtosis and Median computations from StatUtils.  We do not need
these internally (they are only supported in the Stored univariates) and they
are much less commonly used than the other methods in StatUtils.  The median is
also redundant with getPercentiles in AbstractStoreUnivariate and it is not
implemented (while the more general getPercentiles is).","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,21/Jun/03 02:03;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--statPatches.txt;https://issues.apache.org/jira/secure/attachment/12332373/ASF.LICENSE.NOT.GRANTED--statPatches.txt,21/Jun/03 03:53;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--statPatches2.txt;https://issues.apache.org/jira/secure/attachment/12332374/ASF.LICENSE.NOT.GRANTED--statPatches2.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,20964.0,,,2003-06-21 02:33:48.0,,,false,,,,,,,,,,,,,,150334,,,Sat Jun 21 09:45:44 UTC 2003,,,,,,0|i0rwin:,160912,,,,,,,,"21/Jun/03 02:03;phil@steitz.com;Created an attachment (id=6918)
patch to remove methods
","21/Jun/03 02:33;mdiggory@gmail.com;I am all for (1.) 

I feel the first part of (2) needs some more thought. I am using the StatUtils
versions of Skewness and Kurtosis to provide for the storage based solution in
UnivariateImpl. This was currently the ideal location for them as I am
delegating to StatUtils for the other storage cases (mean, var, std) as well, so
its a uniform strategy across all of Univariate. As they are moments just like
mean and var, they can have resonably simple standalone implementations. I was
also optimizing the StatUtil versions of these methods at this time. 

The second part of (2) I'm ok with. 

What do you think about moving percentile and sort functions into StatUtils? Are
you also aware that there are sort utilities for arrays in java.uitl.Arrays?

Also, it would be wise to approach such changes with consideration for whats
happening in the rest of the system. Stripping skew and kurt without checking
their dependencies in UnivariateImpl would end up breaking the build. you should
have encountered compilation problems with UnivariateImpl after making the
changes in this patch. This is where IDE's like Eclipse come in handy, you can
do refactorings, deletions and see what breaks immediately in the IDE.

-Mark

","21/Jun/03 03:35;phil@steitz.com;I want to keep StatUtils as lean as possible, partly because the methods are static.

Regarding the build, all tests run fine for me after the change.  Unfortunately,
the test cases for UnivariateImpl do not execute the paths that would have
caused failure. One more reason to get the path coverage up. Also, I neglected
to execute maven clean before test.  My mistake.  I see now that these methods
are reused, so we have to keep them in StatUtils (unless we decide to drop
skewness and kurtosis from UnivariateImpl).  I will submit a revised patch
keeping these things in; but I am getting a little squemish about so many static
methods. My preference would be to remove skewness and kurtosis from
UnivariateImpl and StatUtils as well.

I do not want to put the percentile computations into StatUtils. Yes, you are
correct, the pre-1.2 sort implementation that I embedded in
abstractStoreUnivariate.getSortedValues should be replaced by the ArrayUtils
sort.  I will submit a separate patch for this.","21/Jun/03 03:53;phil@steitz.com;Created an attachment (id=6921)
Revised patch, leaving skewness, kutosis in StatUtils
","21/Jun/03 09:45;mdiggory@gmail.com;Everything here was commited now, I've also added a new test in
UnivariateImplTest for mean, varaince, skew and kurt (the same one thats in
StoreUnivariateImplTest).

I'm closing it out. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] HypergeometricDistributionImpl cumulativeProbability calculation overflown,MATH-100,12342483,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,microarray@gmail.com,microarray@gmail.com,17/Aug/05 08:41,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Hi, for all that might use this class:

several things I found when using this class to calculate the
cumulative probability. I attached my code FYI. three things:

1. when I used my code to calculate the cumulativeProbability(50) of
5000 200 100 (Population size, number of successes, sample size),
result was greater than 1 (1.0000000000134985);
2. when I calculated cumulativeProbability(50) and
cumulativeProbability(51) for the distribution 5000 200 100, I got the
same results, but it should have been different;
2. the cumulativeProbability(x) returns ""for this distribution, X,
P(X<=x)"", but most of the time (at least in my case) what I care about
is the upper tail (X>=x). based on the above findings, I can't simply
use 1-cumulativeProbability(x-1) to get what I want.

here's what I think might be related to the problem: since the
cumulativeProbability is calculating the lower tail (X<=x), a
distribution like above often has this probability very close to 1;
thus it's difficult to record a number that = 1-1E-50 'cause all you
can do is record sth like 0.9999..... and further digits will be
rounded. to avoid this, I suggest adding a new function to calculate
upper tail or change this to calculate x in a range like (n<=x<=m), in
addition to fix the overflow of the current function.

thank you for your patience to get here. I'm a newbie but I've asked
Java experts in our lab about this. looking into the source code really
isn't up for me......hope someone can fix it, :) BTW I'm using cygwin under
WinXP pro SP2, with Java SDK 1.4.2_09 build b05, and the commons-math I used is
both the 1.0 and the nightly build of 8-15-05. 

the code:
-------------------
import org.apache.commons.math.distribution.HypergeometricDistributionImpl;

class HyperGeometricProbability {

    public static void main(String args[]) {

	if(args.length != 4) {
	    
	    System.out.println(""USAGE: java HyperGeometricProbabilityCalc [population]
[numsuccess] [sample] [overlap]"");
	    
	}
	else {
	    
	    String population = args[0];
	    String numsuccess = args[1];
	    String sample = args[2];
	    String overlap = args[3];
	    
	    int populationI = Integer.parseInt(population);
	    int numsuccessI = Integer.parseInt(numsuccess);
	    int sampleI = Integer.parseInt(sample);
	    int overlapI = Integer.parseInt(overlap);
	    
	    HypergeometricDistributionImpl hDist = new
HypergeometricDistributionImpl(populationI, numsuccessI, sampleI);
	    
	    double raw_probability = 1.0;
	    double cumPro = 1.0;
	    double real_cumPro = 1.0;

	    try {
		if (0 < overlapI && 0 < numsuccessI && 0 < sampleI) {
		    raw_probability = hDist.probability(overlapI);
		    cumPro = hDist.cumulativeProbability(overlapI - 1.0);
		    real_cumPro = 1.0 - cumPro;
			System.out.println(""cumulative probability="" + cumPro + ""\t"" + ""raw
probability="" + raw_probability + ""\t"" + ""real cumulative probability="" + ""\t"" +
real_cumPro);
		    }
	    }
	    catch (Exception e) {
		System.err.println(""BAD PROBABILITY CALCULATION"");
	    }
	    
	}
	
    }
    
}
----------------------------------","Operating System: Windows XP
Platform: PC",,,,,,,,,,,,,,,,,,,,17/Aug/05 08:43;microarray@gmail.com;ASF.LICENSE.NOT.GRANTED--HyperGeometricProbability.java;https://issues.apache.org/jira/secure/attachment/12333722/ASF.LICENSE.NOT.GRANTED--HyperGeometricProbability.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,36215.0,,,2005-08-18 03:36:17.0,,,false,,,,,,,,,,,,,,150333,,,Mon Aug 29 22:53:12 UTC 2005,,,,,,0|i0rwiv:,160913,,,,,,,,"17/Aug/05 08:43;microarray@gmail.com;Created an attachment (id=16073)
the code as a txt file

the same code as in my post. figured this might be easier to work with. ","18/Aug/05 03:36;brentworden;I think we're witnessing the accumulation of numerical error.  The cumulative 
probability method uses the textbook definition for its computation: summing 
the point probabilities.  Each of these probabilities results in a small 
numerical error and the process of summing them just exacerbates the error.

In your case, X ~ hyper(2000, 500, 100) and P(X <= 50) there are 51 small 
errors that, when added together, cause the total sum to be over 1.0.  Also, 
the point probabilities the larger x values (30 - 50) are so small that they 
provide little value to the overall sum, just error.

I think we can address this problem by simply short-circuiting the summation 
whenever it goes above 1.0.  This will solve the problem of cumulative 
probablities greater than 1 and make the upper tail probabilties correct 
because, again in your case, P(X >= 50) is extremely close to zero.

What do others think?
","18/Aug/05 04:18;microarray@gmail.com;I think you're right. it is coming from the accumulation of numerical error. but
I think the solution should be more complicated than a simple short circuit.
let's see an example again:

if we calculate cumulativeProbability(50) for (6000, 200, 100), the upper tail
(X>=50) probability will be 1 - cumulativeProbability(50) = 3.37E-11. but if you
do a sum of the point probabilities using probability(50),(51),...,(100), which
by textbook definition defines the upper tail, you'll get 6.02E-49. 
the discrepency, coming from the accumulation of numerical error, results in a
false estimate of the real cumulative probability, by a factor of E38. true,
both are very small, but in some cases it would matter for sure. 

that's why my suggestion is, either we define a new function to the class to
calculate P(X>=x) for upper tail probability, or we can define the
cumulativeProbability so that it calculates P(m<=x<=n); in this case if you
simply want upper or lower tail, just define m or n separately. and I guess
it'll help to short-circuit the class so that it won't produce p>1 or p<0 cases.

what do you think?

(In reply to comment #2)","19/Aug/05 12:16;brentworden;Addressed.  SVN revision 233412.  Committed to MATH_1_1 branch. Added upper 
tail cumulative probability method to HypergeometricDistributionImpl.  Note, 
this new method was not added to the HypergeometricDistribution interface to 
avoid binary incompatiblities with older versions.","23/Aug/05 04:06;microarray@gmail.com;Thanks for the fix. However I'm still having problems with this. When I compiled
my code again with the new build (I tried the nightly build of 8-20 and 8-21)
and try to calculate the cumulativeProbability of 5000 200 100 50, I still got
the same overflow 1.0000000000134985. 
Initially I guess it's because Brent Worden has added a new method to calculate
the upper tail and the current method is untouched? But I can't find anywhere
about how to use the new method.
This post may sound stupid; can anyone tell me whether I'm doing the right thing
here and the bug is really still here? 

(In reply to comment #4)
> Addressed.  SVN revision 233412.  Committed to MATH_1_1 branch. Added upper 
> tail cumulative probability method to HypergeometricDistributionImpl.  Note, 
> this new method was not added to the HypergeometricDistribution interface to 
> avoid binary incompatiblities with older versions.

","23/Aug/05 05:21;phil@steitz.com;The fix was committed to the MATH_1_1 release branch, not to the svn trunk, so
the nightlies have not picked it up yet.   Once the 1.1 release goes final
(hopefully very soon), we will merge all of the fixes recently made (including
this one) back into the main development trunk.

For now, to test your code, you will have to check out and build using the
MATH_1_1 branch.  If you need help doing this, or would just like a jar to test
with, let me know.","23/Aug/05 07:44;microarray@gmail.com;Ah, I see. so could you please just send me a jar to test with? I don't know how
to do it the other way,:) after all, I'm a newbie to the java world. 
microarray@gmail.com
Thanks!

(In reply to comment #6)","23/Aug/05 07:46;microarray@gmail.com;Oh, I forgot to ask, can you also let me know something about the new upper tail
method that Brent mentioned? Thanks!","23/Aug/05 11:37;brentworden;I just merged the changes, including this fix, in the MATH_1_1 branch to the 
trunk.  The next nightly build should contain your fix.

The actual added method is only found on the HypergeometricDistributionImpl and 
has a signature of:
public double upperCumulativeProbability(int x);

It returns P(X >= x).
",24/Aug/05 00:29;microarray@gmail.com;Thanks a lot! you guys rock :),"25/Aug/05 07:45;microarray@gmail.com;I really hate to post this but I think this bug is somewhat hard to catch:
evolution somehow kicks in......

check this out: 
before I used the new jar I coded a loop to calculate all the probabilities of
the upper tail and sum them to get the upper cumPro, just like textbook
definition, but of course, slow. 
I used the new upperCumulativeProbability method to calculate my example, 5000
200 100 50. used my old looping way calculation and a perl script to cross
check. I got exactly the same answer: upper cumPro = 4.407167010070517E-45. Great!!!
but, as I used it more, ugly bugs showed up: when I calculate 26896 895 55 19,
the new jar with the fix gave me 3.40373618E-10, my old looping way and the perl
script says 6.2588249E-15 is right, and the probability(19)= 5.8805728E-15,
which is a good comfirmation that the second answer is right, because you can
imagine something with a tail of even smaller than E-15 won't accumulate to E-10. 

I have absolutely no idea where this comes from now: the fixed jar can calculate
something as small as 4E-45, as we saw in the first example, which indicates
accumulation of numeric error is not happening here; without any knowledge of
the source code (not that I can't get it, but I won't be able to understand it
at all, as a newbie), I have no idea why it would calculate the second case
wrong. you guys might think ""hey, these are values small enough to be taken as
zero, what's the difference?!"" I guess mathematicall precision is what we're
after here. my looping code can do the job, but it's gonna be tooo slow imagine
you do this over and over. 

Thanks for everyone's patience and effort in solving this.","25/Aug/05 07:49;microarray@gmail.com;oh, another thing I forgot to mention just now: if you calculate something like
1000 100 1 1, the upper cumPro will return 0. basically, if your query number
equals to either the number of success or the sample, then you got 0. for these
cases, the cumPro should just be the probability of this query number. ",26/Aug/05 14:14;brentworden;Fixed.  SVN revision 240613.  Fixed in MATH_1_1 branch.,26/Aug/05 19:27;pkillion@yahoo.com;will this fix be in the 0826 nightly?  does it address both issues raised by mike hu?,"26/Aug/05 21:18;pkillion@yahoo.com;hello - i show no change in the behavior of the upperCumulativeProbability(x) returning 0.0 when x = 
numsuccess or x = sample size.   i am not sure about the other issue (raised by mike hu on 0825).  
perhaps the fix did not make the 0826 build or it is still not fixed.","26/Aug/05 22:25;brentworden;(In reply to comment #15)

I have not had time to get the fix pushed to the trunk, soit will not be in the 
nightly builds yet.  I hope to get around to this today.

Both problems have been resolved.  The problem with returning 0 was caused by a 
simple erroneous boundry value check.  The poor accuracy for small cumulative 
probabilities was the result of a truncation error when performing 1.0 - 
verySmallProbability in certain circumstances.

Of note, the performance for these routines is quite poor for large population 
sizes.  I plan to address these limitations and improve performance post 
release 1.1.


","27/Aug/05 00:39;microarray@gmail.com;OKay, for the love of Java, let's see the details of all the bugs. I tend to
believe they're not all caught yet, but if I have the jar I can definitely test
them out. 

for the current build 8-25-05, the code below
HypergeometricDistributionImpl hDist = new HypergeometricDistributionImpl(26932,
270, 823);

        double probability = hDist.probability(52);
        double utprobability = hDist.upperCumulativeProbability(52);
        double cprobability = hDist.cumulativeProbability(52);

        System.out.println(probability);
        System.out.println(utprobability);
        System.out.println(cprobability);
        System.out.println(1 - cprobability);
will return:
1.018427824183987E-26
-2.437485768780334E-10
1.0000000002437486
-2.437485768780334E-10
this shows that in this example, the upper tail cumPro is just 1 - cumPro, which
is wrong because of the numeric error accumulation we discusse earlier.

but this is obviously not always happening, as when I tried 6000 200 100 50, it
returned 6.02E-49, as I expected. what's the mystery here?

furthermore, if you run the above code but change it so that it runs
HypergeometricDistributionImpl hDist = new HypergeometricDistributionImpl(26932,
823, 270);
the result will be different:
1.0184278236099406E-26
3.7159353372118176E-10
0.99999999962840647
3.7159353372118176E-10
but by textbook definition of hypergeometric distribution, the order of
numSuccess and sample shouldn't matter at all, notice here: A. the raw
probability is slightly different, at the 9th digit; B. this should have
generated what the earlier calculation had, but it's totally different.

so, summarizing the bugs here:
1. upper tail cumulative probability when running some examples, will overflow
and give negative values;
2. upper tail cumPro obviously is not consistently calculating in the same
fashion, sometimes it works, sometimes not;
3. the order of numSuccess and sample sometimes matters to the code, while it
should not, ever;
4. the raw probability, when change the order of numSuccess and sample, will
differ slightly, if it's calculated in exactly the same way then it shouldn't.

I suggest that when you have a fix, test it out further with many more examples,
like what we suggested here, and in reversing order, etc. and if you could send
me the jar or post it here I can test it out as well, rather than waiting for
the nightly build to include it. 

pop numSuccess sample query upperCumPro
26932 823 270 53 1.4160591836816684E-27
26932 270 823 53
6000 200 100 50 6.020909331626761E-49
6000 100 200 50
26896 895 55 15 2.077516591801479E-10
26896 55 895 15","27/Aug/05 00:49;microarray@gmail.com;I saw your post and actually bugzilla told me 
Mid-air collision detected!
Someone else has made changes to this bug at the same time you were trying to.
The changes made were:
pretty cool huh?:)

anyways, it seems like that for some cases your are using 1-small probabilities,
while in the example I mentioned it's not calculated this way: why there seems
to be two ways to calculate this? is it for performance concern? personally I'd
rather get the accurate answer than saving time, and I'm currently doing the
slow looping thing to get the right answer.

thanks!","29/Aug/05 22:53;microarray@gmail.com;I tested the nightly build 8-29-05. it seems like the bugs I mentioned above are
all fixed. will post if I get further problems.

Thanks a lot!",,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] wrong results and stack overflow error from BivariateRegression,MATH-99,12341098,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,skarupo@mail.ru,skarupo@mail.ru,17/Nov/03 19:14,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Example:

import java.util.*;
import org.apache.commons.math.stat.*;

public class RegressionTest
{


	public static void main(String[] args)
	{
	
	
		BivariateRegression br1 = new BivariateRegression();
		BivariateRegression br2 = new BivariateRegression();
		
		
		Random random = new Random(1);
		
		int n = 100;
		
		for (int i = 0; i < n; i++)
		{
		
		
			br1.addData(((double)i)/(n-1), i); //perfectly correlated example
			br2.addData(((double)i)/(n-1), random.nextDouble()); //uncorrelated example
			
		
		
		
		}
		
		
		System.out.println(br1.getSignificance()); //should return 0, but returns NaN instead
		System.out.println(br2.getSignificance()); //should return 1, but some sequences cause stack overflow error!
		
	
	
	}




}","Operating System: All
Platform: PC",,,,,,,,,,,,,,,,,,,,17/Nov/03 19:17;skarupo@mail.ru;ASF.LICENSE.NOT.GRANTED--RegressionTest.class;https://issues.apache.org/jira/secure/attachment/12332623/ASF.LICENSE.NOT.GRANTED--RegressionTest.class,17/Nov/03 19:16;skarupo@mail.ru;ASF.LICENSE.NOT.GRANTED--RegressionTest.java;https://issues.apache.org/jira/secure/attachment/12332622/ASF.LICENSE.NOT.GRANTED--RegressionTest.java,18/Nov/03 14:04;brentworden;ASF.LICENSE.NOT.GRANTED--regression-bug-fixes.txt;https://issues.apache.org/jira/secure/attachment/12332624/ASF.LICENSE.NOT.GRANTED--regression-bug-fixes.txt,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,24747.0,,,2003-11-18 01:51:59.0,,,false,,,,,,,,,,,,,,34205,,,Tue Nov 18 23:07:33 UTC 2003,,,,,,0|i0rwj3:,160914,,,,,,,,"17/Nov/03 19:16;skarupo@mail.ru;Created an attachment (id=9142)
unit test source code
","17/Nov/03 19:17;skarupo@mail.ru;Created an attachment (id=9143)
unit test class file
","18/Nov/03 01:51;brentworden;For the perfect correlation case, the sum of squares error is being computed to 
a negative number because of numerical error (it is a very small number -1e-
11).  So, when the std err. of the slope is computed, a sqrt of a negative 
number is computed resulting in NaN.

When I get a chance, I'll review and research the updating formulas we're using 
to see if they can be improved upon.


For the random correlation case, the Beta.regularizedBeta routine isn't 
converging for large a and small b.  I'll look into that further.","18/Nov/03 14:04;brentworden;Created an attachment (id=9149)
This patch fixes both problems
","18/Nov/03 14:07;brentworden;The 11/18 attachment should fix both problems.  It changes the regularizedBeta 
routine to fix the random correlation case and it modifies the 
BivariateRegression sums of squares updating formulas to fix the perfect 
correlation case.",18/Nov/03 23:07;mdiggory@gmail.com;Applied and Tested,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Integration Source Files,MATH-98,12342444,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,zxg_32@yahoo.com,zxg_32@yahoo.com,08/Aug/05 05:48,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Add SimpsonIntegrator.java
Add SimpsonIntegratorTest.java
Add RombergIntegrator.java
Add RombergIntegratorTest.java
Update TrapezoidIntegrator.java","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,08/Aug/05 05:49;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--Integration.zip;https://issues.apache.org/jira/secure/attachment/12333699/ASF.LICENSE.NOT.GRANTED--Integration.zip,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,36066.0,,,2005-08-14 15:55:17.0,,,false,,,,,,,,,,,,,,150332,,,Sun Aug 14 15:55:17 UTC 2005,,,,,,0|i0rwjb:,160915,,,,,,,,"08/Aug/05 05:49;zxg_32@yahoo.com;Created an attachment (id=15942)
Integration Source File
","14/Aug/05 15:55;phil@steitz.com;

*** This bug has been marked as a duplicate of 36148 ***",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Add content to guidelines section in developer.xml,MATH-97,12340749,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,02/Jun/03 12:53,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch adds content to the guidelines section of developer.xml,
including subsections on coding style, documentation, unit tests and licensing.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,02/Jun/03 12:54;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--developersPatch.txt;https://issues.apache.org/jira/secure/attachment/12332304/ASF.LICENSE.NOT.GRANTED--developersPatch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20404.0,,,2003-06-04 09:44:39.0,,,false,,,,,,,,,,,,,,97387,,,Wed Jun 04 09:44:39 UTC 2003,,,,,,0|i0rwjj:,160916,,,,,,,,"02/Jun/03 12:54;phil@steitz.com;Created an attachment (id=6591)
patch to developers.xml
","04/Jun/03 09:44;mdiggory@gmail.com;Applied the first part of this patch. The links were already commited to the
file earlier.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChiSquareTest does not return,MATH-96,12341492,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,scott@duchin.com,scott@duchin.com,07/Jun/04 07:24,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"When executing the following code it never returns:

try { 
    double[] exp = new double[] {
        3389119.5, 649136.6, 285745.4, 25357364.76, 11291189.78, 543628.0, 
232921.0, 437665.75
    };

    long[] obs = new long[] {
        2372383, 584222, 257170, 17750155, 7903832, 489265, 209628, 393899
    };
    org.apache.commons.math.stat.inference.ChiSquareTestImpl csti =
      new org.apache.commons.math.stat.inference.ChiSquareTestImpl(); 
    double cst = csti.chiSquareTest(exp, obs); 
} catch (org.apache.commons.math.MathException me) {
    me.printStackTrace();
}","Operating System: Windows XP
Platform: PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,29419.0,,,2004-06-07 08:56:57.0,,,false,,,,,,,,,,,,,,34163,,,Tue Jun 08 03:34:45 UTC 2004,,,,,,0|i0rwjr:,160917,,,,,,,,"07/Jun/04 08:56;brentworden;The problem is in the Gamma special functions.  For large x values in 
comparison to a values, the regularizedGammaP function never converges to a 
finite sum because the series terms approach numerical infinity.

IIRC, the algorithm I chose is slow to converge for this particular parameter 
scenario (x >> a).  Long term, I will research a different approach for this 
case and get it integrated into regularizedGammaP.

Short term, I was going to add a infinite term check to short-circut the 
algorithm and either return infinity or throw an exception from 
regularizedGammaP.  Any input on which choice of inifinity handling is 
preferred?
","07/Jun/04 09:33;phil@steitz.com;The ChiSquare test statistic in this case is 3624883 (rounded).  With df = 7,
the p-value returned should be effectively 0.  The code actually does terminate,
when Gamma.regularizedGammaP throws a ConvergenceException.  It takes a *long*
time, however, since the default iteration bound of Integer.MAX_VALUE is used.

It is not obvious to me how to address this.  Reducing the default max
iterations above might be a good idea in general; but this would just cause us
to throw sooneer in the present case, when we should be returning 0.  I am
inclined to suggest that we force 0 for the p-value (or 1 in the ChiSquare cdf)
when the value of the test statistic is greater than say, 1000 * df (which is
500 times the variance) or some such bound.  I will run some tests to see what a
reasonable bound might be.
","07/Jun/04 09:38;phil@steitz.com;In response to Brent: I think that ""returning infinity"" would be better than
throwing.  That way the cdf could return 1 in the present case.  I think this is
better than dealing with the problem at the test or distribution level as I
suggeseted.  ","08/Jun/04 03:34;brentworden;I added a regularized gamma function implementation based on a continued 
fraction.  This approach converges much faster for the large x case and fixes 
the problem experienced by Scott.

I added the example submitted by Scott as a test case and got it and all 
existing test cases to pass with my changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] BrentSolver and SecantSolver request boundary values from function twice,MATH-95,12342257,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,paulf,paulf,25/May/05 00:17,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,1.0,,,,,,,,,,0,,,,,,,,"I'm using the BrentSolver class and I noticed that, when I call the solve() 
method,  it requests the boundary values of the initial interval twice from my 
function. The function is quite expensive so it would be good to avoid these 
two duplicate computations.

I had a look at the source code (see below) and I can see that 
the 'verifyBracketing' check requests the boundary values from the function and 
then the 'solve' method itself also requests those values. It looks like the 
SecantSolver will exhibit this behaviour too. Would it be possible to change 
the code so that the values are requested just once?



BrentSolver.class

   public double solve(double min, double max) throws ConvergenceException, 
        FunctionEvaluationException {
        
        clearResult();
        verifyBracketing(min, max, f);
        
        // Index 0 is the old approximation for the root.
        // Index 1 is the last calculated approximation  for the root.
        // Index 2 is a bracket for the root with respect to x1.
        double x0 = min;
        double x1 = max;
        double y0;
        double y1;
        y0 = f.value(x0);  // ******* REQUESTS BOUNDARY (MIN) VALUE HERE ****
        y1 = f.value(x1);  // ******* REQUESTS BOUNDARY (MAX) VALUE HERE ****


UnivariateRealSolverImpl.class:

    protected void verifyBracketing(double lower, double upper, 
            UnivariateRealFunction f) throws FunctionEvaluationException {
        verifyInterval(lower, upper);
        if (!isBracketing(lower, upper, f)) {
            throw new IllegalArgumentException
            (""Function values at endpoints do not have different signs."" +
                    ""  Endpoints: ["" + lower + "","" + upper + ""]"" + 
                    ""  Values: ["" + f.value(lower) + "","" + f.value(upper) 
+ ""]"");       
        }
    }

    protected boolean isBracketing(double lower, double upper, 
            UnivariateRealFunction f) throws FunctionEvaluationException {
// ******* REQUESTS BOTH BOUNDARY VALUES HERE ****
        return  (f.value(lower) * f.value(upper) < 0);
    }","Operating System: Windows XP
Platform: PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,35042.0,,,2005-05-25 02:41:47.0,,,false,,,,,,,,,,,,,,150331,,,Sat Jun 04 14:43:40 UTC 2005,,,,,,0|i0rwjz:,160918,,,,,,,,"25/May/05 02:41;brentworden;One solution might be to inline isBracketting logic in the verifyBracketing 
method and return a 2-element array comprised of the endpoint evaluations.  The 
solve method could then used these evaluations for initialization.","04/Jun/05 14:43;phil@steitz.com;Should be fixed in nightly builds starting 4 Jun 05.
Thanks for reporting this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Update task list,MATH-94,12340811,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,23/Jun/03 22:37,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,Attaching a patch updating tasks.xml,"Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,23/Jun/03 22:38;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--taskListPatch.txt;https://issues.apache.org/jira/secure/attachment/12332380/ASF.LICENSE.NOT.GRANTED--taskListPatch.txt,25/Jun/03 20:28;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--tasksPatch.txt;https://issues.apache.org/jira/secure/attachment/12332381/ASF.LICENSE.NOT.GRANTED--tasksPatch.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,21015.0,,,2003-06-25 08:51:55.0,,,false,,,,,,,,,,,,,,150330,,,Wed Jun 25 20:34:36 UTC 2003,,,,,,0|i0rwk7:,160919,,,,,,,,"23/Jun/03 22:38;phil@steitz.com;Created an attachment (id=6937)
Patch to tasks.xml
","25/Jun/03 08:51;mdiggory@gmail.com;Phil, This patch contains two patches to 2 different versions of the same file,
one for version 1.4 one for version 1.5. Do you know how it needs to be applied
or can you provide a single patch that covers the entire changes to this file. 
","25/Jun/03 20:28;phil@steitz.com;Created an attachment (id=6970)
corrected patch file
","25/Jun/03 20:30;phil@steitz.com;Oops! I appended the new patch to an old patch file by mistake.  The new patch
that I just added includes only the new stuff. Sorry.  -P","25/Jun/03 20:34;mdiggory@gmail.com;Commited it with one minor change, I already added the array methods to StatUtils.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] PolynomialSplineFunction.value() throws an exception when requesting final datapoint,MATH-93,12342146,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ben@csh.rit.edu,ben@csh.rit.edu,30/Mar/05 08:34,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,1.0,,,,,,,,,,0,,,,,,,,"The javadocs for this function state that

""Throws: FunctionEvaluationException - if v is outside of the domain of of the 
spline function (less than the smallest knot point or greater than or equal to 
the largest knot point)""

I have a series of points, for example ( (12.2,3), (15.4,1.2), (18.9, 4.6)
) that I need to interpolate some data points from. The problem is that I need 
to be able to interpolate from 12.2 through 18.9 inclusive, right now an 
exception is thrown if I try to evaluate for 18.9, which is exactly what the 
javadocs state should happen. Is there a mathematical reason for being able to 
evaluate the first datapoint but not the last one? Can you point me to a 
reference if that is the case? To me it seems to make sense that it should be 
able to determine the value at one of the knot points, "". 

Comments from Brent Worden:
I've looked over some literature and I agree with you that it does not make 
much sense that the function can not be evaluated at one of the knots.

Ben has agreed to spend some time working on a patch.","Operating System: other
Platform: All",,,,,,,,,,,,,,,,,,,,01/Apr/05 23:37;ben@csh.rit.edu;ASF.LICENSE.NOT.GRANTED--SplineFunctionLastKnotPatch.zip;https://issues.apache.org/jira/secure/attachment/12333435/ASF.LICENSE.NOT.GRANTED--SplineFunctionLastKnotPatch.zip,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,34230.0,,,2005-03-30 15:09:09.0,,,false,,,,,,,,,,,,,,34200,,,Sat Apr 02 06:47:53 UTC 2005,,,,,,0|i0rwkf:,160920,,,,,,,,"30/Mar/05 15:09;phil@steitz.com;There is no mathematical reason that I can see now (or remember ;-) that the
spline function should not just return the value at the final knot point.  The
interpolating polynomials are defined on the subintervals determined by the knot
points, closed on the left and open on the right, which is why the last one
(which would give the value at the final knot point) is now undefined at the
final knot point.  This is an implementation detail, however, and unecessary,
(unless I am forgetting something subtle) since the final polynomial should
extend by continuity to the final knot point and return the correct value there.
So I am +1 for this change.  Need to be careful to update all javadoc and unit
tests if we do this.  Patches welcome!

See the javadoc for SplineInterpolator for a reference to the interpolation
algorithm used.","01/Apr/05 23:37;ben@csh.rit.edu;Created an attachment (id=14606)
Fix to issue

I have attached a fix for this issue.  It is fixed by using the last polynomial
to evaluate the final knot point.  Also junit tests have been updated to check
the last knot point.

Ben","02/Apr/05 06:47;brentworden;Patch applied.  Thanks, Ben.
(SVN revision 159727)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][PATCH] inverseCumulativeProbability for PoissonDistributionImpl Fails for large lambda,MATH-92,12342453,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mikael@amazon.com,mikael@amazon.com,10/Aug/05 02:44,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The inverseCumulativeProbability for the Poisson distribution is calculated
through the standard implementation in AbstractIntegerDistribution.  This
implementation fails with a continue fraction stack overflow for large lambda.

Instead, I suggest the following override in PoissonDistributionImpl

/**
 * Calculate the inverse cumulative probability for the Poisson distribution
 * based on a Cornish-Fisher expansion approximation followed up by a line
 * search. 
 * 
 * For the Cornish-Fisher expansion see Abramawitz and Stegun 
 * 'Handbook of Mathmatical Functions' pages 935 and 928
 * 
 * @param prob the target probability
 * @return the desired quantile
 * @throws MathException when things go wrong
 */
private int inverseCumulativeProbability(double prob) throws MathException{
	
    if (prob < 0.0 || prob >= 1.0)
        throw new MathException(""Probability must be in right-side open interval
[0.0, 1.0)"");
	
    if (prob == 0) return 0;  // there is nothing to calculate

    // Use the Cornish-Fisher Expansion with two terms to get a very good
approximation
    // see Abramawitz and Stegun 'Handbook of Mathmatical Functions'
    // pages 935 and 928
    double mu = this.getMean();         // mean
    double sigma = Math.sqrt(mu);       // standard deviation
    double gamma = 1.0/Math.sqrt(mu);   // skewness
		
    double z = new NormalDistributionImpl(0.0,
1.0).inverseCumulativeProbability(prob);
    // this is the actual expansion
    // the floor(... + 0.5) operation is the continuity correction
    int y = (int) Math.floor(mu + sigma*(z + gamma*(z*z - 1.0)/6.0) + 0.5);
		
    // Given this starting point, line search to the right or left.
    // Bisection search is not necessary, since the approximation is rarely 
    // off by more than 1 or 2
    z = this.cumulativeProbability(y);
		
    if ( z > prob) { // missed it to the right, search to the left
        while(true) {
            if (y == 0 || this.cumulativeProbability(y - 1) < prob)
                return y;
            y--;
        }
    } else { // missed it to the left, search to the right
        while(true) {
            y++;
            if (this.cumulativeProbability(y) >= prob)
                return y;
        }
    }
}","Operating System: other
Platform: PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,36105.0,,,2005-08-10 06:57:09.0,,,false,,,,,,,,,,,,,,150329,,,Wed Aug 17 13:53:21 UTC 2005,,,,,,0|i0rwkn:,160921,,,,,,,,"10/Aug/05 06:57;brentworden;Thanks.  

Unfortunately, I tried adding that method to PoissonDistributionImpl but it 
resulted in unit test errors.  Most of the errors dealt with boundry conditions 
and degenerative cases.

Can you try to modify your code so that is passes these test as well as add 
some more tests for your large mean case?

Thanks

","17/Aug/05 13:53;brentworden;Fixed. SVN Revision 233131. Added to MATH_1_1 branch.

Continued fractions convergents were diverging because of infinite intermediate 
computations.  To solve this, if infinite computations are detected, linear 
scaling is performed to lessen the magnitude of these computations.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] DoubleArray: Document RuntimeException,MATH-91,12341448,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tobrien@discursive.com,tobrien@discursive.com,15/May/04 23:14,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Even though we don't have to, it is a good idea in the JavaDoc to document the 
exact conditions under which a RuntimeException such as IllegalArgumentE or 
ArrayOutOfBoundsE can be thrown.  Specifically, getElement() needs an explicit 
description of when aoobe can be thrown.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,29013.0,,,2004-05-17 22:34:13.0,,,false,,,,,,,,,,,,,,150328,,,Mon May 17 22:34:13 UTC 2004,,,,,,0|i0rwkv:,160922,,,,,,,,"17/May/04 22:34;brentworden;If/When this task is undertaken, one might need to add the exceptions to the 
throws clause, otherwise, I think a warning appears in the javadoc report and 
quite possibly in the checkstyle report.  If the exceptions aren't added, I'm 
sure the checkstyle warning could be suppressed but I'm unsure about the 
javadoc warnings.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] BeanUtils dependence in porg.apache.commons.math.stat.univariate,MATH-90,12342634,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Incomplete,,elharo@metalab.unc.edu,elharo@metalab.unc.edu,15/Oct/05 00:52,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,1.1,,,,,,,,,,0,,,,,,,,"The BeanListUnivariateImpl and BeanListUnivariateImpltest classes in the
experiemntal directory depend on beanUtils. However, this jar is not bundled or
referenced as a dependency from project.xml.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,37096.0,,,2005-10-15 11:46:45.0,,,false,,,,,,,,,,,,,,150327,,,Sat Oct 15 11:46:45 UTC 2005,,,,,,0|i0rwl3:,160923,,,,,,,,"15/Oct/05 11:46;phil@steitz.com;The /experimental sources are not included in release or nightly build
distributions.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] PolynomialSplineFunction does not implement DifferentiableUnivariateRealFunction,MATH-89,12342661,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,wboyce@panix.com,wboyce@panix.com,26/Oct/05 23:39,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,1.0,,,,,,,,,,0,,,,,,,,"DifferentiableUnivariateRealFunction declares one method:

UnivariateRealFunction derivative();

PolynomialSplineFunction has this method, but it only implements 
UnivariateRealFunction, not DifferentiableUnivariateRealFunction.

This has not caused any problems for me, but it seems like something that 
should be changed.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,37255.0,,,2005-11-25 01:04:49.0,,,false,,,,,,,,,,,,,,150326,,,Fri Nov 25 01:04:49 UTC 2005,,,,,,0|i0rwlb:,160924,,,,,,,,"25/Nov/05 01:04;phil@steitz.com;Thanks for pointing this out.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Add sign() method to MathUtils,MATH-88,12340757,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,hotfusionman@yahoo.com,hotfusionman@yahoo.com,05/Jun/03 14:20,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"While implementing Ridders' method for root finding, I encountered a need for a
function that would return +1 or -1 depending on the sign of its argument.  I
started a MathUtility class to hold such things, but Phil beat me to the punch
by submitting MathUtils, so I am submitting additions to his class.","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,06/Jun/03 03:59;hotfusionman@yahoo.com;ASF.LICENSE.NOT.GRANTED--MathUtils.diff;https://issues.apache.org/jira/secure/attachment/12332309/ASF.LICENSE.NOT.GRANTED--MathUtils.diff,06/Jun/03 03:58;hotfusionman@yahoo.com;ASF.LICENSE.NOT.GRANTED--MathUtilsTest.diff;https://issues.apache.org/jira/secure/attachment/12332308/ASF.LICENSE.NOT.GRANTED--MathUtilsTest.diff,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,20496.0,,,2003-06-05 21:09:25.0,,,false,,,,,,,,,,,,,,150325,,,Fri Jun 06 10:11:00 UTC 2003,,,,,,0|i0rwlj:,160925,,,,,,,,05/Jun/03 21:09;mdiggory@gmail.com;Is there a patch for this yet?,"06/Jun/03 03:58;hotfusionman@yahoo.com;Created an attachment (id=6654)
In good TDD style, submitting test cases for new sign method before code
","06/Jun/03 03:59;hotfusionman@yahoo.com;Created an attachment (id=6655)
patch to implement sign method
","06/Jun/03 04:40;mdiggory@gmail.com;I'll apply these and run the tests. But here are some comments.

1.) I suspect your using diff directly, I would recommend using cvs diff, this
way your diffs are built directly against the current cvs versions and not a
local copy (which I'm assuming is where the .java~2~ extensions are coming
from), this will also generate a header in your diff that looks similar to the
following

Index: MathUtilsTest.java
===================================================================
RCS file:
/home/cvs/jakarta-commons-sandbox/math/src/test/org/apache/commons/math/MathUtilsTest.java,v
retrieving revision 1.1
diff -u -r1.1 MathUtilsTest.java
--- MathUtilsTest.java  2003-06-03 13:39:19.000000000 -0700
+++ MathUtilsTest.java	2003-06-04 23:45:33.000000000 -0700

2.) Try to generate your patches from the root of the project (at the level of
the math directory), not at the individual file level. Then you do not need to
submit separate patches for each file.

This will help me to be able to quickly apply your patches.

Cheers and thank you for your excellent efforts,
Mark","06/Jun/03 05:58;hotfusionman@yahoo.com;OK, will do.  Still inexperienced with CVS and mainstream software engineering
practices, having come from a world where software engineering is largely
unknown (scientific computing).","06/Jun/03 06:31;mdiggory@gmail.com;Same here, I didn't learn this stuff until I had to participate on software
development projects directly. Hey, it looks good on a resume' when you can say
you've worked with all these tools. :-) 

Here's some good reading ""fodder"" on cvs diff/patch at Apache
http://cocoon.apache.org/2.0/howto/howto-patch.html

Jakarta's isn't quite as detailed.
http://jakarta.apache.org/site/source.html

",06/Jun/03 10:11;mdiggory@gmail.com;Commited and Unit-Tested.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math]TTest should support paired and homoscedastic tests,MATH-87,12341454,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,18/May/04 11:26,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The TTest interface and implementation support only non-paired, unequal variance
tests.  The javadoc should make this clearer.  Support for paired sample tests
and tests under the assumption of equal subpopulation variances should also be
supported.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,29049.0,,,,,,false,,,,,,,,,,,,,,150324,,,Sun Jun 06 02:41:20 UTC 2004,,,,,,0|i0rwlr:,160926,,,,,,,,24/May/04 12:38;phil@steitz.com;Support for paired t-tests has been added.,"06/Jun/04 02:41;phil@steitz.com;Added equalVariances flag to enable homo/heteroscedastic tests, improved
javadoc, updated user guide to reflect changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] improvements to UnivariateImpl,MATH-86,12340723,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,23/May/03 15:16,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch includes the following improvements to Univariate and
UnivariateImpl:

* Improved efficiency of min, max and product maintenance when windowSize is 
  limited by incorporating suggestion posted to commons-dev by Brend Worden 
  (added author credit).  Thanks, Brent!

* Added javadoc specifying NaN contracts for all statistics, definitions for
  geometric and arithmetic means.

* Made some slight modifications to UnivariateImpl to make it consistent with
  NaN contracts

* All interface documentation moved to Univariate. The interface specification
  includes the NaN semantics and a first attempt at clealy defining exactly what
  ""rolling"" means and how this effects what statistics are defined when.

* Added test cases to verify that min, max, product are correctly maintained
  when ""rolling"" and to verify that NaN contracts are satisfied.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,23/May/03 15:18;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--UnivariatePatches.txt;https://issues.apache.org/jira/secure/attachment/12332284/ASF.LICENSE.NOT.GRANTED--UnivariatePatches.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20175.0,,,2003-05-24 02:09:12.0,,,false,,,,,,,,,,,,,,150323,,,Sat May 24 02:09:12 UTC 2003,,,,,,0|i0rwlz:,160927,,,,,,,,"23/May/03 15:18;phil@steitz.com;Created an attachment (id=6466)
Patches for UnivariateImpl, Univariate, UnivariateImplTest
","24/May/03 02:09;tobrien@discursive.com;Patch Submitted, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math]  SimpleRegression getSumSquaredErrors,MATH-85,12343055,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mark.osborn@research.ge.com,mark.osborn@research.ge.com,28/Apr/06 00:00,06/Jul/06 12:11,07/Apr/19 20:38,06/Jul/06 12:11,1.1,,,,,,,,,,0,,,,,,,,"getSumSquaredErrors returns -ve value. See test below:

public void testSimpleRegression() {
		double[] y = {  8915.102, 8919.302, 8923.502};
		double[] x = { 1.107178495, 1.107264895, 1.107351295};
		double[] x2 = { 1.107178495E2, 1.107264895E2, 1.107351295E2};
		SimpleRegression reg = new SimpleRegression();
		for (int i = 0; i < x.length; i++) {
			reg.addData(x[i],y[i]);
		}
		assertTrue(reg.getSumSquaredErrors() >= 0.0); // OK
		reg.clear();
		for (int i = 0; i < x.length; i++) {
			reg.addData(x2[i],y[i]);
		}
		assertTrue(reg.getSumSquaredErrors() >= 0.0); // FAIL
		
	}","Operating System: Windows 2000
Platform: PC",,,,,,,,,,,,,,,,,,,,05/Jul/06 04:19;luc;math-85.patch;https://issues.apache.org/jira/secure/attachment/12336345/math-85.patch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,39432.0,,,2006-07-05 04:15:40.0,,,false,,,,,,,,,,,,,,34220,,,Thu Jul 06 12:11:33 UTC 2006,,,,,,0|i0rwm7:,160928,,,,,,,,"05/Jul/06 04:15;luc;The problem is related to computation accuracy in a corner case.

The data (110.7178495, 8915.102), (110.7264895, 8919.302), (110.7351295, 8923.502) represent three points on a perfect straigth line, with the second point exactly at the middle of the two extreme points. In this case, the sum of the squares of the errors should be exactly 0 as all points lie exactly on the estimated line.

If instead of checking reg.getSumSquaredErrors() >= 0.0 I print the value, I get -7.105427357601002E-15 on my GNU/Linux box. This seems quite fair for me as the computation involves computing a subtraction close to 35.28 - 35.28, where both terms result from several former computations. This is consistent with double precision.

What we observe here is simply a cancellation effect on subtraction. The result is null in the first part of the test (where the x values are 100 times smaller), slightly negative in the second part. I think the null result in the first part is only good fortune (well, it is really related to the orders of magnitude involved: x^2, y^2 and xy).

I suggest to consider this is not a bug.
I will add a patch with a slightly modified test case in a few minutes.",05/Jul/06 04:19;luc;patch adding a test case for issue MATH-85,"05/Jul/06 04:35;psteitz;I agree this is a corner case and the negative result is due to rounding.  The question is, should we force the result to 0 when a negative value is returned by the computation? ",06/Jul/06 12:11;psteitz;Constrained returned result to be non-negative.  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] javadoc and checkstyle changes,MATH-84,12340943,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,06/Sep/03 12:37,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"fixed most all checkstyle errors.  Also, add some more checks to the checkstyle 
config.","Operating System: other
Platform: All",,,,,,,,,,,,,,,,,,,,06/Sep/03 12:38;brentworden;ASF.LICENSE.NOT.GRANTED--checkstyle-patch.txt;https://issues.apache.org/jira/secure/attachment/12332502/ASF.LICENSE.NOT.GRANTED--checkstyle-patch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,22954.0,,,2003-09-07 10:15:58.0,,,false,,,,,,,,,,,,,,150322,,,Sun Sep 07 10:15:58 UTC 2003,,,,,,0|i0rwmf:,160929,,,,,,,,"06/Sep/03 12:38;brentworden;Created an attachment (id=8079)
the patch file
",07/Sep/03 10:15;mdiggory@gmail.com;I'll apply these. Thanks Brent.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] MathUtils Javadoc and minor correction to an indicator method,MATH-83,12341386,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,cschuck@stny.rr.com,cschuck@stny.rr.com,05/Apr/04 09:19,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Some additions and fixes for javadoc in the MathUtils class, and a fix for a
minor bug in the indicator method used for short values.","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,05/Apr/04 09:20;cschuck@stny.rr.com;ASF.LICENSE.NOT.GRANTED--mupatch;https://issues.apache.org/jira/secure/attachment/12332860/ASF.LICENSE.NOT.GRANTED--mupatch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,28194.0,,,2004-04-05 10:49:34.0,,,false,,,,,,,,,,,,,,150321,,,Mon Apr 05 10:49:34 UTC 2004,,,,,,0|i0rwmn:,160930,,,,,,,,"05/Apr/04 09:20;cschuck@stny.rr.com;Created an attachment (id=11130)
unified cvs diff file for the patch
","05/Apr/04 10:49;phil@steitz.com;Patch applied, thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math]  FractionFormatTest doesn't compile under JDK 1.3,MATH-82,12342983,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Won't Fix,,niallp,niallp,21/Mar/06 09:47,23/Jul/12 23:24,07/Apr/19 20:38,11/Jun/07 05:11,,,,,,,,,,,0,,,,,,,,"FractionFormatTest uses NumberFormat.getIntegerInstance() which is a JDK 1.4 
method. Changing this to use getInstance() instead and then using 
setParseIntegerOnly(true) allows it to compile under JDK 1.3 - and the test 
runs and passes.

Its probably a moot point though since ComplexUtilsTest fails using JDK 
1.3.1_04 (on both W2K and Windows XP).","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,21/Mar/06 09:54;niallp;ASF.LICENSE.NOT.GRANTED--TEST-org.apache.commons.math.complex.ComplexUtilsTest.txt;https://issues.apache.org/jira/secure/attachment/12334079/ASF.LICENSE.NOT.GRANTED--TEST-org.apache.commons.math.complex.ComplexUtilsTest.txt,21/Mar/06 09:48;niallp;ASF.LICENSE.NOT.GRANTED--math_FractionFormatTest_JDK13.patch;https://issues.apache.org/jira/secure/attachment/12334078/ASF.LICENSE.NOT.GRANTED--math_FractionFormatTest_JDK13.patch,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,39043.0,,,2006-03-21 12:21:18.0,,,false,,,,,,,,,,,,,,150320,,,Mon Jun 11 05:11:50 UTC 2007,,,,,,0|i0rwmv:,160931,,,,,,,,"21/Mar/06 09:48;niallp;Created an attachment (id=17928)
Patch for FractionFormatTest
","21/Mar/06 09:54;niallp;Created an attachment (id=17929)
Output from ComplexUtilsTest

In case its of interest, heres the output from the ComplexUtilsTest which
failed under JDK 1.3","21/Mar/06 12:21;phil@steitz.com;Thanks for reporting this.  Looks like the compile problem was introduced in
r240383 (after 1.1 release), but ComplexUtils test was included as is in 1.1. 
Will research some more and validate problem on other platforms.  Complile patch
at least should be applied.  ","26/Mar/06 11:54;phil@steitz.com;Patch to fix compile error applied.  Thanks!

The ComplexUtils test failure does not happen on Sun Linux jdk 1.3.1_18 (which
is what the 1.1 release was created with).  This is disturbing, since the test
is there to verify documented behavior.  Looks to me like a bug in the windows
1.3 JDK, since what it causing the failures is that
Math.cos(Double.POSITIVE_INFINITY) is returning a non-NAN value, which according
to the API docs, it should not.

The only remedy that I can think of for this is to conditionally skip the
failing tests if the OS is windows.  Not nice.","27/Oct/06 07:53;aeriform;Not a bug according to Sun.

http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4857011


""We are aware of the issue raised by this bug; it is the same issue
raised in 4807358, the ""almabench"" bug.  Both this bug and 4807358 are
symptoms of the same problem: the sin/cos routines in the client
compiler under 1.3.1 did *not* comply with the Math.{sin/cos}
specification.  As explained below, the way they did not comply could
be the source of a large amount of error.  As a consequence of fixing
this compliance problem (bug 4345903), the x86 sin/cos routines in 1.4
and later are slower than in 1.3.1 client.

The x87 FPU has fsin and fcos instructions to accelerate the
computation of sine and cosine.  However, these instructions have a
number of limitations. First, they only return sensible results over a
limited range of values, +/- 2^63.  Java's sin/cos functions are defined
over the full double range, roughly +/- 2^1023.  Second, even within
the +/-2^63 range, nearly all fsin/fcos implementations perform faulty
*argument reduction,* consequently, the results can be very wrong
outside of a narrow range of +/- pi/4.

...""","27/Oct/06 07:59;aeriform;Math ==>
    public static double sin(double a) {
	return StrictMath.sin(a); // default impl. delegates to StrictMath
    }
    public static double cos(double a) {
	return StrictMath.cos(a); // default impl. delegates to StrictMath
    }

StrictMath ==>
    public static native double sin(double a);
    public static native double cos(double a);","05/Mar/07 19:46;luc;The FractionFormatTest patch has been applied almost one year ago.
The problem with ComplexUtilsTest seems OS-specific and not related to commons-math.
Could we close this bug ?",11/Jun/07 05:11;psteitz;Nothing we can do about incorrect JDK behavior.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] eliminate some author tags,MATH-81,12340842,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,07/Jul/03 14:53,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,The attached patch eliminates my author tags,"Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,07/Jul/03 14:55;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--authorPatch.txt;https://issues.apache.org/jira/secure/attachment/12332407/ASF.LICENSE.NOT.GRANTED--authorPatch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,21360.0,,,2003-07-07 20:45:00.0,,,false,,,,,,,,,,,,,,150319,,,Tue Jul 08 21:56:01 UTC 2003,,,,,,0|i0rwn3:,160932,,,,,,,,"07/Jul/03 14:55;phil@steitz.com;Created an attachment (id=7114)
Patch removing some author tags
","07/Jul/03 20:45;mdiggory@gmail.com;Hey Phil, I want to wait just a hair on this till we get a response from Tim,
his question was a little ambiguous and I'd rather see people get credit where
its due for their efforts (esp those who are submitting patches).","08/Jul/03 06:13;mdiggory@gmail.com;Phil, I was about to apply this patch. Are you sure you want to remove your name
from the contributors section of the project.xml? I'll apply the patch but not
alter the project.xml.

-Mark","08/Jul/03 21:56;tobrien@discursive.com;Just to clarify, I think that some level of attribution is somewhat essential 
in this community.  I think the contributor's list in project.xml is an 
appropriate place for listing contributors.

In general, I'm swayed by the arguments on community@ against author tags at 
the code level.  If we want this effort to scale to more developers, 20+ it 
makes sense to get rid of author tags at the code level and increase 
attribution in our site documentation. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] some initial code,MATH-80,12340705,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,13/May/03 00:22,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Some initial stuff for commons-sandbox/math -- more to follow.

Freq, Univariate, RealMatrix, RealMatriximpl classes plus test cases.

RealMatrixImpl is missing inversion and some other key functionality.  Interface
is also a bit ""spartan""","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,13/May/03 00:25;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--buildPatch.txt;https://issues.apache.org/jira/secure/attachment/12332266/ASF.LICENSE.NOT.GRANTED--buildPatch.txt,13/May/03 00:24;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--src.tar;https://issues.apache.org/jira/secure/attachment/12332265/ASF.LICENSE.NOT.GRANTED--src.tar,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,19859.0,,,2003-05-13 03:14:23.0,,,false,,,,,,,,,,,,,,150318,,,Tue May 13 03:14:23 UTC 2003,,,,,,0|i0rwnb:,160933,,,,,,,,"13/May/03 00:24;phil@steitz.com;Created an attachment (id=6325)
tar including sources for classes and tests
","13/May/03 00:25;phil@steitz.com;Created an attachment (id=6326)
diff to add MathTestSuite to build.xml
",13/May/03 03:14;rdonkin@apache.org;committed. thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Math] homoscedasticTTest method uses non homoscedastic method,MATH-79,12342215,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,gillesgaillard@compuserve.com,gillesgaillard@compuserve.com,29/Apr/05 19:40,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"In TTestImpl, implementation of the method: 
  double homoscedasticTTest(double,double,double,double,double,double)
calls method:
  t(double,double,double,double,double,double)
where one could expect it to call:
  homoscedasticT(double,double,double,double,double,double)","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,34677.0,,,2005-05-02 14:19:05.0,,,false,,,,,,,,,,,,,,34215,,,Mon May 02 14:19:05 UTC 2005,,,,,,0|i0rwnj:,160934,,,,,,,,"02/May/05 14:19;phil@steitz.com;Good catch - yes, this should use the homoscedastic t statistic. Previous test
case masked the error, as the t statistics took the same value.  Code has been
fixed and test cases made more sensitive. 

Fixed in nightly builds starting 02-may-2005.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] serialization tests did not delete temp files,MATH-78,12342452,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,zxg_32@yahoo.com,zxg_32@yahoo.com,09/Aug/05 01:55,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"There are some TestUtils.serializeAndRecover() calls in the test files.
They create temp files like test10487.ser in user temp directory
java.io.tmpdir (total of 127 for a complete build right now). They are
not removed after the build is finished, succeeded or not. They should
be deleted.","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,36084.0,,,2005-08-13 13:08:08.0,,,false,,,,,,,,,,,,,,150317,,,Sat Aug 13 13:08:08 UTC 2005,,,,,,0|i0rwnr:,160935,,,,,,,,"13/Aug/05 13:08;brentworden;Fixed.  SVN revision 232415.  Temp files were not being deleted because open 
file streams were not being closed properly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Interpolation Source Files,MATH-77,12342463,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,zxg_32@yahoo.com,zxg_32@yahoo.com,13/Aug/05 02:48,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Add ExpFunction.java
Add NevilleInterpolator.java
Add NevilleInterpolatorTest.java
Add PolynomialFunctionLagrangeForm.java
Add PolynomialFunctionLagrangeFormTest.java

ExpFunction.java is in the same directory of NevilleInterpolatorTest.java","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,13/Aug/05 02:49;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--Interpolation.zip;https://issues.apache.org/jira/secure/attachment/12333708/ASF.LICENSE.NOT.GRANTED--Interpolation.zip,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,36167.0,,,2005-08-25 06:45:31.0,,,false,,,,,,,,,,,,,,150316,,,Thu Aug 25 06:45:31 UTC 2005,,,,,,0|i0rwnz:,160936,,,,,,,,"13/Aug/05 02:49;zxg_32@yahoo.com;Created an attachment (id=16022)
Interpolation Source Files
","25/Aug/05 06:45;j3322ptm@yahoo.de;Files added (without change, revision pending)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] Improve performance, flexibility of discovery impl",MATH-76,12341458,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,19/May/04 01:43,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Use of commons-discovery to implement pluggable factories in [math] should be 
modified throughout to catch ClassNotFoundException and to cache instances 
(""newInstance"" should be replaced by ""getInstance"").","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,29068.0,,,2004-06-06 07:37:17.0,,,false,,,,,,,,,,,,,,150315,,,Sun Jul 11 01:23:10 UTC 2004,,,,,,0|i0rwo7:,160937,,,,,,,,19/May/04 01:47;phil@steitz.com;*** COM-1307 has been marked as a duplicate of this bug. ***,"06/Jun/04 07:37;mdiggory@gmail.com;My major concern is that there are multiple usecases here, even in your code
example:

DistributionFactory factory = DistributionFactory.newInstance();

vs.

DistributionFactory factory = DistributionFactory.getInstance();

Here, the first case may construct a DistributionFactory where some internal
state/environment is maintained which makes it different than another
DistributionFactory. You can see this sort of case in examples such as SAX and
JAXP where the Factory instance is configurable and produces objects with
different states. 

In another example, with DescriptiveStatistics.newInstance() we have an issue
where the instantiated factory is also the object, it has unique state we want
to maintain multiple instances of the object. Having these methods be
getInstance instead of newInstance would be very limiting.

Right now DistributionFactory has no state that would make it anymore unique
than any other distribution factory. Which makes me think your suggested change
would be fine. But, maybe we should evaluate if we think a Factory is going to
have greater capability or configurability in the future and try not to shoot
ourselves in the foot by limiting it to a singleton status.

Finally, I wonder, if your going to reuse the object as a singleton, do
synchronous issues arise with concurrent access via the methods in the factory?

I feel we should do a review and identify the Factories we would approach this
with and bring up discussion on each. Maybe we should try to identify the types
of factories we want to maintain.  There should be some analysis applied to
determine if the Factory instance is configurable. If it is, then ""newInstance""
should be maintained. If not, the getInstance could be appropriate in cases
where reuse can occur.


Singleton Factory:
factory backed by a singleton instances of itself

Configurable Factory:
factory which can produce uniquely configured instances of itself

???

-Mark","11/Jul/04 01:23;phil@steitz.com;Implementation have been changed to catch NoClassDefFound error.  Consensus is
that caching instances reduces flexibility and should be left to the client. 
Closing bug as FIXED, though the caching part is really WONTFIX.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[bug report] Commons-Math binomial distribution method returns 1 when it should return 0,MATH-75,12341438,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Incomplete,,mdiggory@gmail.com,mdiggory@gmail.com,07/May/04 21:08,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"I wanted to open a ticket for this issue so we can address it formally.

-Mark.

-------- Original Message --------

Dear Mark,

I'm not sure if you're the right person to inform about this, but we believe
there is an error in the results produced by the
org.apache.commons.math.distribution.BinomialDistributionImpl.cumulativeProbability(int
numberOfSuccesses) method.

When the probability of success is zero, and numberOfSuccesses is greater than
zero, we would expect the cumulative probability to be zero. The result returned
is, however, one.

org.apache.commons.math.distribution.BinomialDistributionImpl.probability(int
numberOfSuccesses) behaves as expected.

Obviously this is very easy to workaround, but I thought the development team
should know.

After significant testing on these two functions we are preparing to use the
commons math package in a production application, and would like to thank you
and the other members of the team for starting work on this project. We
understand that the library is in development, and despite the extra integration
work that will be required by us as the library develops, the software you have
developed is already of great use.

Best regards,

David Morgan-Brown
Deutsche Bank

--

This e-mail may contain confidential and/or privileged information. If you are
not the intended recipient (or have received this e-mail in error) please notify
the sender immediately and destroy this e-mail. Any unauthorized copying,
disclosure or distribution of the material in this e-mail is strictly forbidden.



-- 
Mark Diggory
Software Developer
Harvard MIT Data Center
http://www.hmdc.harvard.edu","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,28829.0,,,2004-05-08 02:22:44.0,,,false,,,,,,,,,,,,,,150314,,,Sun May 23 21:53:56 UTC 2004,,,,,,0|i0rwof:,160938,,,,,,,,"08/May/04 02:22;phil@steitz.com;We are, of course thankful for the kind words of encouragement from the 
reporter:-)

I don't think that I agree with the expectation below, however.
If p = 0, then the mass of the binomial distribution is all concentrated on 0 -
- so the *cumulative* distribution function should return 1 for all values 
greater than or equal to 0.  So, I think that the correct result is being 
returned, unless I am missing something.","12/May/04 11:35;phil@steitz.com;Test cases for degenerate cases p = 0, p = 1 have been committed.  The
cumulative distribution function is returning correct values.","12/May/04 21:41;s.m.f@softhome.net;I would have thought it should have returned 1 only for zero and zero for all 
values greater then zero
","12/May/04 21:50;mdiggory@gmail.com;I would like to hold off on closing this. 

If this returned value is disputed, I'd like to see each party document it with
references as to where, why and what they are basing thier results on. I'm not
suggesting that either is correct or incorrect, but both opinions need evidence
to back them up. We want be returning an expected result, if we are deviating
from expected for some reason, lets clearly document it.

-Mark","14/May/04 04:36;brentworden;In general, by definition, the cumulative density function is F(x) = P(X <= x) 
for any distribution X.

So, for the binomial with arbitrary n and p = 1.0, F(0) = P(X <= 0) = P(X = 0) 
= 1.0 (which is what we all agree with).

For, F(1) = P(X <= 1) = P(X = 0) + P(X = 1) = 1.0 + 0.0 = 1.0.

The cumulative probability of 1.0 stands for x > 1, as well.

Furthermore, since P(X = x) is non-negative for every x, the cumulative density 
function is a non-decreasing function.  So, it is impossible for F(0) = 1.0 and 
F(1) to equal anything less than 1.0.

In, short the cumulativeProbability methods are returning the correct values.
","23/May/04 21:53;phil@steitz.com;Brent is correct.   

Cf., for example the cdf the definition here: 
http://mathworld.wolfram.com/DistributionFunction.html

As he points out, it is a basic property of distribution functions that they are
non-decreasing.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Findbugs report,MATH-74,12342529,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,sebb@apache.org,sebb@apache.org,04/Sep/05 22:36,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Just out of curiosity, I ran findbugs 0.9.2-RC3 on the code (see below)

The report shows that there are many uses of == or != for double comparisons.

As far as I can see, most of these can be avoided (e.g. MathUtils.roundUnscaled
has the parameter ""double sign"" which is effectively only used as boolean)","Operating System: All
Platform: PC",,,,,,,,,,,,,,,,,,,,05/Sep/05 02:30;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--commonsmath-fb-comments.txt;https://issues.apache.org/jira/secure/attachment/12333781/ASF.LICENSE.NOT.GRANTED--commonsmath-fb-comments.txt,04/Sep/05 22:38;sebb@apache.org;ASF.LICENSE.NOT.GRANTED--commonsmath-fb.txt;https://issues.apache.org/jira/secure/attachment/12333780/ASF.LICENSE.NOT.GRANTED--commonsmath-fb.txt,25/Nov/05 05:17;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--findBugs1-1-final;https://issues.apache.org/jira/secure/attachment/12333782/ASF.LICENSE.NOT.GRANTED--findBugs1-1-final,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,36491.0,,,2005-09-05 02:29:18.0,,,false,,,,,,,,,,,,,,150313,,,Fri Nov 25 05:19:15 UTC 2005,,,,,,0|i0rwon:,160939,,,,,,,,"04/Sep/05 22:38;sebb@apache.org;Created an attachment (id=16301)
Summary output from Findbugs report
","05/Sep/05 02:29;phil@steitz.com;Thanks!

I am attaching an annotated version.  Some of the issues are real, IMHO and
should be fixed before 1.1.  Other [math] contributors, pls weigh in.","05/Sep/05 02:30;phil@steitz.com;Created an attachment (id=16303)
Annotated findbugs report
","05/Sep/05 05:14;j3322ptm@yahoo.de;> 1,MS: org.apache.commons.math.linear.RealMatrixImpl.TOO_SMALL isn't
> final but should be
> * Should probably be fixed, but can't (backward compat.)
Why can't this be fixed? Is anybody likely to override this?

> 3,FE: Test for floating point equality in
> org.apache.commons.math.analysis.BrentSolver.solve(double,double).
> * By design. Not a real problem.  J - pls confirm.
I copied this from the text book. The case seems mainly to deal with the
initial iteration and for recovering from wild truncation errors.
Getting an estimate so that inverse quadratic interpolation is suspect to
problems for abs(x0-x2)<estimate seems to be a rather complex problem,
and dealing with this properly apparently wont save much.","05/Sep/05 08:58;sebb@apache.org;numElements/internalArray/startIndex - these could perhaps be made volatile
instead of synchronizing the access methods.
","25/Nov/05 05:17;phil@steitz.com;Created an attachment (id=17036)
Findbugs after fixes
",25/Nov/05 05:19;phil@steitz.com;Remaining complaints are by design.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] Fix style, javadoc, test coverage gaps in RandomData",MATH-73,12340752,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,03/Jun/03 14:08,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch fixes style and javadoc errors in RandomData and
RandomDataImpl and gets RandomDataTest as near to 100% coverage as possible
(missed execution paths are traversed only with near zero probability).

The patch also fixes a bug in RandomDataImpl that was, interestingly, discovered
while examining the Clover test coverage report. The reSeedSecure() method was
actually reseeding the non-secure generator.  This error could not have been
caught in any unit test.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,03/Jun/03 14:11;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--RandomDataPatches.txt;https://issues.apache.org/jira/secure/attachment/12332306/ASF.LICENSE.NOT.GRANTED--RandomDataPatches.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20442.0,,,2003-06-04 09:49:00.0,,,false,,,,,,,,,,,,,,150312,,,Wed Jun 04 09:49:00 UTC 2003,,,,,,0|i0rwov:,160940,,,,,,,,"03/Jun/03 14:11;phil@steitz.com;Created an attachment (id=6608)
patches to RandomData, RandomDataImpl, RandomDataTest
","04/Jun/03 09:49;mdiggory@gmail.com;Commited these patches and ran unit tests to verify that the RandomDataTest is
working properly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[MATH] Repetitive Permutations,MATH-72,12341094,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Incomplete,,skarupo@mail.ru,skarupo@mail.ru,14/Nov/03 12:22,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"According to the API doc, the random permutations generated by RandomData.nextPermutation(int, int) are also supposed to be non-repetitive:

http://jakarta.apache.org/commons/sandbox/math/apidocs/org/apache/commons/math/random/RandomData.html#nextPermutation(int,%20int)

In fact, the inplementing class RandomDataImpl may produce identical permutations before all possible ones have been exhausted.

I just noticed this and loaded the lates build (Jar packed on Nov 1 2003) and the problem is still there...

For example, two successive calls to nextPermutation(2,2) may produce
{1, 0}
{1, 0}","Operating System: All
Platform: PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,24699.0,,,2003-11-14 13:30:56.0,,,false,,,,,,,,,,,,,,150311,,,Wed Nov 19 13:36:55 UTC 2003,,,,,,0|i0rwp3:,160941,,,,,,,,"14/Nov/03 13:30;brentworden;""without repetition"" in the javadoc implies a individual item will not be 
selected more than once for each permutation.  So, for nextPermutation(3, 3), 
the sequence (0, 0, 1) or (1, 2, 1) will never should up because they have 
duplicate items.

If I changed the javadoc to ""... whose entries are sampled randomly, without 
replacement, ...,"" would the meaning become more explicit?","14/Nov/03 13:56;skarupo@mail.ru;OK, I see. I think the word permutation already implies that. 

BTW, thanks for writing the code! Very useful.",15/Nov/03 00:43;amamment;Adding math to the summary line....,19/Nov/03 12:40;mdiggory@gmail.com;I'm going to assume this is resolved,"19/Nov/03 13:02;skarupo@mail.ru;Yes, thank you. Was I supposed to verify it or do anything else as the reporter of the bug?","19/Nov/03 13:36;mdiggory@gmail.com;No, as long as the issue was addressed, then I just need to close the Bug.
Thanks for reporting. Keep looking :-)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] user guide additions - root finding,MATH-71,12340996,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,24/Sep/03 02:56,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,Added text to the root finding portion of the user guide.,"Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,24/Sep/03 02:56;brentworden;ASF.LICENSE.NOT.GRANTED--analysis-xdoc.txt;https://issues.apache.org/jira/secure/attachment/12332546/ASF.LICENSE.NOT.GRANTED--analysis-xdoc.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,23367.0,,,2003-09-25 03:15:35.0,,,false,,,,,,,,,,,,,,150310,,,Thu Sep 25 03:15:35 UTC 2003,,,,,,0|i0rwpb:,160942,,,,,,,,"24/Sep/03 02:56;brentworden;Created an attachment (id=8320)
patch file
","25/Sep/03 03:15;j3322ptm@yahoo.de;Applied, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Why are the methods of TTest not static?,MATH-70,12341943,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,m.e.foster@ed.ac.uk,m.e.foster@ed.ac.uk,13/Dec/04 22:56,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,1.0,,,,,,,,,,0,,,,,,,,"To do T-tests on data -- for which many thanks! -- you currently have to do the
following

TTest tester = new TTestImpl();
tester.pairedTTest( ... );

There are two things that are inelegant about this implementation: you have to
explicitly call ""new TTestImpl()"" instead of using a factory method, and I don't
see why there's this whole interface-implementation paradigm at all. As far as I
can see, there's no state stored in the TTest implementation; why isn't it a
singleton class with static methods instead?","Operating System: Linux
Platform: PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,32663.0,,,2004-12-14 06:54:29.0,,,false,,,,,,,,,,,,,,150309,,,Sun Jun 05 13:08:21 UTC 2005,,,,,,0|i0rwpj:,160943,,,,,,,,"14/Dec/04 06:54;phil@steitz.com;Great questions.  The interface / implementation separation is to support the 
strategy pattern for alternative implementations.  Staying away from static 
methods in implementations is for extensibility.  All that said, I agree that 
the setup is a little awkward.  

One way to improve usability would be to introduce a TestUtils class that 
would work similarly to StatUtils -- create singleton instances of the 
implementation classes and expose static methods that delegate to the 
singletons.  Any better ideas?  Patches welcome.","05/Jun/05 13:08;phil@steitz.com;Added factory and static utils class.  Should be fixed in nightly builds
starting 5 Jun 05.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Improve numerics in BivariateRegression,MATH-69,12340806,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,21/Jun/03 07:41,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch implements the Chan-Golub-Leveque updating formulas adapted
to means and regression sums of squares to improve accuracy of statistical
computations in BivariateRegression.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,21/Jun/03 07:42;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--regressionUpdate.txt;https://issues.apache.org/jira/secure/attachment/12332376/ASF.LICENSE.NOT.GRANTED--regressionUpdate.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20979.0,,,2003-06-21 09:16:53.0,,,false,,,,,,,,,,,,,,150308,,,Sat Jun 21 09:16:53 UTC 2003,,,,,,0|i0rwpr:,160944,,,,,,,,"21/Jun/03 07:42;phil@steitz.com;Created an attachment (id=6922)
patches to BivariateRegression, BivariateRegressionTest
","21/Jun/03 09:16;mdiggory@gmail.com;rebuilt, unit tested and commited",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Failure to fail when testing for exceptions in MathUtils,MATH-68,12342633,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,elharo@metalab.unc.edu,elharo@metalab.unc.edu,15/Oct/05 00:45,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,1.1,,,,,,,,,,0,,,,,,,,"In MathUtilsTest I found the following code:

    public void testAddAndCheck() {
        int big = Integer.MAX_VALUE;
        int bigNeg = Integer.MIN_VALUE;
        assertEquals(big, MathUtils.addAndCheck(big, 0));
        try {
            int res = MathUtils.addAndCheck(big, 1);
        } catch (ArithmeticException ex) {}
        try {
            int res = MathUtils.addAndCheck(bigNeg, -1);
        } catch (ArithmeticException ex) {}
    }
    
    public void testMulAndCheck() {
        int big = Integer.MAX_VALUE;
        int bigNeg = Integer.MIN_VALUE;
        assertEquals(big, MathUtils.mulAndCheck(big, 1));
        try {
            int res = MathUtils.mulAndCheck(big, 2);
        } catch (ArithmeticException ex) {}
        try {
            int res = MathUtils.mulAndCheck(bigNeg, 2);
        } catch (ArithmeticException ex) {}
    }
    
    public void testSubAndCheck() {
        int big = Integer.MAX_VALUE;
        int bigNeg = Integer.MIN_VALUE;
        assertEquals(big, MathUtils.subAndCheck(big, 0));
        try {
            int res = MathUtils.subAndCheck(big, -1);
        } catch (ArithmeticException ex) {}
        try {
            int res = MathUtils.subAndCheck(bigNeg, 1);
        } catch (ArithmeticException ex) {}
    }

These tests pass even if the expected excepiton is thrown. All three should be
reworked with fail() statements at the end of the try block like so:

        try {
            int res = MathUtils.subAndCheck(big, -1);
            fail(""Didn't throw exceptions when subtracting one over the maximum"");
        } catch (ArithmeticException ex) {}
        try {
            int res = MathUtils.subAndCheck(bigNeg, 1);
            fail(""Didn't throw exceptions when subtracting one over the maximum"");
        } catch (ArithmeticException ex) {}


I doubt there's a real bug here, but if there is, these tests won't find it.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,37095.0,,,2005-10-16 09:20:33.0,,,false,,,,,,,,,,,,,,150307,,,Sun Oct 16 09:20:33 UTC 2005,,,,,,0|i0rwpz:,160945,,,,,,,,16/Oct/05 09:20;phil@steitz.com;Thanks for reporting this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Completing RealMatrixImpl,MATH-67,12340790,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,15/Jun/03 12:49,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch adds implementations for solve(), inverse() and other missing
method implementations in RealMatrixImpl. The patch also corrects checkstyle
errors and brings test path coverage to as close to 100% as practical.

The one remaining missing method implementation is getRank().","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,15/Jun/03 12:51;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--RealMatrixPatch.txt;https://issues.apache.org/jira/secure/attachment/12332357/ASF.LICENSE.NOT.GRANTED--RealMatrixPatch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20783.0,,,2003-06-16 00:04:36.0,,,false,,,,,,,,,,,,,,150306,,,Mon Jun 16 00:04:36 UTC 2003,,,,,,0|i0rwq7:,160946,,,,,,,,"15/Jun/03 12:51;phil@steitz.com;Created an attachment (id=6836)
patches to RealMatrixImpl, RealMatrix, RealMatrixImplTest
",16/Jun/03 00:04;mdiggory@gmail.com;Patched and Tested.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] fixed some checkstyle warnings,MATH-66,12340761,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Won't Fix,,brentworden,brentworden,06/Jun/03 03:33,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,removed a lot of tabs and 80+ character lines.,"Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,20522.0,,,2003-06-11 10:39:47.0,,,false,,,,,,,,,,,,,,150305,,,Wed Jun 11 10:39:47 UTC 2003,,,,,,0|i0rwqf:,160947,,,,,,,,"11/Jun/03 10:39;tobrien@discursive.com;I think this bug is a mistake, I don't see any patches.  I'm marking is resolved
WONTFIX",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] user guide additions - distribution framework,MATH-65,12340993,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,24/Sep/03 01:29,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,Added text to distribution framework portion of the user guide.,"Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,24/Sep/03 01:30;brentworden;ASF.LICENSE.NOT.GRANTED--stat-xdoc.txt;https://issues.apache.org/jira/secure/attachment/12332545/ASF.LICENSE.NOT.GRANTED--stat-xdoc.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,23361.0,,,2003-09-25 03:15:44.0,,,false,,,,,,,,,,,,,,150304,,,Thu Sep 25 03:15:44 UTC 2003,,,,,,0|i0rwqn:,160948,,,,,,,,"24/Sep/03 01:30;brentworden;Created an attachment (id=8319)
patch file
","25/Sep/03 03:15;j3322ptm@yahoo.de;Applied, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Cleanup and add inferential stats to BivariateRegression,MATH-64,12340778,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,11/Jun/03 01:45,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch includes the following improvements to BivariateRegression

* Fixed all checkstyle errors and eliminated redundant NaN checks.  Now have
100% test path coverage.

* Used distribution framework TDistribution to implement
getSlopeConfidenceInterval and getSignificance methods.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,11/Jun/03 01:46;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--regressionPatch.txt;https://issues.apache.org/jira/secure/attachment/12332340/ASF.LICENSE.NOT.GRANTED--regressionPatch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20657.0,,,2003-06-11 10:35:55.0,,,false,,,,,,,,,,,,,,150303,,,Wed Jun 11 10:35:55 UTC 2003,,,,,,0|i0rwqv:,160949,,,,,,,,"11/Jun/03 01:46;phil@steitz.com;Created an attachment (id=6739)
patches to BivariateRegression, BivariateRegressionTest
",11/Jun/03 10:35;tobrien@discursive.com;Patch committed.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] ConplexFormat tests fail for French locale,MATH-63,12341735,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,21/Sep/04 10:48,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The ComplexFormat tests are failing when run on a French locale system. Test
output reported to commons-dev by Paul Libbrecht:

> cat  target/test-reports/TEST-
org.apache.commons.math.complex.ComplexFormatTest.txt
Testsuite: org.apache.commons.math.complex.ComplexFormatTest
Tests run: 34, Failures: 17, Errors: 0, Time elapsed: 1,09 sec

Testcase: 
testSimpleWithDecimals(org.apache.commons.math.complex.ComplexFormatTest ):   
FAILED
expected:<....23 + 1....> but was:<...,23 + 1,...>
junit.framework.ComparisonFailure: expected:<....23 + 1....> but  was:<...,23 +
1,...>
        at 
org.apache.commons.math.complex.ComplexFormatTest.testSimpleWithDecimals
(ComplexFormatTest.java:49)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase: 
testSimpleWithDecimalsTrunc(org.apache.commons.math.complex.ComplexForma tTest):
      FAILED
expected:<....23 + 1....> but was:<...,23 + 1,...>
junit.framework.ComparisonFailure: expected:<....23 + 1....> but  was:<...,23 +
1,...>
        at 
org.apache.commons.math.complex.ComplexFormatTest.testSimpleWithDecimals
Trunc(ComplexFormatTest.java:56)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase:  testNegativeReal(org.apache.commons.math.complex.ComplexFormatTest):
 FAILED
expected:<....23 + 1....> but was:<...,23 + 1,...>
junit.framework.ComparisonFailure: expected:<....23 + 1....> but  was:<...,23 +
1,...>
        at 
org.apache.commons.math.complex.ComplexFormatTest.testNegativeReal(Compl
exFormatTest.java:63)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase: 
testNegativeImaginary(org.apache.commons.math.complex.ComplexFormatTest) :    
FAILED
expected:<....23 - 1....> but was:<...,23 - 1,...>
junit.framework.ComparisonFailure: expected:<....23 - 1....> but  was:<...,23 -
1,...>
        at 
org.apache.commons.math.complex.ComplexFormatTest.testNegativeImaginary(
ComplexFormatTest.java:70)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase:  testNegativeBoth(org.apache.commons.math.complex.ComplexFormatTest):
 FAILED
expected:<....23 - 1....> but was:<...,23 - 1,...>
junit.framework.ComparisonFailure: expected:<....23 - 1....> but  was:<...,23 -
1,...>
        at 
org.apache.commons.math.complex.ComplexFormatTest.testNegativeBoth(Compl
exFormatTest.java:77)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase:  testZeroReal(org.apache.commons.math.complex.ComplexFormatTest):    
 FAILED
expected:<.......> but was:<...,...>
junit.framework.ComparisonFailure: expected:<.......> but was:<...,...>
        at 
org.apache.commons.math.complex.ComplexFormatTest.testZeroReal(ComplexFo
rmatTest.java:84)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase:  testZeroImaginary(org.apache.commons.math.complex.ComplexFormatTest):
FAILED
expected:<.......> but was:<...,...>
junit.framework.ComparisonFailure: expected:<.......> but was:<...,...>
        at 
org.apache.commons.math.complex.ComplexFormatTest.testZeroImaginary(Comp
lexFormatTest.java:91)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase: 
testStaticFormatComplex(org.apache.commons.math.complex.ComplexFormatTes t):  
FAILED
expected:<....22 - 342....> but was:<...,22 - 342,...>
junit.framework.ComparisonFailure: expected:<....22 - 342....> but  was:<...,22
- 342,...>
        at 
org.apache.commons.math.complex.ComplexFormatTest.testStaticFormatComple
x(ComplexFormatTest.java:105)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase: 
testParseSimpleWithDecimals(org.apache.commons.math.complex.ComplexForma tTest):
      FAILED
Format.parseObject(String) failed
junit.framework.AssertionFailedError: Format.parseObject(String) failed
        at 
org.apache.commons.math.complex.ComplexFormatTest.testParseSimpleWithDec
imals(ComplexFormatTest.java:147)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase: 
testParseSimpleWithDecimalsTrunc(org.apache.commons.math.complex.Complex
FormatTest):  FAILED
Format.parseObject(String) failed
junit.framework.AssertionFailedError: Format.parseObject(String) failed
        at 
org.apache.commons.math.complex.ComplexFormatTest.testParseSimpleWithDec
imalsTrunc(ComplexFormatTest.java:158)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase: 
testParseNegativeReal(org.apache.commons.math.complex.ComplexFormatTest) :    
FAILED
Format.parseObject(String) failed
junit.framework.AssertionFailedError: Format.parseObject(String) failed
        at 
org.apache.commons.math.complex.ComplexFormatTest.testParseNegativeReal(
ComplexFormatTest.java:169)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase: 
testParseNegativeImaginary(org.apache.commons.math.complex.ComplexFormat Test):
       FAILED
Format.parseObject(String) failed
junit.framework.AssertionFailedError: Format.parseObject(String) failed
        at 
org.apache.commons.math.complex.ComplexFormatTest.testParseNegativeImagi
nary(ComplexFormatTest.java:180)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase: 
testParseNegativeBoth(org.apache.commons.math.complex.ComplexFormatTest) :    
FAILED
Format.parseObject(String) failed
junit.framework.AssertionFailedError: Format.parseObject(String) failed
        at 
org.apache.commons.math.complex.ComplexFormatTest.testParseNegativeBoth(
ComplexFormatTest.java:191)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase:  testParseZeroReal(org.apache.commons.math.complex.ComplexFormatTest):
FAILED
Format.parseObject(String) failed
junit.framework.AssertionFailedError: Format.parseObject(String) failed
        at 
org.apache.commons.math.complex.ComplexFormatTest.testParseZeroReal(Comp
lexFormatTest.java:202)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase: 
testParseZeroImaginary(org.apache.commons.math.complex.ComplexFormatTest ):   
FAILED
Format.parseObject(String) failed
junit.framework.AssertionFailedError: Format.parseObject(String) failed
        at 
org.apache.commons.math.complex.ComplexFormatTest.testParseZeroImaginary
(ComplexFormatTest.java:213)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase: 
testParseDifferentImaginaryChar(org.apache.commons.math.complex.ComplexF
ormatTest):   FAILED
Format.parseObject(String) failed
junit.framework.AssertionFailedError: Format.parseObject(String) failed
        at 
org.apache.commons.math.complex.ComplexFormatTest.testParseDifferentImag
inaryChar(ComplexFormatTest.java:224)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)


Testcase:  testFormatNumber(org.apache.commons.math.complex.ComplexFormatTest):
 FAILED
expected:<.......> but was:<...,...>
junit.framework.ComparisonFailure: expected:<.......> but was:<...,...>
        at 
org.apache.commons.math.complex.ComplexFormatTest.testFormatNumber(Compl
exFormatTest.java:330)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at 
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav a:39)
        at 
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,31325.0,,,2004-09-21 11:57:37.0,,,false,,,,,,,,,,,,,,150302,,,Tue Sep 21 11:57:37 UTC 2004,,,,,,0|i0rwr3:,160950,,,,,,,,21/Sep/04 11:57;brentworden;Added locale support to complex format.  Added test cases for specific locales.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] Improve performance, flexibility of discovery impl",MATH-62,12341459,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,phil@steitz.com,phil@steitz.com,19/May/04 01:43,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Use of commons-discovery to implement pluggable factories in [math] should be 
modified throughout to catch ClassNotFoundException and to cache instances 
(""newInstance"" should be replaced by ""getInstance"").","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,29069.0,,,,,,false,,,,,,,,,,,,,,150301,,,Wed May 19 01:47:22 UTC 2004,,,,,,0|i0rwrb:,160951,,,,,,,,"19/May/04 01:47;phil@steitz.com;

*** This bug has been marked as a duplicate of 29068 ***",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Update ComplexUtils.java,MATH-61,12342496,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,zxg_32@yahoo.com,zxg_32@yahoo.com,19/Aug/05 08:29,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Index: ComplexUtils.java
===================================================================
--- ComplexUtils.java   (revision 233396)
+++ ComplexUtils.java   (working copy)
@@ -216,7 +216,10 @@

         double a = z.getReal();
         double b = z.getImaginary();
-
+        if (a == 0.0 && b == 0.0) {
+            return new Complex(0.0, 0.0);
+        }
+
         double t = Math.sqrt((Math.abs(a) + z.abs()) / 2.0);
         if (a >= 0.0) {
             return new Complex(t, b / (2.0 * t));","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,19/Aug/05 08:30;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--ComplexUtils.java;https://issues.apache.org/jira/secure/attachment/12333743/ASF.LICENSE.NOT.GRANTED--ComplexUtils.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,36266.0,,,2005-08-23 06:05:34.0,,,false,,,,,,,,,,,,,,150300,,,Thu Aug 25 07:08:10 UTC 2005,,,,,,0|i0rwrj:,160952,,,,,,,,"19/Aug/05 08:30;zxg_32@yahoo.com;add zero checks to sqrt()
","19/Aug/05 08:30;zxg_32@yahoo.com;Created an attachment (id=16107)
ComplexUtils.java
","23/Aug/05 06:05;phil@steitz.com;Patch looks good to me, with comment about how we handle 0. Not sure
computational formula is optimal, though.","25/Aug/05 07:08;j3322ptm@yahoo.de;Applied. This also fixes a LaguerreSolver test case.
Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] Function math.fraction.ProperFractionFormat.parse(String, ParsePosition) return illogical result",MATH-60,12343081,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,,Fixed,,nhung.nnguyen@gmail.com,nhung.nnguyen@gmail.com,14/May/06 04:20,16/Oct/06 23:27,07/Apr/19 20:38,05/Jun/06 10:16,1.1,,,,,,,,,,0,,,,,,,,"Hello,

I find illogical returned result from function ""Fraction parse(String source, 
ParsePostion pos)"" (in class ProperFractionFormat of the Fraction Package) of 
the Commons Math library. Please see the following code segment for more 
details:

""
ProperFractionFormat properFormat = new ProperFractionFormat();
result = null;
String source = ""1 -1 / 2"";
ParsePosition pos = new ParsePosition(0);

//Test 1 : fail 
public void testParseNegative(){
 
   String source = ""-1 -2 / 3"";
   ParsePosition pos = new ParsePosition(0);

   Fraction actual = properFormat.parse(source, pos);
   assertNull(actual);
}

// Test2: success
public void testParseNegative(){
 
   String source = ""-1 -2 / 3"";
   ParsePosition pos = new ParsePosition(0);

   Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3
   assertEquals(1, source.getNumerator());
   assertEquals(3, source.getDenominator());
}

""

Note: Similarly, when I passed in the following inputs: 
  input 2: (source = 1 2 / -3, pos = 0)
  input 3: ( source =  -1 -2 / 3, pos = 0)

Function ""Fraction parse(String, ParsePosition)"" returned Fraction 1/3 (means 
the result Fraction had numerator = 1 and  denominator = 3)for all 3 inputs 
above.
 
I think the function does not handle parsing the numberator/ denominator 
properly incase input string provide invalid numerator/denominator. 

Thank you!","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,39575.0,,,2006-05-17 13:26:08.0,,,false,,,,,,,,,,,,,,34136,,,Mon Oct 16 23:27:47 UTC 2006,,,,,,0|i0rwrr:,160953,,,,,,,,"17/May/06 13:26;psteitz;Thank you for reporting this.  I agree that the treatment of minus signs in the fractional parts of mixed fractions is not correct.  I think, however, that the correct solution is to throw a ParseException when the fractional part is
negative.  Otherwise, we have to disambiguate expressions such as ""-1 -1/3""  which could mean -4/3 or -2/3.  I think it is better to view these expressions as meaningless.  If there are no objections, I will make that change.","05/Jun/06 10:16;psteitz;Changed ProperFractionFormat to reject embedded minus, per last comment.","14/Oct/06 02:07;nhung.nnguyen@gmail.com;
   [[ Old comment, sent by email on Fri, 19 May 2006 12:29:16 -0500 ]]

Thank you to get back to me.

Have a great weekend,
Nhung


-----------------------------------------------------------------------------
--------------------------
","16/Oct/06 17:42;luc;doesn't the answers provided by Phil Steitz suit you ? This issue is marked as closed, do you want to reopen it ?","16/Oct/06 23:27;nhung.nnguyen@gmail.com;Yes, it's great. I don't need it to be reopenned. Thank you very much!

Nhu


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] chi-squared distribution resubmittal.,MATH-59,12340755,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,05/Jun/03 06:15,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"This is the resubmittal of the chi-squared distribution.  This was removed 
because of the NR references in Gamma.java.  I derived new implementations 
based on formulas found at MathWorld and elsewhere replacing any derivation 
from NR.

If anyone cares, the series representation of incomplete gamma found in NR, is 
derived from the confluent hypergeometric function of the first kind (what the 
he!! is that?).","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,05/Jun/03 06:16;brentworden;ASF.LICENSE.NOT.GRANTED--new-chi-squared-patch.txt;https://issues.apache.org/jira/secure/attachment/12332307/ASF.LICENSE.NOT.GRANTED--new-chi-squared-patch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20488.0,,,2003-06-05 21:08:09.0,,,false,,,,,,,,,,,,,,150299,,,Thu Jun 05 21:08:09 UTC 2003,,,,,,0|i0rwrz:,160954,,,,,,,,"05/Jun/03 06:16;brentworden;Created an attachment (id=6638)
path file
","05/Jun/03 21:08;mdiggory@gmail.com;I've commited these changes to the cvs and run unit testing to verify the
package still builds properly. Looks good.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] modify AbstractStoreUnivariate.getSortedValues to use built-in sort,MATH-58,12340807,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,21/Jun/03 08:17,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Attached patch replaces shell sort in AbstractStoreUnivariate.getSortedValues
with java.util.Arrays sort.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,21/Jun/03 08:18;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--abstractStoreUniariatePatch.txt;https://issues.apache.org/jira/secure/attachment/12332377/ASF.LICENSE.NOT.GRANTED--abstractStoreUniariatePatch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20980.0,,,2003-06-21 09:19:33.0,,,false,,,,,,,,,,,,,,150298,,,Sat Jun 21 09:19:33 UTC 2003,,,,,,0|i0rws7:,160955,,,,,,,,"21/Jun/03 08:18;phil@steitz.com;Created an attachment (id=6923)
Patch to AbstractStoreUnivariate
","21/Jun/03 09:19;mdiggory@gmail.com;patched, unit tested and commited",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] NaN handling in StatsUtil.min, StatsUtil.max methods",MATH-57,12342619,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,mark.osborn@research.ge.com,mark.osborn@research.ge.com,11/Oct/05 20:58,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,1.0,,,,,,,,,,0,,,,,,,,"Data with NaN appears to be incorrectly handled in some cases when calculating 
min and max using StatsUtil package. See examples below:

data={NaN, 4.0, 5.0, NaN}
max (Apache)=NaN <-- correct.
min (Apache)=NaN <-- correct.

data={4.0, 5.0, NaN}
max (Apache)=NaN <-- correct.
min (Apache)=NaN <-- correct.

data={4.0, 5.0, 7.0}
max (Apache)=7.0 <-- correct.
min (Apache)=4.0 <-- correct.

data={NaN, 4.0, 5.0}
max (Apache)=5.0 <-- incorrect? NaN.
min (Apache)=4.0 <-- incorrect? NaN.

data={2.2343, NaN, NaN, 3435.32}
max (Apache)=3435.32 <-- incorrect? NaN.
min (Apache)=3435.32 <-- incorrect? NaN.

data={3435.32, NaN, NaN, 2.2343}
max (Apache)=2.2343 <-- incorrect? NaN.
min (Apache)=2.2343 <-- incorrect? NaN.

data={NaN, NaN, NaN}
max (Apache)=NaN <-- correct.
min (Apache)=NaN <-- correct.","Operating System: Windows 2000
Platform: PC",,,,,,,,,,,,,,,,,,,,15/Oct/05 03:01;mauro@apache.org;ASF.LICENSE.NOT.GRANTED--issue-37019-patch.txt;https://issues.apache.org/jira/secure/attachment/12333845/ASF.LICENSE.NOT.GRANTED--issue-37019-patch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,37019.0,,,2005-10-11 23:05:34.0,,,false,,,,,,,,,,,,,,150297,,,Sun Oct 16 07:18:08 UTC 2005,,,,,,0|i0rwsf:,160956,,,,,,,,11/Oct/05 23:05;phil@steitz.com;Thanks for reporting this.  Looks like a bug.  Patches welcome :-),"13/Oct/05 23:07;mauro@apache.org;It's not clear from the javadocs whether any data value which is NaN
should result in the min/max values being NaN too. 

Javadocs only state that if ALL values are NaN the result is NaN.

Cases can be made for both ignoring spurious NaN or for triggering a NaN result.

Could this be clarified?
","13/Oct/05 23:45;elharo@metalab.unc.edu;The Complex class also has some questionable and undocumented behavior with
respect to NaN (and Inf). It might be worthwhile considering on a general NaN
policy for the math classes. ","14/Oct/05 00:10;kim@kimvdlinde.com;problem arrises from the underlying Max.evaluate(double[]) and
Min.evaluate(double[]) methods, who fail to address the Double.NaN values
similar to the Max.increment(double) methods, e.g. is not addressing it at all
(from class Max):

    public void increment(final double d) {
        if (d > value || Double.isNaN(value)) {
            value = d;
        }
        n++;
    }
and 
    (public double evaluate(final double[] values) is in super class, but
resolves to evaluate(values, 0, values.length) =>

    public double evaluate(final double[] values, final int begin, final int
length) {
        double max = Double.NaN;
        if (test(values, begin, length)) {
            max = values[begin];
            for (int i = begin; i < begin + length; i++) {
                max = (max > values[i]) ? max : values[i];
            }
        }
        return max;
    }

Solution:

    public double evaluate(final double[] values, final int begin, final int
length) {
        double max = Double.NaN;
        if (test(values, begin, length)) {
            max = values[begin];
            for (int i = begin; i < begin + length; i++) {
                max = ((max > values[i]) || Double.isNaN(value)) ? max : values[i];
            }
        }
        return max;
    }

similar to min and this will resolve issue in StatUtils.","14/Oct/05 00:26;kim@kimvdlinde.com;Ok, did some more checks, and the solution before was only dealing with half the
problem:

    public void increment(final double d)
    {
	if (Double.isNaN(value) || Double.isNaN(d))
	    value = Double.NaN;
        else if (d > value)
            value = d;
        n++;
    }

    public double evaluate(final double[] values, final int begin, final int length)
    {
        double max = Double.NaN;
        if (test(values, begin, length))
        {
            max = values[begin];
            for (int i = begin; i < begin + length; i++)
	    {
	        if (Double.isNaN(values[i]))
	            return Double.NaN;
                max = (max > values[i]) ? max : values[i];
            }
        }
        return max;
    }

The solution I added prematurely returns the method as soon as it finds a NaN. ","14/Oct/05 01:23;mauro@apache.org;A simple way of ensuring that all values are not NaN is to check in 
AbstractUnivariateStatistic#test() and throw an IAE if ANY are.

It would have the great advantage of simplicity and applicable to all
UnivariateStatistic impls, but could break backward compat in existing
users' code.



","14/Oct/05 11:58;phil@steitz.com;This is definitely a bug, as the javadoc indicates clearly that NaN values
should have no impact on the result:
""The result is NaN iff all values are NaN (i.e. NaN values have no impact on the
value of the statistic).""
","15/Oct/05 03:01;mauro@apache.org;Created an attachment (id=16699)
Patch to test and fix issue

Attached is a patch which fixes issue as per Javadoc spec,
ie evaluate returns NaN iff all values are NaN, else
NaNs are ignored.
",16/Oct/05 07:18;phil@steitz.com;Patch applied. Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Interpolation Source Files,MATH-56,12342464,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,zxg_32@yahoo.com,zxg_32@yahoo.com,13/Aug/05 10:54,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"rename ExpFunction.java to Expm1Function.java and make changes to it
fix bug in NevilleInterpolatorTest.java
minor changes in PolynomialFunctionLagrangeForm.java

replace these 3 files in COM-2311, others remain the same.","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,13/Aug/05 10:54;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--Interpolator.zip;https://issues.apache.org/jira/secure/attachment/12333709/ASF.LICENSE.NOT.GRANTED--Interpolator.zip,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,36175.0,,,2005-08-25 06:46:19.0,,,false,,,,,,,,,,,,,,150296,,,Thu Aug 25 06:46:19 UTC 2005,,,,,,0|i0rwsn:,160957,,,,,,,,"13/Aug/05 10:54;zxg_32@yahoo.com;Created an attachment (id=16028)
Interpolation Source Files
","25/Aug/05 06:46;j3322ptm@yahoo.de;Files added (without change, revision pending)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Solver Source Files,MATH-55,12342487,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,zxg_32@yahoo.com,zxg_32@yahoo.com,18/Aug/05 03:00,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Add RiddersSolver.java
Add RiddersSolverTest.java","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,22/Aug/05 01:38;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--LaguerreSolver.java;https://issues.apache.org/jira/secure/attachment/12333733/ASF.LICENSE.NOT.GRANTED--LaguerreSolver.java,22/Aug/05 01:39;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--LaguerreSolverTest.java;https://issues.apache.org/jira/secure/attachment/12333734/ASF.LICENSE.NOT.GRANTED--LaguerreSolverTest.java,22/Aug/05 01:37;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--MullerSolver.java;https://issues.apache.org/jira/secure/attachment/12333732/ASF.LICENSE.NOT.GRANTED--MullerSolver.java,20/Aug/05 11:06;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--MullerSolver.java;https://issues.apache.org/jira/secure/attachment/12333731/ASF.LICENSE.NOT.GRANTED--MullerSolver.java,20/Aug/05 02:10;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--MullerSolver.java;https://issues.apache.org/jira/secure/attachment/12333730/ASF.LICENSE.NOT.GRANTED--MullerSolver.java,19/Aug/05 11:38;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--MullerSolver.java;https://issues.apache.org/jira/secure/attachment/12333728/ASF.LICENSE.NOT.GRANTED--MullerSolver.java,19/Aug/05 11:43;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--MullerSolverTest.java;https://issues.apache.org/jira/secure/attachment/12333729/ASF.LICENSE.NOT.GRANTED--MullerSolverTest.java,18/Aug/05 03:00;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--RiddersSolver.java;https://issues.apache.org/jira/secure/attachment/12333726/ASF.LICENSE.NOT.GRANTED--RiddersSolver.java,18/Aug/05 03:01;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--RiddersSolverTest.java;https://issues.apache.org/jira/secure/attachment/12333727/ASF.LICENSE.NOT.GRANTED--RiddersSolverTest.java,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,36233.0,,,2005-08-25 06:48:06.0,,,false,,,,,,,,,,,,,,150295,,,Thu Aug 25 06:48:06 UTC 2005,,,,,,0|i0rwsv:,160958,,,,,,,,"18/Aug/05 03:00;zxg_32@yahoo.com;Created an attachment (id=16084)
RiddersSolver.java
","18/Aug/05 03:01;zxg_32@yahoo.com;Created an attachment (id=16085)
RiddersSolverTest.java
","19/Aug/05 11:37;zxg_32@yahoo.com;Add MullerSolver.java
Add MullerSolverTest.java
","19/Aug/05 11:38;zxg_32@yahoo.com;Created an attachment (id=16108)
MullerSolver.java
","19/Aug/05 11:43;zxg_32@yahoo.com;Created an attachment (id=16109)
MullerSolverTest.java
","20/Aug/05 02:10;zxg_32@yahoo.com;Created an attachment (id=16116)
MullerSolver.java

update MullerSolver.java
remove two lines:

import org.apache.commons.math.complex.Complex;
import org.apache.commons.math.complex.ComplexUtils;

Maybe complex arithmetics will be added in future, but for now
it's not needed.","20/Aug/05 11:06;zxg_32@yahoo.com;Created an attachment (id=16125)
MullerSolver.java

Update MullerSolver.java

simplifies some branching condition statements,
the algorithm logic remains the same.
","22/Aug/05 01:37;zxg_32@yahoo.com;Created an attachment (id=16134)
MullerSolver.java

update MullerSolver.java

minor change, add one line
		oldx = Double.POSITIVE_INFINITY;
","22/Aug/05 01:38;zxg_32@yahoo.com;Add LaguerreSolver.java
Add LaguerreSolverTest.java
","22/Aug/05 01:38;zxg_32@yahoo.com;Created an attachment (id=16135)
LaguerreSolver.java
","22/Aug/05 01:39;zxg_32@yahoo.com;Created an attachment (id=16136)
LaguerreSolverTest.java
","25/Aug/05 06:48;j3322ptm@yahoo.de;Files added (without change, revision pending)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FDistribution -- java stack overflow,MATH-54,12341490,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,scott@duchin.com,scott@duchin.com,07/Jun/04 00:24,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"This bug goes back to at least the beginning of March, tested on June 5 build.
The following block creates a java stack overflow:

try {
    org.apache.commons.math.distribution.FDistributionImpl fd =
    new org.apache.commons.math.distribution.FDistributionImpl(100000., 
100000.);
    double est;
    est = fd.cumulativeProbability(1.);
    _logger.info(""1.="" + est);
} catch (Exception jle) {
    jle.printStackTrace();
    return;
}","Operating System: Windows XP
Platform: PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,29414.0,,,2004-06-11 01:25:35.0,,,false,,,,,,,,,,,,,,34219,,,Fri Jun 11 01:25:35 UTC 2004,,,,,,0|i0rwt3:,160959,,,,,,,,"11/Jun/04 01:25;brentworden;I changed the continued fraction used in regularizedBeta resulting in faster 
convergence.  I added the test case provided by scott and ran all units tests 
with all of them passing.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] improvements to ValueServer,MATH-53,12340765,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,06/Jun/03 15:47,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch includes the following improvements to ValueServer

* add missing setters 
* add fill methods to fill double arrays with data generated by the ValueServer
* close path coverage gaps
* eliminate checkstyle errors
* improve clarity of javadoc","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,06/Jun/03 15:48;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--valueServerPatch.txt;https://issues.apache.org/jira/secure/attachment/12332329/ASF.LICENSE.NOT.GRANTED--valueServerPatch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20543.0,,,2003-06-07 08:39:57.0,,,false,,,,,,,,,,,,,,150294,,,Sat Jun 07 08:39:57 UTC 2003,,,,,,0|i0rwtb:,160960,,,,,,,,"06/Jun/03 15:48;phil@steitz.com;Created an attachment (id=6668)
patches to ValueServer, ValueServerTest
","07/Jun/03 08:39;mdiggory@gmail.com;Applied, Commited and tested.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Integration Source Files,MATH-52,12342441,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,zxg_32@yahoo.com,zxg_32@yahoo.com,07/Aug/05 03:13,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Add TrapezoidIntegrator.java
Add TrapezoidIntegratorTest.java
Update UnivariateRealIntegrator.java
Update UnivariateRealIntegratorImpl.java","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,07/Aug/05 03:14;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--Integration.zip;https://issues.apache.org/jira/secure/attachment/12333691/ASF.LICENSE.NOT.GRANTED--Integration.zip,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,36060.0,,,2005-08-14 15:53:48.0,,,false,,,,,,,,,,,,,,150293,,,Sun Aug 14 15:53:48 UTC 2005,,,,,,0|i0rwtj:,160961,,,,,,,,"07/Aug/05 03:14;zxg_32@yahoo.com;Created an attachment (id=15935)
Integration Source File
","14/Aug/05 15:53;phil@steitz.com;

*** This bug has been marked as a duplicate of 36148 ***",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] EmpiricalDistributionImpl patch to adapt to change in Univariate interface,MATH-51,12340721,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,22/May/03 21:08,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The EmpiricalDistributionImpl that I submitted yesterday assumed the ""old""
Univariate interface, in which getN() returned a double.  The attached patch
inserts the necessary casts to avoid the rounding/truncation errors that were
causing the EmpiricalDistribution and ValueServer unit tests to fail.

The patch also adds a RandomData member variable so that getNext() does not
instantiate a new RandomData instance for each activation","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,22/May/03 21:09;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--EmpiricalDistributionImplPatch.txt;https://issues.apache.org/jira/secure/attachment/12332281/ASF.LICENSE.NOT.GRANTED--EmpiricalDistributionImplPatch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20149.0,,,2003-05-22 22:46:56.0,,,false,,,,,,,,,,,,,,150292,,,Thu May 22 22:46:56 UTC 2003,,,,,,0|i0rwtr:,160962,,,,,,,,"22/May/03 21:09;phil@steitz.com;Created an attachment (id=6456)
Patch for EmpiricalDistributionImpl
","22/May/03 22:46;tobrien@discursive.com;Patch committed, also a related issue with RandomDataTest was fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] Adding references, updating task list in xdocs",MATH-50,12340741,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,30/May/03 22:24,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patches to xdocs

* add the numerical analysis and prob/stat references that have been posted or
  included in javadoc recently, plus a few more generic references.
* update the task list based on accomplishments and discussion

Also updating my own contact info in project.xml","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,30/May/03 22:28;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--projectXMLPatch.txt;https://issues.apache.org/jira/secure/attachment/12332296/ASF.LICENSE.NOT.GRANTED--projectXMLPatch.txt,30/May/03 22:25;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--xdocsPatch.txt;https://issues.apache.org/jira/secure/attachment/12332295/ASF.LICENSE.NOT.GRANTED--xdocsPatch.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,20357.0,,,2003-05-30 22:42:59.0,,,false,,,,,,,,,,,,,,150291,,,Fri May 30 22:42:59 UTC 2003,,,,,,0|i0rwtz:,160963,,,,,,,,"30/May/03 22:25;phil@steitz.com;Created an attachment (id=6567)
xdocs patch to include references, update task list
","30/May/03 22:28;phil@steitz.com;Created an attachment (id=6568)
adding email contact, fixing roles xml
",30/May/03 22:42;tobrien@discursive.com;patches applied,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Non-private serialVersionUIDs,MATH-49,12342647,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,elharo@metalab.unc.edu,elharo@metalab.unc.edu,19/Oct/05 20:43,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Many classes such as BrentSolver use non-private serialversionUIDs. e.g.

    static final long serialVersionUID = 3350616277306882875L;

should be

    private static final long serialVersionUID = 3350616277306882875L;

I'll see if I can patch this.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,19/Oct/05 20:52;elharo@metalab.unc.edu;ASF.LICENSE.NOT.GRANTED--serialVersionUIDPatch;https://issues.apache.org/jira/secure/attachment/12333868/ASF.LICENSE.NOT.GRANTED--serialVersionUIDPatch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,37162.0,,,2005-11-24 04:16:51.0,,,false,,,,,,,,,,,,,,150290,,,Thu Nov 24 04:16:51 UTC 2005,,,,,,0|i0rwu7:,160964,,,,,,,,"19/Oct/05 20:52;elharo@metalab.unc.edu;Created an attachment (id=16752)
privatize serialVersionUID

This is the first time I've made a patch with SVN. Plase double-check to make
sure I didn't do anything too stupid. Thanks.","24/Nov/05 04:16;phil@steitz.com;Patch applied to MATH_1_1 branch, to be merged into trunk.  Thanks!  Sorry for
the latency. 

In the future, it is best to make your patches from the top level src directory
or at least somewhere inside the project tree.

Keep them coming :-)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] chi-squared distribution (part of the distribution collection),MATH-48,12340737,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,28/May/03 05:44,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Here is the begginings of the distribution framework.  It's definately been a 
practice in over-engineering.

It starts with a distribution factory following the abstract factory pattern.  
The value being, if users want to swap out the default distribution 
implementations with their own, why stop them.  The factory is responsible for 
create the distribution instances (only continuous ones for now).

Each distribution has a cummulativeProbabilty method which returns the value of 
the CDF for a given value.  By default, via the abstract distribution base 
class, an inverseCummulativeProbability method is all provided for all 
distributions.  It's implementation is generically handled by applying root 
finding techniques to the cummulativeProbability method.  That way, when new 
distributions are added, authors only need to create the CDF method and the 
inverse is provided automatically.

Currenty, the only distributions are Chi-Squared and Gamma (one actually since 
Chi-Squared is a specialized Gamma) but I have more planned.

FYI, I took the liberty of putting these classes into separate packages to 
limit the package clutter we're starting to experience.  As such I could not 
create a patch file for all my changes.  I will provide a patch for what I can 
and provide a zip archive with all the files in the new packages.

Feel free to modify and include as you see fit.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,28/May/03 05:50;brentworden;ASF.LICENSE.NOT.GRANTED--chi-square-additions.zip;https://issues.apache.org/jira/secure/attachment/12332292/ASF.LICENSE.NOT.GRANTED--chi-square-additions.zip,28/May/03 05:46;brentworden;ASF.LICENSE.NOT.GRANTED--chi-square-patch.diff;https://issues.apache.org/jira/secure/attachment/12332291/ASF.LICENSE.NOT.GRANTED--chi-square-patch.diff,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,20279.0,,,2003-05-28 08:12:06.0,,,false,,,,,,,,,,,,,,150289,,,Fri May 30 02:44:22 UTC 2003,,,,,,0|i0rwuf:,160965,,,,,,,,"28/May/03 05:46;brentworden;Created an attachment (id=6519)
Here's the patch to the existing packages.
","28/May/03 05:50;brentworden;Created an attachment (id=6520)
Here's the archive of all the new files in the new packages.
","28/May/03 08:12;steitzp@yahoo.com;I like the generic CDF inversion capability provided by 
AbstractContinousDistribution, and the ContinuousDistribution interface.  
Personally, I do not see the implementation as over-engineered for what it does 
and if we want to tackle the critical value problem for statistical tests 
generically, I can't see a significantly simpler or more flexible way to do 
it.  It would be nice to extend/modify/replace TestStatistic with a hypothesis-
testing capability using the distribution framework to compute critical values 
for t (need to add this distribution), chi-square and maybe F.  We will also 
obviously have to fill out the docs on the framework.  What will likely be most 
useful to applications are the distribution implementations and any inferential 
machinery that we build on top of them.

I guess there is no postponing the packaging discussion.  stat is sort of 
obvious and I guess stat.distribution makes sense, assuming we want to move 
forward with the framework (which I personally think is a good idea).  In an 
earlier post, I had designated something like ""analysis"" or ""numerical"", which 
would be a good home for the rootfinding stuff.  Probably should take this to 
the list.",30/May/03 02:44;tobrien@discursive.com;Patches applied,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] ""NaN when MathUtils.round(0,2)""",MATH-47,12342500,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,ngupte@aurigalogic.com,ngupte@aurigalogic.com,22/Aug/05 21:08,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,1.1,,,,,,,,,,0,,,,,,,,"If MathUtils.round(a + b, 2) is calledm the resutl is NaN where as it should've
been 0.00","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,36300.0,,,2005-08-22 21:56:03.0,,,false,,,,,,,,,,,,,,150288,,,Mon Aug 22 21:56:03 UTC 2005,,,,,,0|i0rwun:,160966,,,,,,,,"22/Aug/05 21:09;ngupte@aurigalogic.com;Forgot to mention:

If MathUtils.round(a + b, 2) is calledm the resutl is NaN where as it should've
been 0.00 iff (a = b) == 0.","22/Aug/05 21:56;brentworden;Fixed in SVN Revision 234481. Applied to MATH_1_1 branch.  Fixed division by 
zero error in rounding methods.

Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] ""INFINITE_WINDOW"" is misspelled ""INIFINTE_WINDOW"" in Univariate and its users",MATH-46,12340731,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Minor,Fixed,,hotfusionman@yahoo.com,hotfusionman@yahoo.com,26/May/03 14:27,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"""INFINITE_WINDOW"" is misspelled ""INIFINTE_WINDOW"" in Univariate.java and its users, viz., 
UnivariateImpl.java, ListUnivariateImpl.java, and StoreUnivariateImpl.java.  Looks like a copy-and-
paste bug originating in Univariate.java that should be fixed before it propagates further.","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,20234.0,,,2003-05-27 00:44:52.0,,,false,,,,,,,,,,,,,,150287,,,Tue May 27 00:44:52 UTC 2003,,,,,,0|i0rwuv:,160967,,,,,,,,27/May/03 00:44;tobrien@discursive.com;Fixed - the dangers of code completion in a tool like Eclipse....,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Matrix operations make unecessary copies,MATH-45,12341805,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,14/Oct/04 11:20,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The following methods in RealMatrixImpl, BigMatrixImpl make unecessary copies of
the data in the external operands:

  add
  multiply
  preMultiply
  solve
  subtract","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,31713.0,,,,,,false,,,,,,,,,,,,,,150286,,,Mon Nov 08 04:26:59 UTC 2004,,,,,,0|i0rwv3:,160968,,,,,,,,"08/Nov/04 04:26;phil@steitz.com;Committed changes replacing copy - array reference with getEntry. 

We may eventually want to add overloads for RealMatrixImpl/BigMatrixImpl
operands that use getDataRef, which will be faster. This was changed to getData
- resulting in extra copies - when getDataRef was removed from the RealMatrix
interface.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Contradictory asserts in javadoc comments of TTest  and TTestImpl,MATH-44,12342116,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,gillesgaillard@compuserve.com,gillesgaillard@compuserve.com,14/Apr/05 20:30,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Found in javadoc comment of method : 
  public double tTest( X, X )
'For a one-sided test, divide the returned value by 2.'

But found in javadoc comment for method :
  public boolean tTest( X, X, double alpha)
'To perform a 1-sided test, use <code>alpha / 2</code>'

where it should be:
'To perform a 1-sided test, use <code>alpha * 2</code>'

confirmed by code of the boolean valued function:
return (tTest(sampleStats1, sampleStats2) < alpha);","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,34448.0,,,2005-04-17 06:48:28.0,,,false,,,,,,,,,,,,,,150285,,,Sun Apr 17 06:48:28 UTC 2005,,,,,,0|i0rwvb:,160969,,,,,,,,"17/Apr/05 06:48;phil@steitz.com;Thank you for pointing out this error.  
Fixed in nightly builds starting 4/16/05.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] Adding percentiles to StoredUnivariate, AbstractStoreUnivariate",MATH-43,12340744,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,31/May/03 07:13,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Attached patches add getPercentile, getSortedValues to StoredUnivariate
interface and provide implementation in AbstractStoreUnivariate.

Sort uses simple shell sort performed directly on a double[] array copy returned
by getValues. 

Percentile definition used does linear interpolation after computing target
position.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,31/May/03 07:15;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--StoreUnivariatePatches.txt;https://issues.apache.org/jira/secure/attachment/12332299/ASF.LICENSE.NOT.GRANTED--StoreUnivariatePatches.txt,31/May/03 07:18;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--StoreUnivariateTestPatch.txt;https://issues.apache.org/jira/secure/attachment/12332300/ASF.LICENSE.NOT.GRANTED--StoreUnivariateTestPatch.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,20377.0,,,2003-06-04 09:28:13.0,,,false,,,,,,,,,,,,,,150284,,,Wed Jun 04 09:28:13 UTC 2003,,,,,,0|i0rwvj:,160970,,,,,,,,"31/May/03 07:15;phil@steitz.com;Created an attachment (id=6577)
patches to StoreUnivariate, AbstractStoreUnivariate
","31/May/03 07:18;phil@steitz.com;Created an attachment (id=6578)
patch to StoreUnivariateImplTest
","04/Jun/03 09:28;mdiggory@gmail.com;I've applied both patches to the StoreUnivariate** and verified that the Unit
Tests work with the Ant build.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] Function ""Fraction math.fraction.FractionFormat.parse(String, ParsePosition) "" does not handle parsing the numerater properly",MATH-42,12343082,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,nhung.nnguyen@gmail.com,nhung.nnguyen@gmail.com,14/May/06 05:08,14/Oct/06 02:45,07/Apr/19 20:38,05/Jun/06 10:17,1.1,,,,,,,,,,0,,,,,,,,"Hello,

While testing function ""Fraction math.fraction.FractionFormat.parse(String, 
ParsePosition) "", I found it did not handle properly the case the input string 
passed in is incorrect. 

When I passed in a String that represented a Fraction to be parsed, if the 
Fraction embedded in the String had a whole value, while either the numerator 
or denominator had the negative sign, the function just returned 
1/abs(denomitator).

Please see the following code segment for more details:

""   
  NumberFormat nf = null; 
  
  FractionFormat properFormat = FractionFormat.getProperInstance
(Locale.getDefault());
  FractionFormat improperFormat = FractionFormat.getImproperInstance
(Locale.getDefault());
 
//Test 1 : fail 
public void testParseNegative(){
 
   String source = ""1 -2 / 3"";
   ParsePosition pos = new ParsePosition(0);

   Fraction actual = properFormat.parse(source, pos);
   assertNull(actual);
}

// Test2: success
public void testParseNegative(){
 
   String source = ""1 -2 / 3"";
   ParsePosition pos = new ParsePosition(0);

   Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3
   assertEquals(1, source.getNumerator());
   assertEquals(3, source.getDenominator());
}

""

Note: Similarly, when I passed in the following inputs: 
  input 2: (source = 1 2 / -3, pos = 0)
  input 3: ( source =  -1 -2 / 3, pos = 0)

Function ""Fraction parse(String, ParsePosition)"" returned Fraction 1/3 (means 
the result Fraction had numerator = 1 and  denominator = 3)for all 3 inputs 
above.
 
I think the function does not handle parsing the numberator/ denominator 
properly incase input string provides negative numerator or negative 
denominator while there is the whole value.

Thank you!
Nhung","Operating System: other
Platform: Other",,,,,,,,,,,,MATH-60,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,39576.0,,,,,,false,,,,,,,,,,,,,,34218,,,Sat Oct 14 02:45:36 UTC 2006,,,,,,0|i0rwvr:,160971,,,,,,,,"14/Oct/06 02:45;nhung.nnguyen@gmail.com;
   [[ Old comment, sent by email on Mon, 5 Jun 2006 23:54:56 -0500 ]]

Hi,

Thanks to let me know.

Have a good night, and best regards,
Nhung


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Wrong implementation of the spline interpolator,MATH-41,12341143,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Critical,Fixed,,georg.lucas@heidelberg.com,georg.lucas@heidelberg.com,08/Dec/03 16:18,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Here is the (hopefully) correct implementation of the spline interpolator.

        if (c == null) {
            // Number of intervals. The number of data points is N+1.
            int n = xval.length - 1;
            // Check whether the xval vector has ascending values.
            // Separation should be checked too (not implemented: which 
criteria?).
            for (int i = 0; i < n; i++) {
                if (xval[i] >= xval[i + 1]) {
                    throw new IllegalArgumentException(""Dataset must specify 
sorted, ascending x values."");
                }
            }
            // Vectors for the equation system. There are n-1 equations for the 
unknowns s[i] (1<=i<=N-1),
            // which are second order derivatives for the spline at xval[i]. At 
the end points, s[0]=s[N]=0.
            // Vectors are offset by -1, except the lower diagonal vector which 
is offset by -2. Layout:
            // d[0]*s[1]+u[0]*s[2]                                           = b
[0]
            // l[0]*s[1]+d[1]*s[2]+u[1]*s[3]                                 = b
[1]
            //           l[1]*s[2]+d[2]*s[3]+u[2]*s[4]                       = b
[2]
            //                           ...
            //                     l[N-4]*s[N-3]+d[N-3]*s[N-2]+u[N-3]*s[N-1] = b
[N-3]
            //                                   l[N-3]*s[N-2]+d[N-2]*s[N-1] = b
[N-2]
            // Vector b is the right hand side (RHS) of the system.
            double b[] = new double[n - 1];
            // Vector d is diagonal of the matrix and also holds the computed 
solution.
            double d[] = new double[n + 1];
            // Setup right hand side and diagonal.
            double dquot = (yval[1] - yval[0]) / (xval[1] - xval[0]);
            for (int i = 0; i < n - 1; i++) {
                // TODO avoid recomputing the term
                //    (yval[i + 2] - yval[i + 1]) / (xval[i + 2] - xval[i + 1])
                // take it from the previous loop pass. Note: the interesting 
part of performance
                // loss is the range check in the array access, not the 
computation itself.
                double dquotNext = 
                    (yval[i + 2] - yval[i + 1]) / (xval[i + 2] - xval[i + 1]);
                b[i] = 6.0 * (dquotNext - dquot);
                d[i+1] = 2.0 * (xval[i + 2] - xval[i]);
                dquot = dquotNext;
            }
            // u[] and l[] (for the upper and lower diagonal respectively) are 
not
            // really needed, the computation is folded into the system solving 
loops.
            // Keep this for documentation purposes:
            //double u[] = new double[n - 2]; // upper diagonal
            //double l[] = new double[n - 2]; // lower diagonal
            // Set up upper and lower diagonal. Keep the offsets in mind.
            //for (int i = 0; i < n - 2; i++) {
            //  u[i] = xval[i + 2] - xval[i + 1];
            //  l[i] = xval[i + 2] - xval[i + 1];
            //}
            // Solve the system: forward pass.
            for (int i = 0; i < n - 2; i++) {
                double delta = xval[i + 2] - xval[i + 1];
                double deltaquot = delta / d[i+1];
                d[i + 2] -= delta * deltaquot;
                b[i + 1] -= b[i] * deltaquot;
            }
            // Solve the system: backward pass.
            d[n - 1] = b[n - 2] / d[n - 1];
            for (int i = n - 3; i >= 0; i--) {
                d[i+1] = (b[i] - (xval[i + 2] - xval[i + 1]) * d[i + 2]) / d
[i+1];
            }
            // Compute coefficients as usual polynomial coefficients.
            // Not the best with respect to roundoff on evaluation, but simple.
            c = new double[n][4];
            // set the start values (this is for a natural spline)
            d[0] = d[n] = 0;
            
            double delta;
            for (int i = 0; i < n; i++) {
                delta = xval[i + 1] - xval[i];
                c[i][3] = (d[i+1] - d[i]) / delta / 6.0;
                c[i][2] = d[i] / 2.0;
                c[i][1] =
                    (yval[i + 1] - yval[i]) / delta -
                        (d[i+1] + 2.0 * d[i]) * delta / 6.0;
                c[i][0] = yval[i];
            }
        }","Operating System: All
Platform: PC",,,,,,,,,,,,,,,,,,,,10/Dec/03 04:49;georg.lucas@heidelberg.com;ASF.LICENSE.NOT.GRANTED--CubicSplineFunction.java;https://issues.apache.org/jira/secure/attachment/12332651/ASF.LICENSE.NOT.GRANTED--CubicSplineFunction.java,10/Dec/03 04:48;georg.lucas@heidelberg.com;ASF.LICENSE.NOT.GRANTED--InterpolatorTest.java;https://issues.apache.org/jira/secure/attachment/12332650/ASF.LICENSE.NOT.GRANTED--InterpolatorTest.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,25278.0,,,2003-12-08 23:14:34.0,,,false,,,,,,,,,,,,,,150283,,,Sat Apr 03 05:57:39 UTC 2004,,,,,,0|i0rwvz:,160972,,,,,,,,"08/Dec/03 23:14;brentworden;If you'd be so kind, could you provide an example spline problem, preferably 
one online, that you used to determine the coding error?  We could then add 
this to the unit tests.","10/Dec/03 04:48;georg.lucas@heidelberg.com;Created an attachment (id=9468)
New test case for spline interpolator
","10/Dec/03 04:49;georg.lucas@heidelberg.com;Created an attachment (id=9469)
Error correction for last x-value
","27/Jan/04 03:57;mdiggory@gmail.com;Sigh,

There are now issues with this fix and the fact that first/secondDerivatives are
not in the UnivariateRealFunction interface. With all the code flying around,
did we come to a conclusion on how to handle Derivatves and UnivariateRealFunctions?

-Mark","27/Jan/04 05:37;brentworden;I thought we were going to introduce an new inteface to denote differentiable 
functions and morph the spline interpolator to use it.","03/Apr/04 05:57;phil@steitz.com;Implementation has been fixed in CVS and tests have been added, verifying
coefficients and consistency.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] binomail distribution patch,MATH-40,12340913,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,15/Aug/03 23:25,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"I created the binomial distribution.  along with this, I created a discrete 
distribution heirarchy similar to the continuous case.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,15/Aug/03 23:25;brentworden;ASF.LICENSE.NOT.GRANTED--binomial-distribution-patch.txt;https://issues.apache.org/jira/secure/attachment/12332463/ASF.LICENSE.NOT.GRANTED--binomial-distribution-patch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,22464.0,,,2003-08-17 00:09:45.0,,,false,,,,,,,,,,,,,,150282,,,Sun Aug 17 00:24:11 UTC 2003,,,,,,0|i0rww7:,160973,,,,,,,,"15/Aug/03 23:25;brentworden;Created an attachment (id=7834)
patch file
","17/Aug/03 00:09;j3322ptm@yahoo.de;Patch applied, compiled, unit tested and committed.
Thanks!","17/Aug/03 00:24;mdiggory@gmail.com;Just a heads up. Maybe we should make it a practice to take ownership of these
tickets when we work on them so that others do not attempt the same. I attempted
to commit this patch during / just after your commit and got all kinds of
conflict errors. If its status had changed ownership eariler I would not have
gone through the process while you were working on it too. :-)

-Mark",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] replace build.xml with maven-generated version,MATH-39,12340748,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,02/Jun/03 00:02,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The current version of build.xml is missing commons-beanutils and associated
dependencies.  The attached patch replaces the original with a maven-generated
version that includes a target that gets the dependent jars.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,02/Jun/03 00:03;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--buildPatch.txt;https://issues.apache.org/jira/secure/attachment/12332303/ASF.LICENSE.NOT.GRANTED--buildPatch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20396.0,,,,,,false,,,,,,,,,,,,,,150281,,,Mon Jun 02 00:03:20 UTC 2003,,,,,,0|i0rwwf:,160974,,,,,,,,"02/Jun/03 00:03;phil@steitz.com;Created an attachment (id=6587)
patch to build.xml
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractDescriptiveStatistics needs more coverage,MATH-38,12341447,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Won't Fix,,tobrien@discursive.com,tobrien@discursive.com,15/May/04 22:50,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Three method need coverage, getKurtosisClass(), getSortedValues(), and toString
().  This class is at 52%, it should be trivial to increase this to 100% (but 
make sure the tests are proper, coverage for coverage's sake is a useless 
thing).

http://jakarta.apache.org/commons/math/clover/org/apache/commons/math/stat/Abst
ractDescriptiveStatistics.html","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,29012.0,,,2004-05-17 22:34:23.0,,,false,,,,,,,,,,,,,,150280,,,Sun May 23 08:00:37 UTC 2004,,,,,,0|i0rwwn:,160975,,,,,,,,"17/May/04 22:34;brentworden;I added some tests a while back which increased the coverage to 80% and the 
site hasn't be updated to reflect those additions.

However, I noticed one thing for the getKurtosisClass method that needs to be 
addressed.  There is no tolerance used in determining the return value so, even 
very small kurtosis values result in MESOKURTIC never being returned.  Since, 
the kurtosis class is based on the kurtosis, which invariably has numerical 
error, we should allow for some error in the kurtosis class result.
","23/May/04 08:00;tobrien@discursive.com;In response to this, I think the best course of action is the removal of
getKurtosisClass().  As each class is a subjective term used for classification,
we could create a default tolerance, or take the tack that getKurtosisClass() is
not a measure in line with the set of measurements available in DescriptiveStat.
 Having a Kurtosis class in DescStat opens up the door for other classifications
based on skewness, etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Integration Source Files,MATH-37,12342456,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Duplicate,,zxg_32@yahoo.com,zxg_32@yahoo.com,11/Aug/05 03:18,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Update TrapezoidIntegrator.java
Update SimpsonIntegrator.java
Update RombergIntegrator.java
Update SimpsonIntegratorTest.java

Mostly comments changes.","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,11/Aug/05 03:19;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--Integration.zip;https://issues.apache.org/jira/secure/attachment/12333700/ASF.LICENSE.NOT.GRANTED--Integration.zip,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,36132.0,,,2005-08-14 15:55:45.0,,,false,,,,,,,,,,,,,,150279,,,Sun Aug 14 15:55:45 UTC 2005,,,,,,0|i0rwwv:,160976,,,,,,,,"11/Aug/05 03:19;zxg_32@yahoo.com;Created an attachment (id=15998)
Integration Source File
","14/Aug/05 15:55;phil@steitz.com;

*** This bug has been marked as a duplicate of 36148 ***",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] t-test statistic proposal,MATH-36,12340730,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,26/May/03 13:48,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Adds the one sample, t-test statistic to TestStatistic and implementations.
Also add unit tests.","Operating System: other
Platform: All",,,,,,,,,,,,,,,,,,,,26/May/03 14:00;brentworden;ASF.LICENSE.NOT.GRANTED--t-test-unit.diff;https://issues.apache.org/jira/secure/attachment/12332287/ASF.LICENSE.NOT.GRANTED--t-test-unit.diff,26/May/03 13:58;brentworden;ASF.LICENSE.NOT.GRANTED--t-test.diff;https://issues.apache.org/jira/secure/attachment/12332286/ASF.LICENSE.NOT.GRANTED--t-test.diff,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,20231.0,,,2003-05-27 00:36:47.0,,,false,,,,,,,,,,,,,,150278,,,Tue May 27 00:36:47 UTC 2003,,,,,,0|i0rwx3:,160977,,,,,,,,"26/May/03 13:58;brentworden;Created an attachment (id=6493)
t-test patch file.
","26/May/03 14:00;brentworden;Created an attachment (id=6494)
t-test unit test patch file.
","27/May/03 00:36;tobrien@discursive.com;Brent, thanks for the patch.  It has been applied.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Proposal for Poisson distributuion,MATH-35,12341803,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,frno@bredband.net,frno@bredband.net,13/Oct/04 14:34,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,Support for the Poisson distributuion should be added to the commons-math package.,"Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,13/Oct/04 14:37;frno@bredband.net;ASF.LICENSE.NOT.GRANTED--PoissonDistribution.java;https://issues.apache.org/jira/secure/attachment/12333167/ASF.LICENSE.NOT.GRANTED--PoissonDistribution.java,13/Oct/04 14:38;frno@bredband.net;ASF.LICENSE.NOT.GRANTED--PoissonDistributionImpl.java;https://issues.apache.org/jira/secure/attachment/12333168/ASF.LICENSE.NOT.GRANTED--PoissonDistributionImpl.java,13/Oct/04 14:41;frno@bredband.net;ASF.LICENSE.NOT.GRANTED--PoissonDistributionTest.java;https://issues.apache.org/jira/secure/attachment/12333169/ASF.LICENSE.NOT.GRANTED--PoissonDistributionTest.java,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,31688.0,,,2004-11-02 09:37:19.0,,,false,,,,,,,,,,,,,,150277,,,Sun Nov 07 11:35:28 UTC 2004,,,,,,0|i0rwxb:,160978,,,,,,,,"13/Oct/04 14:37;frno@bredband.net;Created an attachment (id=13060)
Proposed interface for the Poisson distribution
","13/Oct/04 14:38;frno@bredband.net;Created an attachment (id=13061)
Implementation of the Poisson distribution
","13/Oct/04 14:41;frno@bredband.net;Created an attachment (id=13062)
JUnit test for the Poisson distribution implementation
","02/Nov/04 09:37;brentworden;Few comments:

1) The createFormat method uses replaceAll, a JDK 1.4 call.  I think [math] is 
meant to be compatible with JDK 1.3 if not JDK 1.2.

2) I see createFormat is being used to force precision on the inverse 
cumulative probs.  Is there a reason this is being done on intermediate 
results?  Is it necessary at all?

3) For the probability method, I would suggest using the ol' Exp[Log[f(x)]] 
trick to avoid numerical overflow.  n! is a very volatile method for even 
moderate n values.

4) Can the parameter be renamed to something a little more descriptive?  I 
would suggest mean.

5) I think the cumulative probability method can be optimized one of two ways, 
or by utilizing both.  Cumulative probabilities for Poisson can be expressed in 
terms of the regularized gamma function which we already have implemented.  The 
performance of this approach would not deteriorate for large x as the current 
loop does.  At a minimum, instead of making calls to probability for each 
summation term, the ratio of successive terms should be used to alleviate the 
need to call n! for every term.

6) Is there a compelling reason not to use the inverseCumulativeProbability 
method provided by the AbstractDiscreteDistribution class?
","02/Nov/04 13:29;phil@steitz.com;Brent,

I agree with all of your points.

1)-2) I think the ""rounding"" should be removed. To document clearly what the
contract means with the implementation in the patch would be difficult and would
depart from the specification in the interface.

3) Using MathUtils.factorialDouble in place of factorial will accomplish this.

4) Mean sounds good to me too.

5) I would favor just using the regularized gamma.

6) At first I thought that this would not work well since we would have to start
the bisection with top endpoint Integer.MAX_VALUE; but that's only 32 cuts out,
so I agree here too.

I will make these changes prior to committing.",07/Nov/04 11:35;phil@steitz.com;Patches with modifications above have been committed.  Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
More coverage needed for CubicSplineFunction,MATH-34,12341446,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Won't Fix,,tobrien@discursive.com,tobrien@discursive.com,15/May/04 22:45,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"firstDerivative() and secondDerivative() are completely untested, and there is 
a validation which is untested in the value() method.  Coverage is at 35%, it 
should be easy to get this to 100%.

http://jakarta.apache.org/commons/math/clover/org/apache/commons/math/analysis/
CubicSplineFunction.html","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,29010.0,,,,,,false,,,,,,,,,,,,,,150276,,,Sun May 23 08:08:18 UTC 2004,,,,,,0|i0rwxj:,160979,,,,,,,,"23/May/04 08:08;tobrien@discursive.com;Ick, never mind, this was a case of an outdated coverage report.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] T-Distribution causing a StackOverflowError,MATH-33,12341290,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Blocker,Cannot Reproduce,,scott@duchin.com,scott@duchin.com,26/Feb/04 14:56,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"I ran the following code:
    TDistributionImpl td = new TDistributionImpl(5.);
    double est;
    est = td.cummulativeProbability(.1);
    System.out.println("".1=""+est);
    est = td.cummulativeProbability(.01);
    System.out.println("".01=""+est);
and I get:
   [junit] .1=0.5378849302066607
   [junit] Failed to invoke suite():java.lang.StackOverflowError
The error happens with different degrees of freedom at different locations.","Operating System: All
Platform: PC",,,,,,,,,,,,,,,,,,,,29/Feb/04 05:57;brentworden;ASF.LICENSE.NOT.GRANTED--t-distribution-bug-27243-patch.txt;https://issues.apache.org/jira/secure/attachment/12332759/ASF.LICENSE.NOT.GRANTED--t-distribution-bug-27243-patch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,27243.0,,,2004-02-29 05:56:19.0,,,false,,,,,,,,,,,,,,34182,,,Wed Mar 03 14:56:07 UTC 2004,,,,,,0|i0rwxr:,160980,,,,,,,,"29/Feb/04 05:56;brentworden;I tried running the unit test against the latest code in CVS and could not 
reproduce the error.  I do recall a StackOverflowError with regards to the 
ContinuedFraction class, that TDistributionImpl uses, that has been resolved.

Scott, if you're not using the latest code from CVS, try synching up and run 
your tests again.

For completeness, I will add your unit test to our test suite.","29/Feb/04 05:57;brentworden;Created an attachment (id=10609)
Adding the patch file for traceability.
","02/Mar/04 14:53;scott@duchin.com;I tried same thing on my home computer and got a java stack overflow.
At work I am running NT 2000 and JDK 1.4.2
At home I am running XP PRO and JDK 1.5 beta
I have just downloaded the nightly build Mar-01
If the nightly builds are not the latest then where is a later jar I can use.","03/Mar/04 13:38;brentworden;I took a look at the nightly build jars and they are most definately out of 
date.  I believe that the nightly build process is going after the sandbox CVS 
module and not commons proper.

So, while we get the nightly build situation addressed, you can either:
1) grab the latest source from the CVS and build it with ant or maven.
or
2) grab the jar I just built from http://www.apache.org/~brentworden/commons-
math-1.0-dev.jar
","03/Mar/04 14:56;scott@duchin.com;The jar you gave me works fine.  Thank you for your patience with me and for 
doing a great job.  Do you have any idea when the nightly build will be fixed?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] MathUtils  round Method issues,MATH-32,12342410,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,vemurys@hotmail.com,vemurys@hotmail.com,28/Jul/05 05:36,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"I was trying to use this class to round my float values.
class ==> org.apache.commons.math.util.MathUtils
method ==> round(float,scale)
Input 30.095 output 30.09 - ( i think should be  30.1)
Input 33.095 putput 33.1 - Correct
Input 50.085 output 50.08 - should have been 50.09
Input 50.185 output 50.19 - Correct
Input 50.005 output 50.01 - correct
Input 30.005 out put 30.0 - wrong
30.645 output - 30.65 - correct
So it seems there is some inconsistency. Not sure if its because of the how 
BigDecimal.ROUND_HALF_UP works. but still the data is inconsistent.","Operating System: Windows XP
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,35904.0,,,2005-07-28 22:25:26.0,,,false,,,,,,,,,,,,,,34149,,,Sat Aug 13 12:53:03 UTC 2005,,,,,,0|i0rwxz:,160981,,,,,,,,"28/Jul/05 22:25;phil@steitz.com;Looks like an byproduct of the BigDecimal conversions, as this should not be the
default behavior.  Thanks for reporting this. Junit test cases would be great.","30/Jul/05 16:31;phil@steitz.com;The problem was due to the (documented) ""unpredictable"" behavior of BigDecimal
constructor when supplied double values.  As a workaround, I changed the
implementation to pass a string argument instead. 

I am leaving this open, since I think we should eliminate use of BigDecimal
altogether and code the algorithms directly.  ",13/Aug/05 12:53;brentworden;Fixed.  SVN Revision 231029.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math} Adding random sampling and permutations,MATH-31,12340738,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,28/May/03 21:08,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch adds the following two capabilities to RandomData:

1. Generating random permutations of integers 

2. Generating random samples (returned as Object arrays) from Collections

Tests validate expected sample distribtution using chi-square tests","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,28/May/03 21:19;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--RandomDataTestPatch.txt;https://issues.apache.org/jira/secure/attachment/12332294/ASF.LICENSE.NOT.GRANTED--RandomDataTestPatch.txt,28/May/03 21:09;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--randomDataPatches.txt;https://issues.apache.org/jira/secure/attachment/12332293/ASF.LICENSE.NOT.GRANTED--randomDataPatches.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,20303.0,,,2003-05-30 02:48:03.0,,,false,,,,,,,,,,,,,,150275,,,Fri May 30 02:48:03 UTC 2003,,,,,,0|i0rwy7:,160982,,,,,,,,"28/May/03 21:09;phil@steitz.com;Created an attachment (id=6546)
Patches to RandomData, RandomDataImpl, RandomDataTest
","28/May/03 21:19;phil@steitz.com;Created an attachment (id=6547)
DOH!  forgot the test patch -- here 'tis
",30/May/03 02:48;tobrien@discursive.com;Patches applied,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] RandomDataImpl not using Random,MATH-30,12340759,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,06/Jun/03 00:11,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"There's one call in RandomDataImpl using Math.random() and not the wrapped 
Random object.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,06/Jun/03 00:11;brentworden;ASF.LICENSE.NOT.GRANTED--random-patch.txt;https://issues.apache.org/jira/secure/attachment/12332311/ASF.LICENSE.NOT.GRANTED--random-patch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20514.0,,,2003-06-06 01:34:11.0,,,false,,,,,,,,,,,,,,150274,,,Fri Jun 06 02:04:56 UTC 2003,,,,,,0|i0rwyf:,160983,,,,,,,,"06/Jun/03 00:11;brentworden;Created an attachment (id=6651)
patch file
","06/Jun/03 01:34;phil@steitz.com;Don't ask me how that got in there :-)

Nice catch.

",06/Jun/03 02:04;mdiggory@gmail.com;Applied the patch. Passed JUnit test...,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] Adding MathUtils class, including binomial coefficients, factorials",MATH-29,12340747,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,01/Jun/03 14:49,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"I am attaching source for a new utility class, org.apache.commons.math.MathUtils
to house useful formulas, extending those that java.lang.Math provides.  Initial
contents are binomial coefficient and factorial methods.  For each of these,
three methods are provided, returning  

   * value as a long
   * value as a double
   * log of the value as a double

Limits on actual parameters for the first two (full value representations) are
specified in the javadoc.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,01/Jun/03 14:51;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--MathUtils.java;https://issues.apache.org/jira/secure/attachment/12332301/ASF.LICENSE.NOT.GRANTED--MathUtils.java,01/Jun/03 14:53;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--MathUtilsTest.java;https://issues.apache.org/jira/secure/attachment/12332302/ASF.LICENSE.NOT.GRANTED--MathUtilsTest.java,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,20390.0,,,2003-06-04 09:34:27.0,,,false,,,,,,,,,,,,,,150273,,,Wed Jun 04 09:34:27 UTC 2003,,,,,,0|i0rwyn:,160984,,,,,,,,"01/Jun/03 14:51;phil@steitz.com;Created an attachment (id=6582)
Source for MathUtils
","01/Jun/03 14:53;phil@steitz.com;Created an attachment (id=6583)
Source for MathUtilsTest
",04/Jun/03 09:34;mdiggory@gmail.com;Added both files tot he cvs and ran the UnitTests to verify build works correctly.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Factorial method throws exception for zero,MATH-28,12341802,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,frno@bredband.net,frno@bredband.net,13/Oct/04 14:07,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The MathUtils.factorial(final int n) method throws an IllegalArgumentException
for n=0 which is clearly incorrect since 0!=1 by definition.","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,31687.0,,,,,,false,,,,,,,,,,,,,,34216,,,2004-10-13 14:07:06.0,,,,,,0|i0rwyv:,160985,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Integration Source Files,MATH-27,12342457,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,zxg_32@yahoo.com,zxg_32@yahoo.com,12/Aug/05 02:38,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Update TrapezoidIntegratorTest.java
Update SimpsonIntegratorTest.java
Update RombergIntegratorTest.java

Some comments changes","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,13/Aug/05 09:04;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--Integration.zip;https://issues.apache.org/jira/secure/attachment/12333702/ASF.LICENSE.NOT.GRANTED--Integration.zip,12/Aug/05 02:38;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--Integration.zip;https://issues.apache.org/jira/secure/attachment/12333701/ASF.LICENSE.NOT.GRANTED--Integration.zip,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,36148.0,,,2005-08-14 15:53:48.0,,,false,,,,,,,,,,,,,,150272,,,Sun Aug 14 15:56:43 UTC 2005,,,,,,0|i0rwz3:,160986,,,,,,,,"12/Aug/05 02:38;zxg_32@yahoo.com;Created an attachment (id=16012)
Integration Source Files
","13/Aug/05 09:04;zxg_32@yahoo.com;Created an attachment (id=16027)
Integration Source Files
",14/Aug/05 15:53;phil@steitz.com;*** COM-2289 has been marked as a duplicate of this bug. ***,14/Aug/05 15:54;phil@steitz.com;*** COM-2269 has been marked as a duplicate of this bug. ***,14/Aug/05 15:54;phil@steitz.com;*** COM-2268 has been marked as a duplicate of this bug. ***,14/Aug/05 15:55;phil@steitz.com;*** COM-2292 has been marked as a duplicate of this bug. ***,14/Aug/05 15:55;phil@steitz.com;*** COM-2304 has been marked as a duplicate of this bug. ***,"14/Aug/05 15:56;phil@steitz.com;Patches applied with one small change - copyright dates all fixed to be 2005.
Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wiki link points to old Commons Wiki,MATH-26,12341487,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,tobrien@discursive.com,tobrien@discursive.com,04/Jun/04 21:59,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"http://jakarta.apache.org/commons/math/ has a link to
http://nagoya.apache.org/wiki/apachewiki.cgi?JakartaCommonsProjectPages, let's
change this to point to http://wiki.apache.org/jakarta-commons","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,29394.0,,,2004-06-05 05:47:08.0,,,false,,,,,,,,,,,,,,150271,,,Wed Jun 09 03:29:16 UTC 2004,,,,,,0|i0rwzb:,160987,,,,,,,,"05/Jun/04 05:47;dirkv;Menu has been updated in cvs, a simple site update will fix this","09/Jun/04 03:29;rdonkin@apache.org;This one's fixed now (but not by me). 

Robert",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] RandomDataTest#testConfig() requires SUN security provider,MATH-25,12342787,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,11/Dec/05 06:19,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"RandomDataTest#testConfig() assumes SUN will be available as a
provider for the SHA-1 PRNG. Depending on your JDK, this may cause
test (and therefore, build) failures.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,37862.0,,,2005-12-11 08:55:37.0,,,false,,,,,,,,,,,,,,150270,,,Sun Dec 11 08:55:37 UTC 2005,,,,,,0|i0rwzj:,160988,,,,,,,,11/Dec/05 08:55;jwcarman;Updated summary.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] ChiSquareTest incorrectly rejects tables containing zeros,MATH-24,12341918,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,05/Dec/04 05:13,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The isNonNegative(long[][] in) function defined in
o.a.c.m.inference.ChiSquareTest incorrectly rejects 0 as well as negative
counts. This causes 2-way tables containing 0 counts to be incorrectly rejected
(methods throw IllegalArgumentException).","Operating System: Linux
Platform: PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,32531.0,,,,,,false,,,,,,,,,,,,,,34154,,,Sun Dec 05 05:58:52 UTC 2004,,,,,,0|i0rwzr:,160989,,,,,,,,"05/Dec/04 05:58;phil@steitz.com;Changes to fix this problem have been committed.

Thanks to Hans van der Heijden for reporting this bug.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] refactored distributions to use new solver framework,MATH-23,12340838,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,03/Jul/03 22:11,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"- Changed abstract distribution to use default solver.
- Removed bisection method from RootFinding.
- Added BisectionSolver
- Fixed some javadoc and checkstyle complaints
- Added, ran, and passed BisectionSolver tests
- Ran and passed all other tests.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,03/Jul/03 22:11;brentworden;ASF.LICENSE.NOT.GRANTED--bisection-patch.diff;https://issues.apache.org/jira/secure/attachment/12332406/ASF.LICENSE.NOT.GRANTED--bisection-patch.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,21313.0,,,2003-07-06 01:08:34.0,,,false,,,,,,,,,,,,,,150269,,,Sun Jul 06 01:08:34 UTC 2003,,,,,,0|i0rwzz:,160990,,,,,,,,"03/Jul/03 22:11;brentworden;Created an attachment (id=7083)
patch file
",06/Jul/03 01:08;mdiggory@gmail.com;Applied and Commited.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math][patch] Update Complex.java,MATH-22,12342477,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,zxg_32@yahoo.com,zxg_32@yahoo.com,17/Aug/05 02:21,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Index: Complex.java
===================================================================
--- Complex.java	(revision 233018)
+++ Complex.java	(working copy)
@@ -66,7 +66,19 @@
         if (isNaN()) {
             return Double.NaN;
         }
-        return Math.sqrt(squareSum());       
+        if (Math.abs(real) < Math.abs(imaginary)) {
+            if (imaginary == 0.0) {
+                return Math.abs(real);
+            }
+            double q = real / imaginary;
+            return (Math.abs(imaginary) * Math.sqrt(1 + q*q));
+        } else {
+            if (real == 0.0) {
+                return Math.abs(imaginary);
+            }
+            double q = imaginary / real;
+            return (Math.abs(real) * Math.sqrt(1 + q*q));
+        }
     }
     
     /**
@@ -108,17 +120,29 @@
         if (isNaN() || rhs.isNaN()) {
             return NaN;
         }
-        
-        if (Math.abs(rhs.getReal()) < Math.abs(rhs.getImaginary())) {
-            double q = rhs.getReal() / rhs.getImaginary();
-            double d = (rhs.getReal() * q) + rhs.getImaginary();
-            return new Complex(((real * q) + imaginary) / d,
-                ((imaginary * q) - real) / d);
+
+        double c = rhs.getReal();
+        double d = rhs.getImaginary();
+        if (c == 0.0 && d == 0.0) {
+            throw new ArithmeticException(""Error: division by zero."");
+        }
+
+        if (Math.abs(c) < Math.abs(d)) {
+            if (d == 0.0) {
+                return new Complex(real/c, imaginary/c);
+            }
+            double q = c / d;
+            double denominator = c * q + d;
+            return new Complex((real * q + imaginary) / denominator,
+                (imaginary * q - real) / denominator);
         } else {
-            double q = rhs.getImaginary() / rhs.getReal();
-            double d = (rhs.getImaginary() * q) + rhs.getReal();
-            return new Complex(((imaginary * q) + real) / d,
-                (imaginary - (real * q)) / d);
+            if (c == 0.0) {
+                return new Complex(imaginary/d, -real/c);
+            }
+            double q = d / c;
+            double denominator = d * q + c;
+            return new Complex((imaginary * q + real) / denominator,
+                (imaginary - real * q) / denominator);
         }
     }","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,17/Aug/05 02:21;zxg_32@yahoo.com;ASF.LICENSE.NOT.GRANTED--Complex.java;https://issues.apache.org/jira/secure/attachment/12333721/ASF.LICENSE.NOT.GRANTED--Complex.java,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,36205.0,,,2005-08-17 14:13:39.0,,,false,,,,,,,,,,,,,,150268,,,Wed Aug 17 14:13:39 UTC 2005,,,,,,0|i0rx07:,160991,,,,,,,,"17/Aug/05 02:21;zxg_32@yahoo.com;Created an attachment (id=16064)
Complex.java
","17/Aug/05 02:24;zxg_32@yahoo.com;rewrite and optimize Complex.java to address the issues of
division by zero and overflow
","17/Aug/05 14:13;brentworden;Patch applied.  SVN Revision 233126.  Applied to branch MATH_1_1.

Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Initial submission for User Guide,MATH-21,12340835,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,02/Jul/03 21:50,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch plus tar provides a shell for the User Guide based on the
previously posted outline and most of the content for the Data Generation section.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,02/Jul/03 21:51;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--userguide.tar;https://issues.apache.org/jira/secure/attachment/12332404/ASF.LICENSE.NOT.GRANTED--userguide.tar,02/Jul/03 21:51;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--xdocPatch.txt;https://issues.apache.org/jira/secure/attachment/12332403/ASF.LICENSE.NOT.GRANTED--xdocPatch.txt,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,21277.0,,,2003-07-06 00:04:30.0,,,false,,,,,,,,,,,,,,150267,,,Sun Jul 06 00:04:30 UTC 2003,,,,,,0|i0rx0f:,160992,,,,,,,,"02/Jul/03 21:51;phil@steitz.com;Created an attachment (id=7055)
Patch to existing xdocs
","02/Jul/03 21:51;phil@steitz.com;Created an attachment (id=7056)
New files to add to xdocs/userguide
","06/Jul/03 00:04;mdiggory@gmail.com;This is great work!  I added them and tested the site generation to verify html
gets generated properly. Thanks Phil :-)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] bivariate regression submission,MATH-20,12340729,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,25/May/03 21:59,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached tar includes an implementation of ordinary least squares regression
with one independent variable. The implementation uses running sums and does not
require the data to be stored in memory.  Since I could not conceive of any
significantly different implementation strategies that did not amount to just
improving efficiency or numerical accuracy of what I am submitting, I did not
abstract the interface.

The test cases validate the computations against NIST reference data and
verified computations. The slope, intercept, their standard errors and r-square
estimates are accurate to within 10E-12 against the reference data set.  MSE and
other ANOVA stats are good at least to within 10E-8.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,25/May/03 22:01;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--regression.tar;https://issues.apache.org/jira/secure/attachment/12332285/ASF.LICENSE.NOT.GRANTED--regression.tar,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,20224.0,,,2003-05-26 09:14:53.0,,,false,,,,,,,,,,,,,,150266,,,Mon May 26 09:14:53 UTC 2003,,,,,,0|i0rx0n:,160993,,,,,,,,"25/May/03 22:01;phil@steitz.com;Created an attachment (id=6487)
tar fiie containing BivariateRegression, BivariateRegressionTest
",26/May/03 09:14;tobrien@discursive.com;Patch committed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] patch to fix percentile bug.,MATH-19,12341003,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,27/Sep/03 11:56,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,,"Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,27/Sep/03 11:56;brentworden;ASF.LICENSE.NOT.GRANTED--percentile-bug.txt;https://issues.apache.org/jira/secure/attachment/12332550/ASF.LICENSE.NOT.GRANTED--percentile-bug.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,23453.0,,,2003-10-16 22:18:15.0,,,false,,,,,,,,,,,,,,150265,,,Thu Oct 16 22:18:15 UTC 2003,,,,,,0|i0rx0v:,160994,,,,,,,,"27/Sep/03 11:56;brentworden;Created an attachment (id=8366)
patch file
",16/Oct/03 22:18;mdiggory@gmail.com;About time I got to this. Thanks Brent,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] ComplexFormat ignores imaginaryFormat,MATH-18,12342827,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,02/Jan/06 14:37,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Constructors for ComplexFormat allow different custom NumberFormats to be
specified for real and imaginary parts.  These are stored in instance fields
realFormat and imaginaryFormat.  The private method formatDouble that formats
real and imaginary parts ignores the actual parameter specifying the format,
using realFormat all the time.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,38091.0,,,2006-01-04 00:32:03.0,,,false,,,,,,,,,,,,,,150264,,,Wed Jan 04 01:48:24 UTC 2006,,,,,,0|i0rx13:,160995,,,,,,,,"04/Jan/06 00:32;brentworden;Fixed in revision 365663.  ComplexFormat was not formatting double values with 
the provided NumberFormat.  Instead, the real part format was always used.",04/Jan/06 01:48;brentworden;actually fixed in revision 365680.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Add significance tests,MATH-17,12340809,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,23/Jun/03 07:09,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch adds t- and chi-square tests to TestStatistic.  It also fixes
checkstyle and clover test coverage gaps.  Tests are implemented using the
distribution framework to provide exact p-values as well as fixed significance
tests, using double[] data arrays or Univariates to describe sample data.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,23/Jun/03 07:10;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--testStatisticPatch.txt;https://issues.apache.org/jira/secure/attachment/12332378/ASF.LICENSE.NOT.GRANTED--testStatisticPatch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,21003.0,,,2003-06-25 08:37:45.0,,,false,,,,,,,,,,,,,,150263,,,Wed Jun 25 08:37:45 UTC 2003,,,,,,0|i0kp1r:,118875,,,,,,,,"23/Jun/03 07:10;phil@steitz.com;Created an attachment (id=6932)
patches to TestStatistic, TestStatisticImpl, TestStatisticTest
","25/Jun/03 08:37;mdiggory@gmail.com;applied, tested.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Add dimension checks to RealMatrixImpl solve and LUDecompose,MATH-16,12340810,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,23/Jun/03 10:52,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch adds a check to RealMatrixImpl.solve() to make sure that the
coefficient matrix is square and a check to RealMatrixImpl.LUDecompose() to make
sure that the row dimension of the matrix is >= the column dimension.  Thanks to
Al Chou for pointing out these gaps.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,23/Jun/03 10:54;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--realMatrixPatch.txt;https://issues.apache.org/jira/secure/attachment/12332379/ASF.LICENSE.NOT.GRANTED--realMatrixPatch.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,21005.0,,,2003-06-25 08:42:14.0,,,false,,,,,,,,,,,,,,150262,,,Wed Jun 25 08:42:14 UTC 2003,,,,,,0|i0rx1b:,160996,,,,,,,,"23/Jun/03 10:54;phil@steitz.com;Created an attachment (id=6934)
patches to RealMatrix (javadoc), RealMatrixImpl, RealMatrixImplTest
",25/Jun/03 08:42;mdiggory@gmail.com;applied and tested.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[math] patch - license spelling correction, convergence exception change",MATH-15,12341026,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,10/Oct/03 11:43,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The Apache license misspelled acknowledgement.  Made correction to license file 
and to all source file headings.

Made ConvergenceException extend MathException and moved ConvergenceException 
to the org.apache.commons.math packages.

The change of exception heirarchy forced changes in other parts of commons-
math.  All affected, low level routines (gamma, beta, cf, ...) now throw a 
MathException.  All affected, distribution routines now catch a MathException 
and return NaN.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,10/Oct/03 11:44;brentworden;ASF.LICENSE.NOT.GRANTED--license-spelling.txt;https://issues.apache.org/jira/secure/attachment/12332576/ASF.LICENSE.NOT.GRANTED--license-spelling.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,23716.0,,,2003-10-16 22:28:27.0,,,false,,,,,,,,,,,,,,150261,,,Thu Oct 16 22:28:27 UTC 2003,,,,,,0|i0rx1j:,160997,,,,,,,,"10/Oct/03 11:44;brentworden;Created an attachment (id=8520)
patch file containing all the changes.
","16/Oct/03 22:28;mdiggory@gmail.com;Had to do some reverse patch trickery to get this applied, please doublecheck my
work by running the tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] Gamma.logGamma performance,MATH-14,12341770,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,kgeis,kgeis,04/Oct/04 14:17,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"I did some profiling on a project and found a lot of time spent in
Gamma.logGamma() (I'm doing a lot of t-tests.)  I found some tweaks to the code
that give me a 65% performance increase.  I'm using JDK 1.5.

Let's see if I can attach a patch here...","Operating System: All
Platform: All",,,,,,,,,,,,,,,,,,,,04/Oct/04 14:19;kgeis;ASF.LICENSE.NOT.GRANTED--gamma-performance.diff;https://issues.apache.org/jira/secure/attachment/12333149/ASF.LICENSE.NOT.GRANTED--gamma-performance.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,31522.0,,,2004-10-08 12:55:49.0,,,false,,,,,,,,,,,,,,150260,,,Fri Oct 08 12:55:49 UTC 2004,,,,,,0|i0rx1r:,160998,,,,,,,,"04/Oct/04 14:19;kgeis;Created an attachment (id=12924)
logGamma performance patch
","04/Oct/04 14:21;kgeis;I would attribute the performance increase as follows: 2 or 3% from reversing
the loop order, and even improvements (~30% each) from moving the log(PI) out of
the function and merging two calls to log.",08/Oct/04 12:55;phil@steitz.com;Patch applied.  Thanks!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] changed the univariate real solver factory,MATH-13,12340877,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,31/Jul/03 04:54,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Made the UnivariateRealSolverFactory an abstract factory with one concrete 
factory: UnivariateRealSolverFactoryImpl.

Removed the solve() convience methods from the factory and created a new 
UnivariateRealSolverUtil class.

Added a few javadoc comments.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,31/Jul/03 04:54;brentworden;ASF.LICENSE.NOT.GRANTED--solver-factory.diff;https://issues.apache.org/jira/secure/attachment/12332436/ASF.LICENSE.NOT.GRANTED--solver-factory.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,22002.0,,,2003-07-31 05:14:25.0,,,false,,,,,,,,,,,,,,150259,,,Thu Jul 31 05:14:25 UTC 2003,,,,,,0|i0rx1z:,160999,,,,,,,,"31/Jul/03 04:54;brentworden;Created an attachment (id=7594)
patch file
",31/Jul/03 05:14;tobrien@discursive.com;Patch applied and site regenerated to boot.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] RunTimeException in EmpiricalDistributionImpl.load(double[]),MATH-12,12342524,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,keithmcdonald@sympatico.ca,keithmcdonald@sympatico.ca,01/Sep/05 05:17,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Overview Description: 

I get the following RunTimeException when loading data into an instance of
EmpiricalDistributionImpl:

java.lang.RuntimeException: Index: 1000, Size: 1000
     at org.apache.commons.math.random.EmpiricalDistributionImpl.load
(EmpiricalDistributionImpl.java:111)
     at BreakEmpiricalDistributionImpl.main(BreakEmpiricalDistributio
nImpl.java:7)
Exception in thread ""main""

Here is a small program to reproduce the problem:

import org.apache.commons.math.random.EmpiricalDistributionImpl;

public class BreakEmpiricalDistributionImpl {
  
  public static void main(String[] args) {
    double[] x = new double[] {9474.94326071674, 2080107.8865462579};
    new EmpiricalDistributionImpl().load(x);
  }
  
}

Build Date & Platform: 

Build 2005-08-31 on Windows 2000 with Sun JDK 1.4.2_09

    Additional Builds and Platforms:","Operating System: Windows 2000
Platform: PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,36450.0,,,2005-09-01 14:27:24.0,,,false,,,,,,,,,,,,,,34187,,,Sun Sep 04 09:41:53 UTC 2005,,,,,,0|i0rx27:,161000,,,,,,,,"01/Sep/05 14:27;phil@steitz.com;The code should probably include a guard for this and throw a more meaningful
exception, but this class is not designed to be used with very small data sets.
Are you trying to produce random draws from a discrete distribution with point
masses on the two values?  This class is designed to use empirical data to
simulate continuous distributions (see the reference in the class header comment).  

As stated in the Usage Notes section of the header comment, the binCount should
be set to a number smaller than than the sample size. In this case, 1 is the
only possible value; but this will not work, since bins of size 1 have no
variance.  So the minimum possible acceptable values are effectively 4
datapoints with binCount = 2. 

The javadoc should probably be updated to make this clear and a binCount -
sample size guard should be added.","01/Sep/05 20:55;keithmcdonald@sympatico.ca;Thank you for responding to this bug report so quickly!

I hit the problem with a larger sample size than the two samples that I gave in
the code for reproducing the problem. I just wanted to provide a small program
that could reproduce the problem easily. I think the problem is caused by the
limitations of the precision of the variable ""delta"". 

When delta is produced by dividing (max - min) / binCount in the method
fillBinStats, regardless of how many samples you have, it is possible to get a
number that, when multiplied by binCount, is slightly smaller than (max - min).
So, when inputArray[i] is equal to max, (inputArray[i] - min) / delta equals a
number slightly larger than binCount. When the Math.ceil method is applied to
this number, you get binCount + 1, which , even after subtracting 1 as is done
in the computeBinStats methods, is too large for binStats.get to take as an
argument and this leads to the RunTimeException.

My suggestion for fixing this would be to add a call to Math.min(x,
binStats.size()-1) before passing the result to binStats.get, where x is the
current expression in computeBinStats whose result is being passed to
binStats.get. This change would need to go into two places: the computeBinStats
method of ArrayDataAdapter and the computeBinStats method of StreamDataAdapter.

Here is a replacement program that reproduces the problem with 10,000 samples.

import org.apache.commons.math.random.EmpiricalDistributionImpl;

public class BreakEmpiricalDistributionImpl {
  
  public static void main(String[] args) {
    final int NUMBER_OF_SAMPLES = 10000;
    double[] x = new double[NUMBER_OF_SAMPLES];
    x[0] = 9474.94326071674;
    x[1] = 2080107.8865462579;
    for (int i = 2; i < NUMBER_OF_SAMPLES; i++) {
      x[i] = Math.max(Math.min(x[0] + x[1] * Math.random(), x[1]), x[0]);
    }
    new EmpiricalDistributionImpl().load(x);
  }
  
}","02/Sep/05 22:40;phil@steitz.com;Thanks for the clarification.  Yes, this looks like a bug. ","04/Sep/05 09:41;phil@steitz.com;Fixed in svn r267527, nightly builds starting 9/3/05 and 1.1 RC3.
Thanks!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SplineInterpolator gives bad interpolation,MATH-11,12341376,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,jfreyss@hotmail.com,jfreyss@hotmail.com,30/Mar/04 00:14,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The CubicSplineFunction does not fit on each interval:

> Replace
>             double dquot = (yval[1] - yval[0]) / (xval[1] - xval[0]);
>             for (int i = 0; i < n - 1; i++) {
>                               double dquotNext =
>                                      (yval[i + 2] - yval[i + 1]) / (xval[i
> + 2] - xval[i + 1]);
>                             b[i] = 6.0 * (dquotNext - dquot);
>                             d[i] = 2.0 * (xval[i + 2] - xval[i]);
>                             dquot = dquotNext;
>                   }
> 
> 
> With
> 
>             //Source: http://mathworld.wolfram.com/CubicSpline.html
>             for (int i = 0; i < n - 1; i++) {
>                   b[i] = 3.0 * (yval[i+1] - yval[i]);
>                   d[i] = (i>0 && i<n-2)? 4.0: 2.0 ;
>             }
>","Operating System: other
Platform: PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,28019.0,,,2004-04-03 05:55:13.0,,,false,,,,,,,,,,,,,,150258,,,Sat Apr 03 05:55:13 UTC 2004,,,,,,0|i0rx2f:,161001,,,,,,,,"03/Apr/04 05:55;phil@steitz.com;The implementation has been fixed and tests have been added to verify
consistency (interpolator assigns correct values at knot points, polynomial
derivatives agree, etc.) 

Fixed in nightly builds starting 4/2/04.

Thanks for reporting this problem.  Reopen if you still get bad results.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] fixed some checkstyle warnings,MATH-10,12340850,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,11/Jul/03 22:44,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Also, added maven.javadoc.links to project.properties so, when maven is used to 
produce javadoc, references to external API's will have valid links.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,11/Jul/03 22:45;brentworden;ASF.LICENSE.NOT.GRANTED--cleanup-patch.diff;https://issues.apache.org/jira/secure/attachment/12332412/ASF.LICENSE.NOT.GRANTED--cleanup-patch.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,21516.0,,,2003-07-11 23:02:49.0,,,false,,,,,,,,,,,,,,150257,,,Fri Jul 11 23:02:49 UTC 2003,,,,,,0|i0rx2n:,161002,,,,,,,,"11/Jul/03 22:45;brentworden;Created an attachment (id=7247)
patch file
","11/Jul/03 23:02;tobrien@discursive.com;Patch applied, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[PATCH] [math] Natural spline interpolation,MATH-9,12340813,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,j3322ptm@yahoo.de,j3322ptm@yahoo.de,24/Jun/03 01:28,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Appended a tgz with new files for interpolation with an natural cubic spline.
Caveats: several TODOs in the source, and the unit tests must be reworked.
The patch depends on the UnivariateRealFunction interface.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,24/Jun/03 10:40;hotfusionman@yahoo.com;ASF.LICENSE.NOT.GRANTED--SplineInterpolator.diff.gz;https://issues.apache.org/jira/secure/attachment/12332384/ASF.LICENSE.NOT.GRANTED--SplineInterpolator.diff.gz,24/Jun/03 01:30;j3322ptm@yahoo.de;ASF.LICENSE.NOT.GRANTED--m.tar.gz;https://issues.apache.org/jira/secure/attachment/12332383/ASF.LICENSE.NOT.GRANTED--m.tar.gz,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,21023.0,,,2003-06-24 10:40:29.0,,,false,,,,,,,,,,,,,,150256,,,Wed Jun 25 08:16:54 UTC 2003,,,,,,0|i0rx2v:,161003,,,,,,,,"24/Jun/03 01:30;j3322ptm@yahoo.de;Created an attachment (id=6940)
Proposed patch
","24/Jun/03 10:40;hotfusionman@yahoo.com;Created an attachment (id=6949)
Proposed enhancement to SplineInterpolator (persistence of c[][] array as instance variable)
","25/Jun/03 08:16;mdiggory@gmail.com;I'll apply this, but its really alot easier to apply ""cvs generated patches""
over tarballs or unix diff. I would prefer this format over all others.

Not only does cvs diff generate patches directly against the cvs copies,
assuring your diffing against whats presently in the cvs, but you can also add
files using it. In otherwords, if you create a file and the run cvs diff on the
directory, when I apply the patch, the file will be created.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math]TTest should not throw IllegalArgumentException for small samples,MATH-8,12341455,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,18/May/04 11:29,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"Currently, the TTest implementation requires that the minimum of the two sample
sizes be at least 5.  This is too restrictive.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,29050.0,,,,,,false,,,,,,,,,,,,,,34139,,,Sun May 23 12:08:01 UTC 2004,,,,,,0|i0rx33:,161004,,,,,,,,23/May/04 12:08;phil@steitz.com;The minimum sample size has been reduced to 2.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] user guide additions - special functions,MATH-7,12341001,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,24/Sep/03 22:17,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,Adds text to the special function portion of the user guide.,"Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,24/Sep/03 22:18;brentworden;ASF.LICENSE.NOT.GRANTED--special-xdoc.txt;https://issues.apache.org/jira/secure/attachment/12332548/ASF.LICENSE.NOT.GRANTED--special-xdoc.txt,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,23389.0,,,2003-09-25 03:15:58.0,,,false,,,,,,,,,,,,,,150255,,,Thu Sep 25 03:15:58 UTC 2003,,,,,,0|i0rx3b:,161005,,,,,,,,"24/Sep/03 22:18;brentworden;Created an attachment (id=8331)
patch file
","25/Sep/03 03:15;j3322ptm@yahoo.de;Applied, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Remove dependency on commons-lang,MATH-6,12341558,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,07/Jul/04 20:01,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The attached patch copies implementation from
o.a.c.l.Exception.NestableException and its dependencies into MathException. 
Test cases from o.a.c.l.AbstractNestableTestCase are also added to
MathExceptionTest.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,07/Jul/04 20:02;phil@steitz.com;ASF.LICENSE.NOT.GRANTED--exceptionPatch;https://issues.apache.org/jira/secure/attachment/12332975/ASF.LICENSE.NOT.GRANTED--exceptionPatch,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,29948.0,,,,,,false,,,,,,,,,,,,,,150254,,,Mon Jul 26 01:30:55 UTC 2004,,,,,,0|i0rx3j:,161006,,,,,,,,"07/Jul/04 20:02;phil@steitz.com;Created an attachment (id=12053)
Patch to add implementation code from commons-lang
","26/Jul/04 01:30;phil@steitz.com;Changes based on [collections] FunctorException committed.  Dependency has been
removed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Complex arithmetic operations do not conform to C99x Annex G,MATH-5,12342629,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Won't Fix,,phil@steitz.com,phil@steitz.com,14/Oct/05 12:23,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The Complex classes do not handle NaN and infinite values consistently with the
C99x Annex G spec, nor is current handling of these values documented sufficiently.

See http://www.open-std.org/jtc1/sc22/wg14/
for a working paper related to the latest draft of the specification.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,37086.0,,,2005-10-14 20:16:47.0,,,false,,,,,,,,,,,,,,150253,,,Thu Nov 24 03:02:14 UTC 2005,,,,,,0|i0rx3r:,161007,,,,,,,,"14/Oct/05 20:16;elharo@metalab.unc.edu;This page might also assist:

http://www.dinkumware.com/refxc.html","14/Oct/05 20:30;elharo@metalab.unc.edu;If the goal of this class is to implement the semantics of the C99 double
_Complex type, then this should be stated explcitly in the JavaDoc.

This is probably a very good goal. The C99 types have been rigorously thought
out, and are relatively well specified. They also give us a good conformance
test for any operations in question: just compare our output to C99.

We just need to be explicit that this is the goal.

On the other hand, the C99 spec is not easily or cheaply available. Cheapest
version I found was about $60 from the British Standards Institute, published by
Wiley. Therefore we shoudl not assume every class user has a copy at their
finger tips. We shoudl reproduce as necessary the semantics in the JavaDoc,
though perhaps indicating that in the event of any conflicts between the JavaDoc
and the C99 spec, the C99 spec is normative, unless we explicitly state
itherwise. (I don't know, but there may be places where Java and C diverge
enough that it makes sense for this class not to be in perfect sync.)
","14/Oct/05 21:41;phil@steitz.com;The minimum requirement is that the javadoc fully specify implemented behavior,
which in the case of 1.0 implementations of complex arithmetic and equals, it
does not. We discussed earlier using C99x Annex G as a guide (similarly to what
some other Java Complex implementations do and similarly to how we have tried to
follow 454 for double arithmetic), for the conisistency reasons mentioned above.
Lack of public availability of the spec is an inconvenience; but that does not
prevent us making our impls consistent with it.  The key is to fully document in
our javadoc what the behavior that we are implementing is.","24/Nov/05 03:02;phil@steitz.com;Current behavior has been documented.  No attempt has been made to implement
C99x Annex G for the following reasons:

1. Spec is not publicly available and we have not been able to get
   permission to quote from the spec in javadoc or to share the spec
   with the development community.

2. Spec is for the C language and Annex G is still not normative.

3. Following the spec complicates and has performance impacts on
   implementations.

4. Following the spec introduces semantic incompatibility with
   version 1.0 of commons-math.

The user guide and javadoc have been updated to reflect the following principles:

* Operations invoked on arguments having any NaN parts always result in
  Complex.NaN returned;
* Infinite values are handled according to the rules for java.lang.Double
  arithmetic, resulting in NaN or infinities returned in parts of Complex
  results per the computational formulas provided in the method javadoc.

So...for now closing as WONTFIX.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Sources will not compile using JDK 1.3,MATH-4,12341920,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,phil@steitz.com,phil@steitz.com,06/Dec/04 14:19,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The following classes contain JDK 1.4-dependent code
MatrixExeption, InvalidMatrixException (nested exception support)
BeanTransformer (Expression)
CertifiedDataAbstractTest (Expression)","Operating System: Linux
Platform: PC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,32538.0,,,,,,false,,,,,,,,,,,,,,150252,,,Mon Dec 06 14:54:36 UTC 2004,,,,,,0|i0rx3z:,161008,,,,,,,,"06/Dec/04 14:54;phil@steitz.com;Changes to eliminate JDK 1.4 dependencies have been committed. 
InvalidMatrixException and MatrixIndexException no longer support nesting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] Corrections to Constructor Fraction(double),MATH-3,12342327,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,j.weimar@tu-bs.de,j.weimar@tu-bs.de,20/Jun/05 21:14,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"The constructor for Fraction fails for doubles which are almost integer.

examples should be inserted in th etest case:

    public void testConstructorDouble() {
        try {
.....            
            assertFraction(0, 1, new Fraction(0.00000000000001));
            assertFraction(2, 5, new Fraction(0.40000000000001));
            assertFraction(15, 1, new Fraction(15.0000000000001));
            
        } catch (ConvergenceException ex) {
            fail(ex.getMessage());
        }
    }

The fix for this problem is to include the following code in the constructor:


        int n = 0;
        boolean stop = false;

        // check for (almost) integer arguments, which should not go
        // to iterations.
        if (Math.abs(a0 - value)<epsilon){
            this.numerator = a0;
            this.denominator = 1;
            return;
        }
        do {
            ++n;","Operating System: other
Platform: All",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,35434.0,,,2005-06-26 10:55:04.0,,,false,,,,,,,,,,,,,,34168,,,Sun Jun 26 10:55:04 UTC 2005,,,,,,0|i0rx47:,161009,,,,,,,,26/Jun/05 10:55;phil@steitz.com;Patches applied. Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] fixed some javadoc and checkstyle warnings,MATH-2,12340773,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,10/Jun/03 03:21,23/Apr/09 02:26,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,,"Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,10/Jun/03 03:22;brentworden;ASF.LICENSE.NOT.GRANTED--beanlistunivariateimpl-checkstyle-patch.txt;https://issues.apache.org/jira/secure/attachment/12332334/ASF.LICENSE.NOT.GRANTED--beanlistunivariateimpl-checkstyle-patch.txt,11/Jun/03 13:51;brentworden;ASF.LICENSE.NOT.GRANTED--checkstyle-patch.txt;https://issues.apache.org/jira/secure/attachment/12332336/ASF.LICENSE.NOT.GRANTED--checkstyle-patch.txt,10/Jun/03 03:21;brentworden;ASF.LICENSE.NOT.GRANTED--freq-checkstyle-patch.txt;https://issues.apache.org/jira/secure/attachment/12332333/ASF.LICENSE.NOT.GRANTED--freq-checkstyle-patch.txt,10/Jun/03 03:22;brentworden;ASF.LICENSE.NOT.GRANTED--randomdataimpl-checkstyle-patch.txt;https://issues.apache.org/jira/secure/attachment/12332335/ASF.LICENSE.NOT.GRANTED--randomdataimpl-checkstyle-patch.txt,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,20627.0,,,2003-06-11 08:36:02.0,,,false,,,,,,,,,,,,,,150251,,,Wed Jun 11 18:32:07 UTC 2003,,,,,,0|i0rx4f:,161010,,,,,,,,"10/Jun/03 03:21;brentworden;Created an attachment (id=6718)
Freq.java patch
","10/Jun/03 03:22;brentworden;Created an attachment (id=6719)
BeanListUnivariateImpl.java patch
","10/Jun/03 03:22;brentworden;Created an attachment (id=6720)
RandomDataImpl.java patch
","11/Jun/03 08:36;mdiggory@gmail.com;Brent, these patches are each on multiple files spanning the entire math
directory, I want to verify that this was this your intention.

The first patch attempts to add the files getting patched were previously added
by your last F-dist patch. It also ""gutts"" the contents of some of the tests
(the FixedDoubleArrayTest class for one).

Was it your intention to have multiple files altered in each of these patches?","11/Jun/03 12:54;brentworden;Oops.  I summitted these things I hast.  I'll clean them up and reopen the 
issue later.","11/Jun/03 13:51;brentworden;Created an attachment (id=6750)
Latest patch (KEEP)
","11/Jun/03 13:52;brentworden;I've attached a leaner patch file.  Try applying ""Latest patch"".  The others 
can be ignored.",11/Jun/03 18:32;mdiggory@gmail.com;Applied the last patch successfully. Ran Unit tests successfully. thanks Brent,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[math] new approach to testing against certified data sets.,MATH-1,12340666,Bug,Closed,MATH,Commons Math,software,issues@commons.apache.org,,http://commons.apache.org/math/,Major,Fixed,,brentworden,brentworden,16/Mar/04 13:42,23/Jul/12 23:24,07/Apr/19 20:38,23/Apr/09 02:26,,,,,,,,,,,0,,,,,,,,"With the talk of testing commons-math against additional data sets, I wanted to 
try to make our certified data tests a little more flexible in terms of the 
data they can load and the summary statistics they can verify.

What I've initially come up with is a simple data format similar to the 
existing NIST data set files.  The valid data file consists of blank lines, # 
style comment lines, certified summary statistic lines, and data value lines.

For statistic verification, blank lines and comments are ignored.

A line is deem a certified summary statistic values if it contains an '=' 
character.  Theses lines are parsed into name/value pairs.  The name 
corresponds to a property of DescriptiveStatistics and/or SummaryStatistics and 
the value is the expected value of the summary statistic.

All other lines not identified as one of the previous three types is deemed a 
data value line.  With these lines a double value is extracted from the line 
and added to both a DescriptiveStatistics object and a SummaryStatistics object.

The actual verification process used bean introspection to access the actual 
summary statistic properties from the loaded DescriptiveStatistics and 
SummaryStatistics objects.  These actual values are then compared to the 
expected summary statistic values.

I'll attach a patch which contains all the changes.  I'll hold off on 
committing it until everyone has had a chance to look it over and provide some 
feedback.","Operating System: other
Platform: Other",,,,,,,,,,,,,,,,,,,,16/Mar/04 13:42;brentworden;ASF.LICENSE.NOT.GRANTED--new-certified-test-data-approach.diff;https://issues.apache.org/jira/secure/attachment/12332235/ASF.LICENSE.NOT.GRANTED--new-certified-test-data-approach.diff,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,27692.0,,,2004-03-17 11:18:58.0,,,false,,,,,,,,,,,,,,150250,,,Thu Mar 18 13:57:26 UTC 2004,,,,,,0|i0rx4n:,161011,,,,,,,,"16/Mar/04 13:42;brentworden;Created an attachment (id=10800)
patch file
","17/Mar/04 11:18;phil@steitz.com;This is very nice.  The one small additional improvement that I would suggest
would be to have CertifiedDataTest expose a ""tolerance"" property that individual
tests could override -- i.e. get rid of the hard-coded 10E-5 in
testCertifiedValues(). ","18/Mar/04 13:57;brentworden;I went ahead and checking in the changes.  I took you're advice and added a 
getMaximumAbsoluteError method to provide a more flexible tolerance check.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
